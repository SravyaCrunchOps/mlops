Documentation | Kubernetes KubernetesDocumentationKubernetes BlogTrainingPartnersCommunityCase StudiesVersionsRelease Information v1.33 v1.32 v1.31 v1.30 v1.29Englishবাংলা (Bengali) 中文 (Chinese) Français (French) Deutsch (German) हिन्दी (Hindi) Bahasa Indonesia (Indonesian) Italiano (Italian) 日本語 (Japanese) 한국어 (Korean) Polski (Polish) Português (Portuguese) Русский (Russian) Español (Spanish) Українська (Ukrainian) Tiếng Việt (Vietnamese) This is the multi-page printable view of this section. Click here to print.Return to the regular view of this page.Documentation1: Kubernetes Documentation1.1: Available Documentation Versions2: Getting started2.1: Learning environment2.2: Production environment2.2.1: Container Runtimes2.2.2: Installing Kubernetes with deployment tools2.2.2.1: Bootstrapping clusters with kubeadm2.2.2.1.1: Installing kubeadm2.2.2.1.2: Troubleshooting kubeadm2.2.2.1.3: Creating a cluster with kubeadm2.2.2.1.4: Customizing components with the kubeadm API2.2.2.1.5: Options for Highly Available Topology2.2.2.1.6: Creating Highly Available Clusters with kubeadm2.2.2.1.7: Set up a High Availability etcd Cluster with kubeadm2.2.2.1.8: Configuring each kubelet in your cluster using kubeadm2.2.2.1.9: Dual-stack support with kubeadm2.2.3: Turnkey Cloud Solutions2.3: Best practices2.3.1: Considerations for large clusters2.3.2: Running in multiple zones2.3.3: Validate node setup2.3.4: Enforcing Pod Security Standards2.3.5: PKI certificates and requirements3: Concepts3.1: Overview3.1.1: Kubernetes Components3.1.2: Objects In Kubernetes3.1.2.1: Kubernetes Object Management3.1.2.2: Object Names and IDs3.1.2.3: Labels and Selectors3.1.2.4: Namespaces3.1.2.5: Annotations3.1.2.6: Field Selectors3.1.2.7: Finalizers3.1.2.8: Owners and Dependents3.1.2.9: Recommended Labels3.1.3: The Kubernetes API3.2: Cluster Architecture3.2.1: Nodes3.2.2: Communication between Nodes and the Control Plane3.2.3: Controllers3.2.4: Leases3.2.5: Cloud Controller Manager3.2.6: About cgroup v23.2.7: Kubernetes Self-Healing3.2.8: Container Runtime Interface (CRI)3.2.9: Garbage Collection3.2.10: Mixed Version Proxy3.3: Containers3.3.1: Images3.3.2: Container Environment3.3.3: Runtime Class3.3.4: Container Lifecycle Hooks3.4: Workloads3.4.1: Pods3.4.1.1: Pod Lifecycle3.4.1.2: Init Containers3.4.1.3: Sidecar Containers3.4.1.4: Ephemeral Containers3.4.1.5: Disruptions3.4.1.6: Pod Quality of Service Classes3.4.1.7: User Namespaces3.4.1.8: Downward API3.4.2: Workload Management3.4.2.1: Deployments3.4.2.2: ReplicaSet3.4.2.3: StatefulSets3.4.2.4: DaemonSet3.4.2.5: Jobs3.4.2.6: Automatic Cleanup for Finished Jobs3.4.2.7: CronJob3.4.2.8: ReplicationController3.4.3: Autoscaling Workloads3.4.4: Managing Workloads3.5: Services, Load Balancing, and Networking3.5.1: Service3.5.2: Ingress3.5.3: Ingress Controllers3.5.4: Gateway API3.5.5: EndpointSlices3.5.6: Network Policies3.5.7: DNS for Services and Pods3.5.8: IPv4/IPv6 dual-stack3.5.9: Topology Aware Routing3.5.10: Networking on Windows3.5.11: Service ClusterIP allocation3.5.12: Service Internal Traffic Policy3.6: Storage3.6.1: Volumes3.6.2: Persistent Volumes3.6.3: Projected Volumes3.6.4: Ephemeral Volumes3.6.5: Storage Classes3.6.6: Volume Attributes Classes3.6.7: Dynamic Volume Provisioning3.6.8: Volume Snapshots3.6.9: Volume Snapshot Classes3.6.10: CSI Volume Cloning3.6.11: Storage Capacity3.6.12: Node-specific Volume Limits3.6.13: Volume Health Monitoring3.6.14: Windows Storage3.7: Configuration3.7.1: Configuration Best Practices3.7.2: ConfigMaps3.7.3: Secrets3.7.4: Liveness, Readiness, and Startup Probes3.7.5: Resource Management for Pods and Containers3.7.6: Organizing Cluster Access Using kubeconfig Files3.7.7: Resource Management for Windows nodes3.8: Security3.8.1: Cloud Native Security and Kubernetes3.8.2: Pod Security Standards3.8.3: Pod Security Admission3.8.4: Service Accounts3.8.5: Pod Security Policies3.8.6: Security For Windows Nodes3.8.7: Controlling Access to the Kubernetes API3.8.8: Role Based Access Control Good Practices3.8.9: Good practices for Kubernetes Secrets3.8.10: Multi-tenancy3.8.11: Hardening Guide - Authentication Mechanisms3.8.12: Kubernetes API Server Bypass Risks3.8.13: Linux kernel security constraints for Pods and containers3.8.14: Security Checklist3.8.15: Application Security Checklist3.9: Policies3.9.1: Limit Ranges3.9.2: Resource Quotas3.9.3: Process ID Limits And Reservations3.9.4: Node Resource Managers3.10: Scheduling, Preemption and Eviction3.10.1: Kubernetes Scheduler3.10.2: Assigning Pods to Nodes3.10.3: Pod Overhead3.10.4: Pod Scheduling Readiness3.10.5: Pod Topology Spread Constraints3.10.6: Taints and Tolerations3.10.7: Scheduling Framework3.10.8: Dynamic Resource Allocation3.10.9: Scheduler Performance Tuning3.10.10: Resource Bin Packing3.10.11: Pod Priority and Preemption3.10.12: Node-pressure Eviction3.10.13: API-initiated Eviction3.11: Cluster Administration3.11.1: Node Shutdowns3.11.2: Node Autoscaling3.11.3: Certificates3.11.4: Cluster Networking3.11.5: Admission Webhook Good Practices3.11.6: Logging Architecture3.11.7: Compatibility Version For Kubernetes Control Plane Components3.11.8: Metrics For Kubernetes System Components3.11.9: Metrics for Kubernetes Object States3.11.10: System Logs3.11.11: Traces For Kubernetes System Components3.11.12: Proxies in Kubernetes3.11.13: API Priority and Fairness3.11.14: Installing Addons3.11.15: Coordinated Leader Election3.12: Windows in Kubernetes3.12.1: Windows containers in Kubernetes3.12.2: Guide for Running Windows Containers in Kubernetes3.13: Extending Kubernetes3.13.1: Compute, Storage, and Networking Extensions3.13.1.1: Network Plugins3.13.1.2: Device Plugins3.13.2: Extending the Kubernetes API3.13.2.1: Custom Resources3.13.2.2: Kubernetes API Aggregation Layer3.13.3: Operator pattern4: Tasks4.1: Install Tools4.1.1: Install and Set Up kubectl on Linux4.1.2: Install and Set Up kubectl on macOS4.1.3: Install and Set Up kubectl on Windows4.2: Administer a Cluster4.2.1: Administration with kubeadm4.2.1.1: Adding Linux worker nodes4.2.1.2: Adding Windows worker nodes4.2.1.3: Upgrading kubeadm clusters4.2.1.4: Upgrading Linux nodes4.2.1.5: Upgrading Windows nodes4.2.1.6: Configuring a cgroup driver4.2.1.7: Certificate Management with kubeadm4.2.1.8: Reconfiguring a kubeadm cluster4.2.1.9: Changing The Kubernetes Package Repository4.2.2: Overprovision Node Capacity For A Cluster4.2.3: Migrating from dockershim4.2.3.1: Changing the Container Runtime on a Node from Docker Engine to containerd4.2.3.2: Find Out What Container Runtime is Used on a Node4.2.3.3: Troubleshooting CNI plugin-related errors4.2.3.4: Check whether dockershim removal affects you4.2.3.5: Migrating telemetry and security agents from dockershim4.2.4: Generate Certificates Manually4.2.5: Manage Memory, CPU, and API Resources4.2.5.1: Configure Default Memory Requests and Limits for a Namespace4.2.5.2: Configure Default CPU Requests and Limits for a Namespace4.2.5.3: Configure Minimum and Maximum Memory Constraints for a Namespace4.2.5.4: Configure Minimum and Maximum CPU Constraints for a Namespace4.2.5.5: Configure Memory and CPU Quotas for a Namespace4.2.5.6: Configure a Pod Quota for a Namespace4.2.6: Install a Network Policy Provider4.2.6.1: Use Antrea for NetworkPolicy4.2.6.2: Use Calico for NetworkPolicy4.2.6.3: Use Cilium for NetworkPolicy4.2.6.4: Use Kube-router for NetworkPolicy4.2.6.5: Romana for NetworkPolicy4.2.6.6: Weave Net for NetworkPolicy4.2.7: Access Clusters Using the Kubernetes API4.2.8: Advertise Extended Resources for a Node4.2.9: Autoscale the DNS Service in a Cluster4.2.10: Change the Access Mode of a PersistentVolume to ReadWriteOncePod4.2.11: Change the default StorageClass4.2.12: Switching from Polling to CRI Event-based Updates to Container Status4.2.13: Change the Reclaim Policy of a PersistentVolume4.2.14: Cloud Controller Manager Administration4.2.15: Configure a kubelet image credential provider4.2.16: Configure Quotas for API Objects4.2.17: Control CPU Management Policies on the Node4.2.18: Control Topology Management Policies on a node4.2.19: Customizing DNS Service4.2.20: Debugging DNS Resolution4.2.21: Declare Network Policy4.2.22: Developing Cloud Controller Manager4.2.23: Enable Or Disable A Kubernetes API4.2.24: Encrypting Confidential Data at Rest4.2.25: Decrypt Confidential Data that is Already Encrypted at Rest4.2.26: Guaranteed Scheduling For Critical Add-On Pods4.2.27: IP Masquerade Agent User Guide4.2.28: Limit Storage Consumption4.2.29: Migrate Replicated Control Plane To Use Cloud Controller Manager4.2.30: Operating etcd clusters for Kubernetes4.2.31: Reserve Compute Resources for System Daemons4.2.32: Running Kubernetes Node Components as a Non-root User4.2.33: Safely Drain a Node4.2.34: Securing a Cluster4.2.35: Set Kubelet Parameters Via A Configuration File4.2.36: Share a Cluster with Namespaces4.2.37: Upgrade A Cluster4.2.38: Use Cascading Deletion in a Cluster4.2.39: Using a KMS provider for data encryption4.2.40: Using CoreDNS for Service Discovery4.2.41: Using NodeLocal DNSCache in Kubernetes Clusters4.2.42: Using sysctls in a Kubernetes Cluster4.2.43: Utilizing the NUMA-aware Memory Manager4.2.44: Verify Signed Kubernetes Artifacts4.3: Configure Pods and Containers4.3.1: Assign Memory Resources to Containers and Pods4.3.2: Assign CPU Resources to Containers and Pods4.3.3: Assign Pod-level CPU and memory resources4.3.4: Configure GMSA for Windows Pods and containers4.3.5: Resize CPU and Memory Resources assigned to Containers4.3.6: Configure RunAsUserName for Windows pods and containers4.3.7: Create a Windows HostProcess Pod4.3.8: Configure Quality of Service for Pods4.3.9: Assign Extended Resources to a Container4.3.10: Configure a Pod to Use a Volume for Storage4.3.11: Configure a Pod to Use a PersistentVolume for Storage4.3.12: Configure a Pod to Use a Projected Volume for Storage4.3.13: Configure a Security Context for a Pod or Container4.3.14: Configure Service Accounts for Pods4.3.15: Pull an Image from a Private Registry4.3.16: Configure Liveness, Readiness and Startup Probes4.3.17: Assign Pods to Nodes4.3.18: Assign Pods to Nodes using Node Affinity4.3.19: Configure Pod Initialization4.3.20: Attach Handlers to Container Lifecycle Events4.3.21: Configure a Pod to Use a ConfigMap4.3.22: Share Process Namespace between Containers in a Pod4.3.23: Use a User Namespace With a Pod4.3.24: Use an Image Volume With a Pod4.3.25: Create static Pods4.3.26: Translate a Docker Compose File to Kubernetes Resources4.3.27: Enforce Pod Security Standards by Configuring the Built-in Admission Controller4.3.28: Enforce Pod Security Standards with Namespace Labels4.3.29: Migrate from PodSecurityPolicy to the Built-In PodSecurity Admission Controller4.4: Monitoring, Logging, and Debugging4.4.1: Troubleshooting Applications4.4.1.1: Debug Pods4.4.1.2: Debug Services4.4.1.3: Debug a StatefulSet4.4.1.4: Determine the Reason for Pod Failure4.4.1.5: Debug Init Containers4.4.1.6: Debug Running Pods4.4.1.7: Get a Shell to a Running Container4.4.2: Troubleshooting Clusters4.4.2.1: Troubleshooting kubectl4.4.2.2: Resource metrics pipeline4.4.2.3: Tools for Monitoring Resources4.4.2.4: Monitor Node Health4.4.2.5: Debugging Kubernetes nodes with crictl4.4.2.6: Auditing4.4.2.7: Debugging Kubernetes Nodes With Kubectl4.4.2.8: Developing and debugging services locally using telepresence4.4.2.9: Windows debugging tips4.5: Manage Kubernetes Objects4.5.1: Declarative Management of Kubernetes Objects Using Configuration Files4.5.2: Declarative Management of Kubernetes Objects Using Kustomize4.5.3: Managing Kubernetes Objects Using Imperative Commands4.5.4: Imperative Management of Kubernetes Objects Using Configuration Files4.5.5: Update API Objects in Place Using kubectl patch4.5.6: Migrate Kubernetes Objects Using Storage Version Migration4.6: Managing Secrets4.6.1: Managing Secrets using kubectl4.6.2: Managing Secrets using Configuration File4.6.3: Managing Secrets using Kustomize4.7: Inject Data Into Applications4.7.1: Define a Command and Arguments for a Container4.7.2: Define Dependent Environment Variables4.7.3: Define Environment Variables for a Container4.7.4: Expose Pod Information to Containers Through Environment Variables4.7.5: Expose Pod Information to Containers Through Files4.7.6: Distribute Credentials Securely Using Secrets4.8: Run Applications4.8.1: Run a Stateless Application Using a Deployment4.8.2: Run a Single-Instance Stateful Application4.8.3: Run a Replicated Stateful Application4.8.4: Scale a StatefulSet4.8.5: Delete a StatefulSet4.8.6: Force Delete StatefulSet Pods4.8.7: Horizontal Pod Autoscaling4.8.8: HorizontalPodAutoscaler Walkthrough4.8.9: Specifying a Disruption Budget for your Application4.8.10: Accessing the Kubernetes API from a Pod4.9: Run Jobs4.9.1: Running Automated Tasks with a CronJob4.9.2: Coarse Parallel Processing Using a Work Queue4.9.3: Fine Parallel Processing Using a Work Queue4.9.4: Indexed Job for Parallel Processing with Static Work Assignment4.9.5: Job with Pod-to-Pod Communication4.9.6: Parallel Processing using Expansions4.9.7: Handling retriable and non-retriable pod failures with Pod failure policy4.10: Access Applications in a Cluster4.10.1: Deploy and Access the Kubernetes Dashboard4.10.2: Accessing Clusters4.10.3: Configure Access to Multiple Clusters4.10.4: Use Port Forwarding to Access Applications in a Cluster4.10.5: Use a Service to Access an Application in a Cluster4.10.6: Connect a Frontend to a Backend Using Services4.10.7: Create an External Load Balancer4.10.8: List All Container Images Running in a Cluster4.10.9: Set up Ingress on Minikube with the NGINX Ingress Controller4.10.10: Communicate Between Containers in the Same Pod Using a Shared Volume4.10.11: Configure DNS for a Cluster4.10.12: Access Services Running on Clusters4.11: Extend Kubernetes4.11.1: Configure the Aggregation Layer4.11.2: Use Custom Resources4.11.2.1: Extend the Kubernetes API with CustomResourceDefinitions4.11.2.2: Versions in CustomResourceDefinitions4.11.3: Set up an Extension API Server4.11.4: Configure Multiple Schedulers4.11.5: Use an HTTP Proxy to Access the Kubernetes API4.11.6: Use a SOCKS5 Proxy to Access the Kubernetes API4.11.7: Set up Konnectivity service4.12: TLS4.12.1: Issue a Certificate for a Kubernetes API Client Using A CertificateSigningRequest4.12.2: Configure Certificate Rotation for the Kubelet4.12.3: Manage TLS Certificates in a Cluster4.12.4: Manual Rotation of CA Certificates4.13: Manage Cluster Daemons4.13.1: Building a Basic DaemonSet4.13.2: Perform a Rolling Update on a DaemonSet4.13.3: Perform a Rollback on a DaemonSet4.13.4: Running Pods on Only Some Nodes4.14: Networking4.14.1: Adding entries to Pod /etc/hosts with HostAliases4.14.2: Extend Service IP Ranges4.14.3: Validate IPv4/IPv6 dual-stack4.15: Extend kubectl with plugins4.16: Manage HugePages4.17: Schedule GPUs5: Tutorials5.1: Hello Minikube5.2: Learn Kubernetes Basics5.2.1: Create a Cluster5.2.1.1: Using Minikube to Create a Cluster5.2.2: Deploy an App5.2.2.1: Using kubectl to Create a Deployment5.2.3: Explore Your App5.2.3.1: Viewing Pods and Nodes5.2.4: Expose Your App Publicly5.2.4.1: Using a Service to Expose Your App5.2.5: Scale Your App5.2.5.1: Running Multiple Instances of Your App5.2.6: Update Your App5.2.6.1: Performing a Rolling Update5.3: Configuration5.3.1: Updating Configuration via a ConfigMap5.3.2: Configuring Redis using a ConfigMap5.3.3: Adopting Sidecar Containers5.4: Security5.4.1: Apply Pod Security Standards at the Cluster Level5.4.2: Apply Pod Security Standards at the Namespace Level5.4.3: Restrict a Container's Access to Resources with AppArmor5.4.4: Restrict a Container's Syscalls with seccomp5.5: Stateless Applications5.5.1: Exposing an External IP Address to Access an Application in a Cluster5.5.2: Example: Deploying PHP Guestbook application with Redis5.6: Stateful Applications5.6.1: StatefulSet Basics5.6.2: Example: Deploying WordPress and MySQL with Persistent Volumes5.6.3: Example: Deploying Cassandra with a StatefulSet5.6.4: Running ZooKeeper, A Distributed System Coordinator5.7: Cluster Management5.7.1: Running Kubelet in Standalone Mode5.7.2: Namespaces Walkthrough5.8: Services5.8.1: Connecting Applications with Services5.8.2: Using Source IP5.8.3: Explore Termination Behavior for Pods And Their Endpoints6: Reference6.1: Glossary6.2: API Overview6.2.1: Declarative API Validation6.2.2: Kubernetes API Concepts6.2.3: Server-Side Apply6.2.4: Client Libraries6.2.5: Common Expression Language in Kubernetes6.2.6: Kubernetes Deprecation Policy6.2.7: Deprecated API Migration Guide6.2.8: Kubernetes API health endpoints6.3: API Access Control6.3.1: Authenticating6.3.2: Authenticating with Bootstrap Tokens6.3.3: Authorization6.3.4: Using RBAC Authorization6.3.5: Using Node Authorization6.3.6: Webhook Mode6.3.7: Using ABAC Authorization6.3.8: Admission Control in Kubernetes6.3.9: Dynamic Admission Control6.3.10: Managing Service Accounts6.3.11: Certificates and Certificate Signing Requests6.3.12: Mapping PodSecurityPolicies to Pod Security Standards6.3.13: Kubelet authentication/authorization6.3.14: TLS bootstrapping6.3.15: Mutating Admission Policy6.3.16: Validating Admission Policy6.4: Well-Known Labels, Annotations and Taints6.4.1: Audit Annotations6.5: Kubernetes API6.5.1: Workload Resources6.5.1.1: Pod6.5.1.2: Binding6.5.1.3: PodTemplate6.5.1.4: ReplicationController6.5.1.5: ReplicaSet6.5.1.6: Deployment6.5.1.7: StatefulSet6.5.1.8: ControllerRevision6.5.1.9: DaemonSet6.5.1.10: Job6.5.1.11: CronJob6.5.1.12: HorizontalPodAutoscaler6.5.1.13: HorizontalPodAutoscaler6.5.1.14: PriorityClass6.5.1.15: DeviceTaintRule v1alpha36.5.1.16: ResourceClaim v1beta26.5.1.17: ResourceClaimTemplate v1beta26.5.1.18: ResourceSlice v1beta26.5.2: Service Resources6.5.2.1: Service6.5.2.2: Endpoints6.5.2.3: EndpointSlice6.5.2.4: Ingress6.5.2.5: IngressClass6.5.3: Config and Storage Resources6.5.3.1: ConfigMap6.5.3.2: Secret6.5.3.3: CSIDriver6.5.3.4: CSINode6.5.3.5: CSIStorageCapacity6.5.3.6: PersistentVolumeClaim6.5.3.7: PersistentVolume6.5.3.8: StorageClass6.5.3.9: StorageVersionMigration v1alpha16.5.3.10: Volume6.5.3.11: VolumeAttachment6.5.3.12: VolumeAttributesClass v1beta16.5.4: Authentication Resources6.5.4.1: ServiceAccount6.5.4.2: TokenRequest6.5.4.3: TokenReview6.5.4.4: CertificateSigningRequest6.5.4.5: ClusterTrustBundle v1beta16.5.4.6: SelfSubjectReview6.5.5: Authorization Resources6.5.5.1: LocalSubjectAccessReview6.5.5.2: SelfSubjectAccessReview6.5.5.3: SelfSubjectRulesReview6.5.5.4: SubjectAccessReview6.5.5.5: ClusterRole6.5.5.6: ClusterRoleBinding6.5.5.7: Role6.5.5.8: RoleBinding6.5.6: Policy Resources6.5.6.1: FlowSchema6.5.6.2: LimitRange6.5.6.3: ResourceQuota6.5.6.4: NetworkPolicy6.5.6.5: PodDisruptionBudget6.5.6.6: PriorityLevelConfiguration6.5.6.7: ValidatingAdmissionPolicy6.5.6.8: ValidatingAdmissionPolicyBinding6.5.6.9: MutatingAdmissionPolicy v1alpha16.5.6.10: MutatingAdmissionPolicyBinding v1alpha16.5.7: Extend Resources6.5.7.1: CustomResourceDefinition6.5.7.2: DeviceClass v1beta26.5.7.3: MutatingWebhookConfiguration6.5.7.4: ValidatingWebhookConfiguration6.5.8: Cluster Resources6.5.8.1: APIService6.5.8.2: ComponentStatus6.5.8.3: Event6.5.8.4: IPAddress6.5.8.5: Lease6.5.8.6: LeaseCandidate v1beta16.5.8.7: Namespace6.5.8.8: Node6.5.8.9: RuntimeClass6.5.8.10: ServiceCIDR6.5.9: Common Definitions6.5.9.1: DeleteOptions6.5.9.2: LabelSelector6.5.9.3: ListMeta6.5.9.4: LocalObjectReference6.5.9.5: NodeSelectorRequirement6.5.9.6: ObjectFieldSelector6.5.9.7: ObjectMeta6.5.9.8: ObjectReference6.5.9.9: Patch6.5.9.10: Quantity6.5.9.11: ResourceFieldSelector6.5.9.12: Status6.5.9.13: TypedLocalObjectReference6.5.10: Common Parameters6.6: Instrumentation6.6.1: Kubernetes Component SLI Metrics6.6.2: CRI Pod & Container Metrics6.6.3: Node metrics data6.6.4: Kubernetes z-pages6.6.5: Kubernetes Metrics Reference6.7: Kubernetes Issues and Security6.7.1: Kubernetes Issue Tracker6.7.2: Kubernetes Security and Disclosure Information6.7.3: Official CVE Feed6.8: Node Reference Information6.8.1: Kubelet Checkpoint API6.8.2: Linux Kernel Version Requirements6.8.3: Articles on dockershim Removal and on Using CRI-compatible Runtimes6.8.4: Node Labels Populated By The Kubelet6.8.5: Local Files And Paths Used By The Kubelet6.8.6: Kubelet Configuration Directory Merging6.8.7: Kubelet Device Manager API Versions6.8.8: Kubelet Systemd Watchdog6.8.9: Node Status6.8.10: Seccomp and Kubernetes6.9: Networking Reference6.9.1: Protocols for Services6.9.2: Ports and Protocols6.9.3: Virtual IPs and Service Proxies6.10: Setup tools6.10.1: Kubeadm6.10.1.1: Kubeadm Generated6.10.1.1.1: 6.10.1.1.2: 6.10.1.1.2.1: 6.10.1.1.2.2: 6.10.1.1.2.3: 6.10.1.1.2.4: 6.10.1.1.2.5: 6.10.1.1.2.6: 6.10.1.1.2.7: 6.10.1.1.2.8: 6.10.1.1.2.9: 6.10.1.1.2.10: 6.10.1.1.2.11: 6.10.1.1.2.12: 6.10.1.1.2.13: 6.10.1.1.2.14: 6.10.1.1.2.15: 6.10.1.1.2.16: 6.10.1.1.3: 6.10.1.1.4: 6.10.1.1.4.1: 6.10.1.1.4.2: 6.10.1.1.4.3: 6.10.1.1.4.4: 6.10.1.1.4.5: 6.10.1.1.4.6: 6.10.1.1.4.7: 6.10.1.1.4.8: 6.10.1.1.4.9: 6.10.1.1.4.10: 6.10.1.1.5: 6.10.1.1.5.1: 6.10.1.1.5.2: 6.10.1.1.5.3: 6.10.1.1.5.4: 6.10.1.1.5.5: 6.10.1.1.5.6: 6.10.1.1.5.7: 6.10.1.1.5.8: 6.10.1.1.5.9: 6.10.1.1.5.10: 6.10.1.1.5.11: 6.10.1.1.5.12: 6.10.1.1.5.13: 6.10.1.1.5.14: 6.10.1.1.5.15: 6.10.1.1.5.16: 6.10.1.1.5.17: 6.10.1.1.5.18: 6.10.1.1.5.19: 6.10.1.1.5.20: 6.10.1.1.5.21: 6.10.1.1.5.22: 6.10.1.1.5.23: 6.10.1.1.5.24: 6.10.1.1.5.25: 6.10.1.1.5.26: 6.10.1.1.5.27: 6.10.1.1.5.28: 6.10.1.1.5.29: 6.10.1.1.5.30: 6.10.1.1.5.31: 6.10.1.1.5.32: 6.10.1.1.5.33: 6.10.1.1.5.34: 6.10.1.1.5.35: 6.10.1.1.5.36: 6.10.1.1.5.37: 6.10.1.1.5.38: 6.10.1.1.5.39: 6.10.1.1.5.40: 6.10.1.1.5.41: 6.10.1.1.5.42: 6.10.1.1.5.43: 6.10.1.1.5.44: 6.10.1.1.5.45: 6.10.1.1.6: 6.10.1.1.6.1: 6.10.1.1.6.2: 6.10.1.1.6.3: 6.10.1.1.6.4: 6.10.1.1.6.5: 6.10.1.1.6.6: 6.10.1.1.6.7: 6.10.1.1.6.8: 6.10.1.1.6.9: 6.10.1.1.6.10: 6.10.1.1.6.11: 6.10.1.1.6.12: 6.10.1.1.6.13: 6.10.1.1.6.14: 6.10.1.1.7: 6.10.1.1.7.1: 6.10.1.1.8: 6.10.1.1.8.1: 6.10.1.1.8.2: 6.10.1.1.8.3: 6.10.1.1.8.4: 6.10.1.1.9: 6.10.1.1.9.1: 6.10.1.1.9.2: 6.10.1.1.9.3: 6.10.1.1.9.4: 6.10.1.1.10: 6.10.1.1.10.1: 6.10.1.1.10.2: 6.10.1.1.10.3: 6.10.1.1.10.4: 6.10.1.1.10.5: 6.10.1.1.10.6: 6.10.1.1.10.7: 6.10.1.1.10.8: 6.10.1.1.10.9: 6.10.1.1.10.10: 6.10.1.1.10.11: 6.10.1.1.10.12: 6.10.1.1.10.13: 6.10.1.1.10.14: 6.10.1.1.10.15: 6.10.1.1.10.16: 6.10.1.1.10.17: 6.10.1.1.10.18: 6.10.1.1.10.19: 6.10.1.1.10.20: 6.10.1.1.10.21: 6.10.1.1.10.22: 6.10.1.1.10.23: 6.10.1.1.10.24: 6.10.1.1.10.25: 6.10.1.1.10.26: 6.10.1.1.10.27: 6.10.1.1.11: 6.10.1.1.12: 6.10.1.2: kubeadm init6.10.1.3: kubeadm join6.10.1.4: kubeadm upgrade6.10.1.5: kubeadm upgrade phases6.10.1.6: kubeadm config6.10.1.7: kubeadm reset6.10.1.8: kubeadm token6.10.1.9: kubeadm version6.10.1.10: kubeadm alpha6.10.1.11: kubeadm certs6.10.1.12: kubeadm init phase6.10.1.13: kubeadm join phase6.10.1.14: kubeadm kubeconfig6.10.1.15: kubeadm reset phase6.10.1.16: Implementation details6.11: Command line tool (kubectl)6.11.1: Introduction to kubectl6.11.2: kubectl Quick Reference6.11.3: kubectl reference6.11.3.1: kubectl6.11.3.2: kubectl annotate6.11.3.3: kubectl api-resources6.11.3.4: kubectl api-versions6.11.3.5: kubectl apply6.11.3.5.1: kubectl apply edit-last-applied6.11.3.5.2: kubectl apply set-last-applied6.11.3.5.3: kubectl apply view-last-applied6.11.3.6: kubectl attach6.11.3.7: kubectl auth6.11.3.7.1: kubectl auth can-i6.11.3.7.2: kubectl auth reconcile6.11.3.7.3: kubectl auth whoami6.11.3.8: kubectl autoscale6.11.3.9: kubectl certificate6.11.3.9.1: kubectl certificate approve6.11.3.9.2: kubectl certificate deny6.11.3.10: kubectl cluster-info6.11.3.10.1: kubectl cluster-info dump6.11.3.11: kubectl completion6.11.3.12: kubectl config6.11.3.12.1: kubectl config current-context6.11.3.12.2: kubectl config delete-cluster6.11.3.12.3: kubectl config delete-context6.11.3.12.4: kubectl config delete-user6.11.3.12.5: kubectl config get-clusters6.11.3.12.6: kubectl config get-contexts6.11.3.12.7: kubectl config get-users6.11.3.12.8: kubectl config rename-context6.11.3.12.9: kubectl config set6.11.3.12.10: kubectl config set-cluster6.11.3.12.11: kubectl config set-context6.11.3.12.12: kubectl config set-credentials6.11.3.12.13: kubectl config unset6.11.3.12.14: kubectl config use-context6.11.3.12.15: kubectl config view6.11.3.13: kubectl cordon6.11.3.14: kubectl cp6.11.3.15: kubectl create6.11.3.15.1: kubectl create clusterrole6.11.3.15.2: kubectl create clusterrolebinding6.11.3.15.3: kubectl create configmap6.11.3.15.4: kubectl create cronjob6.11.3.15.5: kubectl create deployment6.11.3.15.6: kubectl create ingress6.11.3.15.7: kubectl create job6.11.3.15.8: kubectl create namespace6.11.3.15.9: kubectl create poddisruptionbudget6.11.3.15.10: kubectl create priorityclass6.11.3.15.11: kubectl create quota6.11.3.15.12: kubectl create role6.11.3.15.13: kubectl create rolebinding6.11.3.15.14: kubectl create secret6.11.3.15.15: kubectl create secret docker-registry6.11.3.15.16: kubectl create secret generic6.11.3.15.17: kubectl create secret tls6.11.3.15.18: kubectl create service6.11.3.15.19: kubectl create service clusterip6.11.3.15.20: kubectl create service externalname6.11.3.15.21: kubectl create service loadbalancer6.11.3.15.22: kubectl create service nodeport6.11.3.15.23: kubectl create serviceaccount6.11.3.15.24: kubectl create token6.11.3.16: kubectl debug6.11.3.17: kubectl delete6.11.3.18: kubectl describe6.11.3.19: kubectl diff6.11.3.20: kubectl drain6.11.3.21: kubectl edit6.11.3.22: kubectl events6.11.3.23: kubectl exec6.11.3.24: kubectl explain6.11.3.25: kubectl expose6.11.3.26: kubectl get6.11.3.27: kubectl kustomize6.11.3.28: kubectl label6.11.3.29: kubectl logs6.11.3.30: kubectl options6.11.3.31: kubectl patch6.11.3.32: kubectl plugin6.11.3.32.1: kubectl plugin list6.11.3.33: kubectl port-forward6.11.3.34: kubectl proxy6.11.3.35: kubectl replace6.11.3.36: kubectl rollout6.11.3.36.1: kubectl rollout history6.11.3.36.2: kubectl rollout pause6.11.3.36.3: kubectl rollout restart6.11.3.36.4: kubectl rollout resume6.11.3.36.5: kubectl rollout status6.11.3.36.6: kubectl rollout undo6.11.3.37: kubectl run6.11.3.38: kubectl scale6.11.3.39: kubectl set6.11.3.39.1: kubectl set env6.11.3.39.2: kubectl set image6.11.3.39.3: kubectl set resources6.11.3.39.4: kubectl set selector6.11.3.39.5: kubectl set serviceaccount6.11.3.39.6: kubectl set subject6.11.3.40: kubectl taint6.11.3.41: kubectl top6.11.3.41.1: kubectl top node6.11.3.41.2: kubectl top pod6.11.3.42: kubectl uncordon6.11.3.43: kubectl version6.11.3.44: kubectl wait6.11.4: kubectl Commands6.11.5: kubectl6.11.6: JSONPath Support6.11.7: kubectl for Docker Users6.11.8: kubectl Usage Conventions6.12: Component tools6.12.1: Feature Gates6.12.2: Feature Gates (removed)6.12.3: kubelet6.12.4: kube-apiserver6.12.5: kube-controller-manager6.12.6: kube-proxy6.12.7: kube-scheduler6.13: Debug cluster6.13.1: Flow control6.14: Configuration APIs6.14.1: Client Authentication (v1)6.14.2: Client Authentication (v1beta1)6.14.3: Event Rate Limit Configuration (v1alpha1)6.14.4: Image Policy API (v1alpha1)6.14.5: kube-apiserver Admission (v1)6.14.6: kube-apiserver Audit Configuration (v1)6.14.7: kube-apiserver Configuration (v1)6.14.8: kube-apiserver Configuration (v1alpha1)6.14.9: kube-apiserver Configuration (v1beta1)6.14.10: kube-controller-manager Configuration (v1alpha1)6.14.11: kube-proxy Configuration (v1alpha1)6.14.12: kube-scheduler Configuration (v1)6.14.13: kubeadm Configuration (v1beta3)6.14.14: kubeadm Configuration (v1beta4)6.14.15: kubeconfig (v1)6.14.16: Kubelet Configuration (v1)6.14.17: Kubelet Configuration (v1alpha1)6.14.18: Kubelet Configuration (v1beta1)6.14.19: Kubelet CredentialProvider (v1)6.14.20: WebhookAdmission Configuration (v1)6.15: External APIs6.15.1: Kubernetes Custom Metrics (v1beta2)6.15.2: Kubernetes External Metrics (v1beta1)6.15.3: Kubernetes Metrics (v1beta1)6.16: Scheduling6.16.1: Scheduler Configuration6.16.2: Scheduling Policies6.17: Other Tools7: Contribute to Kubernetes7.1: Contribute to Kubernetes Documentation7.2: Contributing to Kubernetes blogs7.2.1: Submitting articles to Kubernetes blogs7.2.2: Blog guidelines7.2.3: Blog article mirroring7.2.4: Post-release communications7.2.5: Helping as a blog writing buddy7.3: Suggesting content improvements7.4: Contributing new content7.4.1: Opening a pull request7.4.2: Documenting a feature for a release7.4.3: Submitting case studies7.5: Reviewing changes7.5.1: Reviewing pull requests7.5.2: Reviewing for approvers and reviewers7.6: Localizing Kubernetes documentation7.7: Participating in SIG Docs7.7.1: Roles and responsibilities7.7.2: Issue Wranglers7.7.3: PR wranglers7.8: Documentation style overview7.8.1: Documentation Content Guide7.8.2: Documentation Style Guide7.8.3: Diagram Guide7.8.4: Writing a new topic7.8.5: Page content types7.8.6: Content organization7.8.7: Custom Hugo Shortcodes7.9: Updating Reference Documentation7.9.1: Reference Documentation Quickstart7.9.2: Contributing to the Upstream Kubernetes Code7.9.3: Generating Reference Documentation for the Kubernetes API7.9.4: Generating Reference Documentation for kubectl Commands7.9.5: Generating Reference Documentation for Metrics7.9.6: Generating Reference Pages for Kubernetes Components and Tools7.9.7: 7.10: Advanced contributing7.11: Viewing Site Analytics8: Docs smoke test page1 - Kubernetes DocumentationKubernetes is an open source container orchestration engine for automating deployment, scaling, and management of containerized applications. The open source project is hosted by the Cloud Native Computing Foundation.1.1 - Available Documentation VersionsThis website contains documentation for the current version of Kubernetes and the four previous versions of Kubernetes.The availability of documentation for a Kubernetes version is separate from whether that release is currently supported. Read Support period to learn about which versions of Kubernetes are officially supported, and for how long.2 - Getting startedThis section lists the different ways to set up and run Kubernetes. When you install Kubernetes, choose an installation type based on: ease of maintenance, security, control, available resources, and expertise required to operate and manage a cluster.You can download Kubernetes to deploy a Kubernetes cluster on a local machine, into the cloud, or for your own datacenter.Several Kubernetes components such as kube-apiserver or kube-proxy can also be deployed as container images within the cluster.It is recommended to run Kubernetes components as container images wherever that is possible, and to have Kubernetes manage those components. Components that run containers - notably, the kubelet - can't be included in this category.If you don't want to manage a Kubernetes cluster yourself, you could pick a managed service, including certified platforms. There are also other standardized and custom solutions across a wide range of cloud and bare metal environments.Learning environmentIf you're learning Kubernetes, use the tools supported by the Kubernetes community, or tools in the ecosystem to set up a Kubernetes cluster on a local machine. See Install tools.Production environmentWhen evaluating a solution for a production environment, consider which aspects of operating a Kubernetes cluster (or abstractions) you want to manage yourself and which you prefer to hand off to a provider.For a cluster you're managing yourself, the officially supported tool for deploying Kubernetes is kubeadm.What's nextDownload KubernetesDownload and install tools including kubectlSelect a container runtime for your new clusterLearn about best practices for cluster setupKubernetes is designed for its control plane to run on Linux. Within your cluster you can run applications on Linux or other operating systems, including Windows.Learn to set up clusters with Windows nodes2.1 - Learning environment2.2 - Production environmentCreate a production-quality Kubernetes clusterA production-quality Kubernetes cluster requires planning and preparation. If your Kubernetes cluster is to run critical workloads, it must be configured to be resilient. This page explains steps you can take to set up a production-ready cluster, or to promote an existing cluster for production use. If you're already familiar with production setup and want the links, skip to What's next.Production considerationsTypically, a production Kubernetes cluster environment has more requirements than a personal learning, development, or test environment Kubernetes. A production environment may require secure access by many users, consistent availability, and the resources to adapt to changing demands.As you decide where you want your production Kubernetes environment to live (on premises or in a cloud) and the amount of management you want to take on or hand to others, consider how your requirements for a Kubernetes cluster are influenced by the following issues:Availability: A single-machine Kubernetes learning environment has a single point of failure. Creating a highly available cluster means considering:Separating the control plane from the worker nodes.Replicating the control plane components on multiple nodes.Load balancing traffic to the cluster’s API server.Having enough worker nodes available, or able to quickly become available, as changing workloads warrant it.Scale: If you expect your production Kubernetes environment to receive a stable amount of demand, you might be able to set up for the capacity you need and be done. However, if you expect demand to grow over time or change dramatically based on things like season or special events, you need to plan how to scale to relieve increased pressure from more requests to the control plane and worker nodes or scale down to reduce unused resources.Security and access management: You have full admin privileges on your own Kubernetes learning cluster. But shared clusters with important workloads, and more than one or two users, require a more refined approach to who and what can access cluster resources. You can use role-based access control (RBAC) and other security mechanisms to make sure that users and workloads can get access to the resources they need, while keeping workloads, and the cluster itself, secure. You can set limits on the resources that users and workloads can access by managing policies and container resources.Before building a Kubernetes production environment on your own, consider handing off some or all of this job to Turnkey Cloud Solutions providers or other Kubernetes Partners. Options include:Serverless: Just run workloads on third-party equipment without managing a cluster at all. You will be charged for things like CPU usage, memory, and disk requests.Managed control plane: Let the provider manage the scale and availability of the cluster's control plane, as well as handle patches and upgrades.Managed worker nodes: Configure pools of nodes to meet your needs, then the provider makes sure those nodes are available and ready to implement upgrades when needed.Integration: There are providers that integrate Kubernetes with other services you may need, such as storage, container registries, authentication methods, and development tools.Whether you build a production Kubernetes cluster yourself or work with partners, review the following sections to evaluate your needs as they relate to your cluster’s control plane, worker nodes, user access, and workload resources.Production cluster setupIn a production-quality Kubernetes cluster, the control plane manages the cluster from services that can be spread across multiple computers in different ways. Each worker node, however, represents a single entity that is configured to run Kubernetes pods.Production control planeThe simplest Kubernetes cluster has the entire control plane and worker node services running on the same machine. You can grow that environment by adding worker nodes, as reflected in the diagram illustrated in Kubernetes Components. If the cluster is meant to be available for a short period of time, or can be discarded if something goes seriously wrong, this might meet your needs.If you need a more permanent, highly available cluster, however, you should consider ways of extending the control plane. By design, one-machine control plane services running on a single machine are not highly available. If keeping the cluster up and running and ensuring that it can be repaired if something goes wrong is important, consider these steps:Choose deployment tools: You can deploy a control plane using tools such as kubeadm, kops, and kubespray. See Installing Kubernetes with deployment tools to learn tips for production-quality deployments using each of those deployment methods. Different Container Runtimes are available to use with your deployments.Manage certificates: Secure communications between control plane services are implemented using certificates. Certificates are automatically generated during deployment or you can generate them using your own certificate authority. See PKI certificates and requirements for details.Configure load balancer for apiserver: Configure a load balancer to distribute external API requests to the apiserver service instances running on different nodes. See Create an External Load Balancer for details.Separate and backup etcd service: The etcd services can either run on the same machines as other control plane services or run on separate machines, for extra security and availability. Because etcd stores cluster configuration data, backing up the etcd database should be done regularly to ensure that you can repair that database if needed. See the etcd FAQ for details on configuring and using etcd. See Operating etcd clusters for Kubernetes and Set up a High Availability etcd cluster with kubeadm for details.Create multiple control plane systems: For high availability, the control plane should not be limited to a single machine. If the control plane services are run by an init service (such as systemd), each service should run on at least three machines. However, running control plane services as pods in Kubernetes ensures that the replicated number of services that you request will always be available. The scheduler should be fault tolerant, but not highly available. Some deployment tools set up Raft consensus algorithm to do leader election of Kubernetes services. If the primary goes away, another service elects itself and take over.Span multiple zones: If keeping your cluster available at all times is critical, consider creating a cluster that runs across multiple data centers, referred to as zones in cloud environments. Groups of zones are referred to as regions. By spreading a cluster across multiple zones in the same region, it can improve the chances that your cluster will continue to function even if one zone becomes unavailable. See Running in multiple zones for details.Manage on-going features: If you plan to keep your cluster over time, there are tasks you need to do to maintain its health and security. For example, if you installed with kubeadm, there are instructions to help you with Certificate Management and Upgrading kubeadm clusters. See Administer a Cluster for a longer list of Kubernetes administrative tasks.To learn about available options when you run control plane services, see kube-apiserver, kube-controller-manager, and kube-scheduler component pages. For highly available control plane examples, see Options for Highly Available topology, Creating Highly Available clusters with kubeadm, and Operating etcd clusters for Kubernetes. See Backing up an etcd cluster for information on making an etcd backup plan.Production worker nodesProduction-quality workloads need to be resilient and anything they rely on needs to be resilient (such as CoreDNS). Whether you manage your own control plane or have a cloud provider do it for you, you still need to consider how you want to manage your worker nodes (also referred to simply as nodes).Configure nodes: Nodes can be physical or virtual machines. If you want to create and manage your own nodes, you can install a supported operating system, then add and run the appropriate Node services. Consider:The demands of your workloads when you set up nodes by having appropriate memory, CPU, and disk speed and storage capacity available.Whether generic computer systems will do or you have workloads that need GPU processors, Windows nodes, or VM isolation.Validate nodes: See Valid node setup for information on how to ensure that a node meets the requirements to join a Kubernetes cluster.Add nodes to the cluster: If you are managing your own cluster you can add nodes by setting up your own machines and either adding them manually or having them register themselves to the cluster’s apiserver. See the Nodes section for information on how to set up Kubernetes to add nodes in these ways.Scale nodes: Have a plan for expanding the capacity your cluster will eventually need. See Considerations for large clusters to help determine how many nodes you need, based on the number of pods and containers you need to run. If you are managing nodes yourself, this can mean purchasing and installing your own physical equipment.Autoscale nodes: Read Node Autoscaling to learn about the tools available to automatically manage your nodes and the capacity they provide.Set up node health checks: For important workloads, you want to make sure that the nodes and pods running on those nodes are healthy. Using the Node Problem Detector daemon, you can ensure your nodes are healthy.Production user managementIn production, you may be moving from a model where you or a small group of people are accessing the cluster to where there may potentially be dozens or hundreds of people. In a learning environment or platform prototype, you might have a single administrative account for everything you do. In production, you will want more accounts with different levels of access to different namespaces.Taking on a production-quality cluster means deciding how you want to selectively allow access by other users. In particular, you need to select strategies for validating the identities of those who try to access your cluster (authentication) and deciding if they have permissions to do what they are asking (authorization):Authentication: The apiserver can authenticate users using client certificates, bearer tokens, an authenticating proxy, or HTTP basic auth. You can choose which authentication methods you want to use. Using plugins, the apiserver can leverage your organization’s existing authentication methods, such as LDAP or Kerberos. See Authentication for a description of these different methods of authenticating Kubernetes users.Authorization: When you set out to authorize your regular users, you will probably choose between RBAC and ABAC authorization. See Authorization Overview to review different modes for authorizing user accounts (as well as service account access to your cluster):Role-based access control (RBAC): Lets you assign access to your cluster by allowing specific sets of permissions to authenticated users. Permissions can be assigned for a specific namespace (Role) or across the entire cluster (ClusterRole). Then using RoleBindings and ClusterRoleBindings, those permissions can be attached to particular users.Attribute-based access control (ABAC): Lets you create policies based on resource attributes in the cluster and will allow or deny access based on those attributes. Each line of a policy file identifies versioning properties (apiVersion and kind) and a map of spec properties to match the subject (user or group), resource property, non-resource property (/version or /apis), and readonly. See Examples for details.As someone setting up authentication and authorization on your production Kubernetes cluster, here are some things to consider:Set the authorization mode: When the Kubernetes API server (kube-apiserver) starts, supported authorization modes must be set using an --authorization-config file or the --authorization-mode flag. For example, that flag in the kube-adminserver.yaml file (in /etc/kubernetes/manifests) could be set to Node,RBAC. This would allow Node and RBAC authorization for authenticated requests.Create user certificates and role bindings (RBAC): If you are using RBAC authorization, users can create a CertificateSigningRequest (CSR) that can be signed by the cluster CA. Then you can bind Roles and ClusterRoles to each user. See Certificate Signing Requests for details.Create policies that combine attributes (ABAC): If you are using ABAC authorization, you can assign combinations of attributes to form policies to authorize selected users or groups to access particular resources (such as a pod), namespace, or apiGroup. For more information, see Examples.Consider Admission Controllers: Additional forms of authorization for requests that can come in through the API server include Webhook Token Authentication. Webhooks and other special authorization types need to be enabled by adding Admission Controllers to the API server.Set limits on workload resourcesDemands from production workloads can cause pressure both inside and outside of the Kubernetes control plane. Consider these items when setting up for the needs of your cluster's workloads:Set namespace limits: Set per-namespace quotas on things like memory and CPU. See Manage Memory, CPU, and API Resources for details.Prepare for DNS demand: If you expect workloads to massively scale up, your DNS service must be ready to scale up as well. See Autoscale the DNS service in a Cluster.Create additional service accounts: User accounts determine what users can do on a cluster, while a service account defines pod access within a particular namespace. By default, a pod takes on the default service account from its namespace. See Managing Service Accounts for information on creating a new service account. For example, you might want to:Add secrets that a pod could use to pull images from a particular container registry. See Configure Service Accounts for Pods for an example.Assign RBAC permissions to a service account. See ServiceAccount permissions for details.What's nextDecide if you want to build your own production Kubernetes or obtain one from available Turnkey Cloud Solutions or Kubernetes Partners.If you choose to build your own cluster, plan how you want to handle certificates and set up high availability for features such as etcd and the API server.Choose from kubeadm, kops or Kubespray deployment methods.Configure user management by determining your Authentication and Authorization methods.Prepare for application workloads by setting up resource limits, DNS autoscaling and service accounts.2.2.1 - Container RuntimesNote: Dockershim has been removed from the Kubernetes project as of release 1.24. Read the Dockershim Removal FAQ for further details.You need to install a container runtime into each node in the cluster so that Pods can run there. This page outlines what is involved and describes related tasks for setting up nodes.Kubernetes 1.33 requires that you use a runtime that conforms with the Container Runtime Interface (CRI).See CRI version support for more information.This page provides an outline of how to use several common container runtimes with Kubernetes.containerdCRI-ODocker EngineMirantis Container RuntimeNote:Kubernetes releases before v1.24 included a direct integration with Docker Engine, using a component named dockershim. That special direct integration is no longer part of Kubernetes (this removal was announced as part of the v1.20 release). You can read Check whether Dockershim removal affects you to understand how this removal might affect you. To learn about migrating from using dockershim, see Migrating from dockershim.If you are running a version of Kubernetes other than v1.33, check the documentation for that version.Install and configure prerequisitesNetwork configurationBy default, the Linux kernel does not allow IPv4 packets to be routed between interfaces. Most Kubernetes cluster networking implementations will change this setting (if needed), but some might expect the administrator to do it for them. (Some might also expect other sysctl parameters to be set, kernel modules to be loaded, etc; consult the documentation for your specific network implementation.)Enable IPv4 packet forwardingTo manually enable IPv4 packet forwarding:# sysctl params required by setup, params persist across reboots cat < /etc/containerd/config.toml as specified in getting-started.md and then set the configuration parameters specified above accordingly.If you apply this change, make sure to restart containerd:sudo systemctl restart containerd When using kubeadm, manually configure the cgroup driver for kubelet.In Kubernetes v1.28, you can enable automatic detection of the cgroup driver as an alpha feature. See systemd cgroup driver for more details.Overriding the sandbox (pause) imageIn your containerd config you can overwrite the sandbox image by setting the following config:[plugins."io.containerd.grpc.v1.cri"] sandbox_image = "registry.k8s.io/pause:3.10" You might need to restart containerd as well once you've updated the config file: systemctl restart containerd.CRI-OThis section contains the necessary steps to install CRI-O as a container runtime.To install CRI-O, follow CRI-O Install Instructions.cgroup driverCRI-O uses the systemd cgroup driver per default, which is likely to work fine for you. To switch to the cgroupfs cgroup driver, either edit /etc/crio/crio.conf or place a drop-in configuration in /etc/crio/crio.conf.d/02-cgroup-manager.conf, for example:[crio.runtime] conmon_cgroup = "pod" cgroup_manager = "cgroupfs" You should also note the changed conmon_cgroup, which has to be set to the value pod when using CRI-O with cgroupfs. It is generally necessary to keep the cgroup driver configuration of the kubelet (usually done via kubeadm) and CRI-O in sync.In Kubernetes v1.28, you can enable automatic detection of the cgroup driver as an alpha feature. See systemd cgroup driver for more details.For CRI-O, the CRI socket is /var/run/crio/crio.sock by default.Overriding the sandbox (pause) imageIn your CRI-O config you can set the following config value:[crio.image] pause_image="registry.k8s.io/pause:3.10" This config option supports live configuration reload to apply this change: systemctl reload crio or by sending SIGHUP to the crio process.Docker EngineNote:These instructions assume that you are using the cri-dockerd adapter to integrate Docker Engine with Kubernetes.On each of your nodes, install Docker for your Linux distribution as per Install Docker Engine.Install cri-dockerd, following the directions in the install section of the documentation.For cri-dockerd, the CRI socket is /run/cri-dockerd.sock by default.Mirantis Container RuntimeMirantis Container Runtime (MCR) is a commercially available container runtime that was formerly known as Docker Enterprise Edition.You can use Mirantis Container Runtime with Kubernetes using the open source cri-dockerd component, included with MCR.To learn more about how to install Mirantis Container Runtime, visit MCR Deployment Guide.Check the systemd unit named cri-docker.socket to find out the path to the CRI socket.Overriding the sandbox (pause) imageThe cri-dockerd adapter accepts a command line argument for specifying which container image to use as the Pod infrastructure container (“pause image”). The command line argument to use is --pod-infra-container-image.What's nextAs well as a container runtime, your cluster will need a working network plugin.2.2.2 - Installing Kubernetes with deployment toolsThere are many methods and tools for setting up your own production Kubernetes cluster. For example:kubeadmCluster API: A Kubernetes sub-project focused on providing declarative APIs and tooling to simplify provisioning, upgrading, and operating multiple Kubernetes clusters.kops: An automated cluster provisioning tool. For tutorials, best practices, configuration options and information on reaching out to the community, please check the kOps website for details.kubespray: A composition of Ansible playbooks, inventory, provisioning tools, and domain knowledge for generic OS/Kubernetes clusters configuration management tasks. You can reach out to the community on Slack channel #kubespray.2.2.2.1 - Bootstrapping clusters with kubeadm2.2.2.1.1 - Installing kubeadm This page shows how to install the kubeadm toolbox. For information on how to create a cluster with kubeadm once you have performed this installation process, see the Creating a cluster with kubeadm page.This installation guide is for Kubernetes v1.33. If you want to use a different Kubernetes version, please refer to the following pages instead:Installing kubeadm (Kubernetes v1.32)Installing kubeadm (Kubernetes v1.31)Installing kubeadm (Kubernetes v1.30)Installing kubeadm (Kubernetes v1.29)Before you beginA compatible Linux host. The Kubernetes project provides generic instructions for Linux distributions based on Debian and Red Hat, and those distributions without a package manager.2 GB or more of RAM per machine (any less will leave little room for your apps).2 CPUs or more for control plane machines.Full network connectivity between all machines in the cluster (public or private network is fine).Unique hostname, MAC address, and product_uuid for every node. See here for more details.Certain ports are open on your machines. See here for more details.Note:The kubeadm installation is done via binaries that use dynamic linking and assumes that your target system provides glibc. This is a reasonable assumption on many Linux distributions (including Debian, Ubuntu, Fedora, CentOS, etc.) but it is not always the case with custom and lightweight distributions which don't include glibc by default, such as Alpine Linux. The expectation is that the distribution either includes glibc or a compatibility layer that provides the expected symbols.Check your OS versionNote: This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the content guide before submitting a change. More information.LinuxWindowsThe kubeadm project supports LTS kernels. See List of LTS kernels.You can get the kernel version using the command uname -rFor more information, see Linux Kernel Requirements.The kubeadm project supports recent kernel versions. For a list of recent kernels, see Windows Server Release Information.You can get the kernel version (also called the OS version) using the command systeminfoFor more information, see Windows OS version compatibility.A Kubernetes cluster created by kubeadm depends on software that use kernel features. This software includes, but is not limited to the container runtime, the kubelet, and a Container Network Interface plugin.To help you avoid unexpected errors as a result of an unsupported kernel version, kubeadm runs the SystemVerification pre-flight check. This check fails if the kernel version is not supported.You may choose to skip the check, if you know that your kernel provides the required features, even though kubeadm does not support its version.Verify the MAC address and product_uuid are unique for every nodeYou can get the MAC address of the network interfaces using the command ip link or ifconfig -aThe product_uuid can be checked by using the command sudo cat /sys/class/dmi/id/product_uuidIt is very likely that hardware devices will have unique addresses, although some virtual machines may have identical values. Kubernetes uses these values to uniquely identify the nodes in the cluster. If these values are not unique to each node, the installation process may fail.Check network adaptersIf you have more than one network adapter, and your Kubernetes components are not reachable on the default route, we recommend you add IP route(s) so Kubernetes cluster addresses go via the appropriate adapter.Check required portsThese required ports need to be open in order for Kubernetes components to communicate with each other. You can use tools like netcat to check if a port is open. For example:nc 127.0.0.1 6443 -zv -w 2 The pod network plugin you use may also require certain ports to be open. Since this differs with each pod network plugin, please see the documentation for the plugins about what port(s) those need.Swap configurationThe default behavior of a kubelet is to fail to start if swap memory is detected on a node. This means that swap should either be disabled or tolerated by kubelet.To tolerate swap, add failSwapOn: false to kubelet configuration or as a command line argument. Note: even if failSwapOn: false is provided, workloads wouldn't have swap access by default. This can be changed by setting a swapBehavior, again in the kubelet configuration file. To use swap, set a swapBehavior other than the default NoSwap setting. See Swap memory management for more details.To disable swap, sudo swapoff -a can be used to disable swapping temporarily. To make this change persistent across reboots, make sure swap is disabled in config files like /etc/fstab, systemd.swap, depending how it was configured on your system.Installing a container runtimeTo run containers in Pods, Kubernetes uses a container runtime.By default, Kubernetes uses the Container Runtime Interface (CRI) to interface with your chosen container runtime.If you don't specify a runtime, kubeadm automatically tries to detect an installed container runtime by scanning through a list of known endpoints.If multiple or no container runtimes are detected kubeadm will throw an error and will request that you specify which one you want to use.See container runtimes for more information.Note:Docker Engine does not implement the CRI which is a requirement for a container runtime to work with Kubernetes. For that reason, an additional service cri-dockerd has to be installed. cri-dockerd is a project based on the legacy built-in Docker Engine support that was removed from the kubelet in version 1.24.The tables below include the known endpoints for supported operating systems:LinuxWindowsLinux container runtimesRuntimePath to Unix domain socketcontainerdunix:///var/run/containerd/containerd.sockCRI-Ounix:///var/run/crio/crio.sockDocker Engine (using cri-dockerd)unix:///var/run/cri-dockerd.sockWindows container runtimesRuntimePath to Windows named pipecontainerdnpipe:////./pipe/containerd-containerdDocker Engine (using cri-dockerd)npipe:////./pipe/cri-dockerdInstalling kubeadm, kubelet and kubectlYou will install these packages on all of your machines:kubeadm: the command to bootstrap the cluster.kubelet: the component that runs on all of the machines in your cluster and does things like starting pods and containers.kubectl: the command line util to talk to your cluster.kubeadm will not install or manage kubelet or kubectl for you, so you will need to ensure they match the version of the Kubernetes control plane you want kubeadm to install for you. If you do not, there is a risk of a version skew occurring that can lead to unexpected, buggy behaviour. However, one minor version skew between the kubelet and the control plane is supported, but the kubelet version may never exceed the API server version. For example, the kubelet running 1.7.0 should be fully compatible with a 1.8.0 API server, but not vice versa.For information about installing kubectl, see Install and set up kubectl.Warning:These instructions exclude all Kubernetes packages from any system upgrades. This is because kubeadm and Kubernetes require special attention to upgrade.For more information on version skews, see:Kubernetes version and version-skew policyKubeadm-specific version skew policyNote: The legacy package repositories (apt.kubernetes.io and yum.kubernetes.io) have been deprecated and frozen starting from September 13, 2023. Using the new package repositories hosted at pkgs.k8s.io is strongly recommended and required in order to install Kubernetes versions released after September 13, 2023. The deprecated legacy repositories, and their contents, might be removed at any time in the future and without a further notice period. The new package repositories provide downloads for Kubernetes versions starting with v1.24.0.Note:There's a dedicated package repository for each Kubernetes minor version. If you want to install a minor version other than v1.33, please see the installation guide for your desired minor version.Debian-based distributionsRed Hat-based distributionsWithout a package managerThese instructions are for Kubernetes v1.33.Update the apt package index and install packages needed to use the Kubernetes apt repository:sudo apt-get update # apt-transport-https may be a dummy package; if so, you can skip that package sudo apt-get install -y apt-transport-https ca-certificates curl gpg Download the public signing key for the Kubernetes package repositories. The same signing key is used for all repositories so you can disregard the version in the URL:# If the directory `/etc/apt/keyrings` does not exist, it should be created before the curl command, read the note below. # sudo mkdir -p -m 755 /etc/apt/keyrings curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.33/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg Note:In releases older than Debian 12 and Ubuntu 22.04, directory /etc/apt/keyrings does not exist by default, and it should be created before the curl command.Add the appropriate Kubernetes apt repository. Please note that this repository have packages only for Kubernetes 1.33; for other Kubernetes minor versions, you need to change the Kubernetes minor version in the URL to match your desired minor version (you should also check that you are reading the documentation for the version of Kubernetes that you plan to install).# This overwrites any existing configuration in /etc/apt/sources.list.d/kubernetes.list echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.33/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list Update the apt package index, install kubelet, kubeadm and kubectl, and pin their version:sudo apt-get update sudo apt-get install -y kubelet kubeadm kubectl sudo apt-mark hold kubelet kubeadm kubectl (Optional) Enable the kubelet service before running kubeadm:sudo systemctl enable --now kubelet Set SELinux to permissive mode:These instructions are for Kubernetes 1.33.# Set SELinux in permissive mode (effectively disabling it) sudo setenforce 0 sudo sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config Caution:Setting SELinux in permissive mode by running setenforce 0 and sed ... effectively disables it. This is required to allow containers to access the host filesystem; for example, some cluster network plugins require that. You have to do this until SELinux support is improved in the kubelet.You can leave SELinux enabled if you know how to configure it but it may require settings that are not supported by kubeadm.Add the Kubernetes yum repository. The exclude parameter in the repository definition ensures that the packages related to Kubernetes are not upgraded upon running yum update as there's a special procedure that must be followed for upgrading Kubernetes. Please note that this repository have packages only for Kubernetes 1.33; for other Kubernetes minor versions, you need to change the Kubernetes minor version in the URL to match your desired minor version (you should also check that you are reading the documentation for the version of Kubernetes that you plan to install).# This overwrites any existing configuration in /etc/yum.repos.d/kubernetes.repo cat < kubelet.conf. $NODE must be set to the name of the existing failed node in the cluster. Modify the resulted kubelet.conf manually to adjust the cluster name and server endpoint, or pass kubeconfig user --config (see Generating kubeconfig files for additional users). If your cluster does not have the ca.key you must sign the embedded certificates in the kubelet.conf externally.Copy this resulted kubelet.conf to /etc/kubernetes/kubelet.conf on the failed node.Restart the kubelet (systemctl restart kubelet) on the failed node and wait for /var/lib/kubelet/pki/kubelet-client-current.pem to be recreated.Manually edit the kubelet.conf to point to the rotated kubelet client certificates, by replacing client-certificate-data and client-key-data with:client-certificate: /var/lib/kubelet/pki/kubelet-client-current.pem client-key: /var/lib/kubelet/pki/kubelet-client-current.pem Restart the kubelet.Make sure the node becomes Ready.Default NIC When using flannel as the pod network in VagrantThe following error might indicate that something was wrong in the pod network:Error from server (NotFound): the server could not find the requested resource If you're using flannel as the pod network inside Vagrant, then you will have to specify the default interface name for flannel.Vagrant typically assigns two interfaces to all VMs. The first, for which all hosts are assigned the IP address 10.0.2.15, is for external traffic that gets NATed.This may lead to problems with flannel, which defaults to the first interface on a host. This leads to all hosts thinking they have the same public IP address. To prevent this, pass the --iface eth1 flag to flannel so that the second interface is chosen.Non-public IP used for containersIn some situations kubectl logs and kubectl run commands may return with the following errors in an otherwise functional cluster:Error from server: Get https://10.19.0.41:10250/containerLogs/default/mysql-ddc65b868-glc5m/mysql: dial tcp 10.19.0.41:10250: getsockopt: no route to host This may be due to Kubernetes using an IP that can not communicate with other IPs on the seemingly same subnet, possibly by policy of the machine provider.DigitalOcean assigns a public IP to eth0 as well as a private one to be used internally as anchor for their floating IP feature, yet kubelet will pick the latter as the node's InternalIP instead of the public one.Use ip addr show to check for this scenario instead of ifconfig because ifconfig will not display the offending alias IP address. Alternatively an API endpoint specific to DigitalOcean allows to query for the anchor IP from the droplet:curl http://169.254.169.254/metadata/v1/interfaces/public/0/anchor_ipv4/address The workaround is to tell kubelet which IP to use using --node-ip. When using DigitalOcean, it can be the public one (assigned to eth0) or the private one (assigned to eth1) should you want to use the optional private network. The kubeletExtraArgs section of the kubeadm NodeRegistrationOptions structure can be used for this.Then restart kubelet:systemctl daemon-reload systemctl restart kubelet coredns pods have CrashLoopBackOff or Error stateIf you have nodes that are running SELinux with an older version of Docker, you might experience a scenario where the coredns pods are not starting. To solve that, you can try one of the following options:Upgrade to a newer version of Docker.Disable SELinux.Modify the coredns deployment to set allowPrivilegeEscalation to true:kubectl -n kube-system get deployment coredns -o yaml | \ sed 's/allowPrivilegeEscalation: false/allowPrivilegeEscalation: true/g' | \ kubectl apply -f - Another cause for CoreDNS to have CrashLoopBackOff is when a CoreDNS Pod deployed in Kubernetes detects a loop. A number of workarounds are available to avoid Kubernetes trying to restart the CoreDNS Pod every time CoreDNS detects the loop and exits.Warning:Disabling SELinux or setting allowPrivilegeEscalation to true can compromise the security of your cluster.etcd pods restart continuallyIf you encounter the following error:rpc error: code = 2 desc = oci runtime error: exec failed: container_linux.go:247: starting container process caused "process_linux.go:110: decoding init error from pipe caused \"read parent: connection reset by peer\"" This issue appears if you run CentOS 7 with Docker 1.13.1.84. This version of Docker can prevent the kubelet from executing into the etcd container.To work around the issue, choose one of these options:Roll back to an earlier version of Docker, such as 1.13.1-75yum downgrade docker-1.13.1-75.git8633870.el7.centos.x86_64 docker-client-1.13.1-75.git8633870.el7.centos.x86_64 docker-common-1.13.1-75.git8633870.el7.centos.x86_64 Install one of the more recent recommended versions, such as 18.06:sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo yum install docker-ce-18.06.1.ce-3.el7.x86_64 Not possible to pass a comma separated list of values to arguments inside a --component-extra-args flagkubeadm init flags such as --component-extra-args allow you to pass custom arguments to a control-plane component like the kube-apiserver. However, this mechanism is limited due to the underlying type used for parsing the values (mapStringString).If you decide to pass an argument that supports multiple, comma-separated values such as \--apiserver-extra-args "enable-admission-plugins=LimitRanger,NamespaceExists" this flag will fail with flag: malformed pair, expect string=string. This happens because the list of arguments for \--apiserver-extra-args expects key=value pairs and in this case NamespacesExists is considered as a key that is missing a value.Alternatively, you can try separating the key=value pairs like so: \--apiserver-extra-args "enable-admission-plugins=LimitRanger,enable-admission-plugins=NamespaceExists" but this will result in the key enable-admission-plugins only having the value of NamespaceExists.A known workaround is to use the kubeadm configuration file.kube-proxy scheduled before node is initialized by cloud-controller-managerIn cloud provider scenarios, kube-proxy can end up being scheduled on new worker nodes before the cloud-controller-manager has initialized the node addresses. This causes kube-proxy to fail to pick up the node's IP address properly and has knock-on effects to the proxy function managing load balancers.The following error can be seen in kube-proxy Pods:server.go:610] Failed to retrieve node IP: host IP unknown; known addresses: [] proxier.go:340] invalid nodeIP, initializing kube-proxy with 127.0.0.1 as nodeIP A known solution is to patch the kube-proxy DaemonSet to allow scheduling it on control-plane nodes regardless of their conditions, keeping it off of other nodes until their initial guarding conditions abate:kubectl -n kube-system patch ds kube-proxy -p='{ "spec": { "template": { "spec": { "tolerations": [ { "key": "CriticalAddonsOnly", "operator": "Exists" }, { "effect": "NoSchedule", "key": "node-role.kubernetes.io/control-plane" } ] } } } }' The tracking issue for this problem is here./usr is mounted read-only on nodesOn Linux distributions such as Fedora CoreOS or Flatcar Container Linux, the directory /usr is mounted as a read-only filesystem. For flex-volume support, Kubernetes components like the kubelet and kube-controller-manager use the default path of /usr/libexec/kubernetes/kubelet-plugins/volume/exec/, yet the flex-volume directory must be writeable for the feature to work.Note:FlexVolume was deprecated in the Kubernetes v1.23 release.To workaround this issue, you can configure the flex-volume directory using the kubeadm configuration file.On the primary control-plane Node (created using kubeadm init), pass the following file using --config:apiVersion: kubeadm.k8s.io/v1beta4 kind: InitConfiguration nodeRegistration: kubeletExtraArgs: \- name: "volume-plugin-dir" value: "/opt/libexec/kubernetes/kubelet-plugins/volume/exec/" \--- apiVersion: kubeadm.k8s.io/v1beta4 kind: ClusterConfiguration controllerManager: extraArgs: \- name: "flex-volume-plugin-dir" value: "/opt/libexec/kubernetes/kubelet-plugins/volume/exec/" On joining Nodes:apiVersion: kubeadm.k8s.io/v1beta4 kind: JoinConfiguration nodeRegistration: kubeletExtraArgs: \- name: "volume-plugin-dir" value: "/opt/libexec/kubernetes/kubelet-plugins/volume/exec/" Alternatively, you can modify /etc/fstab to make the /usr mount writeable, but please be advised that this is modifying a design principle of the Linux distribution.kubeadm upgrade plan prints out context deadline exceeded error messageThis error message is shown when upgrading a Kubernetes cluster with kubeadm in the case of running an external etcd. This is not a critical bug and happens because older versions of kubeadm perform a version check on the external etcd cluster. You can proceed with kubeadm upgrade apply ....This issue is fixed as of version 1.19.kubeadm reset unmounts /var/lib/kubeletIf /var/lib/kubelet is being mounted, performing a kubeadm reset will effectively unmount it.To workaround the issue, re-mount the /var/lib/kubelet directory after performing the kubeadm reset operation.This is a regression introduced in kubeadm 1.15. The issue is fixed in 1.20.Cannot use the metrics-server securely in a kubeadm clusterIn a kubeadm cluster, the metrics-server can be used insecurely by passing the --kubelet-insecure-tls to it. This is not recommended for production clusters.If you want to use TLS between the metrics-server and the kubelet there is a problem, since kubeadm deploys a self-signed serving certificate for the kubelet. This can cause the following errors on the side of the metrics-server:x509: certificate signed by unknown authority x509: certificate is valid for IP-foo not IP-bar See Enabling signed kubelet serving certificates to understand how to configure the kubelets in a kubeadm cluster to have properly signed serving certificates.Also see How to run the metrics-server securely.Upgrade fails due to etcd hash not changingOnly applicable to upgrading a control plane node with a kubeadm binary v1.28.3 or later, where the node is currently managed by kubeadm versions v1.28.0, v1.28.1 or v1.28.2.Here is the error message you may encounter:[upgrade/etcd] Failed to upgrade etcd: couldn't upgrade control plane. kubeadm has tried to recover everything into the earlier state. Errors faced: static Pod hash for component etcd on Node kinder-upgrade-control-plane-1 did not change after 5m0s: timed out waiting for the condition [upgrade/etcd] Waiting for previous etcd to become available I0907 10:10:09.109104 3704 etcd.go:588] [etcd] attempting to see if all cluster endpoints ([https://172.17.0.6:2379/ https://172.17.0.4:2379/ https://172.17.0.3:2379/]) are available 1/10 [upgrade/etcd] Etcd was rolled back and is now available static Pod hash for component etcd on Node kinder-upgrade-control-plane-1 did not change after 5m0s: timed out waiting for the condition couldn't upgrade control plane. kubeadm has tried to recover everything into the earlier state. Errors faced k8s.io/kubernetes/cmd/kubeadm/app/phases/upgrade.rollbackOldManifests cmd/kubeadm/app/phases/upgrade/staticpods.go:525 k8s.io/kubernetes/cmd/kubeadm/app/phases/upgrade.upgradeComponent cmd/kubeadm/app/phases/upgrade/staticpods.go:254 k8s.io/kubernetes/cmd/kubeadm/app/phases/upgrade.performEtcdStaticPodUpgrade cmd/kubeadm/app/phases/upgrade/staticpods.go:338 ... The reason for this failure is that the affected versions generate an etcd manifest file with unwanted defaults in the PodSpec. This will result in a diff from the manifest comparison, and kubeadm will expect a change in the Pod hash, but the kubelet will never update the hash.There are two way to workaround this issue if you see it in your cluster:The etcd upgrade can be skipped between the affected versions and v1.28.3 (or later) by using:kubeadm upgrade {apply|node} [version] --etcd-upgrade=false This is not recommended in case a new etcd version was introduced by a later v1.28 patch version.Before upgrade, patch the manifest for the etcd static pod, to remove the problematic defaulted attributes:diff --git a/etc/kubernetes/manifests/etcd_defaults.yaml b/etc/kubernetes/manifests/etcd_origin.yaml index d807ccbe0aa..46b35f00e15 100644 \--- a/etc/kubernetes/manifests/etcd_defaults.yaml +++ b/etc/kubernetes/manifests/etcd_origin.yaml @@ -43,7 +43,6 @@ spec: scheme: HTTP initialDelaySeconds: 10 periodSeconds: 10 \- successThreshold: 1 timeoutSeconds: 15 name: etcd resources: @@ -59,26 +58,18 @@ spec: scheme: HTTP initialDelaySeconds: 10 periodSeconds: 10 \- successThreshold: 1 timeoutSeconds: 15 \- terminationMessagePath: /dev/termination-log \- terminationMessagePolicy: File volumeMounts: \- mountPath: /var/lib/etcd name: etcd-data \- mountPath: /etc/kubernetes/pki/etcd name: etcd-certs \- dnsPolicy: ClusterFirst \- enableServiceLinks: true hostNetwork: true priority: 2000001000 priorityClassName: system-node-critical \- restartPolicy: Always \- schedulerName: default-scheduler securityContext: seccompProfile: type: RuntimeDefault \- terminationGracePeriodSeconds: 30 volumes: \- hostPath: path: /etc/kubernetes/pki/etcd More information can be found in the tracking issue for this bug.2.2.2.1.3 - Creating a cluster with kubeadm Using kubeadm, you can create a minimum viable Kubernetes cluster that conforms to best practices. In fact, you can use kubeadm to set up a cluster that will pass the Kubernetes Conformance tests. kubeadm also supports other cluster lifecycle functions, such as bootstrap tokens and cluster upgrades.The kubeadm tool is good if you need:A simple way for you to try out Kubernetes, possibly for the first time.A way for existing users to automate setting up a cluster and test their application.A building block in other ecosystem and/or installer tools with a larger scope.You can install and use kubeadm on various machines: your laptop, a set of cloud servers, a Raspberry Pi, and more. Whether you're deploying into the cloud or on-premises, you can integrate kubeadm into provisioning systems such as Ansible or Terraform.Before you beginTo follow this guide, you need:One or more machines running a deb/rpm-compatible Linux OS; for example: Ubuntu or CentOS.2 GiB or more of RAM per machine--any less leaves little room for your apps.At least 2 CPUs on the machine that you use as a control-plane node.Full network connectivity among all machines in the cluster. You can use either a public or a private network.You also need to use a version of kubeadm that can deploy the version of Kubernetes that you want to use in your new cluster.Kubernetes' version and version skew support policy applies to kubeadm as well as to Kubernetes overall. Check that policy to learn about what versions of Kubernetes and kubeadm are supported. This page is written for Kubernetes v1.33.The kubeadm tool's overall feature state is General Availability (GA). Some sub-features are still under active development. The implementation of creating the cluster may change slightly as the tool evolves, but the overall implementation should be pretty stable.Note:Any commands under kubeadm alpha are, by definition, supported on an alpha level.ObjectivesInstall a single control-plane Kubernetes clusterInstall a Pod network on the cluster so that your Pods can talk to each otherInstructionsPreparing the hostsComponent installationInstall a container runtime and kubeadm on all the hosts. For detailed instructions and other prerequisites, see Installing kubeadm.Note:If you have already installed kubeadm, see the first two steps of the Upgrading Linux nodes document for instructions on how to upgrade kubeadm.When you upgrade, the kubelet restarts every few seconds as it waits in a crashloop for kubeadm to tell it what to do. This crashloop is expected and normal. After you initialize your control-plane, the kubelet runs normally.Network setupkubeadm similarly to other Kubernetes components tries to find a usable IP on the network interfaces associated with a default gateway on a host. Such an IP is then used for the advertising and/or listening performed by a component.To find out what this IP is on a Linux host you can use:ip route show # Look for a line starting with "default via" Note:If two or more default gateways are present on the host, a Kubernetes component will try to use the first one it encounters that has a suitable global unicast IP address. While making this choice, the exact ordering of gateways might vary between different operating systems and kernel versions.Kubernetes components do not accept custom network interface as an option, therefore a custom IP address must be passed as a flag to all components instances that need such a custom configuration.Note:If the host does not have a default gateway and if a custom IP address is not passed to a Kubernetes component, the component may exit with an error.To configure the API server advertise address for control plane nodes created with both init and join, the flag --apiserver-advertise-address can be used. Preferably, this option can be set in the kubeadm API as InitConfiguration.localAPIEndpoint and JoinConfiguration.controlPlane.localAPIEndpoint.For kubelets on all nodes, the --node-ip option can be passed in .nodeRegistration.kubeletExtraArgs inside a kubeadm configuration file (InitConfiguration or JoinConfiguration).For dual-stack see Dual-stack support with kubeadm.The IP addresses that you assign to control plane components become part of their X.509 certificates' subject alternative name fields. Changing these IP addresses would require signing new certificates and restarting the affected components, so that the change in certificate files is reflected. See Manual certificate renewal for more details on this topic.Warning:The Kubernetes project recommends against this approach (configuring all component instances with custom IP addresses). Instead, the Kubernetes maintainers recommend to setup the host network, so that the default gateway IP is the one that Kubernetes components auto-detect and use. On Linux nodes, you can use commands such as ip route to configure networking; your operating system might also provide higher level network management tools. If your node's default gateway is a public IP address, you should configure packet filtering or other security measures that protect the nodes and your cluster.Preparing the required container imagesThis step is optional and only applies in case you wish kubeadm init and kubeadm join to not download the default container images which are hosted at registry.k8s.io.Kubeadm has commands that can help you pre-pull the required images when creating a cluster without an internet connection on its nodes. See Running kubeadm without an internet connection for more details.Kubeadm allows you to use a custom image repository for the required images. See Using custom images for more details.Initializing your control-plane nodeThe control-plane node is the machine where the control plane components run, including etcd (the cluster database) and the API Server (which the kubectl command line tool communicates with).(Recommended) If you have plans to upgrade this single control-plane kubeadm cluster to high availability you should specify the --control-plane-endpoint to set the shared endpoint for all control-plane nodes. Such an endpoint can be either a DNS name or an IP address of a load-balancer.Choose a Pod network add-on, and verify whether it requires any arguments to be passed to kubeadm init. Depending on which third-party provider you choose, you might need to set the --pod-network-cidr to a provider-specific value. See Installing a Pod network add-on.(Optional) kubeadm tries to detect the container runtime by using a list of well known endpoints. To use different container runtime or if there are more than one installed on the provisioned node, specify the --cri-socket argument to kubeadm. See Installing a runtime.To initialize the control-plane node run:kubeadm init  Considerations about apiserver-advertise-address and ControlPlaneEndpointWhile --apiserver-advertise-address can be used to set the advertised address for this particular control-plane node's API server, --control-plane-endpoint can be used to set the shared endpoint for all control-plane nodes.--control-plane-endpoint allows both IP addresses and DNS names that can map to IP addresses. Please contact your network administrator to evaluate possible solutions with respect to such mapping.Here is an example mapping:192.168.0.102 cluster-endpoint Where 192.168.0.102 is the IP address of this node and cluster-endpoint is a custom DNS name that maps to this IP. This will allow you to pass --control-plane-endpoint=cluster-endpoint to kubeadm init and pass the same DNS name to kubeadm join. Later you can modify cluster-endpoint to point to the address of your load-balancer in a high availability scenario.Turning a single control plane cluster created without --control-plane-endpoint into a highly available cluster is not supported by kubeadm.More informationFor more information about kubeadm init arguments, see the kubeadm reference guide.To configure kubeadm init with a configuration file see Using kubeadm init with a configuration file.To customize control plane components, including optional IPv6 assignment to liveness probe for control plane components and etcd server, provide extra arguments to each component as documented in custom arguments.To reconfigure a cluster that has already been created see Reconfiguring a kubeadm cluster.To run kubeadm init again, you must first tear down the cluster.If you join a node with a different architecture to your cluster, make sure that your deployed DaemonSets have container image support for this architecture.kubeadm init first runs a series of prechecks to ensure that the machine is ready to run Kubernetes. These prechecks expose warnings and exit on errors. kubeadm init then downloads and installs the cluster control plane components. This may take several minutes. After it finishes you should see:Your Kubernetes control-plane has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config You should now deploy a Pod network to the cluster. Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at: /docs/concepts/cluster-administration/addons/ You can now join any number of machines by running the following on each node as root: kubeadm join : \--token  \--discovery-token-ca-cert-hash sha256: To make kubectl work for your non-root user, run these commands, which are also part of the kubeadm init output:mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config Alternatively, if you are the root user, you can run:export KUBECONFIG=/etc/kubernetes/admin.conf Warning:The kubeconfig file admin.conf that kubeadm init generates contains a certificate with Subject: O = kubeadm:cluster-admins, CN = kubernetes-admin. The group kubeadm:cluster-admins is bound to the built-in cluster-admin ClusterRole. Do not share the admin.conf file with anyone.kubeadm init generates another kubeconfig file super-admin.conf that contains a certificate with Subject: O = system:masters, CN = kubernetes-super-admin. system:masters is a break-glass, super user group that bypasses the authorization layer (for example RBAC). Do not share the super-admin.conf file with anyone. It is recommended to move the file to a safe location.See Generating kubeconfig files for additional users on how to use kubeadm kubeconfig user to generate kubeconfig files for additional users.Make a record of the kubeadm join command that kubeadm init outputs. You need this command to join nodes to your cluster.The token is used for mutual authentication between the control-plane node and the joining nodes. The token included here is secret. Keep it safe, because anyone with this token can add authenticated nodes to your cluster. These tokens can be listed, created, and deleted with the kubeadm token command. See the kubeadm reference guide.Installing a Pod network add-onCaution:This section contains important information about networking setup and deployment order. Read all of this advice carefully before proceeding.You must deploy a Container Network Interface (CNI) based Pod network add-on so that your Pods can communicate with each other. Cluster DNS (CoreDNS) will not start up before a network is installed.Take care that your Pod network must not overlap with any of the host networks: you are likely to see problems if there is any overlap. (If you find a collision between your network plugin's preferred Pod network and some of your host networks, you should think of a suitable CIDR block to use instead, then use that during kubeadm init with \--pod-network-cidr and as a replacement in your network plugin's YAML).By default, kubeadm sets up your cluster to use and enforce use of RBAC (role based access control). Make sure that your Pod network plugin supports RBAC, and so do any manifests that you use to deploy it.If you want to use IPv6--either dual-stack, or single-stack IPv6 only networking--for your cluster, make sure that your Pod network plugin supports IPv6. IPv6 support was added to CNI in v0.6.0.Note:Kubeadm should be CNI agnostic and the validation of CNI providers is out of the scope of our current e2e testing. If you find an issue related to a CNI plugin you should log a ticket in its respective issue tracker instead of the kubeadm or kubernetes issue trackers.Several external projects provide Kubernetes Pod networks using CNI, some of which also support Network Policy.See a list of add-ons that implement the Kubernetes networking model.Please refer to the Installing Addons page for a non-exhaustive list of networking addons supported by Kubernetes. You can install a Pod network add-on with the following command on the control-plane node or a node that has the kubeconfig credentials:kubectl apply -f  Note:Only a few CNI plugins support Windows. More details and setup instructions can be found in Adding Windows worker nodes.You can install only one Pod network per cluster.Once a Pod network has been installed, you can confirm that it is working by checking that the CoreDNS Pod is Running in the output of kubectl get pods --all-namespaces. And once the CoreDNS Pod is up and running, you can continue by joining your nodes.If your network is not working or CoreDNS is not in the Running state, check out the troubleshooting guide for kubeadm.Managed node labelsBy default, kubeadm enables the NodeRestriction admission controller that restricts what labels can be self-applied by kubelets on node registration. The admission controller documentation covers what labels are permitted to be used with the kubelet --node-labels option. The node-role.kubernetes.io/control-plane label is such a restricted label and kubeadm manually applies it using a privileged client after a node has been created. To do that manually you can do the same by using kubectl label and ensure it is using a privileged kubeconfig such as the kubeadm managed /etc/kubernetes/admin.conf.Control plane node isolationBy default, your cluster will not schedule Pods on the control plane nodes for security reasons. If you want to be able to schedule Pods on the control plane nodes, for example for a single machine Kubernetes cluster, run:kubectl taint nodes --all node-role.kubernetes.io/control-plane- The output will look something like:node "test-01" untainted ... This will remove the node-role.kubernetes.io/control-plane:NoSchedule taint from any nodes that have it, including the control plane nodes, meaning that the scheduler will then be able to schedule Pods everywhere.Additionally, you can execute the following command to remove the node.kubernetes.io/exclude-from-external-load-balancers label from the control plane node, which excludes it from the list of backend servers:kubectl label nodes --all node.kubernetes.io/exclude-from-external-load-balancers- Adding more control plane nodesSee Creating Highly Available Clusters with kubeadm for steps on creating a high availability kubeadm cluster by adding more control plane nodes.Adding worker nodesThe worker nodes are where your workloads run.The following pages show how to add Linux and Windows worker nodes to the cluster by using the kubeadm join command:Adding Linux worker nodesAdding Windows worker nodes(Optional) Controlling your cluster from machines other than the control-plane nodeIn order to get a kubectl on some other computer (e.g. laptop) to talk to your cluster, you need to copy the administrator kubeconfig file from your control-plane node to your workstation like this:scp root@:/etc/kubernetes/admin.conf . kubectl --kubeconfig ./admin.conf get nodes Note:The example above assumes SSH access is enabled for root. If that is not the case, you can copy the admin.conf file to be accessible by some other user and scp using that other user instead.The admin.conf file gives the user superuser privileges over the cluster. This file should be used sparingly. For normal users, it's recommended to generate an unique credential to which you grant privileges. You can do this with the kubeadm kubeconfig user --client-name  command. That command will print out a KubeConfig file to STDOUT which you should save to a file and distribute to your user. After that, grant privileges by using kubectl create (cluster)rolebinding.(Optional) Proxying API Server to localhostIf you want to connect to the API Server from outside the cluster, you can use kubectl proxy:scp root@:/etc/kubernetes/admin.conf . kubectl --kubeconfig ./admin.conf proxy You can now access the API Server locally at http://localhost:8001/api/v1Clean upIf you used disposable servers for your cluster, for testing, you can switch those off and do no further clean up. You can use kubectl config delete-cluster to delete your local references to the cluster.However, if you want to deprovision your cluster more cleanly, you should first drain the node and make sure that the node is empty, then deconfigure the node.Remove the nodeTalking to the control-plane node with the appropriate credentials, run:kubectl drain  \--delete-emptydir-data --force --ignore-daemonsets Before removing the node, reset the state installed by kubeadm:kubeadm reset The reset process does not reset or clean up iptables rules or IPVS tables. If you wish to reset iptables, you must do so manually:iptables -F && iptables -t nat -F && iptables -t mangle -F && iptables -X If you want to reset the IPVS tables, you must run the following command:ipvsadm -C Now remove the node:kubectl delete node  If you wish to start over, run kubeadm init or kubeadm join with the appropriate arguments.Clean up the control planeYou can use kubeadm reset on the control plane host to trigger a best-effort clean up.See the kubeadm reset reference documentation for more information about this subcommand and its options.Version skew policyWhile kubeadm allows version skew against some components that it manages, it is recommended that you match the kubeadm version with the versions of the control plane components, kube-proxy and kubelet.kubeadm's skew against the Kubernetes versionkubeadm can be used with Kubernetes components that are the same version as kubeadm or one version older. The Kubernetes version can be specified to kubeadm by using the \--kubernetes-version flag of kubeadm init or the ClusterConfiguration.kubernetesVersion field when using --config. This option will control the versions of kube-apiserver, kube-controller-manager, kube-scheduler and kube-proxy.Example:kubeadm is at 1.33kubernetesVersion must be at 1.33 or 1.32kubeadm's skew against the kubeletSimilarly to the Kubernetes version, kubeadm can be used with a kubelet version that is the same version as kubeadm or three versions older.Example:kubeadm is at 1.33kubelet on the host must be at 1.33, 1.32, 1.31 or 1.30kubeadm's skew against kubeadmThere are certain limitations on how kubeadm commands can operate on existing nodes or whole clusters managed by kubeadm.If new nodes are joined to the cluster, the kubeadm binary used for kubeadm join must match the last version of kubeadm used to either create the cluster with kubeadm init or to upgrade the same node with kubeadm upgrade. Similar rules apply to the rest of the kubeadm commands with the exception of kubeadm upgrade.Example for kubeadm join:kubeadm version 1.33 was used to create a cluster with kubeadm initJoining nodes must use a kubeadm binary that is at version 1.33Nodes that are being upgraded must use a version of kubeadm that is the same MINOR version or one MINOR version newer than the version of kubeadm used for managing the node.Example for kubeadm upgrade:kubeadm version 1.32 was used to create or upgrade the nodeThe version of kubeadm used for upgrading the node must be at 1.32 or 1.33To learn more about the version skew between the different Kubernetes component see the Version Skew Policy.LimitationsCluster resilienceThe cluster created here has a single control-plane node, with a single etcd database running on it. This means that if the control-plane node fails, your cluster may lose data and may need to be recreated from scratch.Workarounds:Regularly back up etcd. The etcd data directory configured by kubeadm is at /var/lib/etcd on the control-plane node.Use multiple control-plane nodes. You can read Options for Highly Available topology to pick a cluster topology that provides high-availability.Platform compatibilitykubeadm deb/rpm packages and binaries are built for amd64, arm (32-bit), arm64, ppc64le, and s390x following the multi-platform proposal.Multiplatform container images for the control plane and addons are also supported since v1.12.Only some of the network providers offer solutions for all platforms. Please consult the list of network providers above or the documentation from each provider to figure out whether the provider supports your chosen platform.TroubleshootingIf you are running into difficulties with kubeadm, please consult our troubleshooting docs.What's nextVerify that your cluster is running properly with SonobuoySee Upgrading kubeadm clusters for details about upgrading your cluster using kubeadm.Learn about advanced kubeadm usage in the kubeadm reference documentationLearn more about Kubernetes concepts and kubectl.See the Cluster Networking page for a bigger list of Pod network add-ons.See the list of add-ons to explore other add-ons, including tools for logging, monitoring, network policy, visualization & control of your Kubernetes cluster.Configure how your cluster handles logs for cluster events and from applications running in Pods. See Logging Architecture for an overview of what is involved.FeedbackFor bugs, visit the kubeadm GitHub issue trackerFor support, visit the #kubeadm Slack channelGeneral SIG Cluster Lifecycle development Slack channel: #sig-cluster-lifecycleSIG Cluster Lifecycle SIG informationSIG Cluster Lifecycle mailing list: kubernetes-sig-cluster-lifecycle2.2.2.1.4 - Customizing components with the kubeadm APIThis page covers how to customize the components that kubeadm deploys. For control plane components you can use flags in the ClusterConfiguration structure or patches per-node. For the kubelet and kube-proxy you can use KubeletConfiguration and KubeProxyConfiguration, accordingly.All of these options are possible via the kubeadm configuration API. For more details on each field in the configuration you can navigate to our API reference pages.Note:Customizing the CoreDNS deployment of kubeadm is currently not supported. You must manually patch the kube-system/coredns ConfigMap and recreate the CoreDNS Pods after that. Alternatively, you can skip the default CoreDNS deployment and deploy your own variant. For more details on that see Using init phases with kubeadm.Note:To reconfigure a cluster that has already been created see Reconfiguring a kubeadm cluster.Customizing the control plane with flags in ClusterConfigurationThe kubeadm ClusterConfiguration object exposes a way for users to override the default flags passed to control plane components such as the APIServer, ControllerManager, Scheduler and Etcd. The components are defined using the following structures:apiServercontrollerManagerscheduleretcdThese structures contain a common extraArgs field, that consists of name / value pairs. To override a flag for a control plane component:Add the appropriate extraArgs to your configuration.Add flags to the extraArgs field.Run kubeadm init with --config .Note:You can generate a ClusterConfiguration object with default values by running kubeadm config print init-defaults and saving the output to a file of your choice.Note:The ClusterConfiguration object is currently global in kubeadm clusters. This means that any flags that you add, will apply to all instances of the same component on different nodes. To apply individual configuration per component on different nodes you can use patches.Note:Duplicate flags (keys), or passing the same flag --foo multiple times, is currently not supported. To workaround that you must use patches.APIServer flagsFor details, see the reference documentation for kube-apiserver.Example usage:apiVersion: kubeadm.k8s.io/v1beta4 kind: ClusterConfiguration kubernetesVersion: v1.16.0 apiServer: extraArgs: \- name: "enable-admission-plugins" value: "AlwaysPullImages,DefaultStorageClass" \- name: "audit-log-path" value: "/home/johndoe/audit.log" ControllerManager flagsFor details, see the reference documentation for kube-controller-manager.Example usage:apiVersion: kubeadm.k8s.io/v1beta4 kind: ClusterConfiguration kubernetesVersion: v1.16.0 controllerManager: extraArgs: \- name: "cluster-signing-key-file" value: "/home/johndoe/keys/ca.key" \- name: "deployment-controller-sync-period" value: "50" Scheduler flagsFor details, see the reference documentation for kube-scheduler.Example usage:apiVersion: kubeadm.k8s.io/v1beta4 kind: ClusterConfiguration kubernetesVersion: v1.16.0 scheduler: extraArgs: \- name: "config" value: "/etc/kubernetes/scheduler-config.yaml" extraVolumes: \- name: schedulerconfig hostPath: /home/johndoe/schedconfig.yaml mountPath: /etc/kubernetes/scheduler-config.yaml readOnly: true pathType: "File" Etcd flagsFor details, see the etcd server documentation.Example usage:apiVersion: kubeadm.k8s.io/v1beta4 kind: ClusterConfiguration etcd: local: extraArgs: \- name: "election-timeout" value: 1000 Customizing with patchesFEATURE STATE: Kubernetes v1.22 [beta]Kubeadm allows you to pass a directory with patch files to InitConfiguration and JoinConfiguration on individual nodes. These patches can be used as the last customization step before component configuration is written to disk.You can pass this file to kubeadm init with --config :apiVersion: kubeadm.k8s.io/v1beta4 kind: InitConfiguration patches: directory: /home/user/somedir Note:For kubeadm init you can pass a file containing both a ClusterConfiguration and InitConfiguration separated by ---.You can pass this file to kubeadm join with --config :apiVersion: kubeadm.k8s.io/v1beta4 kind: JoinConfiguration patches: directory: /home/user/somedir The directory must contain files named target[suffix][+patchtype].extension. For example, kube-apiserver0+merge.yaml or just etcd.json.target can be one of kube-apiserver, kube-controller-manager, kube-scheduler, etcd and kubeletconfiguration.suffix is an optional string that can be used to determine which patches are applied first alpha-numerically.patchtype can be one of strategic, merge or json and these must match the patching formats supported by kubectl. The default patchtype is strategic.extension must be either json or yaml.Note:If you are using kubeadm upgrade to upgrade your kubeadm nodes you must again provide the same patches, so that the customization is preserved after upgrade. To do that you can use the --patches flag, which must point to the same directory. kubeadm upgrade currently does not support a configuration API structure that can be used for the same purpose.Customizing the kubeletTo customize the kubelet you can add a KubeletConfiguration next to the ClusterConfiguration or InitConfiguration separated by --- within the same configuration file. This file can then be passed to kubeadm init and kubeadm will apply the same base KubeletConfiguration to all nodes in the cluster.For applying instance-specific configuration over the base KubeletConfiguration you can use the kubeletconfiguration patch target.Alternatively, you can use kubelet flags as overrides by passing them in the nodeRegistration.kubeletExtraArgs field supported by both InitConfiguration and JoinConfiguration. Some kubelet flags are deprecated, so check their status in the kubelet reference documentation before using them.For additional details see Configuring each kubelet in your cluster using kubeadmCustomizing kube-proxyTo customize kube-proxy you can pass a KubeProxyConfiguration next your ClusterConfiguration or InitConfiguration to kubeadm init separated by ---.For more details you can navigate to our API reference pages.Note:kubeadm deploys kube-proxy as a DaemonSet, which means that the KubeProxyConfiguration would apply to all instances of kube-proxy in the cluster.2.2.2.1.5 - Options for Highly Available TopologyThis page explains the two options for configuring the topology of your highly available (HA) Kubernetes clusters.You can set up an HA cluster:With stacked control plane nodes, where etcd nodes are colocated with control plane nodesWith external etcd nodes, where etcd runs on separate nodes from the control planeYou should carefully consider the advantages and disadvantages of each topology before setting up an HA cluster.Note:kubeadm bootstraps the etcd cluster statically. Read the etcd Clustering Guide for more details.Stacked etcd topologyA stacked HA cluster is a topology where the distributed data storage cluster provided by etcd is stacked on top of the cluster formed by the nodes managed by kubeadm that run control plane components.Each control plane node runs an instance of the kube-apiserver, kube-scheduler, and kube-controller-manager. The kube-apiserver is exposed to worker nodes using a load balancer.Each control plane node creates a local etcd member and this etcd member communicates only with the kube-apiserver of this node. The same applies to the local kube-controller-manager and kube-scheduler instances.This topology couples the control planes and etcd members on the same nodes. It is simpler to set up than a cluster with external etcd nodes, and simpler to manage for replication.However, a stacked cluster runs the risk of failed coupling. If one node goes down, both an etcd member and a control plane instance are lost, and redundancy is compromised. You can mitigate this risk by adding more control plane nodes.You should therefore run a minimum of three stacked control plane nodes for an HA cluster.This is the default topology in kubeadm. A local etcd member is created automatically on control plane nodes when using kubeadm init and kubeadm join --control-plane.External etcd topologyAn HA cluster with external etcd is a topology where the distributed data storage cluster provided by etcd is external to the cluster formed by the nodes that run control plane components.Like the stacked etcd topology, each control plane node in an external etcd topology runs an instance of the kube-apiserver, kube-scheduler, and kube-controller-manager. And the kube-apiserver is exposed to worker nodes using a load balancer. However, etcd members run on separate hosts, and each etcd host communicates with the kube-apiserver of each control plane node.This topology decouples the control plane and etcd member. It therefore provides an HA setup where losing a control plane instance or an etcd member has less impact and does not affect the cluster redundancy as much as the stacked HA topology.However, this topology requires twice the number of hosts as the stacked HA topology. A minimum of three hosts for control plane nodes and three hosts for etcd nodes are required for an HA cluster with this topology.What's nextSet up a highly available cluster with kubeadm2.2.2.1.6 - Creating Highly Available Clusters with kubeadmThis page explains two different approaches to setting up a highly available Kubernetes cluster using kubeadm:With stacked control plane nodes. This approach requires less infrastructure. The etcd members and control plane nodes are co-located.With an external etcd cluster. This approach requires more infrastructure. The control plane nodes and etcd members are separated.Before proceeding, you should carefully consider which approach best meets the needs of your applications and environment. Options for Highly Available topology outlines the advantages and disadvantages of each.If you encounter issues with setting up the HA cluster, please report these in the kubeadm issue tracker.See also the upgrade documentation.Caution:This page does not address running your cluster on a cloud provider. In a cloud environment, neither approach documented here works with Service objects of type LoadBalancer, or with dynamic PersistentVolumes.Before you beginThe prerequisites depend on which topology you have selected for your cluster's control plane:Stacked etcdExternal etcdYou need:Three or more machines that meet kubeadm's minimum requirements for the control-plane nodes. Having an odd number of control plane nodes can help with leader selection in the case of machine or zone failure.including a container runtime, already set up and workingThree or more machines that meet kubeadm's minimum requirements for the workersincluding a container runtime, already set up and workingFull network connectivity between all machines in the cluster (public or private network)Superuser privileges on all machines using sudoYou can use a different tool; this guide uses sudo in the examples.SSH access from one device to all nodes in the systemkubeadm and kubelet already installed on all machines.See Stacked etcd topology for context.You need:Three or more machines that meet kubeadm's minimum requirements for the control-plane nodes. Having an odd number of control plane nodes can help with leader selection in the case of machine or zone failure.including a container runtime, already set up and workingThree or more machines that meet kubeadm's minimum requirements for the workersincluding a container runtime, already set up and workingFull network connectivity between all machines in the cluster (public or private network)Superuser privileges on all machines using sudoYou can use a different tool; this guide uses sudo in the examples.SSH access from one device to all nodes in the systemkubeadm and kubelet already installed on all machines.And you also need:Three or more additional machines, that will become etcd cluster members. Having an odd number of members in the etcd cluster is a requirement for achieving optimal voting quorum.These machines again need to have kubeadm and kubelet installed.These machines also require a container runtime, that is already set up and working.See External etcd topology for context.Container imagesEach host should have access read and fetch images from the Kubernetes container image registry, registry.k8s.io. If you want to deploy a highly-available cluster where the hosts do not have access to pull images, this is possible. You must ensure by some other means that the correct container images are already available on the relevant hosts.Command line interfaceTo manage Kubernetes once your cluster is set up, you should install kubectl on your PC. It is also useful to install the kubectl tool on each control plane node, as this can be helpful for troubleshooting.First steps for both methodsCreate load balancer for kube-apiserverNote:There are many configurations for load balancers. The following example is only one option. Your cluster requirements may need a different configuration.Create a kube-apiserver load balancer with a name that resolves to DNS.In a cloud environment you should place your control plane nodes behind a TCP forwarding load balancer. This load balancer distributes traffic to all healthy control plane nodes in its target list. The health check for an apiserver is a TCP check on the port the kube-apiserver listens on (default value :6443).It is not recommended to use an IP address directly in a cloud environment.The load balancer must be able to communicate with all control plane nodes on the apiserver port. It must also allow incoming traffic on its listening port.Make sure the address of the load balancer always matches the address of kubeadm's ControlPlaneEndpoint.Read the Options for Software Load Balancing guide for more details.Add the first control plane node to the load balancer, and test the connection:nc -zv -w 2  A connection refused error is expected because the API server is not yet running. A timeout, however, means the load balancer cannot communicate with the control plane node. If a timeout occurs, reconfigure the load balancer to communicate with the control plane node.Add the remaining control plane nodes to the load balancer target group.Stacked control plane and etcd nodesSteps for the first control plane nodeInitialize the control plane:sudo kubeadm init --control-plane-endpoint "LOAD_BALANCER_DNS:LOAD_BALANCER_PORT" --upload-certs You can use the --kubernetes-version flag to set the Kubernetes version to use. It is recommended that the versions of kubeadm, kubelet, kubectl and Kubernetes match.The --control-plane-endpoint flag should be set to the address or DNS and port of the load balancer.The --upload-certs flag is used to upload the certificates that should be shared across all the control-plane instances to the cluster. If instead, you prefer to copy certs across control-plane nodes manually or using automation tools, please remove this flag and refer to Manual certificate distribution section below.Note:The kubeadm init flags --config and --certificate-key cannot be mixed, therefore if you want to use the kubeadm configuration you must add the certificateKey field in the appropriate config locations (under InitConfiguration and JoinConfiguration: controlPlane).Note:Some CNI network plugins require additional configuration, for example specifying the pod IP CIDR, while others do not. See the CNI network documentation. To add a pod CIDR pass the flag --pod-network-cidr, or if you are using a kubeadm configuration file set the podSubnet field under the networking object of ClusterConfiguration.The output looks similar to:... You can now join any number of control-plane node by running the following command on each as a root: kubeadm join 192.168.0.200:6443 --token 9vr73a.a8uxyaju799qwdjv --discovery-token-ca-cert-hash sha256:7c2e69131a36ae2a042a339b33381c6d0d43887e2de83720eff5359e26aec866 --control-plane --certificate-key f8902e114ef118304e561c3ecd4d0b543adc226b7a07f675f56564185ffe0c07 Please note that the certificate-key gives access to cluster sensitive data, keep it secret! As a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use kubeadm init phase upload-certs to reload certs afterward. Then you can join any number of worker nodes by running the following on each as root: kubeadm join 192.168.0.200:6443 --token 9vr73a.a8uxyaju799qwdjv --discovery-token-ca-cert-hash sha256:7c2e69131a36ae2a042a339b33381c6d0d43887e2de83720eff5359e26aec866 Copy this output to a text file. You will need it later to join control plane and worker nodes to the cluster.When --upload-certs is used with kubeadm init, the certificates of the primary control plane are encrypted and uploaded in the kubeadm-certs Secret.To re-upload the certificates and generate a new decryption key, use the following command on a control plane node that is already joined to the cluster:sudo kubeadm init phase upload-certs --upload-certs You can also specify a custom --certificate-key during init that can later be used by join. To generate such a key you can use the following command:kubeadm certs certificate-key The certificate key is a hex encoded string that is an AES key of size 32 bytes.Note:The kubeadm-certs Secret and the decryption key expire after two hours.Caution:As stated in the command output, the certificate key gives access to cluster sensitive data, keep it secret!Apply the CNI plugin of your choice: Follow these instructions to install the CNI provider. Make sure the configuration corresponds to the Pod CIDR specified in the kubeadm configuration file (if applicable).Note:You must pick a network plugin that suits your use case and deploy it before you move on to next step. If you don't do this, you will not be able to launch your cluster properly.Type the following and watch the pods of the control plane components get started:kubectl get pod -n kube-system -w Steps for the rest of the control plane nodesFor each additional control plane node you should:Execute the join command that was previously given to you by the kubeadm init output on the first node. It should look something like this:sudo kubeadm join 192.168.0.200:6443 --token 9vr73a.a8uxyaju799qwdjv --discovery-token-ca-cert-hash sha256:7c2e69131a36ae2a042a339b33381c6d0d43887e2de83720eff5359e26aec866 --control-plane --certificate-key f8902e114ef118304e561c3ecd4d0b543adc226b7a07f675f56564185ffe0c07 The --control-plane flag tells kubeadm join to create a new control plane.The --certificate-key ... will cause the control plane certificates to be downloaded from the kubeadm-certs Secret in the cluster and be decrypted using the given key.You can join multiple control-plane nodes in parallel.External etcd nodesSetting up a cluster with external etcd nodes is similar to the procedure used for stacked etcd with the exception that you should setup etcd first, and you should pass the etcd information in the kubeadm config file.Set up the etcd clusterFollow these instructions to set up the etcd cluster.Set up SSH as described here.Copy the following files from any etcd node in the cluster to the first control plane node:export CONTROL_PLANE="ubuntu@10.0.0.7" scp /etc/kubernetes/pki/etcd/ca.crt "${CONTROL_PLANE}": scp /etc/kubernetes/pki/apiserver-etcd-client.crt "${CONTROL_PLANE}": scp /etc/kubernetes/pki/apiserver-etcd-client.key "${CONTROL_PLANE}": Replace the value of CONTROL_PLANE with the user@host of the first control-plane node.Set up the first control plane nodeCreate a file called kubeadm-config.yaml with the following contents:--- apiVersion: kubeadm.k8s.io/v1beta4 kind: ClusterConfiguration kubernetesVersion: stable controlPlaneEndpoint: "LOAD_BALANCER_DNS:LOAD_BALANCER_PORT" # change this (see below) etcd: external: endpoints: \- https://ETCD_0_IP:2379 # change ETCD_0_IP appropriately \- https://ETCD_1_IP:2379 # change ETCD_1_IP appropriately \- https://ETCD_2_IP:2379 # change ETCD_2_IP appropriately caFile: /etc/kubernetes/pki/etcd/ca.crt certFile: /etc/kubernetes/pki/apiserver-etcd-client.crt keyFile: /etc/kubernetes/pki/apiserver-etcd-client.key Note:The difference between stacked etcd and external etcd here is that the external etcd setup requires a configuration file with the etcd endpoints under the external object for etcd. In the case of the stacked etcd topology, this is managed automatically.Replace the following variables in the config template with the appropriate values for your cluster:LOAD_BALANCER_DNSLOAD_BALANCER_PORTETCD_0_IPETCD_1_IPETCD_2_IPThe following steps are similar to the stacked etcd setup:Run sudo kubeadm init --config kubeadm-config.yaml --upload-certs on this node.Write the output join commands that are returned to a text file for later use.Apply the CNI plugin of your choice.Note:You must pick a network plugin that suits your use case and deploy it before you move on to next step. If you don't do this, you will not be able to launch your cluster properly.Steps for the rest of the control plane nodesThe steps are the same as for the stacked etcd setup:Make sure the first control plane node is fully initialized.Join each control plane node with the join command you saved to a text file. It's recommended to join the control plane nodes one at a time.Don't forget that the decryption key from --certificate-key expires after two hours, by default.Common tasks after bootstrapping control planeInstall workersWorker nodes can be joined to the cluster with the command you stored previously as the output from the kubeadm init command:sudo kubeadm join 192.168.0.200:6443 --token 9vr73a.a8uxyaju799qwdjv --discovery-token-ca-cert-hash sha256:7c2e69131a36ae2a042a339b33381c6d0d43887e2de83720eff5359e26aec866 Manual certificate distributionIf you choose to not use kubeadm init with the --upload-certs flag this means that you are going to have to manually copy the certificates from the primary control plane node to the joining control plane nodes.There are many ways to do this. The following example uses ssh and scp:SSH is required if you want to control all nodes from a single machine.Enable ssh-agent on your main device that has access to all other nodes in the system:eval $(ssh-agent) Add your SSH identity to the session:ssh-add ~/.ssh/path_to_private_key SSH between nodes to check that the connection is working correctly.When you SSH to any node, add the -A flag. This flag allows the node that you have logged into via SSH to access the SSH agent on your PC. Consider alternative methods if you do not fully trust the security of your user session on the node.ssh -A 10.0.0.7 When using sudo on any node, make sure to preserve the environment so SSH forwarding works:sudo -E -s After configuring SSH on all the nodes you should run the following script on the first control plane node after running kubeadm init. This script will copy the certificates from the first control plane node to the other control plane nodes:In the following example, replace CONTROL_PLANE_IPS with the IP addresses of the other control plane nodes.USER=ubuntu # customizable CONTROL_PLANE_IPS="10.0.0.7 10.0.0.8" for host in ${CONTROL_PLANE_IPS}; do scp /etc/kubernetes/pki/ca.crt "${USER}"@$host: scp /etc/kubernetes/pki/ca.key "${USER}"@$host: scp /etc/kubernetes/pki/sa.key "${USER}"@$host: scp /etc/kubernetes/pki/sa.pub "${USER}"@$host: scp /etc/kubernetes/pki/front-proxy-ca.crt "${USER}"@$host: scp /etc/kubernetes/pki/front-proxy-ca.key "${USER}"@$host: scp /etc/kubernetes/pki/etcd/ca.crt "${USER}"@$host:etcd-ca.crt # Skip the next line if you are using external etcd scp /etc/kubernetes/pki/etcd/ca.key "${USER}"@$host:etcd-ca.key done Caution:Copy only the certificates in the above list. kubeadm will take care of generating the rest of the certificates with the required SANs for the joining control-plane instances. If you copy all the certificates by mistake, the creation of additional nodes could fail due to a lack of required SANs.Then on each joining control plane node you have to run the following script before running kubeadm join. This script will move the previously copied certificates from the home directory to /etc/kubernetes/pki:USER=ubuntu # customizable mkdir -p /etc/kubernetes/pki/etcd mv /home/${USER}/ca.crt /etc/kubernetes/pki/ mv /home/${USER}/ca.key /etc/kubernetes/pki/ mv /home/${USER}/sa.pub /etc/kubernetes/pki/ mv /home/${USER}/sa.key /etc/kubernetes/pki/ mv /home/${USER}/front-proxy-ca.crt /etc/kubernetes/pki/ mv /home/${USER}/front-proxy-ca.key /etc/kubernetes/pki/ mv /home/${USER}/etcd-ca.crt /etc/kubernetes/pki/etcd/ca.crt # Skip the next line if you are using external etcd mv /home/${USER}/etcd-ca.key /etc/kubernetes/pki/etcd/ca.key 2.2.2.1.7 - Set up a High Availability etcd Cluster with kubeadmBy default, kubeadm runs a local etcd instance on each control plane node. It is also possible to treat the etcd cluster as external and provision etcd instances on separate hosts. The differences between the two approaches are covered in the Options for Highly Available topology page.This task walks through the process of creating a high availability external etcd cluster of three members that can be used by kubeadm during cluster creation.Before you beginThree hosts that can talk to each other over TCP ports 2379 and 2380. This document assumes these default ports. However, they are configurable through the kubeadm config file.Each host must have systemd and a bash compatible shell installed.Each host must have a container runtime, kubelet, and kubeadm installed.Each host should have access to the Kubernetes container image registry (registry.k8s.io) or list/pull the required etcd image using kubeadm config images list/pull. This guide will set up etcd instances as static pods managed by a kubelet.Some infrastructure to copy files between hosts. For example ssh and scp can satisfy this requirement.Setting up the clusterThe general approach is to generate all certs on one node and only distribute the necessary files to the other nodes.Note:kubeadm contains all the necessary cryptographic machinery to generate the certificates described below; no other cryptographic tooling is required for this example.Note:The examples below use IPv4 addresses but you can also configure kubeadm, the kubelet and etcd to use IPv6 addresses. Dual-stack is supported by some Kubernetes options, but not by etcd. For more details on Kubernetes dual-stack support see Dual-stack support with kubeadm.Configure the kubelet to be a service manager for etcd.Note:You must do this on every host where etcd should be running.Since etcd was created first, you must override the service priority by creating a new unit file that has higher precedence than the kubeadm-provided kubelet unit file.cat << EOF > /etc/systemd/system/kubelet.service.d/kubelet.conf # Replace "systemd" with the cgroup driver of your container runtime. The default value in the kubelet is "cgroupfs". # Replace the value of "containerRuntimeEndpoint" for a different container runtime if needed. # apiVersion: kubelet.config.k8s.io/v1beta1 kind: KubeletConfiguration authentication: anonymous: enabled: false webhook: enabled: false authorization: mode: AlwaysAllow cgroupDriver: systemd address: 127.0.0.1 containerRuntimeEndpoint: unix:///var/run/containerd/containerd.sock staticPodPath: /etc/kubernetes/manifests EOF cat << EOF > /etc/systemd/system/kubelet.service.d/20-etcd-service-manager.conf [Service] ExecStart= ExecStart=/usr/bin/kubelet --config=/etc/systemd/system/kubelet.service.d/kubelet.conf Restart=always EOF systemctl daemon-reload systemctl restart kubelet Check the kubelet status to ensure it is running.systemctl status kubelet Create configuration files for kubeadm.Generate one kubeadm configuration file for each host that will have an etcd member running on it using the following script.# Update HOST0, HOST1 and HOST2 with the IPs of your hosts export HOST0=10.0.0.6 export HOST1=10.0.0.7 export HOST2=10.0.0.8 # Update NAME0, NAME1 and NAME2 with the hostnames of your hosts export NAME0="infra0" export NAME1="infra1" export NAME2="infra2" # Create temp directories to store files that will end up on other hosts mkdir -p /tmp/${HOST0}/ /tmp/${HOST1}/ /tmp/${HOST2}/ HOSTS=(${HOST0} ${HOST1} ${HOST2}) NAMES=(${NAME0} ${NAME1} ${NAME2}) for i in "${!HOSTS[@]}"; do HOST=${HOSTS[$i]} NAME=${NAMES[$i]} cat << EOF > /tmp/${HOST}/kubeadmcfg.yaml \--- apiVersion: "kubeadm.k8s.io/v1beta4" kind: InitConfiguration nodeRegistration: name: ${NAME} localAPIEndpoint: advertiseAddress: ${HOST} \--- apiVersion: "kubeadm.k8s.io/v1beta4" kind: ClusterConfiguration etcd: local: serverCertSANs: \- "${HOST}" peerCertSANs: \- "${HOST}" extraArgs: \- name: initial-cluster value: ${NAMES[0]}=https://${HOSTS[0]}:2380,${NAMES[1]}=https://${HOSTS[1]}:2380,${NAMES[2]}=https://${HOSTS[2]}:2380 \- name: initial-cluster-state value: new \- name: name value: ${NAME} \- name: listen-peer-urls value: https://${HOST}:2380 \- name: listen-client-urls value: https://${HOST}:2379 \- name: advertise-client-urls value: https://${HOST}:2379 \- name: initial-advertise-peer-urls value: https://${HOST}:2380 EOF done Generate the certificate authority.If you already have a CA then the only action that is copying the CA's crt and key file to /etc/kubernetes/pki/etcd/ca.crt and /etc/kubernetes/pki/etcd/ca.key. After those files have been copied, proceed to the next step, "Create certificates for each member".If you do not already have a CA then run this command on $HOST0 (where you generated the configuration files for kubeadm).kubeadm init phase certs etcd-ca This creates two files:/etc/kubernetes/pki/etcd/ca.crt/etc/kubernetes/pki/etcd/ca.keyCreate certificates for each member.kubeadm init phase certs etcd-server --config=/tmp/${HOST2}/kubeadmcfg.yaml kubeadm init phase certs etcd-peer --config=/tmp/${HOST2}/kubeadmcfg.yaml kubeadm init phase certs etcd-healthcheck-client --config=/tmp/${HOST2}/kubeadmcfg.yaml kubeadm init phase certs apiserver-etcd-client --config=/tmp/${HOST2}/kubeadmcfg.yaml cp -R /etc/kubernetes/pki /tmp/${HOST2}/ # cleanup non-reusable certificates find /etc/kubernetes/pki -not -name ca.crt -not -name ca.key -type f -delete kubeadm init phase certs etcd-server --config=/tmp/${HOST1}/kubeadmcfg.yaml kubeadm init phase certs etcd-peer --config=/tmp/${HOST1}/kubeadmcfg.yaml kubeadm init phase certs etcd-healthcheck-client --config=/tmp/${HOST1}/kubeadmcfg.yaml kubeadm init phase certs apiserver-etcd-client --config=/tmp/${HOST1}/kubeadmcfg.yaml cp -R /etc/kubernetes/pki /tmp/${HOST1}/ find /etc/kubernetes/pki -not -name ca.crt -not -name ca.key -type f -delete kubeadm init phase certs etcd-server --config=/tmp/${HOST0}/kubeadmcfg.yaml kubeadm init phase certs etcd-peer --config=/tmp/${HOST0}/kubeadmcfg.yaml kubeadm init phase certs etcd-healthcheck-client --config=/tmp/${HOST0}/kubeadmcfg.yaml kubeadm init phase certs apiserver-etcd-client --config=/tmp/${HOST0}/kubeadmcfg.yaml # No need to move the certs because they are for HOST0 # clean up certs that should not be copied off this host find /tmp/${HOST2} -name ca.key -type f -delete find /tmp/${HOST1} -name ca.key -type f -delete Copy certificates and kubeadm configs.The certificates have been generated and now they must be moved to their respective hosts.USER=ubuntu HOST=${HOST1} scp -r /tmp/${HOST}/* ${USER}@${HOST}: ssh ${USER}@${HOST} USER@HOST $ sudo -Es root@HOST $ chown -R root:root pki root@HOST $ mv pki /etc/kubernetes/ Ensure all expected files exist.The complete list of required files on $HOST0 is:/tmp/${HOST0} └── kubeadmcfg.yaml \--- /etc/kubernetes/pki ├── apiserver-etcd-client.crt ├── apiserver-etcd-client.key └── etcd ├── ca.crt ├── ca.key ├── healthcheck-client.crt ├── healthcheck-client.key ├── peer.crt ├── peer.key ├── server.crt └── server.key On $HOST1:$HOME └── kubeadmcfg.yaml \--- /etc/kubernetes/pki ├── apiserver-etcd-client.crt ├── apiserver-etcd-client.key └── etcd ├── ca.crt ├── healthcheck-client.crt ├── healthcheck-client.key ├── peer.crt ├── peer.key ├── server.crt └── server.key On $HOST2:$HOME └── kubeadmcfg.yaml \--- /etc/kubernetes/pki ├── apiserver-etcd-client.crt ├── apiserver-etcd-client.key └── etcd ├── ca.crt ├── healthcheck-client.crt ├── healthcheck-client.key ├── peer.crt ├── peer.key ├── server.crt └── server.key Create the static pod manifests.Now that the certificates and configs are in place it's time to create the manifests. On each host run the kubeadm command to generate a static manifest for etcd.root@HOST0 $ kubeadm init phase etcd local --config=/tmp/${HOST0}/kubeadmcfg.yaml root@HOST1 $ kubeadm init phase etcd local --config=$HOME/kubeadmcfg.yaml root@HOST2 $ kubeadm init phase etcd local --config=$HOME/kubeadmcfg.yaml Optional: Check the cluster health.If etcdctl isn't available, you can run this tool inside a container image. You would do that directly with your container runtime using a tool such as crictl run and not through KubernetesETCDCTL_API=3 etcdctl \ \--cert /etc/kubernetes/pki/etcd/peer.crt \ \--key /etc/kubernetes/pki/etcd/peer.key \ \--cacert /etc/kubernetes/pki/etcd/ca.crt \ \--endpoints https://${HOST0}:2379 endpoint health ... https://[HOST0 IP]:2379 is healthy: successfully committed proposal: took = 16.283339ms https://[HOST1 IP]:2379 is healthy: successfully committed proposal: took = 19.44402ms https://[HOST2 IP]:2379 is healthy: successfully committed proposal: took = 35.926451ms Set ${HOST0}to the IP address of the host you are testing.What's nextOnce you have an etcd cluster with 3 working members, you can continue setting up a highly available control plane using the external etcd method with kubeadm.2.2.2.1.8 - Configuring each kubelet in your cluster using kubeadmNote: Dockershim has been removed from the Kubernetes project as of release 1.24. Read the Dockershim Removal FAQ for further details.FEATURE STATE: Kubernetes v1.11 [stable]The lifecycle of the kubeadm CLI tool is decoupled from the kubelet, which is a daemon that runs on each node within the Kubernetes cluster. The kubeadm CLI tool is executed by the user when Kubernetes is initialized or upgraded, whereas the kubelet is always running in the background.Since the kubelet is a daemon, it needs to be maintained by some kind of an init system or service manager. When the kubelet is installed using DEBs or RPMs, systemd is configured to manage the kubelet. You can use a different service manager instead, but you need to configure it manually.Some kubelet configuration details need to be the same across all kubelets involved in the cluster, while other configuration aspects need to be set on a per-kubelet basis to accommodate the different characteristics of a given machine (such as OS, storage, and networking). You can manage the configuration of your kubelets manually, but kubeadm now provides a KubeletConfiguration API type for managing your kubelet configurations centrally.Kubelet configuration patternsThe following sections describe patterns to kubelet configuration that are simplified by using kubeadm, rather than managing the kubelet configuration for each Node manually.Propagating cluster-level configuration to each kubeletYou can provide the kubelet with default values to be used by kubeadm init and kubeadm join commands. Interesting examples include using a different container runtime or setting the default subnet used by services.If you want your services to use the subnet 10.96.0.0/12 as the default for services, you can pass the --service-cidr parameter to kubeadm:kubeadm init --service-cidr 10.96.0.0/12 Virtual IPs for services are now allocated from this subnet. You also need to set the DNS address used by the kubelet, using the --cluster-dns flag. This setting needs to be the same for every kubelet on every manager and Node in the cluster. The kubelet provides a versioned, structured API object that can configure most parameters in the kubelet and push out this configuration to each running kubelet in the cluster. This object is called KubeletConfiguration. The KubeletConfiguration allows the user to specify flags such as the cluster DNS IP addresses expressed as a list of values to a camelCased key, illustrated by the following example:apiVersion: kubelet.config.k8s.io/v1beta1 kind: KubeletConfiguration clusterDNS: \- 10.96.0.10 For more details on the KubeletConfiguration have a look at this section.Providing instance-specific configuration detailsSome hosts require specific kubelet configurations due to differences in hardware, operating system, networking, or other host-specific parameters. The following list provides a few examples.The path to the DNS resolution file, as specified by the --resolv-conf kubelet configuration flag, may differ among operating systems, or depending on whether you are using systemd-resolved. If this path is wrong, DNS resolution will fail on the Node whose kubelet is configured incorrectly.The Node API object .metadata.name is set to the machine's hostname by default, unless you are using a cloud provider. You can use the --hostname-override flag to override the default behavior if you need to specify a Node name different from the machine's hostname.Currently, the kubelet cannot automatically detect the cgroup driver used by the container runtime, but the value of --cgroup-driver must match the cgroup driver used by the container runtime to ensure the health of the kubelet.To specify the container runtime you must set its endpoint with the \--container-runtime-endpoint= flag.The recommended way of applying such instance-specific configuration is by using KubeletConfiguration patches.Configure kubelets using kubeadmIt is possible to configure the kubelet that kubeadm will start if a custom KubeletConfiguration API object is passed with a configuration file like so kubeadm ... --config some-config-file.yaml.By calling kubeadm config print init-defaults --component-configs KubeletConfiguration you can see all the default values for this structure.It is also possible to apply instance-specific patches over the base KubeletConfiguration. Have a look at Customizing the kubelet for more details.Workflow when using kubeadm initWhen you call kubeadm init, the kubelet configuration is marshalled to disk at /var/lib/kubelet/config.yaml, and also uploaded to a kubelet-config ConfigMap in the kube-system namespace of the cluster. A kubelet configuration file is also written to /etc/kubernetes/kubelet.conf with the baseline cluster-wide configuration for all kubelets in the cluster. This configuration file points to the client certificates that allow the kubelet to communicate with the API server. This addresses the need to propagate cluster-level configuration to each kubelet.To address the second pattern of providing instance-specific configuration details, kubeadm writes an environment file to /var/lib/kubelet/kubeadm-flags.env, which contains a list of flags to pass to the kubelet when it starts. The flags are presented in the file like this:KUBELET_KUBEADM_ARGS="--flag1=value1 --flag2=value2 ..." In addition to the flags used when starting the kubelet, the file also contains dynamic parameters such as the cgroup driver and whether to use a different container runtime socket (--cri-socket).After marshalling these two files to disk, kubeadm attempts to run the following two commands, if you are using systemd:systemctl daemon-reload && systemctl restart kubelet If the reload and restart are successful, the normal kubeadm init workflow continues.Workflow when using kubeadm joinWhen you run kubeadm join, kubeadm uses the Bootstrap Token credential to perform a TLS bootstrap, which fetches the credential needed to download the kubelet-config ConfigMap and writes it to /var/lib/kubelet/config.yaml. The dynamic environment file is generated in exactly the same way as kubeadm init.Next, kubeadm runs the following two commands to load the new configuration into the kubelet:systemctl daemon-reload && systemctl restart kubelet After the kubelet loads the new configuration, kubeadm writes the /etc/kubernetes/bootstrap-kubelet.conf KubeConfig file, which contains a CA certificate and Bootstrap Token. These are used by the kubelet to perform the TLS Bootstrap and obtain a unique credential, which is stored in /etc/kubernetes/kubelet.conf.When the /etc/kubernetes/kubelet.conf file is written, the kubelet has finished performing the TLS Bootstrap. Kubeadm deletes the /etc/kubernetes/bootstrap-kubelet.conf file after completing the TLS Bootstrap.The kubelet drop-in file for systemdkubeadm ships with configuration for how systemd should run the kubelet. Note that the kubeadm CLI command never touches this drop-in file.This configuration file installed by the kubeadm package is written to /usr/lib/systemd/system/kubelet.service.d/10-kubeadm.conf and is used by systemd. It augments the basic kubelet.service.If you want to override that further, you can make a directory /etc/systemd/system/kubelet.service.d/ (not /usr/lib/systemd/system/kubelet.service.d/) and put your own customizations into a file there. For example, you might add a new local file /etc/systemd/system/kubelet.service.d/local-overrides.conf to override the unit settings configured by kubeadm.Here is what you are likely to find in /usr/lib/systemd/system/kubelet.service.d/10-kubeadm.conf:Note:The contents below are just an example. If you don't want to use a package manager follow the guide outlined in the (Without a package manager) section.[Service] Environment="KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf" Environment="KUBELET_CONFIG_ARGS=--config=/var/lib/kubelet/config.yaml" # This is a file that "kubeadm init" and "kubeadm join" generate at runtime, populating # the KUBELET_KUBEADM_ARGS variable dynamically EnvironmentFile=-/var/lib/kubelet/kubeadm-flags.env # This is a file that the user can use for overrides of the kubelet args as a last resort. Preferably, # the user should use the .NodeRegistration.KubeletExtraArgs object in the configuration files instead. # KUBELET_EXTRA_ARGS should be sourced from this file. EnvironmentFile=-/etc/default/kubelet ExecStart= ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS This file specifies the default locations for all of the files managed by kubeadm for the kubelet.The KubeConfig file to use for the TLS Bootstrap is /etc/kubernetes/bootstrap-kubelet.conf, but it is only used if /etc/kubernetes/kubelet.conf does not exist.The KubeConfig file with the unique kubelet identity is /etc/kubernetes/kubelet.conf.The file containing the kubelet's ComponentConfig is /var/lib/kubelet/config.yaml.The dynamic environment file that contains KUBELET_KUBEADM_ARGS is sourced from /var/lib/kubelet/kubeadm-flags.env.The file that can contain user-specified flag overrides with KUBELET_EXTRA_ARGS is sourced from /etc/default/kubelet (for DEBs), or /etc/sysconfig/kubelet (for RPMs). KUBELET_EXTRA_ARGS is last in the flag chain and has the highest priority in the event of conflicting settings.Kubernetes binaries and package contentsThe DEB and RPM packages shipped with the Kubernetes releases are:Package nameDescriptionkubeadmInstalls the /usr/bin/kubeadm CLI tool and the kubelet drop-in file for the kubelet.kubeletInstalls the /usr/bin/kubelet binary.kubectlInstalls the /usr/bin/kubectl binary.cri-toolsInstalls the /usr/bin/crictl binary from the cri-tools git repository.kubernetes-cniInstalls the /opt/cni/bin binaries from the plugins git repository.2.2.2.1.9 - Dual-stack support with kubeadmFEATURE STATE: Kubernetes v1.23 [stable]Your Kubernetes cluster includes dual-stack networking, which means that cluster networking lets you use either address family. In a cluster, the control plane can assign both an IPv4 address and an IPv6 address to a single Pod or a Service.Before you beginYou need to have installed the kubeadm tool, following the steps from Installing kubeadm.For each server that you want to use as a node, make sure it allows IPv6 forwarding.Enable IPv6 packet forwardingTo check if IPv6 packet forwarding is enabled:sysctl net.ipv6.conf.all.forwarding If the output is net.ipv6.conf.all.forwarding = 1 it is already enabled. Otherwise it is not enabled yet.To manually enable IPv6 packet forwarding:# sysctl params required by setup, params persist across reboots cat <, , localhost, 127.0.0.1kube-etcd-peeretcd-caserver, client, , localhost, 127.0.0.1kube-etcd-healthcheck-clientetcd-caclientkube-apiserver-etcd-clientetcd-caclientkube-apiserverkubernetes-caserver, , 1kube-apiserver-kubelet-clientkubernetes-casystem:mastersclientfront-proxy-clientkubernetes-front-proxy-caclientNote:Instead of using the super-user group system:masters for kube-apiserver-kubelet-client a less privileged group can be used. kubeadm uses the kubeadm:cluster-admins group for that purpose.where kind maps to one or more of the x509 key usage, which is also documented in the .spec.usages of a CertificateSigningRequest type:kindKey usageserverdigital signature, key encipherment, server authclientdigital signature, key encipherment, client authNote:Hosts/SAN listed above are the recommended ones for getting a working cluster; if required by a specific setup, it is possible to add additional SANs on all the server certificates.Note:For kubeadm users only:The scenario where you are copying to your cluster CA certificates without private keys is referred as external CA in the kubeadm documentation.If you are comparing the above list with a kubeadm generated PKI, please be aware that kube-etcd, kube-etcd-peer and kube-etcd-healthcheck-client certificates are not generated in case of external etcd.Certificate pathsCertificates should be placed in a recommended path (as used by kubeadm). Paths should be specified using the given argument regardless of location.DefaultCNrecommendedkeypathrecommendedcertpathcommandkeyargumentcertargumentetcd-caetcd/ca.keyetcd/ca.crtkube-apiserver--etcd-cafilekube-apiserver-etcd-clientapiserver-etcd-client.keyapiserver-etcd-client.crtkube-apiserver--etcd-keyfile--etcd-certfilekubernetes-caca.keyca.crtkube-apiserver--client-ca-filekubernetes-caca.keyca.crtkube-controller-manager--cluster-signing-key-file--client-ca-file,--root-ca-file,--cluster-signing-cert-filekube-apiserverapiserver.keyapiserver.crtkube-apiserver--tls-private-key-file--tls-cert-filekube-apiserver-kubelet-clientapiserver-kubelet-client.keyapiserver-kubelet-client.crtkube-apiserver--kubelet-client-key--kubelet-client-certificatefront-proxy-cafront-proxy-ca.keyfront-proxy-ca.crtkube-apiserver--requestheader-client-ca-filefront-proxy-cafront-proxy-ca.keyfront-proxy-ca.crtkube-controller-manager--requestheader-client-ca-filefront-proxy-clientfront-proxy-client.keyfront-proxy-client.crtkube-apiserver--proxy-client-key-file--proxy-client-cert-fileetcd-caetcd/ca.keyetcd/ca.crtetcd--trusted-ca-file,--peer-trusted-ca-filekube-etcdetcd/server.keyetcd/server.crtetcd--key-file--cert-filekube-etcd-peeretcd/peer.keyetcd/peer.crtetcd--peer-key-file--peer-cert-fileetcd-caetcd/ca.crtetcdctl--cacertkube-etcd-healthcheck-clientetcd/healthcheck-client.keyetcd/healthcheck-client.crtetcdctl--key--certSame considerations apply for the service account key pair:private key pathpublic key pathcommandargumentsa.keykube-controller-manager--service-account-private-key-filesa.pubkube-apiserver--service-account-key-fileThe following example illustrates the file paths from the previous tables you need to provide if you are generating all of your own keys and certificates:/etc/kubernetes/pki/etcd/ca.key /etc/kubernetes/pki/etcd/ca.crt /etc/kubernetes/pki/apiserver-etcd-client.key /etc/kubernetes/pki/apiserver-etcd-client.crt /etc/kubernetes/pki/ca.key /etc/kubernetes/pki/ca.crt /etc/kubernetes/pki/apiserver.key /etc/kubernetes/pki/apiserver.crt /etc/kubernetes/pki/apiserver-kubelet-client.key /etc/kubernetes/pki/apiserver-kubelet-client.crt /etc/kubernetes/pki/front-proxy-ca.key /etc/kubernetes/pki/front-proxy-ca.crt /etc/kubernetes/pki/front-proxy-client.key /etc/kubernetes/pki/front-proxy-client.crt /etc/kubernetes/pki/etcd/server.key /etc/kubernetes/pki/etcd/server.crt /etc/kubernetes/pki/etcd/peer.key /etc/kubernetes/pki/etcd/peer.crt /etc/kubernetes/pki/etcd/healthcheck-client.key /etc/kubernetes/pki/etcd/healthcheck-client.crt /etc/kubernetes/pki/sa.key /etc/kubernetes/pki/sa.pub Configure certificates for user accountsYou must manually configure these administrator accounts and service accounts:FilenameCredential nameDefault CNO (in Subject)admin.confdefault-adminkubernetes-adminsuper-admin.confdefault-super-adminkubernetes-super-adminsystem:masterskubelet.confdefault-authsystem:node: (see note)system:nodescontroller-manager.confdefault-controller-managersystem:kube-controller-managerscheduler.confdefault-schedulersystem:kube-schedulerNote:The value of  for kubelet.conf must match precisely the value of the node name provided by the kubelet as it registers with the apiserver. For further details, read the Node Authorization.Note:In the above example  is implementation specific. Some tools sign the certificate in the default admin.conf to be part of the system:masters group. system:masters is a break-glass, super user group can bypass the authorization layer of Kubernetes, such as RBAC. Also some tools do not generate a separate super-admin.conf with a certificate bound to this super user group.kubeadm generates two separate administrator certificates in kubeconfig files. One is in admin.conf and has Subject: O = kubeadm:cluster-admins, CN = kubernetes-admin. kubeadm:cluster-admins is a custom group bound to the cluster-admin ClusterRole. This file is generated on all kubeadm managed control plane machines.Another is in super-admin.conf that has Subject: O = system:masters, CN = kubernetes-super-admin. This file is generated only on the node where kubeadm init was called.For each configuration, generate an x509 certificate/key pair with the given Common Name (CN) and Organization (O).Run kubectl as follows for each configuration:KUBECONFIG= kubectl config set-cluster default-cluster --server=https://:6443 --certificate-authority  \--embed-certs KUBECONFIG= kubectl config set-credentials  \--client-key .pem --client-certificate .pem --embed-certs KUBECONFIG= kubectl config set-context default-system --cluster default-cluster --user  KUBECONFIG= kubectl config use-context default-system These files are used as follows:FilenameCommandCommentadmin.confkubectlConfigures administrator user for the clustersuper-admin.confkubectlConfigures super administrator user for the clusterkubelet.confkubeletOne required for each node in the cluster.controller-manager.confkube-controller-managerMust be added to manifest in manifests/kube-controller-manager.yamlscheduler.confkube-schedulerMust be added to manifest in manifests/kube-scheduler.yamlThe following files illustrate full paths to the files listed in the previous table:/etc/kubernetes/admin.conf /etc/kubernetes/super-admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf any other IP or DNS name you contact your cluster on (as used by kubeadm the load balancer stable IP and/or DNS name, kubernetes, kubernetes.default, kubernetes.default.svc, kubernetes.default.svc.cluster, kubernetes.default.svc.cluster.local) ↩︎3 - ConceptsThe Concepts section helps you learn about the parts of the Kubernetes system and the abstractions Kubernetes uses to represent your cluster, and helps you obtain a deeper understanding of how Kubernetes works.3.1 - OverviewKubernetes is a portable, extensible, open source platform for managing containerized workloads and services, that facilitates both declarative configuration and automation. It has a large, rapidly growing ecosystem. Kubernetes services, support, and tools are widely available.This page is an overview of Kubernetes.The name Kubernetes originates from Greek, meaning helmsman or pilot. K8s as an abbreviation results from counting the eight letters between the "K" and the "s". Google open-sourced the Kubernetes project in 2014. Kubernetes combines over 15 years of Google's experience running production workloads at scale with best-of-breed ideas and practices from the community.Why you need Kubernetes and what it can doContainers are a good way to bundle and run your applications. In a production environment, you need to manage the containers that run the applications and ensure that there is no downtime. For example, if a container goes down, another container needs to start. Wouldn't it be easier if this behavior was handled by a system?That's how Kubernetes comes to the rescue! Kubernetes provides you with a framework to run distributed systems resiliently. It takes care of scaling and failover for your application, provides deployment patterns, and more. For example: Kubernetes can easily manage a canary deployment for your system.Kubernetes provides you with:Service discovery and load balancing Kubernetes can expose a container using the DNS name or using their own IP address. If traffic to a container is high, Kubernetes is able to load balance and distribute the network traffic so that the deployment is stable.Storage orchestration Kubernetes allows you to automatically mount a storage system of your choice, such as local storages, public cloud providers, and more.Automated rollouts and rollbacks You can describe the desired state for your deployed containers using Kubernetes, and it can change the actual state to the desired state at a controlled rate. For example, you can automate Kubernetes to create new containers for your deployment, remove existing containers and adopt all their resources to the new container.Automatic bin packing You provide Kubernetes with a cluster of nodes that it can use to run containerized tasks. You tell Kubernetes how much CPU and memory (RAM) each container needs. Kubernetes can fit containers onto your nodes to make the best use of your resources.Self-healing Kubernetes restarts containers that fail, replaces containers, kills containers that don't respond to your user-defined health check, and doesn't advertise them to clients until they are ready to serve.Secret and configuration management Kubernetes lets you store and manage sensitive information, such as passwords, OAuth tokens, and SSH keys. You can deploy and update secrets and application configuration without rebuilding your container images, and without exposing secrets in your stack configuration.Batch execution In addition to services, Kubernetes can manage your batch and CI workloads, replacing containers that fail, if desired.Horizontal scaling Scale your application up and down with a simple command, with a UI, or automatically based on CPU usage.IPv4/IPv6 dual-stack Allocation of IPv4 and IPv6 addresses to Pods and ServicesDesigned for extensibility Add features to your Kubernetes cluster without changing upstream source code.What Kubernetes is notKubernetes is not a traditional, all-inclusive PaaS (Platform as a Service) system. Since Kubernetes operates at the container level rather than at the hardware level, it provides some generally applicable features common to PaaS offerings, such as deployment, scaling, load balancing, and lets users integrate their logging, monitoring, and alerting solutions. However, Kubernetes is not monolithic, and these default solutions are optional and pluggable. Kubernetes provides the building blocks for building developer platforms, but preserves user choice and flexibility where it is important.Kubernetes:Does not limit the types of applications supported. Kubernetes aims to support an extremely diverse variety of workloads, including stateless, stateful, and data-processing workloads. If an application can run in a container, it should run great on Kubernetes.Does not deploy source code and does not build your application. Continuous Integration, Delivery, and Deployment (CI/CD) workflows are determined by organization cultures and preferences as well as technical requirements.Does not provide application-level services, such as middleware (for example, message buses), data-processing frameworks (for example, Spark), databases (for example, MySQL), caches, nor cluster storage systems (for example, Ceph) as built-in services. Such components can run on Kubernetes, and/or can be accessed by applications running on Kubernetes through portable mechanisms, such as the Open Service Broker.Does not dictate logging, monitoring, or alerting solutions. It provides some integrations as proof of concept, and mechanisms to collect and export metrics.Does not provide nor mandate a configuration language/system (for example, Jsonnet). It provides a declarative API that may be targeted by arbitrary forms of declarative specifications.Does not provide nor adopt any comprehensive machine configuration, maintenance, management, or self-healing systems.Additionally, Kubernetes is not a mere orchestration system. In fact, it eliminates the need for orchestration. The technical definition of orchestration is execution of a defined workflow: first do A, then B, then C. In contrast, Kubernetes comprises a set of independent, composable control processes that continuously drive the current state towards the provided desired state. It shouldn't matter how you get from A to C. Centralized control is also not required. This results in a system that is easier to use and more powerful, robust, resilient, and extensible.Historical context for KubernetesLet's take a look at why Kubernetes is so useful by going back in time.Traditional deployment era:Early on, organizations ran applications on physical servers. There was no way to define resource boundaries for applications in a physical server, and this caused resource allocation issues. For example, if multiple applications run on a physical server, there can be instances where one application would take up most of the resources, and as a result, the other applications would underperform. A solution for this would be to run each application on a different physical server. But this did not scale as resources were underutilized, and it was expensive for organizations to maintain many physical servers.Virtualized deployment era:As a solution, virtualization was introduced. It allows you to run multiple Virtual Machines (VMs) on a single physical server's CPU. Virtualization allows applications to be isolated between VMs and provides a level of security as the information of one application cannot be freely accessed by another application.Virtualization allows better utilization of resources in a physical server and allows better scalability because an application can be added or updated easily, reduces hardware costs, and much more. With virtualization you can present a set of physical resources as a cluster of disposable virtual machines.Each VM is a full machine running all the components, including its own operating system, on top of the virtualized hardware.Container deployment era:Containers are similar to VMs, but they have relaxed isolation properties to share the Operating System (OS) among the applications. Therefore, containers are considered lightweight. Similar to a VM, a container has its own filesystem, share of CPU, memory, process space, and more. As they are decoupled from the underlying infrastructure, they are portable across clouds and OS distributions.Containers have become popular because they provide extra benefits, such as:Agile application creation and deployment: increased ease and efficiency of container image creation compared to VM image use.Continuous development, integration, and deployment: provides for reliable and frequent container image build and deployment with quick and efficient rollbacks (due to image immutability).Dev and Ops separation of concerns: create application container images at build/release time rather than deployment time, thereby decoupling applications from infrastructure.Observability: not only surfaces OS-level information and metrics, but also application health and other signals.Environmental consistency across development, testing, and production: runs the same on a laptop as it does in the cloud.Cloud and OS distribution portability: runs on Ubuntu, RHEL, CoreOS, on-premises, on major public clouds, and anywhere else.Application-centric management: raises the level of abstraction from running an OS on virtual hardware to running an application on an OS using logical resources.Loosely coupled, distributed, elastic, liberated micro-services: applications are broken into smaller, independent pieces and can be deployed and managed dynamically – not a monolithic stack running on one big single-purpose machine.Resource isolation: predictable application performance.Resource utilization: high efficiency and density.What's nextTake a look at the Kubernetes ComponentsTake a look at the The Kubernetes APITake a look at the Cluster ArchitectureReady to Get Started?3.1.1 - Kubernetes ComponentsAn overview of the key components that make up a Kubernetes cluster.This page provides a high-level overview of the essential components that make up a Kubernetes cluster.The components of a Kubernetes clusterCore ComponentsA Kubernetes cluster consists of a control plane and one or more worker nodes. Here's a brief overview of the main components:Control Plane ComponentsManage the overall state of the cluster:kube-apiserverThe core component server that exposes the Kubernetes HTTP APIetcdConsistent and highly-available key value store for all API server datakube-schedulerLooks for Pods not yet bound to a node, and assigns each Pod to a suitable node.kube-controller-managerRuns controllers to implement Kubernetes API behavior.cloud-controller-manager (optional)Integrates with underlying cloud provider(s).Node ComponentsRun on every node, maintaining running pods and providing the Kubernetes runtime environment:kubeletEnsures that Pods are running, including their containers.kube-proxy (optional)Maintains network rules on nodes to implement Services.Container runtimeSoftware responsible for running containers. Read Container Runtimes to learn more.🛇 This item links to a third party project or product that is not part of Kubernetes itself. More informationYour cluster may require additional software on each node; for example, you might also run systemd on a Linux node to supervise local components.AddonsAddons extend the functionality of Kubernetes. A few important examples include:DNSFor cluster-wide DNS resolutionWeb UI (Dashboard)For cluster management via a web interfaceContainer Resource MonitoringFor collecting and storing container metricsCluster-level LoggingFor saving container logs to a central log storeFlexibility in ArchitectureKubernetes allows for flexibility in how these components are deployed and managed. The architecture can be adapted to various needs, from small development environments to large-scale production deployments.For more detailed information about each component and various ways to configure your cluster architecture, see the Cluster Architecture page.3.1.2 - Objects In KubernetesKubernetes objects are persistent entities in the Kubernetes system. Kubernetes uses these entities to represent the state of your cluster. Learn about the Kubernetes object model and how to work with these objects.This page explains how Kubernetes objects are represented in the Kubernetes API, and how you can express them in .yaml format.Understanding Kubernetes objectsKubernetes objects are persistent entities in the Kubernetes system. Kubernetes uses these entities to represent the state of your cluster. Specifically, they can describe:What containerized applications are running (and on which nodes)The resources available to those applicationsThe policies around how those applications behave, such as restart policies, upgrades, and fault-toleranceA Kubernetes object is a "record of intent"--once you create the object, the Kubernetes system will constantly work to ensure that the object exists. By creating an object, you're effectively telling the Kubernetes system what you want your cluster's workload to look like; this is your cluster's desired state.To work with Kubernetes objects—whether to create, modify, or delete them—you'll need to use the Kubernetes API. When you use the kubectl command-line interface, for example, the CLI makes the necessary Kubernetes API calls for you. You can also use the Kubernetes API directly in your own programs using one of the Client Libraries.Object spec and statusAlmost every Kubernetes object includes two nested object fields that govern the object's configuration: the object spec and the object status. For objects that have a spec, you have to set this when you create the object, providing a description of the characteristics you want the resource to have: its desired state.The status describes the current state of the object, supplied and updated by the Kubernetes system and its components. The Kubernetes control plane continually and actively manages every object's actual state to match the desired state you supplied.For example: in Kubernetes, a Deployment is an object that can represent an application running on your cluster. When you create the Deployment, you might set the Deployment spec to specify that you want three replicas of the application to be running. The Kubernetes system reads the Deployment spec and starts three instances of your desired application--updating the status to match your spec. If any of those instances should fail (a status change), the Kubernetes system responds to the difference between spec and status by making a correction--in this case, starting a replacement instance.For more information on the object spec, status, and metadata, see the Kubernetes API Conventions.Describing a Kubernetes objectWhen you create an object in Kubernetes, you must provide the object spec that describes its desired state, as well as some basic information about the object (such as a name). When you use the Kubernetes API to create the object (either directly or via kubectl), that API request must include that information as JSON in the request body. Most often, you provide the information to kubectl in a file known as a manifest. By convention, manifests are YAML (you could also use JSON format). Tools such as kubectl convert the information from a manifest into JSON or another supported serialization format when making the API request over HTTP.Here's an example manifest that shows the required fields and object spec for a Kubernetes Deployment:application/deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment spec: selector: matchLabels: app: nginx replicas: 2 # tells deployment to run 2 pods matching the template template: metadata: labels: app: nginx spec: containers: \- name: nginx image: nginx:1.14.2 ports: \- containerPort: 80 One way to create a Deployment using a manifest file like the one above is to use the kubectl apply command in the kubectl command-line interface, passing the .yaml file as an argument. Here's an example:kubectl apply -f https://k8s.io/examples/application/deployment.yaml The output is similar to this:deployment.apps/nginx-deployment created Required fieldsIn the manifest (YAML or JSON file) for the Kubernetes object you want to create, you'll need to set values for the following fields:apiVersion - Which version of the Kubernetes API you're using to create this objectkind - What kind of object you want to createmetadata - Data that helps uniquely identify the object, including a name string, UID, and optional namespacespec - What state you desire for the objectThe precise format of the object spec is different for every Kubernetes object, and contains nested fields specific to that object. The Kubernetes API Reference can help you find the spec format for all of the objects you can create using Kubernetes.For example, see the spec field for the Pod API reference. For each Pod, the .spec field specifies the pod and its desired state (such as the container image name for each container within that pod). Another example of an object specification is the spec field for the StatefulSet API. For StatefulSet, the .spec field specifies the StatefulSet and its desired state. Within the .spec of a StatefulSet is a template for Pod objects. That template describes Pods that the StatefulSet controller will create in order to satisfy the StatefulSet specification. Different kinds of objects can also have different .status; again, the API reference pages detail the structure of that .status field, and its content for each different type of object.Note:See Configuration Best Practices for additional information on writing YAML configuration files.Server side field validationStarting with Kubernetes v1.25, the API server offers server side field validation that detects unrecognized or duplicate fields in an object. It provides all the functionality of kubectl --validate on the server side.The kubectl tool uses the --validate flag to set the level of field validation. It accepts the values ignore, warn, and strict while also accepting the values true (equivalent to strict) and false (equivalent to ignore). The default validation setting for kubectl is --validate=true.StrictStrict field validation, errors on validation failureWarnField validation is performed, but errors are exposed as warnings rather than failing the requestIgnoreNo server side field validation is performedWhen kubectl cannot connect to an API server that supports field validation it will fall back to using client-side validation. Kubernetes 1.27 and later versions always offer field validation; older Kubernetes releases might not. If your cluster is older than v1.27, check the documentation for your version of Kubernetes.What's nextIf you're new to Kubernetes, read more about the following:Pods which are the most important basic Kubernetes objects.Deployment objects.Controllers in Kubernetes.kubectl and kubectl commands.Kubernetes Object Management explains how to use kubectl to manage objects. You might need to install kubectl if you don't already have it available.To learn about the Kubernetes API in general, visit:Kubernetes API overviewTo learn about objects in Kubernetes in more depth, read other pages in this section:3.1.2.1 - Kubernetes Object ManagementThe kubectl command-line tool supports several different ways to create and manage Kubernetes objects. This document provides an overview of the different approaches. Read the Kubectl book for details of managing objects by Kubectl.Management techniquesWarning:A Kubernetes object should be managed using only one technique. Mixing and matching techniques for the same object results in undefined behavior.Management techniqueOperates onRecommended environmentSupported writersLearning curveImperative commandsLive objectsDevelopment projects1+LowestImperative object configurationIndividual filesProduction projects1ModerateDeclarative object configurationDirectories of filesProduction projects1+HighestImperative commandsWhen using imperative commands, a user operates directly on live objects in a cluster. The user provides operations to the kubectl command as arguments or flags.This is the recommended way to get started or to run a one-off task in a cluster. Because this technique operates directly on live objects, it provides no history of previous configurations.ExamplesRun an instance of the nginx container by creating a Deployment object:kubectl create deployment nginx --image nginx Trade-offsAdvantages compared to object configuration:Commands are expressed as a single action word.Commands require only a single step to make changes to the cluster.Disadvantages compared to object configuration:Commands do not integrate with change review processes.Commands do not provide an audit trail associated with changes.Commands do not provide a source of records except for what is live.Commands do not provide a template for creating new objects.Imperative object configurationIn imperative object configuration, the kubectl command specifies the operation (create, replace, etc.), optional flags and at least one file name. The file specified must contain a full definition of the object in YAML or JSON format.See the API reference for more details on object definitions.Warning:The imperative replace command replaces the existing spec with the newly provided one, dropping all changes to the object missing from the configuration file. This approach should not be used with resource types whose specs are updated independently of the configuration file. Services of type LoadBalancer, for example, have their externalIPs field updated independently from the configuration by the cluster.ExamplesCreate the objects defined in a configuration file:kubectl create -f nginx.yaml Delete the objects defined in two configuration files:kubectl delete -f nginx.yaml -f redis.yaml Update the objects defined in a configuration file by overwriting the live configuration:kubectl replace -f nginx.yaml Trade-offsAdvantages compared to imperative commands:Object configuration can be stored in a source control system such as Git.Object configuration can integrate with processes such as reviewing changes before push and audit trails.Object configuration provides a template for creating new objects.Disadvantages compared to imperative commands:Object configuration requires basic understanding of the object schema.Object configuration requires the additional step of writing a YAML file.Advantages compared to declarative object configuration:Imperative object configuration behavior is simpler and easier to understand.As of Kubernetes version 1.5, imperative object configuration is more mature.Disadvantages compared to declarative object configuration:Imperative object configuration works best on files, not directories.Updates to live objects must be reflected in configuration files, or they will be lost during the next replacement.Declarative object configurationWhen using declarative object configuration, a user operates on object configuration files stored locally, however the user does not define the operations to be taken on the files. Create, update, and delete operations are automatically detected per-object by kubectl. This enables working on directories, where different operations might be needed for different objects.Note:Declarative object configuration retains changes made by other writers, even if the changes are not merged back to the object configuration file. This is possible by using the patch API operation to write only observed differences, instead of using the replace API operation to replace the entire object configuration.ExamplesProcess all object configuration files in the configs directory, and create or patch the live objects. You can first diff to see what changes are going to be made, and then apply:kubectl diff -f configs/ kubectl apply -f configs/ Recursively process directories:kubectl diff -R -f configs/ kubectl apply -R -f configs/ Trade-offsAdvantages compared to imperative object configuration:Changes made directly to live objects are retained, even if they are not merged back into the configuration files.Declarative object configuration has better support for operating on directories and automatically detecting operation types (create, patch, delete) per-object.Disadvantages compared to imperative object configuration:Declarative object configuration is harder to debug and understand results when they are unexpected.Partial updates using diffs create complex merge and patch operations.What's nextManaging Kubernetes Objects Using Imperative CommandsImperative Management of Kubernetes Objects Using Configuration FilesDeclarative Management of Kubernetes Objects Using Configuration FilesDeclarative Management of Kubernetes Objects Using KustomizeKubectl Command ReferenceKubectl BookKubernetes API Reference3.1.2.2 - Object Names and IDsEach object in your cluster has a Name that is unique for that type of resource. Every Kubernetes object also has a UID that is unique across your whole cluster.For example, you can only have one Pod named myapp-1234 within the same namespace, but you can have one Pod and one Deployment that are each named myapp-1234.For non-unique user-provided attributes, Kubernetes provides labels and annotations.NamesA client-provided string that refers to an object in a resource URL, such as /api/v1/pods/some-name.Only one object of a given kind can have a given name at a time. However, if you delete the object, you can make a new object with the same name.Names must be unique across all API versions of the same resource. API resources are distinguished by their API group, resource type, namespace (for namespaced resources), and name. In other words, API version is irrelevant in this context.Note:In cases when objects represent a physical entity, like a Node representing a physical host, when the host is re-created under the same name without deleting and re-creating the Node, Kubernetes treats the new host as the old one, which may lead to inconsistencies.The server may generate a name when generateName is provided instead of name in a resource create request. When generateName is used, the provided value is used as a name prefix, which server appends a generated suffix to. Even though the name is generated, it may conflict with existing names resulting in a HTTP 409 response. This became far less likely to happen in Kubernetes v1.31 and later, since the server will make up to 8 attempt to generate a unique name before returning a HTTP 409 response.Below are four types of commonly used name constraints for resources.DNS Subdomain NamesMost resource types require a name that can be used as a DNS subdomain name as defined in RFC 1123. This means the name must:contain no more than 253 characterscontain only lowercase alphanumeric characters, '-' or '.'start with an alphanumeric characterend with an alphanumeric characterRFC 1123 Label NamesSome resource types require their names to follow the DNS label standard as defined in RFC 1123. This means the name must:contain at most 63 characterscontain only lowercase alphanumeric characters or '-'start with an alphanumeric characterend with an alphanumeric characterRFC 1035 Label NamesSome resource types require their names to follow the DNS label standard as defined in RFC 1035. This means the name must:contain at most 63 characterscontain only lowercase alphanumeric characters or '-'start with an alphabetic characterend with an alphanumeric characterNote:The only difference between the RFC 1035 and RFC 1123 label standards is that RFC 1123 labels are allowed to start with a digit, whereas RFC 1035 labels can start with a lowercase alphabetic character only.Path Segment NamesSome resource types require their names to be able to be safely encoded as a path segment. In other words, the name may not be "." or ".." and the name may not contain "/" or "%".Here's an example manifest for a Pod named nginx-demo.apiVersion: v1 kind: Pod metadata: name: nginx-demo spec: containers: \- name: nginx image: nginx:1.14.2 ports: \- containerPort: 80 Note:Some resource types have additional restrictions on their names.UIDsA Kubernetes systems-generated string to uniquely identify objects.Every object created over the whole lifetime of a Kubernetes cluster has a distinct UID. It is intended to distinguish between historical occurrences of similar entities.Kubernetes UIDs are universally unique identifiers (also known as UUIDs). UUIDs are standardized as ISO/IEC 9834-8 and as ITU-T X.667.What's nextRead about labels and annotations in Kubernetes.See the Identifiers and Names in Kubernetes design document.3.1.2.3 - Labels and SelectorsLabels are key/value pairs that are attached to objects such as Pods. Labels are intended to be used to specify identifying attributes of objects that are meaningful and relevant to users, but do not directly imply semantics to the core system. Labels can be used to organize and to select subsets of objects. Labels can be attached to objects at creation time and subsequently added and modified at any time. Each object can have a set of key/value labels defined. Each Key must be unique for a given object."metadata": { "labels": { "key1" : "value1", "key2" : "value2" } } Labels allow for efficient queries and watches and are ideal for use in UIs and CLIs. Non-identifying information should be recorded using annotations.MotivationLabels enable users to map their own organizational structures onto system objects in a loosely coupled fashion, without requiring clients to store these mappings.Service deployments and batch processing pipelines are often multi-dimensional entities (e.g., multiple partitions or deployments, multiple release tracks, multiple tiers, multiple micro-services per tier). Management often requires cross-cutting operations, which breaks encapsulation of strictly hierarchical representations, especially rigid hierarchies determined by the infrastructure rather than by users.Example labels:"release" : "stable", "release" : "canary""environment" : "dev", "environment" : "qa", "environment" : "production""tier" : "frontend", "tier" : "backend", "tier" : "cache""partition" : "customerA", "partition" : "customerB""track" : "daily", "track" : "weekly"These are examples of commonly used labels; you are free to develop your own conventions. Keep in mind that label Key must be unique for a given object.Syntax and character setLabels are key/value pairs. Valid label keys have two segments: an optional prefix and name, separated by a slash (/). The name segment is required and must be 63 characters or less, beginning and ending with an alphanumeric character ([a-z0-9A-Z]) with dashes (-), underscores (_), dots (.), and alphanumerics between. The prefix is optional. If specified, the prefix must be a DNS subdomain: a series of DNS labels separated by dots (.), not longer than 253 characters in total, followed by a slash (/).If the prefix is omitted, the label Key is presumed to be private to the user. Automated system components (e.g. kube-scheduler, kube-controller-manager, kube-apiserver, kubectl, or other third-party automation) which add labels to end-user objects must specify a prefix.The kubernetes.io/ and k8s.io/ prefixes are reserved for Kubernetes core components.Valid label value:must be 63 characters or less (can be empty),unless empty, must begin and end with an alphanumeric character ([a-z0-9A-Z]),could contain dashes (-), underscores (_), dots (.), and alphanumerics between.For example, here's a manifest for a Pod that has two labels environment: production and app: nginx:apiVersion: v1 kind: Pod metadata: name: label-demo labels: environment: production app: nginx spec: containers: \- name: nginx image: nginx:1.14.2 ports: \- containerPort: 80 Label selectorsUnlike names and UIDs, labels do not provide uniqueness. In general, we expect many objects to carry the same label(s).Via a label selector, the client/user can identify a set of objects. The label selector is the core grouping primitive in Kubernetes.The API currently supports two types of selectors: equality-based and set-based. A label selector can be made of multiple requirements which are comma-separated. In the case of multiple requirements, all must be satisfied so the comma separator acts as a logical AND (&&) operator.The semantics of empty or non-specified selectors are dependent on the context, and API types that use selectors should document the validity and meaning of them.Note:For some API types, such as ReplicaSets, the label selectors of two instances must not overlap within a namespace, or the controller can see that as conflicting instructions and fail to determine how many replicas should be present.Caution:For both equality-based and set-based conditions there is no logical OR (||) operator. Ensure your filter statements are structured accordingly.Equality-based requirementEquality- or inequality-based requirements allow filtering by label keys and values. Matching objects must satisfy all of the specified label constraints, though they may have additional labels as well. Three kinds of operators are admitted =,==,!=. The first two represent equality (and are synonyms), while the latter represents inequality. For example:environment = production tier != frontend The former selects all resources with key equal to environment and value equal to production. The latter selects all resources with key equal to tier and value distinct from frontend, and all resources with no labels with the tier key. One could filter for resources in production excluding frontend using the comma operator: environment=production,tier!=frontendOne usage scenario for equality-based label requirement is for Pods to specify node selection criteria. For example, the sample Pod below selects nodes where the accelerator label exists and is set to nvidia-tesla-p100.apiVersion: v1 kind: Pod metadata: name: cuda-test spec: containers: \- name: cuda-test image: "registry.k8s.io/cuda-vector-add:v0.1" resources: limits: nvidia.com/gpu: 1 nodeSelector: accelerator: nvidia-tesla-p100 Set-based requirementSet-based label requirements allow filtering keys according to a set of values. Three kinds of operators are supported: in,notin and exists (only the key identifier). For example:environment in (production, qa) tier notin (frontend, backend) partition !partition The first example selects all resources with key equal to environment and value equal to production or qa.The second example selects all resources with key equal to tier and values other than frontend and backend, and all resources with no labels with the tier key.The third example selects all resources including a label with key partition; no values are checked.The fourth example selects all resources without a label with key partition; no values are checked.Similarly the comma separator acts as an AND operator. So filtering resources with a partition key (no matter the value) and with environment different than qa can be achieved using partition,environment notin (qa). The set-based label selector is a general form of equality since environment=production is equivalent to environment in (production); similarly for != and notin.Set-based requirements can be mixed with equality-based requirements. For example: partition in (customerA, customerB),environment!=qa.APILIST and WATCH filteringFor list and watch operations, you can specify label selectors to filter the sets of objects returned; you specify the filter using a query parameter. (To learn in detail about watches in Kubernetes, read efficient detection of changes). Both requirements are permitted (presented here as they would appear in a URL query string):equality-based requirements: ?labelSelector=environment%3Dproduction,tier%3Dfrontendset-based requirements: ?labelSelector=environment+in+%28production%2Cqa%29%2Ctier+in+%28frontend%29Both label selector styles can be used to list or watch resources via a REST client. For example, targeting apiserver with kubectl and using equality-based one may write:kubectl get pods -l environment=production,tier=frontend or using set-based requirements:kubectl get pods -l 'environment in (production),tier in (frontend)' As already mentioned set-based requirements are more expressive. For instance, they can implement the OR operator on values:kubectl get pods -l 'environment in (production, qa)' or restricting negative matching via notin operator:kubectl get pods -l 'environment,environment notin (frontend)' Set references in API objectsSome Kubernetes objects, such as services and replicationcontrollers, also use label selectors to specify sets of other resources, such as pods.Service and ReplicationControllerThe set of pods that a service targets is defined with a label selector. Similarly, the population of pods that a replicationcontroller should manage is also defined with a label selector.Label selectors for both objects are defined in json or yaml files using maps, and only equality-based requirement selectors are supported:"selector": { "component" : "redis", } orselector: component: redis This selector (respectively in json or yaml format) is equivalent to component=redis or component in (redis).Resources that support set-based requirementsNewer resources, such as Job, Deployment, ReplicaSet, and DaemonSet, support set-based requirements as well.selector: matchLabels: component: redis matchExpressions: \- { key: tier, operator: In, values: [cache] } \- { key: environment, operator: NotIn, values: [dev] } matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels map is equivalent to an element of matchExpressions, whose key field is "key", the operator is "In", and the values array contains only "value". matchExpressions is a list of pod selector requirements. Valid operators include In, NotIn, Exists, and DoesNotExist. The values set must be non-empty in the case of In and NotIn. All of the requirements, from both matchLabels and matchExpressions are ANDed together -- they must all be satisfied in order to match.Selecting sets of nodesOne use case for selecting over labels is to constrain the set of nodes onto which a pod can schedule. See the documentation on node selection for more information.Using labels effectivelyYou can apply a single label to any resources, but this is not always the best practice. There are many scenarios where multiple labels should be used to distinguish resource sets from one another.For instance, different applications would use different values for the app label, but a multi-tier application, such as the guestbook example, would additionally need to distinguish each tier. The frontend could carry the following labels:labels: app: guestbook tier: frontend while the Redis master and replica would have different tier labels, and perhaps even an additional role label:labels: app: guestbook tier: backend role: master andlabels: app: guestbook tier: backend role: replica The labels allow for slicing and dicing the resources along any dimension specified by a label:kubectl apply -f examples/guestbook/all-in-one/guestbook-all-in-one.yaml kubectl get pods -Lapp -Ltier -Lrole NAME READY STATUS RESTARTS AGE APP TIER ROLE guestbook-fe-4nlpb 1/1 Running 0 1m guestbook frontend  guestbook-fe-ght6d 1/1 Running 0 1m guestbook frontend  guestbook-fe-jpy62 1/1 Running 0 1m guestbook frontend  guestbook-redis-master-5pg3b 1/1 Running 0 1m guestbook backend master guestbook-redis-replica-2q2yf 1/1 Running 0 1m guestbook backend replica guestbook-redis-replica-qgazl 1/1 Running 0 1m guestbook backend replica my-nginx-divi2 1/1 Running 0 29m nginx  my-nginx-o0ef1 1/1 Running 0 29m nginx  kubectl get pods -lapp=guestbook,role=replica NAME READY STATUS RESTARTS AGE guestbook-redis-replica-2q2yf 1/1 Running 0 3m guestbook-redis-replica-qgazl 1/1 Running 0 3m Updating labelsSometimes you may want to relabel existing pods and other resources before creating new resources. This can be done with kubectl label. For example, if you want to label all your NGINX Pods as frontend tier, run:kubectl label pods -l app=nginx tier=fe pod/my-nginx-2035384211-j5fhi labeled pod/my-nginx-2035384211-u2c7e labeled pod/my-nginx-2035384211-u3t6x labeled This first filters all pods with the label "app=nginx", and then labels them with the "tier=fe". To see the pods you labeled, run:kubectl get pods -l app=nginx -L tier NAME READY STATUS RESTARTS AGE TIER my-nginx-2035384211-j5fhi 1/1 Running 0 23m fe my-nginx-2035384211-u2c7e 1/1 Running 0 23m fe my-nginx-2035384211-u3t6x 1/1 Running 0 23m fe This outputs all "app=nginx" pods, with an additional label column of pods' tier (specified with -L or --label-columns).For more information, please see kubectl label.What's nextLearn how to add a label to a nodeFind Well-known labels, Annotations and TaintsSee Recommended labelsEnforce Pod Security Standards with Namespace LabelsRead a blog on Writing a Controller for Pod Labels3.1.2.4 - NamespacesIn Kubernetes, namespaces provide a mechanism for isolating groups of resources within a single cluster. Names of resources need to be unique within a namespace, but not across namespaces. Namespace-based scoping is applicable only for namespaced objects (e.g. Deployments, Services, etc.) and not for cluster-wide objects (e.g. StorageClass, Nodes, PersistentVolumes, etc.).When to Use Multiple NamespacesNamespaces are intended for use in environments with many users spread across multiple teams, or projects. For clusters with a few to tens of users, you should not need to create or think about namespaces at all. Start using namespaces when you need the features they provide.Namespaces provide a scope for names. Names of resources need to be unique within a namespace, but not across namespaces. Namespaces cannot be nested inside one another and each Kubernetes resource can only be in one namespace.Namespaces are a way to divide cluster resources between multiple users (via resource quota).It is not necessary to use multiple namespaces to separate slightly different resources, such as different versions of the same software: use labels to distinguish resources within the same namespace.Note:For a production cluster, consider not using the default namespace. Instead, make other namespaces and use those.Initial namespacesKubernetes starts with four initial namespaces:defaultKubernetes includes this namespace so that you can start using your new cluster without first creating a namespace.kube-node-leaseThis namespace holds Lease objects associated with each node. Node leases allow the kubelet to send heartbeats so that the control plane can detect node failure.kube-publicThis namespace is readable by all clients (including those not authenticated). This namespace is mostly reserved for cluster usage, in case that some resources should be visible and readable publicly throughout the whole cluster. The public aspect of this namespace is only a convention, not a requirement.kube-systemThe namespace for objects created by the Kubernetes system.Working with NamespacesCreation and deletion of namespaces are described in the Admin Guide documentation for namespaces.Note:Avoid creating namespaces with the prefix kube-, since it is reserved for Kubernetes system namespaces.Viewing namespacesYou can list the current namespaces in a cluster using:kubectl get namespace NAME STATUS AGE default Active 1d kube-node-lease Active 1d kube-public Active 1d kube-system Active 1d Setting the namespace for a requestTo set the namespace for a current request, use the --namespace flag.For example:kubectl run nginx --image=nginx --namespace= kubectl get pods --namespace= Setting the namespace preferenceYou can permanently save the namespace for all subsequent kubectl commands in that context.kubectl config set-context --current --namespace= # Validate it kubectl config view --minify | grep namespace: Namespaces and DNSWhen you create a Service, it creates a corresponding DNS entry. This entry is of the form ..svc.cluster.local, which means that if a container only uses , it will resolve to the service which is local to a namespace. This is useful for using the same configuration across multiple namespaces such as Development, Staging and Production. If you want to reach across namespaces, you need to use the fully qualified domain name (FQDN).As a result, all namespace names must be valid RFC 1123 DNS labels.Warning:By creating namespaces with the same name as public top-level domains, Services in these namespaces can have short DNS names that overlap with public DNS records. Workloads from any namespace performing a DNS lookup without a trailing dot will be redirected to those services, taking precedence over public DNS.To mitigate this, limit privileges for creating namespaces to trusted users. If required, you could additionally configure third-party security controls, such as admission webhooks, to block creating any namespace with the name of public TLDs.Not all objects are in a namespaceMost Kubernetes resources (e.g. pods, services, replication controllers, and others) are in some namespaces. However namespace resources are not themselves in a namespace. And low-level resources, such as nodes and persistentVolumes, are not in any namespace.To see which Kubernetes resources are and aren't in a namespace:# In a namespace kubectl api-resources --namespaced=true # Not in a namespace kubectl api-resources --namespaced=false Automatic labellingFEATURE STATE: Kubernetes 1.22 [stable]The Kubernetes control plane sets an immutable label kubernetes.io/metadata.name on all namespaces. The value of the label is the namespace name.What's nextLearn more about creating a new namespace.Learn more about deleting a namespace.3.1.2.5 - AnnotationsYou can use Kubernetes annotations to attach arbitrary non-identifying metadata to objects. Clients such as tools and libraries can retrieve this metadata.Attaching metadata to objectsYou can use either labels or annotations to attach metadata to Kubernetes objects. Labels can be used to select objects and to find collections of objects that satisfy certain conditions. In contrast, annotations are not used to identify and select objects. The metadata in an annotation can be small or large, structured or unstructured, and can include characters not permitted by labels. It is possible to use labels as well as annotations in the metadata of the same object.Annotations, like labels, are key/value maps:"metadata": { "annotations": { "key1" : "value1", "key2" : "value2" } } Note:The keys and the values in the map must be strings. In other words, you cannot use numeric, boolean, list or other types for either the keys or the values.Here are some examples of information that could be recorded in annotations:Fields managed by a declarative configuration layer. Attaching these fields as annotations distinguishes them from default values set by clients or servers, and from auto-generated fields and fields set by auto-sizing or auto-scaling systems.Build, release, or image information like timestamps, release IDs, git branch, PR numbers, image hashes, and registry address.Pointers to logging, monitoring, analytics, or audit repositories.Client library or tool information that can be used for debugging purposes: for example, name, version, and build information.User or tool/system provenance information, such as URLs of related objects from other ecosystem components.Lightweight rollout tool metadata: for example, config or checkpoints.Phone or pager numbers of persons responsible, or directory entries that specify where that information can be found, such as a team web site.Directives from the end-user to the implementations to modify behavior or engage non-standard features.Instead of using annotations, you could store this type of information in an external database or directory, but that would make it much harder to produce shared client libraries and tools for deployment, management, introspection, and the like.Syntax and character setAnnotations are key/value pairs. Valid annotation keys have two segments: an optional prefix and name, separated by a slash (/). The name segment is required and must be 63 characters or less, beginning and ending with an alphanumeric character ([a-z0-9A-Z]) with dashes (-), underscores (_), dots (.), and alphanumerics between. The prefix is optional. If specified, the prefix must be a DNS subdomain: a series of DNS labels separated by dots (.), not longer than 253 characters in total, followed by a slash (/).If the prefix is omitted, the annotation Key is presumed to be private to the user. Automated system components (e.g. kube-scheduler, kube-controller-manager, kube-apiserver, kubectl, or other third-party automation) which add annotations to end-user objects must specify a prefix.The kubernetes.io/ and k8s.io/ prefixes are reserved for Kubernetes core components.For example, here's a manifest for a Pod that has the annotation imageregistry: https://hub.docker.com/ :apiVersion: v1 kind: Pod metadata: name: annotations-demo annotations: imageregistry: "https://hub.docker.com/" spec: containers: \- name: nginx image: nginx:1.14.2 ports: \- containerPort: 80 What's nextLearn more about Labels and Selectors.Find Well-known labels, Annotations and Taints3.1.2.6 - Field SelectorsField selectors let you select Kubernetes objects based on the value of one or more resource fields. Here are some examples of field selector queries:metadata.name=my-servicemetadata.namespace!=defaultstatus.phase=PendingThis kubectl command selects all Pods for which the value of the status.phase field is Running:kubectl get pods --field-selector status.phase=Running Note:Field selectors are essentially resource filters. By default, no selectors/filters are applied, meaning that all resources of the specified type are selected. This makes the kubectl queries kubectl get pods and kubectl get pods --field-selector "" equivalent.Supported fieldsSupported field selectors vary by Kubernetes resource type. All resource types support the metadata.name and metadata.namespace fields. Using unsupported field selectors produces an error. For example:kubectl get ingress --field-selector foo.bar=baz Error from server (BadRequest): Unable to find "ingresses" that match label selector "", field selector "foo.bar=baz": "foo.bar" is not a known field selector: only "metadata.name", "metadata.namespace" List of supported fieldsKindFieldsPodspec.nodeNamespec.restartPolicyspec.schedulerNamespec.serviceAccountNamespec.hostNetworkstatus.phasestatus.podIPstatus.nominatedNodeNameEventinvolvedObject.kindinvolvedObject.namespaceinvolvedObject.nameinvolvedObject.uidinvolvedObject.apiVersioninvolvedObject.resourceVersioninvolvedObject.fieldPathreasonreportingComponentsourcetypeSecrettypeNamespacestatus.phaseReplicaSetstatus.replicasReplicationControllerstatus.replicasJobstatus.successfulNodespec.unschedulableCertificateSigningRequestspec.signerNameCustom resources fieldsAll custom resource types support the metadata.name and metadata.namespace fields.Additionally, the spec.versions[*].selectableFields field of a CustomResourceDefinition declares which other fields in a custom resource may be used in field selectors. See selectable fields for custom resources for more information about how to use field selectors with CustomResourceDefinitions.Supported operatorsYou can use the =, ==, and != operators with field selectors (= and == mean the same thing). This kubectl command, for example, selects all Kubernetes Services that aren't in the default namespace:kubectl get services --all-namespaces --field-selector metadata.namespace!=default Note:Set-based operators (in, notin, exists) are not supported for field selectors.Chained selectorsAs with label and other selectors, field selectors can be chained together as a comma-separated list. This kubectl command selects all Pods for which the status.phase does not equal Running and the spec.restartPolicy field equals Always:kubectl get pods --field-selector=status.phase!=Running,spec.restartPolicy=Always Multiple resource typesYou can use field selectors across multiple resource types. This kubectl command selects all Statefulsets and Services that are not in the default namespace:kubectl get statefulsets,services --all-namespaces --field-selector metadata.namespace!=default 3.1.2.7 - FinalizersFinalizers are namespaced keys that tell Kubernetes to wait until specific conditions are met before it fully deletes resources marked for deletion. Finalizers alert controllers to clean up resources the deleted object owned.When you tell Kubernetes to delete an object that has finalizers specified for it, the Kubernetes API marks the object for deletion by populating .metadata.deletionTimestamp, and returns a 202 status code (HTTP "Accepted"). The target object remains in a terminating state while the control plane, or other components, take the actions defined by the finalizers. After these actions are complete, the controller removes the relevant finalizers from the target object. When the metadata.finalizers field is empty, Kubernetes considers the deletion complete and deletes the object.You can use finalizers to control garbage collection of resources. For example, you can define a finalizer to clean up related resources or infrastructure before the controller deletes the target resource.You can use finalizers to control garbage collection of objects by alerting controllers to perform specific cleanup tasks before deleting the target resource.Finalizers don't usually specify the code to execute. Instead, they are typically lists of keys on a specific resource similar to annotations. Kubernetes specifies some finalizers automatically, but you can also specify your own.How finalizers workWhen you create a resource using a manifest file, you can specify finalizers in the metadata.finalizers field. When you attempt to delete the resource, the API server handling the delete request notices the values in the finalizers field and does the following:Modifies the object to add a metadata.deletionTimestamp field with the time you started the deletion.Prevents the object from being removed until all items are removed from its metadata.finalizers fieldReturns a 202 status code (HTTP "Accepted")The controller managing that finalizer notices the update to the object setting the metadata.deletionTimestamp, indicating deletion of the object has been requested. The controller then attempts to satisfy the requirements of the finalizers specified for that resource. Each time a finalizer condition is satisfied, the controller removes that key from the resource's finalizers field. When the finalizers field is emptied, an object with a deletionTimestamp field set is automatically deleted. You can also use finalizers to prevent deletion of unmanaged resources.A common example of a finalizer is kubernetes.io/pv-protection, which prevents accidental deletion of PersistentVolume objects. When a PersistentVolume object is in use by a Pod, Kubernetes adds the pv-protection finalizer. If you try to delete the PersistentVolume, it enters a Terminating status, but the controller can't delete it because the finalizer exists. When the Pod stops using the PersistentVolume, Kubernetes clears the pv-protection finalizer, and the controller deletes the volume.Note:When you DELETE an object, Kubernetes adds the deletion timestamp for that object and then immediately starts to restrict changes to the .metadata.finalizers field for the object that is now pending deletion. You can remove existing finalizers (deleting an entry from the finalizers list) but you cannot add a new finalizer. You also cannot modify the deletionTimestamp for an object once it is set.After the deletion is requested, you can not resurrect this object. The only way is to delete it and make a new similar object.Owner references, labels, and finalizersLike labels, owner references describe the relationships between objects in Kubernetes, but are used for a different purpose. When a controller manages objects like Pods, it uses labels to track changes to groups of related objects. For example, when a Job creates one or more Pods, the Job controller applies labels to those pods and tracks changes to any Pods in the cluster with the same label.The Job controller also adds owner references to those Pods, pointing at the Job that created the Pods. If you delete the Job while these Pods are running, Kubernetes uses the owner references (not labels) to determine which Pods in the cluster need cleanup.Kubernetes also processes finalizers when it identifies owner references on a resource targeted for deletion.In some situations, finalizers can block the deletion of dependent objects, which can cause the targeted owner object to remain for longer than expected without being fully deleted. In these situations, you should check finalizers and owner references on the target owner and dependent objects to troubleshoot the cause.Note:In cases where objects are stuck in a deleting state, avoid manually removing finalizers to allow deletion to continue. Finalizers are usually added to resources for a reason, so forcefully removing them can lead to issues in your cluster. This should only be done when the purpose of the finalizer is understood and is accomplished in another way (for example, manually cleaning up some dependent object).What's nextRead Using Finalizers to Control Deletion on the Kubernetes blog.3.1.2.8 - Owners and DependentsIn Kubernetes, some objects are owners of other objects. For example, a ReplicaSet is the owner of a set of Pods. These owned objects are dependents of their owner.Ownership is different from the labels and selectors mechanism that some resources also use. For example, consider a Service that creates EndpointSlice objects. The Service uses labels to allow the control plane to determine which EndpointSlice objects are used for that Service. In addition to the labels, each EndpointSlice that is managed on behalf of a Service has an owner reference. Owner references help different parts of Kubernetes avoid interfering with objects they don’t control.Owner references in object specificationsDependent objects have a metadata.ownerReferences field that references their owner object. A valid owner reference consists of the object name and a UID within the same namespace as the dependent object. Kubernetes sets the value of this field automatically for objects that are dependents of other objects like ReplicaSets, DaemonSets, Deployments, Jobs and CronJobs, and ReplicationControllers. You can also configure these relationships manually by changing the value of this field. However, you usually don't need to and can allow Kubernetes to automatically manage the relationships.Dependent objects also have an ownerReferences.blockOwnerDeletion field that takes a boolean value and controls whether specific dependents can block garbage collection from deleting their owner object. Kubernetes automatically sets this field to true if a controller (for example, the Deployment controller) sets the value of the metadata.ownerReferences field. You can also set the value of the blockOwnerDeletion field manually to control which dependents block garbage collection.A Kubernetes admission controller controls user access to change this field for dependent resources, based on the delete permissions of the owner. This control prevents unauthorized users from delaying owner object deletion.Note:Cross-namespace owner references are disallowed by design. Namespaced dependents can specify cluster-scoped or namespaced owners. A namespaced owner must exist in the same namespace as the dependent. If it does not, the owner reference is treated as absent, and the dependent is subject to deletion once all owners are verified absent.Cluster-scoped dependents can only specify cluster-scoped owners. In v1.20+, if a cluster-scoped dependent specifies a namespaced kind as an owner, it is treated as having an unresolvable owner reference, and is not able to be garbage collected.In v1.20+, if the garbage collector detects an invalid cross-namespace ownerReference, or a cluster-scoped dependent with an ownerReference referencing a namespaced kind, a warning Event with a reason of OwnerRefInvalidNamespace and an involvedObject of the invalid dependent is reported. You can check for that kind of Event by running kubectl get events -A --field-selector=reason=OwnerRefInvalidNamespace.Ownership and finalizersWhen you tell Kubernetes to delete a resource, the API server allows the managing controller to process any finalizer rules for the resource. Finalizers prevent accidental deletion of resources your cluster may still need to function correctly. For example, if you try to delete a PersistentVolume that is still in use by a Pod, the deletion does not happen immediately because the PersistentVolume has the kubernetes.io/pv-protection finalizer on it. Instead, the volume remains in the Terminating status until Kubernetes clears the finalizer, which only happens after the PersistentVolume is no longer bound to a Pod.Kubernetes also adds finalizers to an owner resource when you use either foreground or orphan cascading deletion. In foreground deletion, it adds the foreground finalizer so that the controller must delete dependent resources that also have ownerReferences.blockOwnerDeletion=true before it deletes the owner. If you specify an orphan deletion policy, Kubernetes adds the orphan finalizer so that the controller ignores dependent resources after it deletes the owner object.What's nextLearn more about Kubernetes finalizers.Learn about garbage collection.Read the API reference for object metadata.3.1.2.9 - Recommended LabelsYou can visualize and manage Kubernetes objects with more tools than kubectl and the dashboard. A common set of labels allows tools to work interoperably, describing objects in a common manner that all tools can understand.In addition to supporting tooling, the recommended labels describe applications in a way that can be queried.The metadata is organized around the concept of an application. Kubernetes is not a platform as a service (PaaS) and doesn't have or enforce a formal notion of an application. Instead, applications are informal and described with metadata. The definition of what an application contains is loose.Note:These are recommended labels. They make it easier to manage applications but aren't required for any core tooling.Shared labels and annotations share a common prefix: app.kubernetes.io. Labels without a prefix are private to users. The shared prefix ensures that shared labels do not interfere with custom user labels.LabelsIn order to take full advantage of using these labels, they should be applied on every resource object.KeyDescriptionExampleTypeapp.kubernetes.io/nameThe name of the applicationmysqlstringapp.kubernetes.io/instanceA unique name identifying the instance of an applicationmysql-abcxyzstringapp.kubernetes.io/versionThe current version of the application (e.g., a SemVer 1.0, revision hash, etc.)5.7.21stringapp.kubernetes.io/componentThe component within the architecturedatabasestringapp.kubernetes.io/part-ofThe name of a higher level application this one is part ofwordpressstringapp.kubernetes.io/managed-byThe tool being used to manage the operation of an applicationHelmstringTo illustrate these labels in action, consider the following StatefulSet object:# This is an excerpt apiVersion: apps/v1 kind: StatefulSet metadata: labels: app.kubernetes.io/name: mysql app.kubernetes.io/instance: mysql-abcxyz app.kubernetes.io/version: "5.7.21" app.kubernetes.io/component: database app.kubernetes.io/part-of: wordpress app.kubernetes.io/managed-by: Helm Applications And Instances Of ApplicationsAn application can be installed one or more times into a Kubernetes cluster and, in some cases, the same namespace. For example, WordPress can be installed more than once where different websites are different installations of WordPress.The name of an application and the instance name are recorded separately. For example, WordPress has a app.kubernetes.io/name of wordpress while it has an instance name, represented as app.kubernetes.io/instance with a value of wordpress-abcxyz. This enables the application and instance of the application to be identifiable. Every instance of an application must have a unique name.ExamplesTo illustrate different ways to use these labels the following examples have varying complexity.A Simple Stateless ServiceConsider the case for a simple stateless service deployed using Deployment and Service objects. The following two snippets represent how the labels could be used in their simplest form.The Deployment is used to oversee the pods running the application itself.apiVersion: apps/v1 kind: Deployment metadata: labels: app.kubernetes.io/name: myservice app.kubernetes.io/instance: myservice-abcxyz ... The Service is used to expose the application.apiVersion: v1 kind: Service metadata: labels: app.kubernetes.io/name: myservice app.kubernetes.io/instance: myservice-abcxyz ... Web Application With A DatabaseConsider a slightly more complicated application: a web application (WordPress) using a database (MySQL), installed using Helm. The following snippets illustrate the start of objects used to deploy this application.The start to the following Deployment is used for WordPress:apiVersion: apps/v1 kind: Deployment metadata: labels: app.kubernetes.io/name: wordpress app.kubernetes.io/instance: wordpress-abcxyz app.kubernetes.io/version: "4.9.4" app.kubernetes.io/managed-by: Helm app.kubernetes.io/component: server app.kubernetes.io/part-of: wordpress ... The Service is used to expose WordPress:apiVersion: v1 kind: Service metadata: labels: app.kubernetes.io/name: wordpress app.kubernetes.io/instance: wordpress-abcxyz app.kubernetes.io/version: "4.9.4" app.kubernetes.io/managed-by: Helm app.kubernetes.io/component: server app.kubernetes.io/part-of: wordpress ... MySQL is exposed as a StatefulSet with metadata for both it and the larger application it belongs to:apiVersion: apps/v1 kind: StatefulSet metadata: labels: app.kubernetes.io/name: mysql app.kubernetes.io/instance: mysql-abcxyz app.kubernetes.io/version: "5.7.21" app.kubernetes.io/managed-by: Helm app.kubernetes.io/component: database app.kubernetes.io/part-of: wordpress ... The Service is used to expose MySQL as part of WordPress:apiVersion: v1 kind: Service metadata: labels: app.kubernetes.io/name: mysql app.kubernetes.io/instance: mysql-abcxyz app.kubernetes.io/version: "5.7.21" app.kubernetes.io/managed-by: Helm app.kubernetes.io/component: database app.kubernetes.io/part-of: wordpress ... With the MySQL StatefulSet and Service you'll notice information about both MySQL and WordPress, the broader application, are included.3.1.3 - The Kubernetes APIThe Kubernetes API lets you query and manipulate the state of objects in Kubernetes. The core of Kubernetes' control plane is the API server and the HTTP API that it exposes. Users, the different parts of your cluster, and external components all communicate with one another through the API server.The core of Kubernetes' control plane is the API server. The API server exposes an HTTP API that lets end users, different parts of your cluster, and external components communicate with one another.The Kubernetes API lets you query and manipulate the state of API objects in Kubernetes (for example: Pods, Namespaces, ConfigMaps, and Events).Most operations can be performed through the kubectl command-line interface or other command-line tools, such as kubeadm, which in turn use the API. However, you can also access the API directly using REST calls. Kubernetes provides a set of client libraries for those looking to write applications using the Kubernetes API.Each Kubernetes cluster publishes the specification of the APIs that the cluster serves. There are two mechanisms that Kubernetes uses to publish these API specifications; both are useful to enable automatic interoperability. For example, the kubectl tool fetches and caches the API specification for enabling command-line completion and other features. The two supported mechanisms are as follows:The Discovery API provides information about the Kubernetes APIs: API names, resources, versions, and supported operations. This is a Kubernetes specific term as it is a separate API from the Kubernetes OpenAPI. It is intended to be a brief summary of the available resources and it does not detail specific schema for the resources. For reference about resource schemas, please refer to the OpenAPI document.The Kubernetes OpenAPI Document provides (full) OpenAPI v2.0 and 3.0 schemas for all Kubernetes API endpoints. The OpenAPI v3 is the preferred method for accessing OpenAPI as it provides a more comprehensive and accurate view of the API. It includes all the available API paths, as well as all resources consumed and produced for every operations on every endpoints. It also includes any extensibility components that a cluster supports. The data is a complete specification and is significantly larger than that from the Discovery API.Discovery APIKubernetes publishes a list of all group versions and resources supported via the Discovery API. This includes the following for each resource:NameCluster or namespaced scopeEndpoint URL and supported verbsAlternative namesGroup, version, kindThe API is available in both aggregated and unaggregated form. The aggregated discovery serves two endpoints, while the unaggregated discovery serves a separate endpoint for each group version.Aggregated discoveryFEATURE STATE: Kubernetes v1.30 [stable] (enabled by default: true)Kubernetes offers stable support for aggregated discovery, publishing all resources supported by a cluster through two endpoints (/api and /apis). Requesting this endpoint drastically reduces the number of requests sent to fetch the discovery data from the cluster. You can access the data by requesting the respective endpoints with an Accept header indicating the aggregated discovery resource: Accept: application/json;v=v2;g=apidiscovery.k8s.io;as=APIGroupDiscoveryList.Without indicating the resource type using the Accept header, the default response for the /api and /apis endpoint is an unaggregated discovery document.The discovery document for the built-in resources can be found in the Kubernetes GitHub repository. This Github document can be used as a reference of the base set of the available resources if a Kubernetes cluster is not available to query.The endpoint also supports ETag and protobuf encoding.Unaggregated discoveryWithout discovery aggregation, discovery is published in levels, with the root endpoints publishing discovery information for downstream documents.A list of all group versions supported by a cluster is published at the /api and /apis endpoints. Example:{ "kind": "APIGroupList", "apiVersion": "v1", "groups": [ { "name": "apiregistration.k8s.io", "versions": [ { "groupVersion": "apiregistration.k8s.io/v1", "version": "v1" } ], "preferredVersion": { "groupVersion": "apiregistration.k8s.io/v1", "version": "v1" } }, { "name": "apps", "versions": [ { "groupVersion": "apps/v1", "version": "v1" } ], "preferredVersion": { "groupVersion": "apps/v1", "version": "v1" } }, ... } Additional requests are needed to obtain the discovery document for each group version at /apis// (for example: /apis/rbac.authorization.k8s.io/v1alpha1), which advertises the list of resources served under a particular group version. These endpoints are used by kubectl to fetch the list of resources supported by a cluster.OpenAPI interface definitionFor details about the OpenAPI specifications, see the OpenAPI documentation.Kubernetes serves both OpenAPI v2.0 and OpenAPI v3.0. OpenAPI v3 is the preferred method of accessing the OpenAPI because it offers a more comprehensive (lossless) representation of Kubernetes resources. Due to limitations of OpenAPI version 2, certain fields are dropped from the published OpenAPI including but not limited to default, nullable, oneOf.OpenAPI V2The Kubernetes API server serves an aggregated OpenAPI v2 spec via the /openapi/v2 endpoint. You can request the response format using request headers as follows:Valid request header values for OpenAPI v2 queriesHeaderPossible valuesNotesAccept-Encodinggzipnot supplying this header is also acceptableAcceptapplication/com.github.proto-openapi.spec.v2@v1.0+protobufmainly for intra-cluster useapplication/jsondefault*serves application/jsonWarning:The validation rules published as part of OpenAPI schemas may not be complete, and usually aren't. Additional validation occurs within the API server. If you want precise and complete verification, a kubectl apply --dry-run=server runs all the applicable validation (and also activates admission-time checks).OpenAPI V3FEATURE STATE: Kubernetes v1.27 [stable] (enabled by default: true)Kubernetes supports publishing a description of its APIs as OpenAPI v3.A discovery endpoint /openapi/v3 is provided to see a list of all group/versions available. This endpoint only returns JSON. These group/versions are provided in the following format:{ "paths": { ..., "api/v1": { "serverRelativeURL": "/openapi/v3/api/v1?hash=CC0E9BFD992D8C59AEC98A1E2336F899E8318D3CF4C68944C3DEC640AF5AB52D864AC50DAA8D145B3494F75FA3CFF939FCBDDA431DAD3CA79738B297795818CF" }, "apis/admissionregistration.k8s.io/v1": { "serverRelativeURL": "/openapi/v3/apis/admissionregistration.k8s.io/v1?hash=E19CC93A116982CE5422FC42B590A8AFAD92CDE9AE4D59B5CAAD568F083AD07946E6CB5817531680BCE6E215C16973CD39003B0425F3477CFD854E89A9DB6597" }, .... } } The relative URLs are pointing to immutable OpenAPI descriptions, in order to improve client-side caching. The proper HTTP caching headers are also set by the API server for that purpose (Expires to 1 year in the future, and Cache-Control to immutable). When an obsolete URL is used, the API server returns a redirect to the newest URL.The Kubernetes API server publishes an OpenAPI v3 spec per Kubernetes group version at the /openapi/v3/apis//?hash= endpoint.Refer to the table below for accepted request headers.Valid request header values for OpenAPI v3 queriesHeaderPossible valuesNotesAccept-Encodinggzipnot supplying this header is also acceptableAcceptapplication/com.github.proto-openapi.spec.v3@v1.0+protobufmainly for intra-cluster useapplication/jsondefault*serves application/jsonA Golang implementation to fetch the OpenAPI V3 is provided in the package k8s.io/client-go/openapi3.Kubernetes 1.33 publishes OpenAPI v2.0 and v3.0; there are no plans to support 3.1 in the near future.Protobuf serializationKubernetes implements an alternative Protobuf based serialization format that is primarily intended for intra-cluster communication. For more information about this format, see the Kubernetes Protobuf serialization design proposal and the Interface Definition Language (IDL) files for each schema located in the Go packages that define the API objects.PersistenceKubernetes stores the serialized state of objects by writing them into etcd.API groups and versioningTo make it easier to eliminate fields or restructure resource representations, Kubernetes supports multiple API versions, each at a different API path, such as /api/v1 or /apis/rbac.authorization.k8s.io/v1alpha1.Versioning is done at the API level rather than at the resource or field level to ensure that the API presents a clear, consistent view of system resources and behavior, and to enable controlling access to end-of-life and/or experimental APIs.To make it easier to evolve and to extend its API, Kubernetes implements API groups that can be enabled or disabled.API resources are distinguished by their API group, resource type, namespace (for namespaced resources), and name. The API server handles the conversion between API versions transparently: all the different versions are actually representations of the same persisted data. The API server may serve the same underlying data through multiple API versions.For example, suppose there are two API versions, v1 and v1beta1, for the same resource. If you originally created an object using the v1beta1 version of its API, you can later read, update, or delete that object using either the v1beta1 or the v1 API version, until the v1beta1 version is deprecated and removed. At that point you can continue accessing and modifying the object using the v1 API.API changesAny system that is successful needs to grow and change as new use cases emerge or existing ones change. Therefore, Kubernetes has designed the Kubernetes API to continuously change and grow. The Kubernetes project aims to not break compatibility with existing clients, and to maintain that compatibility for a length of time so that other projects have an opportunity to adapt.In general, new API resources and new resource fields can be added often and frequently. Elimination of resources or fields requires following the API deprecation policy.Kubernetes makes a strong commitment to maintain compatibility for official Kubernetes APIs once they reach general availability (GA), typically at API version v1. Additionally, Kubernetes maintains compatibility with data persisted via beta API versions of official Kubernetes APIs, and ensures that data can be converted and accessed via GA API versions when the feature goes stable.If you adopt a beta API version, you will need to transition to a subsequent beta or stable API version once the API graduates. The best time to do this is while the beta API is in its deprecation period, since objects are simultaneously accessible via both API versions. Once the beta API completes its deprecation period and is no longer served, the replacement API version must be used.Note:Although Kubernetes also aims to maintain compatibility for alpha APIs versions, in some circumstances this is not possible. If you use any alpha API versions, check the release notes for Kubernetes when upgrading your cluster, in case the API did change in incompatible ways that require deleting all existing alpha objects prior to upgrade.Refer to API versions reference for more details on the API version level definitions.API ExtensionThe Kubernetes API can be extended in one of two ways:Custom resources let you declaratively define how the API server should provide your chosen resource API.You can also extend the Kubernetes API by implementing an aggregation layer.What's nextLearn how to extend the Kubernetes API by adding your own CustomResourceDefinition.Controlling Access To The Kubernetes API describes how the cluster manages authentication and authorization for API access.Learn about API endpoints, resource types and samples by reading API Reference.Learn about what constitutes a compatible change, and how to change the API, from API changes.3.2 - Cluster ArchitectureThe architectural concepts behind Kubernetes.A Kubernetes cluster consists of a control plane plus a set of worker machines, called nodes, that run containerized applications. Every cluster needs at least one worker node in order to run Pods.The worker node(s) host the Pods that are the components of the application workload. The control plane manages the worker nodes and the Pods in the cluster. In production environments, the control plane usually runs across multiple computers and a cluster usually runs multiple nodes, providing fault-tolerance and high availability.This document outlines the various components you need to have for a complete and working Kubernetes cluster.Figure 1. Kubernetes cluster components.About this architectureThe diagram in Figure 1 presents an example reference architecture for a Kubernetes cluster. The actual distribution of components can vary based on specific cluster setups and requirements.In the diagram, each node runs the kube-proxy component. You need a network proxy component on each node to ensure that the Service API and associated behaviors are available on your cluster network. However, some network plugins provide their own, third party implementation of proxying. When you use that kind of network plugin, the node does not need to run kube-proxy.Control plane componentsThe control plane's components make global decisions about the cluster (for example, scheduling), as well as detecting and responding to cluster events (for example, starting up a new pod when a Deployment's replicas field is unsatisfied).Control plane components can be run on any machine in the cluster. However, for simplicity, setup scripts typically start all control plane components on the same machine, and do not run user containers on this machine. See Creating Highly Available clusters with kubeadm for an example control plane setup that runs across multiple machines.kube-apiserverThe API server is a component of the Kubernetes control plane that exposes the Kubernetes API. The API server is the front end for the Kubernetes control plane.The main implementation of a Kubernetes API server is kube-apiserver. kube-apiserver is designed to scale horizontally—that is, it scales by deploying more instances. You can run several instances of kube-apiserver and balance traffic between those instances.etcdConsistent and highly-available key value store used as Kubernetes' backing store for all cluster data.If your Kubernetes cluster uses etcd as its backing store, make sure you have a back up plan for the data.You can find in-depth information about etcd in the official documentation.kube-schedulerControl plane component that watches for newly created Pods with no assigned node, and selects a node for them to run on.Factors taken into account for scheduling decisions include: individual and collective resource requirements, hardware/software/policy constraints, affinity and anti-affinity specifications, data locality, inter-workload interference, and deadlines.kube-controller-managerControl plane component that runs controller processes.Logically, each controller is a separate process, but to reduce complexity, they are all compiled into a single binary and run in a single process.There are many different types of controllers. Some examples of them are:Node controller: Responsible for noticing and responding when nodes go down.Job controller: Watches for Job objects that represent one-off tasks, then creates Pods to run those tasks to completion.EndpointSlice controller: Populates EndpointSlice objects (to provide a link between Services and Pods).ServiceAccount controller: Create default ServiceAccounts for new namespaces.The above is not an exhaustive list.cloud-controller-managerA Kubernetes control plane component that embeds cloud-specific control logic. The cloud controller manager lets you link your cluster into your cloud provider's API, and separates out the components that interact with that cloud platform from components that only interact with your cluster.The cloud-controller-manager only runs controllers that are specific to your cloud provider. If you are running Kubernetes on your own premises, or in a learning environment inside your own PC, the cluster does not have a cloud controller manager.As with the kube-controller-manager, the cloud-controller-manager combines several logically independent control loops into a single binary that you run as a single process. You can scale horizontally (run more than one copy) to improve performance or to help tolerate failures.The following controllers can have cloud provider dependencies:Node controller: For checking the cloud provider to determine if a node has been deleted in the cloud after it stops respondingRoute controller: For setting up routes in the underlying cloud infrastructureService controller: For creating, updating and deleting cloud provider load balancersNode componentsNode components run on every node, maintaining running pods and providing the Kubernetes runtime environment.kubeletAn agent that runs on each node in the cluster. It makes sure that containers are running in a Pod.The kubelet takes a set of PodSpecs that are provided through various mechanisms and ensures that the containers described in those PodSpecs are running and healthy. The kubelet doesn't manage containers which were not created by Kubernetes.kube-proxy (optional)kube-proxy is a network proxy that runs on each node in your cluster, implementing part of the Kubernetes Service concept.kube-proxy maintains network rules on nodes. These network rules allow network communication to your Pods from network sessions inside or outside of your cluster.kube-proxy uses the operating system packet filtering layer if there is one and it's available. Otherwise, kube-proxy forwards the traffic itself.If you use a network plugin that implements packet forwarding for Services by itself, and providing equivalent behavior to kube-proxy, then you do not need to run kube-proxy on the nodes in your cluster.Container runtimeA fundamental component that empowers Kubernetes to run containers effectively. It is responsible for managing the execution and lifecycle of containers within the Kubernetes environment.Kubernetes supports container runtimes such as containerd, CRI-O, and any other implementation of the Kubernetes CRI (Container Runtime Interface).AddonsAddons use Kubernetes resources (DaemonSet, Deployment, etc) to implement cluster features. Because these are providing cluster-level features, namespaced resources for addons belong within the kube-system namespace.Selected addons are described below; for an extended list of available addons, please see Addons.DNSWhile the other addons are not strictly required, all Kubernetes clusters should have cluster DNS, as many examples rely on it.Cluster DNS is a DNS server, in addition to the other DNS server(s) in your environment, which serves DNS records for Kubernetes services.Containers started by Kubernetes automatically include this DNS server in their DNS searches.Web UI (Dashboard)Dashboard is a general purpose, web-based UI for Kubernetes clusters. It allows users to manage and troubleshoot applications running in the cluster, as well as the cluster itself.Container resource monitoringContainer Resource Monitoring records generic time-series metrics about containers in a central database, and provides a UI for browsing that data.Cluster-level LoggingA cluster-level logging mechanism is responsible for saving container logs to a central log store with a search/browsing interface.Network pluginsNetwork plugins are software components that implement the container network interface (CNI) specification. They are responsible for allocating IP addresses to pods and enabling them to communicate with each other within the cluster.Architecture variationsWhile the core components of Kubernetes remain consistent, the way they are deployed and managed can vary. Understanding these variations is crucial for designing and maintaining Kubernetes clusters that meet specific operational needs.Control plane deployment optionsThe control plane components can be deployed in several ways:Traditional deploymentControl plane components run directly on dedicated machines or VMs, often managed as systemd services.Static PodsControl plane components are deployed as static Pods, managed by the kubelet on specific nodes. This is a common approach used by tools like kubeadm.Self-hostedThe control plane runs as Pods within the Kubernetes cluster itself, managed by Deployments and StatefulSets or other Kubernetes primitives.Managed Kubernetes servicesCloud providers often abstract away the control plane, managing its components as part of their service offering.Workload placement considerationsThe placement of workloads, including the control plane components, can vary based on cluster size, performance requirements, and operational policies:In smaller or development clusters, control plane components and user workloads might run on the same nodes.Larger production clusters often dedicate specific nodes to control plane components, separating them from user workloads.Some organizations run critical add-ons or monitoring tools on control plane nodes.Cluster management toolsTools like kubeadm, kops, and Kubespray offer different approaches to deploying and managing clusters, each with its own method of component layout and management.The flexibility of Kubernetes architecture allows organizations to tailor their clusters to specific needs, balancing factors such as operational complexity, performance, and management overhead.Customization and extensibilityKubernetes architecture allows for significant customization:Custom schedulers can be deployed to work alongside the default Kubernetes scheduler or to replace it entirely.API servers can be extended with CustomResourceDefinitions and API Aggregation.Cloud providers can integrate deeply with Kubernetes using the cloud-controller-manager.The flexibility of Kubernetes architecture allows organizations to tailor their clusters to specific needs, balancing factors such as operational complexity, performance, and management overhead.What's nextLearn more about the following:Nodes and their communication with the control plane.Kubernetes controllers.kube-scheduler which is the default scheduler for Kubernetes.Etcd's official documentation.Several container runtimes in Kubernetes.Integrating with cloud providers using cloud-controller-manager.kubectl commands.3.2.1 - NodesKubernetes runs your workload by placing containers into Pods to run on Nodes. A node may be a virtual or physical machine, depending on the cluster. Each node is managed by the control plane and contains the services necessary to run Pods.Typically you have several nodes in a cluster; in a learning or resource-limited environment, you might have only one node.The components on a node include the kubelet, a container runtime, and the kube-proxy.ManagementThere are two main ways to have Nodes added to the API server:The kubelet on a node self-registers to the control planeYou (or another human user) manually add a Node objectAfter you create a Node object, or the kubelet on a node self-registers, the control plane checks whether the new Node object is valid. For example, if you try to create a Node from the following JSON manifest:{ "kind": "Node", "apiVersion": "v1", "metadata": { "name": "10.240.79.157", "labels": { "name": "my-first-k8s-node" } } } Kubernetes creates a Node object internally (the representation). Kubernetes checks that a kubelet has registered to the API server that matches the metadata.name field of the Node. If the node is healthy (i.e. all necessary services are running), then it is eligible to run a Pod. Otherwise, that node is ignored for any cluster activity until it becomes healthy.Note:Kubernetes keeps the object for the invalid Node and continues checking to see whether it becomes healthy.You, or a controller, must explicitly delete the Node object to stop that health checking.The name of a Node object must be a valid DNS subdomain name.Node name uniquenessThe name identifies a Node. Two Nodes cannot have the same name at the same time. Kubernetes also assumes that a resource with the same name is the same object. In case of a Node, it is implicitly assumed that an instance using the same name will have the same state (e.g. network settings, root disk contents) and attributes like node labels. This may lead to inconsistencies if an instance was modified without changing its name. If the Node needs to be replaced or updated significantly, the existing Node object needs to be removed from API server first and re-added after the update.Self-registration of NodesWhen the kubelet flag --register-node is true (the default), the kubelet will attempt to register itself with the API server. This is the preferred pattern, used by most distros.For self-registration, the kubelet is started with the following options:--kubeconfig - Path to credentials to authenticate itself to the API server.--cloud-provider - How to talk to a cloud provider to read metadata about itself.--register-node - Automatically register with the API server.--register-with-taints - Register the node with the given list of taints (comma separated =:).No-op if register-node is false.--node-ip - Optional comma-separated list of the IP addresses for the node. You can only specify a single address for each address family. For example, in a single-stack IPv4 cluster, you set this value to be the IPv4 address that the kubelet should use for the node. See configure IPv4/IPv6 dual stack for details of running a dual-stack cluster.If you don't provide this argument, the kubelet uses the node's default IPv4 address, if any; if the node has no IPv4 addresses then the kubelet uses the node's default IPv6 address.--node-labels - Labels to add when registering the node in the cluster (see label restrictions enforced by the NodeRestriction admission plugin).--node-status-update-frequency - Specifies how often kubelet posts its node status to the API server.When the Node authorization mode and NodeRestriction admission plugin are enabled, kubelets are only authorized to create/modify their own Node resource.Note:As mentioned in the Node name uniqueness section, when Node configuration needs to be updated, it is a good practice to re-register the node with the API server. For example, if the kubelet is being restarted with a new set of --node-labels, but the same Node name is used, the change will not take effect, as labels are only set (or modified) upon Node registration with the API server.Pods already scheduled on the Node may misbehave or cause issues if the Node configuration will be changed on kubelet restart. For example, already running Pod may be tainted against the new labels assigned to the Node, while other Pods, that are incompatible with that Pod will be scheduled based on this new label. Node re-registration ensures all Pods will be drained and properly re-scheduled.Manual Node administrationYou can create and modify Node objects using kubectl.When you want to create Node objects manually, set the kubelet flag --register-node=false.You can modify Node objects regardless of the setting of --register-node. For example, you can set labels on an existing Node or mark it unschedulable.You can set optional node role(s) for nodes by adding one or more node-role.kubernetes.io/:  labels to the node where characters of  are limited by the syntax rules for labels.Kubernetes ignores the label value for node roles; by convention, you can set it to the same string you used for the node role in the label key.You can use labels on Nodes in conjunction with node selectors on Pods to control scheduling. For example, you can constrain a Pod to only be eligible to run on a subset of the available nodes.Marking a node as unschedulable prevents the scheduler from placing new pods onto that Node but does not affect existing Pods on the Node. This is useful as a preparatory step before a node reboot or other maintenance.To mark a Node unschedulable, run:kubectl cordon $NODENAME See Safely Drain a Node for more details.Note:Pods that are part of a DaemonSet tolerate being run on an unschedulable Node. DaemonSets typically provide node-local services that should run on the Node even if it is being drained of workload applications.Node statusA Node's status contains the following information:AddressesConditionsCapacity and AllocatableInfoYou can use kubectl to view a Node's status and other details:kubectl describe node  See Node Status for more details.Node heartbeatsHeartbeats, sent by Kubernetes nodes, help your cluster determine the availability of each node, and to take action when failures are detected.For nodes there are two forms of heartbeats:Updates to the .status of a Node.Lease objects within the kube-node-lease namespace. Each Node has an associated Lease object.Node controllerThe node controller is a Kubernetes control plane component that manages various aspects of nodes.The node controller has multiple roles in a node's life. The first is assigning a CIDR block to the node when it is registered (if CIDR assignment is turned on).The second is keeping the node controller's internal list of nodes up to date with the cloud provider's list of available machines. When running in a cloud environment and whenever a node is unhealthy, the node controller asks the cloud provider if the VM for that node is still available. If not, the node controller deletes the node from its list of nodes.The third is monitoring the nodes' health. The node controller is responsible for:In the case that a node becomes unreachable, updating the Ready condition in the Node's .status field. In this case the node controller sets the Ready condition to Unknown.If a node remains unreachable: triggering API-initiated eviction for all of the Pods on the unreachable node. By default, the node controller waits 5 minutes between marking the node as Unknown and submitting the first eviction request.By default, the node controller checks the state of each node every 5 seconds. This period can be configured using the --node-monitor-period flag on the kube-controller-manager component.Rate limits on evictionIn most cases, the node controller limits the eviction rate to \--node-eviction-rate (default 0.1) per second, meaning it won't evict pods from more than 1 node per 10 seconds.The node eviction behavior changes when a node in a given availability zone becomes unhealthy. The node controller checks what percentage of nodes in the zone are unhealthy (the Ready condition is Unknown or False) at the same time:If the fraction of unhealthy nodes is at least --unhealthy-zone-threshold (default 0.55), then the eviction rate is reduced.If the cluster is small (i.e. has less than or equal to \--large-cluster-size-threshold nodes - default 50), then evictions are stopped.Otherwise, the eviction rate is reduced to --secondary-node-eviction-rate (default 0.01) per second.The reason these policies are implemented per availability zone is because one availability zone might become partitioned from the control plane while the others remain connected. If your cluster does not span multiple cloud provider availability zones, then the eviction mechanism does not take per-zone unavailability into account.A key reason for spreading your nodes across availability zones is so that the workload can be shifted to healthy zones when one entire zone goes down. Therefore, if all nodes in a zone are unhealthy, then the node controller evicts at the normal rate of --node-eviction-rate. The corner case is when all zones are completely unhealthy (none of the nodes in the cluster are healthy). In such a case, the node controller assumes that there is some problem with connectivity between the control plane and the nodes, and doesn't perform any evictions. (If there has been an outage and some nodes reappear, the node controller does evict pods from the remaining nodes that are unhealthy or unreachable).The node controller is also responsible for evicting pods running on nodes with NoExecute taints, unless those pods tolerate that taint. The node controller also adds taints corresponding to node problems like node unreachable or not ready. This means that the scheduler won't place Pods onto unhealthy nodes.Resource capacity trackingNode objects track information about the Node's resource capacity: for example, the amount of memory available and the number of CPUs. Nodes that self register report their capacity during registration. If you manually add a Node, then you need to set the node's capacity information when you add it.The Kubernetes scheduler ensures that there are enough resources for all the Pods on a Node. The scheduler checks that the sum of the requests of containers on the node is no greater than the node's capacity. That sum of requests includes all containers managed by the kubelet, but excludes any containers started directly by the container runtime, and also excludes any processes running outside of the kubelet's control.Note:If you want to explicitly reserve resources for non-Pod processes, see reserve resources for system daemons.Node topologyFEATURE STATE: Kubernetes v1.27 [stable] (enabled by default: true)If you have enabled the TopologyManager feature gate, then the kubelet can use topology hints when making resource assignment decisions. See Control Topology Management Policies on a Node for more information.Swap memory managementFEATURE STATE: Kubernetes v1.30 [beta] (enabled by default: true)To enable swap on a node, the NodeSwap feature gate must be enabled on the kubelet (default is true), and the --fail-swap-on command line flag or failSwapOn configuration setting must be set to false. To allow Pods to utilize swap, swapBehavior should not be set to NoSwap (which is the default behavior) in the kubelet config.Warning:When the memory swap feature is turned on, Kubernetes data such as the content of Secret objects that were written to tmpfs now could be swapped to disk.A user can also optionally configure memorySwap.swapBehavior in order to specify how a node will use swap memory. For example,memorySwap: swapBehavior: LimitedSwap NoSwap (default): Kubernetes workloads will not use swap.LimitedSwap: The utilization of swap memory by Kubernetes workloads is subject to limitations. Only Pods of Burstable QoS are permitted to employ swap.If configuration for memorySwap is not specified and the feature gate is enabled, by default the kubelet will apply the same behaviour as the NoSwap setting.With LimitedSwap, Pods that do not fall under the Burstable QoS classification (i.e. BestEffort/Guaranteed Qos Pods) are prohibited from utilizing swap memory. To maintain the aforementioned security and node health guarantees, these Pods are not permitted to use swap memory when LimitedSwap is in effect.Prior to detailing the calculation of the swap limit, it is necessary to define the following terms:nodeTotalMemory: The total amount of physical memory available on the node.totalPodsSwapAvailable: The total amount of swap memory on the node that is available for use by Pods (some swap memory may be reserved for system use).containerMemoryRequest: The container's memory request.Swap limitation is configured as: (containerMemoryRequest / nodeTotalMemory) * totalPodsSwapAvailable.It is important to note that, for containers within Burstable QoS Pods, it is possible to opt-out of swap usage by specifying memory requests that are equal to memory limits. Containers configured in this manner will not have access to swap memory.Swap is supported only with cgroup v2, cgroup v1 is not supported.For more information, and to assist with testing and provide feedback, please see the blog-post about Kubernetes 1.28: NodeSwap graduates to Beta1, KEP-2400 and its design proposal.What's nextLearn more about the following:Components that make up a node.API definition for Node.Node section of the architecture design document.Graceful/non-graceful node shutdown.Node autoscaling to manage the number and size of nodes in your cluster.Taints and Tolerations.Node Resource Managers.Resource Management for Windows nodes.3.2.2 - Communication between Nodes and the Control PlaneThis document catalogs the communication paths between the API server and the Kubernetes cluster. The intent is to allow users to customize their installation to harden the network configuration such that the cluster can be run on an untrusted network (or on fully public IPs on a cloud provider).Node to Control PlaneKubernetes has a "hub-and-spoke" API pattern. All API usage from nodes (or the pods they run) terminates at the API server. None of the other control plane components are designed to expose remote services. The API server is configured to listen for remote connections on a secure HTTPS port (typically 443) with one or more forms of client authentication enabled. One or more forms of authorization should be enabled, especially if anonymous requests or service account tokens are allowed.Nodes should be provisioned with the public root certificate for the cluster such that they can connect securely to the API server along with valid client credentials. A good approach is that the client credentials provided to the kubelet are in the form of a client certificate. See kubelet TLS bootstrapping for automated provisioning of kubelet client certificates.Pods that wish to connect to the API server can do so securely by leveraging a service account so that Kubernetes will automatically inject the public root certificate and a valid bearer token into the pod when it is instantiated. The kubernetes service (in default namespace) is configured with a virtual IP address that is redirected (via kube-proxy) to the HTTPS endpoint on the API server.The control plane components also communicate with the API server over the secure port.As a result, the default operating mode for connections from the nodes and pod running on the nodes to the control plane is secured by default and can run over untrusted and/or public networks.Control plane to nodeThere are two primary communication paths from the control plane (the API server) to the nodes. The first is from the API server to the kubelet process which runs on each node in the cluster. The second is from the API server to any node, pod, or service through the API server's proxy functionality.API server to kubeletThe connections from the API server to the kubelet are used for:Fetching logs for pods.Attaching (usually through kubectl) to running pods.Providing the kubelet's port-forwarding functionality.These connections terminate at the kubelet's HTTPS endpoint. By default, the API server does not verify the kubelet's serving certificate, which makes the connection subject to man-in-the-middle attacks and unsafe to run over untrusted and/or public networks.To verify this connection, use the --kubelet-certificate-authority flag to provide the API server with a root certificate bundle to use to verify the kubelet's serving certificate.If that is not possible, use SSH tunneling between the API server and kubelet if required to avoid connecting over an untrusted or public network.Finally, Kubelet authentication and/or authorization should be enabled to secure the kubelet API.API server to nodes, pods, and servicesThe connections from the API server to a node, pod, or service default to plain HTTP connections and are therefore neither authenticated nor encrypted. They can be run over a secure HTTPS connection by prefixing https: to the node, pod, or service name in the API URL, but they will not validate the certificate provided by the HTTPS endpoint nor provide client credentials. So while the connection will be encrypted, it will not provide any guarantees of integrity. These connections are not currently safe to run over untrusted or public networks.SSH tunnelsKubernetes supports SSH tunnels to protect the control plane to nodes communication paths. In this configuration, the API server initiates an SSH tunnel to each node in the cluster (connecting to the SSH server listening on port 22) and passes all traffic destined for a kubelet, node, pod, or service through the tunnel. This tunnel ensures that the traffic is not exposed outside of the network in which the nodes are running.Note:SSH tunnels are currently deprecated, so you shouldn't opt to use them unless you know what you are doing. The Konnectivity service is a replacement for this communication channel.Konnectivity serviceFEATURE STATE: Kubernetes v1.18 [beta]As a replacement to the SSH tunnels, the Konnectivity service provides TCP level proxy for the control plane to cluster communication. The Konnectivity service consists of two parts: the Konnectivity server in the control plane network and the Konnectivity agents in the nodes network. The Konnectivity agents initiate connections to the Konnectivity server and maintain the network connections. After enabling the Konnectivity service, all control plane to nodes traffic goes through these connections.Follow the Konnectivity service task to set up the Konnectivity service in your cluster.What's nextRead about the Kubernetes control plane componentsLearn more about Hubs and Spoke modelLearn how to Secure a ClusterLearn more about the Kubernetes APISet up Konnectivity serviceUse Port Forwarding to Access Applications in a ClusterLearn how to Fetch logs for Pods, use kubectl port-forward3.2.3 - ControllersIn robotics and automation, a control loop is a non-terminating loop that regulates the state of a system.Here is one example of a control loop: a thermostat in a room.When you set the temperature, that's telling the thermostat about your desired state. The actual room temperature is the current state. The thermostat acts to bring the current state closer to the desired state, by turning equipment on or off.In Kubernetes, controllers are control loops that watch the state of your cluster, then make or request changes where needed. Each controller tries to move the current cluster state closer to the desired state.Controller patternA controller tracks at least one Kubernetes resource type. These objects have a spec field that represents the desired state. The controller(s) for that resource are responsible for making the current state come closer to that desired state.The controller might carry the action out itself; more commonly, in Kubernetes, a controller will send messages to the API server that have useful side effects. You'll see examples of this below.Control via API serverThe Job controller is an example of a Kubernetes built-in controller. Built-in controllers manage state by interacting with the cluster API server.Job is a Kubernetes resource that runs a Pod, or perhaps several Pods, to carry out a task and then stop.(Once scheduled, Pod objects become part of the desired state for a kubelet).When the Job controller sees a new task it makes sure that, somewhere in your cluster, the kubelets on a set of Nodes are running the right number of Pods to get the work done. The Job controller does not run any Pods or containers itself. Instead, the Job controller tells the API server to create or remove Pods. Other components in the control plane act on the new information (there are new Pods to schedule and run), and eventually the work is done.After you create a new Job, the desired state is for that Job to be completed. The Job controller makes the current state for that Job be nearer to your desired state: creating Pods that do the work you wanted for that Job, so that the Job is closer to completion.Controllers also update the objects that configure them. For example: once the work is done for a Job, the Job controller updates that Job object to mark it Finished.(This is a bit like how some thermostats turn a light off to indicate that your room is now at the temperature you set).Direct controlIn contrast with Job, some controllers need to make changes to things outside of your cluster.For example, if you use a control loop to make sure there are enough Nodes in your cluster, then that controller needs something outside the current cluster to set up new Nodes when needed.Controllers that interact with external state find their desired state from the API server, then communicate directly with an external system to bring the current state closer in line.(There actually is a controller that horizontally scales the nodes in your cluster.)The important point here is that the controller makes some changes to bring about your desired state, and then reports the current state back to your cluster's API server. Other control loops can observe that reported data and take their own actions.In the thermostat example, if the room is very cold then a different controller might also turn on a frost protection heater. With Kubernetes clusters, the control plane indirectly works with IP address management tools, storage services, cloud provider APIs, and other services by extending Kubernetes to implement that.Desired versus current stateKubernetes takes a cloud-native view of systems, and is able to handle constant change.Your cluster could be changing at any point as work happens and control loops automatically fix failures. This means that, potentially, your cluster never reaches a stable state.As long as the controllers for your cluster are running and able to make useful changes, it doesn't matter if the overall state is stable or not.DesignAs a tenet of its design, Kubernetes uses lots of controllers that each manage a particular aspect of cluster state. Most commonly, a particular control loop (controller) uses one kind of resource as its desired state, and has a different kind of resource that it manages to make that desired state happen. For example, a controller for Jobs tracks Job objects (to discover new work) and Pod objects (to run the Jobs, and then to see when the work is finished). In this case something else creates the Jobs, whereas the Job controller creates Pods.It's useful to have simple controllers rather than one, monolithic set of control loops that are interlinked. Controllers can fail, so Kubernetes is designed to allow for that.Note:There can be several controllers that create or update the same kind of object. Behind the scenes, Kubernetes controllers make sure that they only pay attention to the resources linked to their controlling resource.For example, you can have Deployments and Jobs; these both create Pods. The Job controller does not delete the Pods that your Deployment created, because there is information (labels) the controllers can use to tell those Pods apart.Ways of running controllersKubernetes comes with a set of built-in controllers that run inside the kube-controller-manager. These built-in controllers provide important core behaviors.The Deployment controller and Job controller are examples of controllers that come as part of Kubernetes itself ("built-in" controllers). Kubernetes lets you run a resilient control plane, so that if any of the built-in controllers were to fail, another part of the control plane will take over the work.You can find controllers that run outside the control plane, to extend Kubernetes. Or, if you want, you can write a new controller yourself. You can run your own controller as a set of Pods, or externally to Kubernetes. What fits best will depend on what that particular controller does.What's nextRead about the Kubernetes control planeDiscover some of the basic Kubernetes objectsLearn more about the Kubernetes APIIf you want to write your own controller, see Kubernetes extension patterns and the sample-controller repository.3.2.4 - LeasesDistributed systems often have a need for leases, which provide a mechanism to lock shared resources and coordinate activity between members of a set. In Kubernetes, the lease concept is represented by Lease objects in the coordination.k8s.io API Group, which are used for system-critical capabilities such as node heartbeats and component-level leader election.Node heartbeatsKubernetes uses the Lease API to communicate kubelet node heartbeats to the Kubernetes API server. For every Node , there is a Lease object with a matching name in the kube-node-lease namespace. Under the hood, every kubelet heartbeat is an update request to this Lease object, updating the spec.renewTime field for the Lease. The Kubernetes control plane uses the time stamp of this field to determine the availability of this Node.See Node Lease objects for more details.Leader electionKubernetes also uses Leases to ensure only one instance of a component is running at any given time. This is used by control plane components like kube-controller-manager and kube-scheduler in HA configurations, where only one instance of the component should be actively running while the other instances are on stand-by.Read coordinated leader election to learn about how Kubernetes builds on the Lease API to select which component instance acts as leader.API server identityFEATURE STATE: Kubernetes v1.26 [beta] (enabled by default: true)Starting in Kubernetes v1.26, each kube-apiserver uses the Lease API to publish its identity to the rest of the system. While not particularly useful on its own, this provides a mechanism for clients to discover how many instances of kube-apiserver are operating the Kubernetes control plane. Existence of kube-apiserver leases enables future capabilities that may require coordination between each kube-apiserver.You can inspect Leases owned by each kube-apiserver by checking for lease objects in the kube-system namespace with the name apiserver-. Alternatively you can use the label selector apiserver.kubernetes.io/identity=kube-apiserver:kubectl -n kube-system get lease -l apiserver.kubernetes.io/identity=kube-apiserver NAME HOLDER AGE apiserver-07a5ea9b9b072c4a5f3d1c3702 apiserver-07a5ea9b9b072c4a5f3d1c3702_0c8914f7-0f35-440e-8676-7844977d3a05 5m33s apiserver-7be9e061c59d368b3ddaf1376e apiserver-7be9e061c59d368b3ddaf1376e_84f2a85d-37c1-4b14-b6b9-603e62e4896f 4m23s apiserver-1dfef752bcb36637d2763d1868 apiserver-1dfef752bcb36637d2763d1868_c5ffa286-8a9a-45d4-91e7-61118ed58d2e 4m43s The SHA256 hash used in the lease name is based on the OS hostname as seen by that API server. Each kube-apiserver should be configured to use a hostname that is unique within the cluster. New instances of kube-apiserver that use the same hostname will take over existing Leases using a new holder identity, as opposed to instantiating new Lease objects. You can check the hostname used by kube-apisever by checking the value of the kubernetes.io/hostname label:kubectl -n kube-system get lease apiserver-07a5ea9b9b072c4a5f3d1c3702 -o yaml apiVersion: coordination.k8s.io/v1 kind: Lease metadata: creationTimestamp: "2023-07-02T13:16:48Z" labels: apiserver.kubernetes.io/identity: kube-apiserver kubernetes.io/hostname: master-1 name: apiserver-07a5ea9b9b072c4a5f3d1c3702 namespace: kube-system resourceVersion: "334899" uid: 90870ab5-1ba9-4523-b215-e4d4e662acb1 spec: holderIdentity: apiserver-07a5ea9b9b072c4a5f3d1c3702_0c8914f7-0f35-440e-8676-7844977d3a05 leaseDurationSeconds: 3600 renewTime: "2023-07-04T21:58:48.065888Z" Expired leases from kube-apiservers that no longer exist are garbage collected by new kube-apiservers after 1 hour.You can disable API server identity leases by disabling the APIServerIdentity feature gate.WorkloadsYour own workload can define its own use of Leases. For example, you might run a custom controller where a primary or leader member performs operations that its peers do not. You define a Lease so that the controller replicas can select or elect a leader, using the Kubernetes API for coordination. If you do use a Lease, it's a good practice to define a name for the Lease that is obviously linked to the product or component. For example, if you have a component named Example Foo, use a Lease named example-foo.If a cluster operator or another end user could deploy multiple instances of a component, select a name prefix and pick a mechanism (such as hash of the name of the Deployment) to avoid name collisions for the Leases.You can use another approach so long as it achieves the same outcome: different software products do not conflict with one another.3.2.5 - Cloud Controller ManagerFEATURE STATE: Kubernetes v1.11 [beta]Cloud infrastructure technologies let you run Kubernetes on public, private, and hybrid clouds. Kubernetes believes in automated, API-driven infrastructure without tight coupling between components.The cloud-controller-manager is a Kubernetes control plane component that embeds cloud-specific control logic. The cloud controller manager lets you link your cluster into your cloud provider's API, and separates out the components that interact with that cloud platform from components that only interact with your cluster.By decoupling the interoperability logic between Kubernetes and the underlying cloud infrastructure, the cloud-controller-manager component enables cloud providers to release features at a different pace compared to the main Kubernetes project.The cloud-controller-manager is structured using a plugin mechanism that allows different cloud providers to integrate their platforms with Kubernetes.DesignThe cloud controller manager runs in the control plane as a replicated set of processes (usually, these are containers in Pods). Each cloud-controller-manager implements multiple controllers in a single process.Note:You can also run the cloud controller manager as a Kubernetes addon rather than as part of the control plane.Cloud controller manager functionsThe controllers inside the cloud controller manager include:Node controllerThe node controller is responsible for updating Node objects when new servers are created in your cloud infrastructure. The node controller obtains information about the hosts running inside your tenancy with the cloud provider. The node controller performs the following functions:Update a Node object with the corresponding server's unique identifier obtained from the cloud provider API.Annotating and labelling the Node object with cloud-specific information, such as the region the node is deployed into and the resources (CPU, memory, etc) that it has available.Obtain the node's hostname and network addresses.Verifying the node's health. In case a node becomes unresponsive, this controller checks with your cloud provider's API to see if the server has been deactivated / deleted / terminated. If the node has been deleted from the cloud, the controller deletes the Node object from your Kubernetes cluster.Some cloud provider implementations split this into a node controller and a separate node lifecycle controller.Route controllerThe route controller is responsible for configuring routes in the cloud appropriately so that containers on different nodes in your Kubernetes cluster can communicate with each other.Depending on the cloud provider, the route controller might also allocate blocks of IP addresses for the Pod network.Service controllerServices integrate with cloud infrastructure components such as managed load balancers, IP addresses, network packet filtering, and target health checking. The service controller interacts with your cloud provider's APIs to set up load balancers and other infrastructure components when you declare a Service resource that requires them.AuthorizationThis section breaks down the access that the cloud controller manager requires on various API objects, in order to perform its operations.Node controllerThe Node controller only works with Node objects. It requires full access to read and modify Node objects.v1/Node:getlistcreateupdatepatchwatchdeleteRoute controllerThe route controller listens to Node object creation and configures routes appropriately. It requires Get access to Node objects.v1/Node:getService controllerThe service controller watches for Service object create, update and delete events and then configures load balancers for those Services appropriately.To access Services, it requires list, and watch access. To update Services, it requires patch and update access to the status subresource.v1/Service:listgetwatchpatchupdateOthersThe implementation of the core of the cloud controller manager requires access to create Event objects, and to ensure secure operation, it requires access to create ServiceAccounts.v1/Event:createpatchupdatev1/ServiceAccount:createThe RBAC ClusterRole for the cloud controller manager looks like:apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: cloud-controller-manager rules: \- apiGroups: \- "" resources: \- events verbs: \- create \- patch \- update \- apiGroups: \- "" resources: \- nodes verbs: \- '*' \- apiGroups: \- "" resources: \- nodes/status verbs: \- patch \- apiGroups: \- "" resources: \- services verbs: \- list \- watch \- apiGroups: \- "" resources: \- services/status verbs: \- patch \- update \- apiGroups: \- "" resources: \- serviceaccounts verbs: \- create \- apiGroups: \- "" resources: \- persistentvolumes verbs: \- get \- list \- update \- watch What's nextCloud Controller Manager Administration has instructions on running and managing the cloud controller manager.To upgrade a HA control plane to use the cloud controller manager, see Migrate Replicated Control Plane To Use Cloud Controller Manager.Want to know how to implement your own cloud controller manager, or extend an existing project?The cloud controller manager uses Go interfaces, specifically, CloudProvider interface defined in cloud.go from kubernetes/cloud-provider to allow implementations from any cloud to be plugged in.The implementation of the shared controllers highlighted in this document (Node, Route, and Service), and some scaffolding along with the shared cloudprovider interface, is part of the Kubernetes core. Implementations specific to cloud providers are outside the core of Kubernetes and implement the CloudProvider interface.For more information about developing plugins, see Developing Cloud Controller Manager.3.2.6 - About cgroup v2On Linux, control groups constrain resources that are allocated to processes.The kubelet and the underlying container runtime need to interface with cgroups to enforce resource management for pods and containers which includes cpu/memory requests and limits for containerized workloads.There are two versions of cgroups in Linux: cgroup v1 and cgroup v2. cgroup v2 is the new generation of the cgroup API.What is cgroup v2?FEATURE STATE: Kubernetes v1.25 [stable]cgroup v2 is the next version of the Linux cgroup API. cgroup v2 provides a unified control system with enhanced resource management capabilities.cgroup v2 offers several improvements over cgroup v1, such as the following:Single unified hierarchy design in APISafer sub-tree delegation to containersNewer features like Pressure Stall InformationEnhanced resource allocation management and isolation across multiple resourcesUnified accounting for different types of memory allocations (network memory, kernel memory, etc)Accounting for non-immediate resource changes such as page cache write backsSome Kubernetes features exclusively use cgroup v2 for enhanced resource management and isolation. For example, the MemoryQoS feature improves memory QoS and relies on cgroup v2 primitives.Using cgroup v2The recommended way to use cgroup v2 is to use a Linux distribution that enables and uses cgroup v2 by default.To check if your distribution uses cgroup v2, refer to Identify cgroup version on Linux nodes.Requirementscgroup v2 has the following requirements:OS distribution enables cgroup v2Linux Kernel version is 5.8 or laterContainer runtime supports cgroup v2. For example:containerd v1.4 and latercri-o v1.20 and laterThe kubelet and the container runtime are configured to use the systemd cgroup driverLinux Distribution cgroup v2 supportFor a list of Linux distributions that use cgroup v2, refer to the cgroup v2 documentationContainer Optimized OS (since M97)Ubuntu (since 21.10, 22.04+ recommended)Debian GNU/Linux (since Debian 11 bullseye)Fedora (since 31)Arch Linux (since April 2021)RHEL and RHEL-like distributions (since 9)To check if your distribution is using cgroup v2, refer to your distribution's documentation or follow the instructions in Identify the cgroup version on Linux nodes.You can also enable cgroup v2 manually on your Linux distribution by modifying the kernel cmdline boot arguments. If your distribution uses GRUB, systemd.unified_cgroup_hierarchy=1 should be added in GRUB_CMDLINE_LINUX under /etc/default/grub, followed by sudo update-grub. However, the recommended approach is to use a distribution that already enables cgroup v2 by default.Migrating to cgroup v2To migrate to cgroup v2, ensure that you meet the requirements, then upgrade to a kernel version that enables cgroup v2 by default.The kubelet automatically detects that the OS is running on cgroup v2 and performs accordingly with no additional configuration required.There should not be any noticeable difference in the user experience when switching to cgroup v2, unless users are accessing the cgroup file system directly, either on the node or from within the containers.cgroup v2 uses a different API than cgroup v1, so if there are any applications that directly access the cgroup file system, they need to be updated to newer versions that support cgroup v2. For example:Some third-party monitoring and security agents may depend on the cgroup filesystem. Update these agents to versions that support cgroup v2.If you run cAdvisor as a stand-alone DaemonSet for monitoring pods and containers, update it to v0.43.0 or later.If you deploy Java applications, prefer to use versions which fully support cgroup v2:OpenJDK / HotSpot: jdk8u372, 11.0.16, 15 and laterIBM Semeru Runtimes: 8.0.382.0, 11.0.20.0, 17.0.8.0, and laterIBM Java: 8.0.8.6 and laterIf you are using the uber-go/automaxprocs package, make sure the version you use is v1.5.1 or higher.Identify the cgroup version on Linux NodesThe cgroup version depends on the Linux distribution being used and the default cgroup version configured on the OS. To check which cgroup version your distribution uses, run the stat -fc %T /sys/fs/cgroup/ command on the node:stat -fc %T /sys/fs/cgroup/ For cgroup v2, the output is cgroup2fs.For cgroup v1, the output is tmpfs.What's nextLearn more about cgroupsLearn more about container runtimeLearn more about cgroup drivers3.2.7 - Kubernetes Self-HealingKubernetes is designed with self-healing capabilities that help maintain the health and availability of workloads. It automatically replaces failed containers, reschedules workloads when nodes become unavailable, and ensures that the desired state of the system is maintained.Self-Healing capabilitiesContainer-level restarts: If a container inside a Pod fails, Kubernetes restarts it based on the restartPolicy.Replica replacement: If a Pod in a Deployment or StatefulSet fails, Kubernetes creates a replacement Pod to maintain the specified number of replicas. If a Pod fails that is part of a DaemonSet fails, the control plane creates a replacement Pod to run on the same node.Persistent storage recovery: If a node is running a Pod with a PersistentVolume (PV) attached, and the node fails, Kubernetes can reattach the volume to a new Pod on a different node.Load balancing for Services: If a Pod behind a Service fails, Kubernetes automatically removes it from the Service's endpoints to route traffic only to healthy Pods.Here are some of the key components that provide Kubernetes self-healing:kubelet: Ensures that containers are running, and restarts those that fail.ReplicaSet, StatefulSet and DaemonSet controller: Maintains the desired number of Pod replicas.PersistentVolume controller: Manages volume attachment and detachment for stateful workloads.ConsiderationsStorage Failures: If a persistent volume becomes unavailable, recovery steps may be required.Application Errors: Kubernetes can restart containers, but underlying application issues must be addressed separately.What's nextRead more about PodsLearn about Kubernetes ControllersExplore PersistentVolumesRead about node autoscaling. Node autoscaling also provides automatic healing if or when nodes fail in your cluster.3.2.8 - Container Runtime Interface (CRI)The CRI is a plugin interface which enables the kubelet to use a wide variety of container runtimes, without having a need to recompile the cluster components.You need a working container runtime on each Node in your cluster, so that the kubelet can launch Pods and their containers.The Container Runtime Interface (CRI) is the main protocol for the communication between the kubelet and Container Runtime.The Kubernetes Container Runtime Interface (CRI) defines the main gRPC protocol for the communication between the node components kubelet and container runtime.The APIFEATURE STATE: Kubernetes v1.23 [stable]The kubelet acts as a client when connecting to the container runtime via gRPC. The runtime and image service endpoints have to be available in the container runtime, which can be configured separately within the kubelet by using the \--image-service-endpoint command line flags.For Kubernetes v1.33, the kubelet prefers to use CRI v1. If a container runtime does not support v1 of the CRI, then the kubelet tries to negotiate any older supported version. The v1.33 kubelet can also negotiate CRI v1alpha2, but this version is considered as deprecated. If the kubelet cannot negotiate a supported CRI version, the kubelet gives up and doesn't register as a node.UpgradingWhen upgrading Kubernetes, the kubelet tries to automatically select the latest CRI version on restart of the component. If that fails, then the fallback will take place as mentioned above. If a gRPC re-dial was required because the container runtime has been upgraded, then the container runtime must also support the initially selected version or the redial is expected to fail. This requires a restart of the kubelet.What's nextLearn more about the CRI protocol definition3.2.9 - Garbage CollectionGarbage collection is a collective term for the various mechanisms Kubernetes uses to clean up cluster resources. This allows the clean up of resources like the following:Terminated podsCompleted JobsObjects without owner referencesUnused containers and container imagesDynamically provisioned PersistentVolumes with a StorageClass reclaim policy of DeleteStale or expired CertificateSigningRequests (CSRs)Nodes deleted in the following scenarios:On a cloud when the cluster uses a cloud controller managerOn-premises when the cluster uses an addon similar to a cloud controller managerNode Lease objectsOwners and dependentsMany objects in Kubernetes link to each other through owner references. Owner references tell the control plane which objects are dependent on others. Kubernetes uses owner references to give the control plane, and other API clients, the opportunity to clean up related resources before deleting an object. In most cases, Kubernetes manages owner references automatically.Ownership is different from the labels and selectors mechanism that some resources also use. For example, consider a Service that creates EndpointSlice objects. The Service uses labels to allow the control plane to determine which EndpointSlice objects are used for that Service. In addition to the labels, each EndpointSlice that is managed on behalf of a Service has an owner reference. Owner references help different parts of Kubernetes avoid interfering with objects they don’t control.Note:Cross-namespace owner references are disallowed by design. Namespaced dependents can specify cluster-scoped or namespaced owners. A namespaced owner must exist in the same namespace as the dependent. If it does not, the owner reference is treated as absent, and the dependent is subject to deletion once all owners are verified absent.Cluster-scoped dependents can only specify cluster-scoped owners. In v1.20+, if a cluster-scoped dependent specifies a namespaced kind as an owner, it is treated as having an unresolvable owner reference, and is not able to be garbage collected.In v1.20+, if the garbage collector detects an invalid cross-namespace ownerReference, or a cluster-scoped dependent with an ownerReference referencing a namespaced kind, a warning Event with a reason of OwnerRefInvalidNamespace and an involvedObject of the invalid dependent is reported. You can check for that kind of Event by running kubectl get events -A --field-selector=reason=OwnerRefInvalidNamespace.Cascading deletionKubernetes checks for and deletes objects that no longer have owner references, like the pods left behind when you delete a ReplicaSet. When you delete an object, you can control whether Kubernetes deletes the object's dependents automatically, in a process called cascading deletion. There are two types of cascading deletion, as follows:Foreground cascading deletionBackground cascading deletionYou can also control how and when garbage collection deletes resources that have owner references using Kubernetes finalizers.Foreground cascading deletionIn foreground cascading deletion, the owner object you're deleting first enters a deletion in progress state. In this state, the following happens to the owner object:The Kubernetes API server sets the object's metadata.deletionTimestamp field to the time the object was marked for deletion.The Kubernetes API server also sets the metadata.finalizers field to foregroundDeletion.The object remains visible through the Kubernetes API until the deletion process is complete.After the owner object enters the deletion in progress state, the controller deletes dependents it knows about. After deleting all the dependent objects it knows about, the controller deletes the owner object. At this point, the object is no longer visible in the Kubernetes API.During foreground cascading deletion, the only dependents that block owner deletion are those that have the ownerReference.blockOwnerDeletion=true field and are in the garbage collection controller cache. The garbage collection controller cache may not contain objects whose resource type cannot be listed / watched successfully, or objects that are created concurrent with deletion of an owner object. See Use foreground cascading deletion to learn more.Background cascading deletionIn background cascading deletion, the Kubernetes API server deletes the owner object immediately and the garbage collector controller (custom or default) cleans up the dependent objects in the background. If a finalizer exists, it ensures that objects are not deleted until all necessary clean-up tasks are completed. By default, Kubernetes uses background cascading deletion unless you manually use foreground deletion or choose to orphan the dependent objects.See Use background cascading deletion to learn more.Orphaned dependentsWhen Kubernetes deletes an owner object, the dependents left behind are called orphan objects. By default, Kubernetes deletes dependent objects. To learn how to override this behaviour, see Delete owner objects and orphan dependents.Garbage collection of unused containers and imagesThe kubelet performs garbage collection on unused images every five minutes and on unused containers every minute. You should avoid using external garbage collection tools, as these can break the kubelet behavior and remove containers that should exist.To configure options for unused container and image garbage collection, tune the kubelet using a configuration file and change the parameters related to garbage collection using the KubeletConfiguration resource type.Container image lifecycleKubernetes manages the lifecycle of all images through its image manager, which is part of the kubelet, with the cooperation of cadvisor. The kubelet considers the following disk usage limits when making garbage collection decisions:HighThresholdPercentLowThresholdPercentDisk usage above the configured HighThresholdPercent value triggers garbage collection, which deletes images in order based on the last time they were used, starting with the oldest first. The kubelet deletes images until disk usage reaches the LowThresholdPercent value.Garbage collection for unused container imagesFEATURE STATE: Kubernetes v1.30 [beta] (enabled by default: true)As a beta feature, you can specify the maximum time a local image can be unused for, regardless of disk usage. This is a kubelet setting that you configure for each node.To configure the setting, you need to set a value for the imageMaximumGCAge field in the kubelet configuration file.The value is specified as a Kubernetes duration. See duration in the glossary for more details.For example, you can set the configuration field to 12h45m, which means 12 hours and 45 minutes.Note:This feature does not track image usage across kubelet restarts. If the kubelet is restarted, the tracked image age is reset, causing the kubelet to wait the full imageMaximumGCAge duration before qualifying images for garbage collection based on image age.Container garbage collectionThe kubelet garbage collects unused containers based on the following variables, which you can define:MinAge: the minimum age at which the kubelet can garbage collect a container. Disable by setting to 0.MaxPerPodContainer: the maximum number of dead containers each Pod can have. Disable by setting to less than 0.MaxContainers: the maximum number of dead containers the cluster can have. Disable by setting to less than 0.In addition to these variables, the kubelet garbage collects unidentified and deleted containers, typically starting with the oldest first.MaxPerPodContainer and MaxContainers may potentially conflict with each other in situations where retaining the maximum number of containers per Pod (MaxPerPodContainer) would go outside the allowable total of global dead containers (MaxContainers). In this situation, the kubelet adjusts MaxPerPodContainer to address the conflict. A worst-case scenario would be to downgrade MaxPerPodContainer to 1 and evict the oldest containers. Additionally, containers owned by pods that have been deleted are removed once they are older than MinAge.Note:The kubelet only garbage collects the containers it manages.Configuring garbage collectionYou can tune garbage collection of resources by configuring options specific to the controllers managing those resources. The following pages show you how to configure garbage collection:Configuring cascading deletion of Kubernetes objectsConfiguring cleanup of finished JobsWhat's nextLearn more about ownership of Kubernetes objects.Learn more about Kubernetes finalizers.Learn about the TTL controller that cleans up finished Jobs.3.2.10 - Mixed Version ProxyFEATURE STATE: Kubernetes v1.28 [alpha] (enabled by default: false)Kubernetes 1.33 includes an alpha feature that lets an API Server proxy a resource requests to other peer API servers. This is useful when there are multiple API servers running different versions of Kubernetes in one cluster (for example, during a long-lived rollout to a new release of Kubernetes).This enables cluster administrators to configure highly available clusters that can be upgraded more safely, by directing resource requests (made during the upgrade) to the correct kube-apiserver. That proxying prevents users from seeing unexpected 404 Not Found errors that stem from the upgrade process.This mechanism is called the Mixed Version Proxy.Enabling the Mixed Version ProxyEnsure that UnknownVersionInteroperabilityProxy feature gate is enabled when you start the API Server:kube-apiserver \ \--feature-gates=UnknownVersionInteroperabilityProxy=true \ # required command line arguments for this feature \--peer-ca-file= \--proxy-client-cert-file=, \--proxy-client-key-file=, \--requestheader-client-ca-file=, # requestheader-allowed-names can be set to blank to allow any Common Name \--requestheader-allowed-names=, # optional flags for this feature \--peer-advertise-ip=`IP of this kube-apiserver that should be used by peers to proxy requests` \--peer-advertise-port=`port of this kube-apiserver that should be used by peers to proxy requests` # …and other flags as usual Proxy transport and authentication between API serversThe source kube-apiserver reuses the existing APIserver client authentication flags \--proxy-client-cert-file and --proxy-client-key-file to present its identity that will be verified by its peer (the destination kube-apiserver). The destination API server verifies that peer connection based on the configuration you specify using the \--requestheader-client-ca-file command line argument.To authenticate the destination server's serving certs, you must configure a certificate authority bundle by specifying the --peer-ca-file command line argument to the source API server.Configuration for peer API server connectivityTo set the network location of a kube-apiserver that peers will use to proxy requests, use the \--peer-advertise-ip and --peer-advertise-port command line arguments to kube-apiserver or specify these fields in the API server configuration file. If these flags are unspecified, peers will use the value from either --advertise-address or \--bind-address command line argument to the kube-apiserver. If those too, are unset, the host's default interface is used.Mixed version proxyingWhen you enable mixed version proxying, the aggregation layer loads a special filter that does the following:When a resource request reaches an API server that cannot serve that API (either because it is at a version pre-dating the introduction of the API or the API is turned off on the API server) the API server attempts to send the request to a peer API server that can serve the requested API. It does so by identifying API groups / versions / resources that the local server doesn't recognise, and tries to proxy those requests to a peer API server that is capable of handling the request.If the peer API server fails to respond, the source API server responds with 503 ("Service Unavailable") error.How it works under the hoodWhen an API Server receives a resource request, it first checks which API servers can serve the requested resource. This check happens using the internal StorageVersion API.If the resource is known to the API server that received the request (for example, GET /api/v1/pods/some-pod), the request is handled locally.If there is no internal StorageVersion object found for the requested resource (for example, GET /my-api/v1/my-resource) and the configured APIService specifies proxying to an extension API server, that proxying happens following the usual flow for extension APIs.If a valid internal StorageVersion object is found for the requested resource (for example, GET /batch/v1/jobs) and the API server trying to handle the request (the handling API server) has the batch API disabled, then the handling API server fetches the peer API servers that do serve the relevant API group / version / resource (api/v1/batch in this case) using the information in the fetched StorageVersion object. The handling API server then proxies the request to one of the matching peer kube-apiservers that are aware of the requested resource.If there is no peer known for that API group / version / resource, the handling API server passes the request to its own handler chain which should eventually return a 404 ("Not Found") response.If the handling API server has identified and selected a peer API server, but that peer fails to respond (for reasons such as network connectivity issues, or a data race between the request being received and a controller registering the peer's info into the control plane), then the handling API server responds with a 503 ("Service Unavailable") error.3.3 - ContainersTechnology for packaging an application along with its runtime dependencies.This page will discuss containers and container images, as well as their use in operations and solution development.The word container is an overloaded term. Whenever you use the word, check whether your audience uses the same definition.Each container that you run is repeatable; the standardization from having dependencies included means that you get the same behavior wherever you run it.Containers decouple applications from the underlying host infrastructure. This makes deployment easier in different cloud or OS environments.Each node in a Kubernetes cluster runs the containers that form the Pods assigned to that node. Containers in a Pod are co-located and co-scheduled to run on the same node.Container imagesA container image is a ready-to-run software package containing everything needed to run an application: the code and any runtime it requires, application and system libraries, and default values for any essential settings.Containers are intended to be stateless and immutable: you should not change the code of a container that is already running. If you have a containerized application and want to make changes, the correct process is to build a new image that includes the change, then recreate the container to start from the updated image.Container runtimesA fundamental component that empowers Kubernetes to run containers effectively. It is responsible for managing the execution and lifecycle of containers within the Kubernetes environment.Kubernetes supports container runtimes such as containerd, CRI-O, and any other implementation of the Kubernetes CRI (Container Runtime Interface).Usually, you can allow your cluster to pick the default container runtime for a Pod. If you need to use more than one container runtime in your cluster, you can specify the RuntimeClass for a Pod to make sure that Kubernetes runs those containers using a particular container runtime.You can also use RuntimeClass to run different Pods with the same container runtime but with different settings.3.3.1 - ImagesA container image represents binary data that encapsulates an application and all its software dependencies. Container images are executable software bundles that can run standalone and that make very well defined assumptions about their runtime environment.You typically create a container image of your application and push it to a registry before referring to it in a Pod.This page provides an outline of the container image concept.Note:If you are looking for the container images for a Kubernetes release (such as v1.33, the latest minor release), visit Download Kubernetes.Image namesContainer images are usually given a name such as pause, example/mycontainer, or kube-apiserver. Images can also include a registry hostname; for example: fictional.registry.example/imagename, and possibly a port number as well; for example: fictional.registry.example:10443/imagename.If you don't specify a registry hostname, Kubernetes assumes that you mean the Docker public registry. You can change this behaviour by setting default image registry in container runtime configuration.After the image name part you can add a tag or digest (in the same way you would when using with commands like docker or podman). Tags let you identify different versions of the same series of images. Digests are a unique identifier for a specific version of an image. Digests are hashes of the image's content, and are immutable. Tags can be moved to point to different images, but digests are fixed.Image tags consist of lowercase and uppercase letters, digits, underscores (_), periods (.), and dashes (-). It can be up to 128 characters long. And must follow the next regex pattern: [a-zA-Z0-9_][a-zA-Z0-9._-]{0,127} You can read more about and find validation regex in the OCI Distribution Specification. If you don't specify a tag, Kubernetes assumes you mean the tag latest.Image digests consists of a hash algorithm (such as sha256) and a hash value. For example: sha256:1ff6c18fbef2045af6b9c16bf034cc421a29027b800e4f9b68ae9b1cb3e9ae07 You can find more information about digests format in the OCI Image Specification.Some image name examples that Kubernetes can use are:busybox - Image name only, no tag or digest. Kubernetes will use Docker public registry and latest tag. (Same as docker.io/library/busybox:latest)busybox:1.32.0 - Image name with tag. Kubernetes will use Docker public registry. (Same as docker.io/library/busybox:1.32.0)registry.k8s.io/pause:latest - Image name with a custom registry and latest tag.registry.k8s.io/pause:3.5 - Image name with a custom registry and non-latest tag.registry.k8s.io/pause@sha256:1ff6c18fbef2045af6b9c16bf034cc421a29027b800e4f9b68ae9b1cb3e9ae07 - Image name with digest.registry.k8s.io/pause:3.5@sha256:1ff6c18fbef2045af6b9c16bf034cc421a29027b800e4f9b68ae9b1cb3e9ae07 - Image name with tag and digest. Only digest will be used for pulling.Updating imagesWhen you first create a Deployment, StatefulSet, Pod, or other object that includes a Pod template, then by default the pull policy of all containers in that pod will be set to IfNotPresent if it is not explicitly specified. This policy causes the kubelet to skip pulling an image if it already exists.Image pull policyThe imagePullPolicy for a container and the tag of the image affect when the kubelet attempts to pull (download) the specified image.Here's a list of the values you can set for imagePullPolicy and the effects these values have:IfNotPresentthe image is pulled only if it is not already present locally.Alwaysevery time the kubelet launches a container, the kubelet queries the container image registry to resolve the name to an image digest. If the kubelet has a container image with that exact digest cached locally, the kubelet uses its cached image; otherwise, the kubelet pulls the image with the resolved digest, and uses that image to launch the container.Neverthe kubelet does not try fetching the image. If the image is somehow already present locally, the kubelet attempts to start the container; otherwise, startup fails. See pre-pulled images for more details.The caching semantics of the underlying image provider make even imagePullPolicy: Always efficient, as long as the registry is reliably accessible. Your container runtime can notice that the image layers already exist on the node so that they don't need to be downloaded again.Note:You should avoid using the :latest tag when deploying containers in production as it is harder to track which version of the image is running and more difficult to roll back properly.Instead, specify a meaningful tag such as v1.42.0 and/or a digest.To make sure the Pod always uses the same version of a container image, you can specify the image's digest; replace : with @ (for example, image@sha256:45b23dee08af5e43a7fea6c4cf9c25ccf269ee113168c19722f87876677c5cb2).When using image tags, if the image registry were to change the code that the tag on that image represents, you might end up with a mix of Pods running the old and new code. An image digest uniquely identifies a specific version of the image, so Kubernetes runs the same code every time it starts a container with that image name and digest specified. Specifying an image by digest fixes the code that you run so that a change at the registry cannot lead to that mix of versions.There are third-party admission controllers that mutate Pods (and pod templates) when they are created, so that the running workload is defined based on an image digest rather than a tag. That might be useful if you want to make sure that all your workload is running the same code no matter what tag changes happen at the registry.Default image pull policyWhen you (or a controller) submit a new Pod to the API server, your cluster sets the imagePullPolicy field when specific conditions are met:if you omit the imagePullPolicy field, and you specify the digest for the container image, the imagePullPolicy is automatically set to IfNotPresent.if you omit the imagePullPolicy field, and the tag for the container image is :latest, imagePullPolicy is automatically set to Always;if you omit the imagePullPolicy field, and you don't specify the tag for the container image, imagePullPolicy is automatically set to Always;if you omit the imagePullPolicy field, and you specify the tag for the container image that isn't :latest, the imagePullPolicy is automatically set to IfNotPresent.Note:The value of imagePullPolicy of the container is always set when the object is first created, and is not updated if the image's tag or digest later changes.For example, if you create a Deployment with an image whose tag is not :latest, and later update that Deployment's image to a :latest tag, the imagePullPolicy field will not change to Always. You must manually change the pull policy of any object after its initial creation.Required image pullIf you would like to always force a pull, you can do one of the following:Set the imagePullPolicy of the container to Always.Omit the imagePullPolicy and use :latest as the tag for the image to use; Kubernetes will set the policy to Always when you submit the Pod.Omit the imagePullPolicy and the tag for the image to use; Kubernetes will set the policy to Always when you submit the Pod.Enable the AlwaysPullImages admission controller.ImagePullBackOffWhen a kubelet starts creating containers for a Pod using a container runtime, it might be possible the container is in Waiting state because of ImagePullBackOff.The status ImagePullBackOff means that a container could not start because Kubernetes could not pull a container image (for reasons such as invalid image name, or pulling from a private registry without imagePullSecret). The BackOff part indicates that Kubernetes will keep trying to pull the image, with an increasing back-off delay.Kubernetes raises the delay between each attempt until it reaches a compiled-in limit, which is 300 seconds (5 minutes).Image pull per runtime classFEATURE STATE: Kubernetes v1.29 [alpha] (enabled by default: false)Kubernetes includes alpha support for performing image pulls based on the RuntimeClass of a Pod.If you enable the RuntimeClassInImageCriApi feature gate, the kubelet references container images by a tuple of (image name, runtime handler) rather than just the image name or digest. Your container runtime may adapt its behavior based on the selected runtime handler. Pulling images based on runtime class will be helpful for VM based containers like windows hyperV containers.Serial and parallel image pullsBy default, kubelet pulls images serially. In other words, kubelet sends only one image pull request to the image service at a time. Other image pull requests have to wait until the one being processed is complete.Nodes make image pull decisions in isolation. Even when you use serialized image pulls, two different nodes can pull the same image in parallel.If you would like to enable parallel image pulls, you can set the field serializeImagePulls to false in the kubelet configuration. With serializeImagePulls set to false, image pull requests will be sent to the image service immediately, and multiple images will be pulled at the same time.When enabling parallel image pulls, please make sure the image service of your container runtime can handle parallel image pulls.The kubelet never pulls multiple images in parallel on behalf of one Pod. For example, if you have a Pod that has an init container and an application container, the image pulls for the two containers will not be parallelized. However, if you have two Pods that use different images, the kubelet pulls the images in parallel on behalf of the two different Pods, when parallel image pulls is enabled.Maximum parallel image pullsFEATURE STATE: Kubernetes v1.32 [beta]When serializeImagePulls is set to false, the kubelet defaults to no limit on the maximum number of images being pulled at the same time. If you would like to limit the number of parallel image pulls, you can set the field maxParallelImagePulls in kubelet configuration. With maxParallelImagePulls set to n, only n images can be pulled at the same time, and any image pull beyond n will have to wait until at least one ongoing image pull is complete.Limiting the number parallel image pulls would prevent image pulling from consuming too much network bandwidth or disk I/O, when parallel image pulling is enabled.You can set maxParallelImagePulls to a positive number that is greater than or equal to 1. If you set maxParallelImagePulls to be greater than or equal to 2, you must set the serializeImagePulls to false. The kubelet will fail to start with invalid maxParallelImagePulls settings.Multi-architecture images with image indexesAs well as providing binary images, a container registry can also serve a container image index. An image index can point to multiple image manifests for architecture-specific versions of a container. The idea is that you can have a name for an image (for example: pause, example/mycontainer, kube-apiserver) and allow different systems to fetch the right binary image for the machine architecture they are using.Kubernetes itself typically names container images with a suffix -$(ARCH). For backward compatibility, please generate the older images with suffixes. The idea is to generate say pause image which has the manifest for all the arch(es) and say pause-amd64 which is backwards compatible for older configurations or YAML files which may have hard coded the images with suffixes.Using a private registryPrivate registries may require keys to read images from them.Credentials can be provided in several ways:Configuring Nodes to Authenticate to a Private Registryall pods can read any configured private registriesrequires node configuration by cluster administratorKubelet Credential Provider to dynamically fetch credentials for private registrieskubelet can be configured to use credential provider exec plugin for the respective private registry.Pre-pulled Imagesall pods can use any images cached on a noderequires root access to all nodes to set upSpecifying ImagePullSecrets on a Podonly pods which provide their own keys can access the private registryVendor-specific or local extensionsif you're using a custom node configuration, you (or your cloud provider) can implement your mechanism for authenticating the node to the container registry.These options are explained in more detail below.Configuring nodes to authenticate to a private registrySpecific instructions for setting credentials depends on the container runtime and registry you chose to use. You should refer to your solution's documentation for the most accurate information.For an example of configuring a private container image registry, see the Pull an Image from a Private Registry task. That example uses a private registry in Docker Hub.Kubelet credential provider for authenticated image pullsNote:This approach is especially suitable when kubelet needs to fetch registry credentials dynamically. Most commonly used for registries provided by cloud providers where auth tokens are short-lived.You can configure the kubelet to invoke a plugin binary to dynamically fetch registry credentials for a container image. This is the most robust and versatile way to fetch credentials for private registries, but also requires kubelet-level configuration to enable.See Configure a kubelet image credential provider for more details.Interpretation of config.jsonThe interpretation of config.json varies between the original Docker implementation and the Kubernetes interpretation. In Docker, the auths keys can only specify root URLs, whereas Kubernetes allows glob URLs as well as prefix-matched paths. The only limitation is that glob patterns (*) have to include the dot (.) for each subdomain. The amount of matched subdomains has to be equal to the amount of glob patterns (*.), for example:*.kubernetes.io will not match kubernetes.io, but abc.kubernetes.io*.*.kubernetes.io will not match abc.kubernetes.io, but abc.def.kubernetes.ioprefix.*.io will match prefix.kubernetes.io*-good.kubernetes.io will match prefix-good.kubernetes.ioThis means that a config.json like this is valid:{ "auths": { "my-registry.io/images": { "auth": "…" }, "*.my-registry.io/images": { "auth": "…" } } } Image pull operations would now pass the credentials to the CRI container runtime for every valid pattern. For example the following container image names would match successfully:my-registry.io/imagesmy-registry.io/images/my-imagemy-registry.io/images/another-imagesub.my-registry.io/images/my-imageBut not:a.sub.my-registry.io/images/my-imagea.b.sub.my-registry.io/images/my-imageThe kubelet performs image pulls sequentially for every found credential. This means, that multiple entries in config.json for different paths are possible, too:{ "auths": { "my-registry.io/images": { "auth": "…" }, "my-registry.io/images/subpath": { "auth": "…" } } } If now a container specifies an image my-registry.io/images/subpath/my-image to be pulled, then the kubelet will try to download them from both authentication sources if one of them fails.Pre-pulled imagesNote:This approach is suitable if you can control node configuration. It will not work reliably if your cloud provider manages nodes and replaces them automatically.By default, the kubelet tries to pull each image from the specified registry. However, if the imagePullPolicy property of the container is set to IfNotPresent or Never, then a local image is used (preferentially or exclusively, respectively).If you want to rely on pre-pulled images as a substitute for registry authentication, you must ensure all nodes in the cluster have the same pre-pulled images.This can be used to preload certain images for speed or as an alternative to authenticating to a private registry.Note:FEATURE STATE: Kubernetes v1.33 [alpha] (enabled by default: false)Access to pre-pulled images may be authorized according to image pull credential verificationSpecifying imagePullSecrets on a PodNote:This is the recommended approach to run containers based on images in private registries.Kubernetes supports specifying container image registry keys on a Pod. imagePullSecrets must all be in the same namespace as the Pod. The referenced Secrets must be of type kubernetes.io/dockercfg or kubernetes.io/dockerconfigjson.Ensure Image Pull Credential VerificationFEATURE STATE: Kubernetes v1.33 [alpha] (enabled by default: false)If the KubeletEnsureSecretPulledImages feature gate is enabled, Kubernetes will validate image credentials for every image that requires credentials to be pulled, even if that image is already present on the node. This validation ensures that images in a pod request which have not been successfully pulled with the provided credentials must re-pull the images from the registry. Additionally, image pulls that re-use the same credentials which previously resulted in a successful image pull will not need to re-pull from the registry and are instead validated locally without accessing the registry (provided the image is available locally). This is controlled by theimagePullCredentialsVerificationPolicy field in the Kubelet configuration.This configuration controls when image pull credentials must be verified if the image is already present on the node:NeverVerify: Mimics the behavior of having this feature gate disabled. If the image is present locally, image pull credentials are not verified.NeverVerifyPreloadedImages: Images pulled outside the kubelet are not verified, but all other images will have their credentials verified. This is the default behavior.NeverVerifyAllowListedImages: Images pulled outside the kubelet and mentioned within the preloadedImagesVerificationAllowlist specified in the kubelet config are not verified.AlwaysVerify: All images will have their credentials verified before they can be used.This verification applies to pre-pulled images, images pulled using node-wide secrets, and images pulled using pod-level secrets.Note:In the case of credential rotation, the credentials previously used to pull the image will continue to verify without the need to access the registry. New or rotated credentials will require the image to be re-pulled from the registry.Creating a Secret with a Docker configYou need to know the username, registry password and client email address for authenticating to the registry, as well as its hostname. Run the following command, substituting the appropriate uppercase values:kubectl create secret docker-registry  \ \--docker-server=DOCKER_REGISTRY_SERVER \ \--docker-username=DOCKER_USER \ \--docker-password=DOCKER_PASSWORD \ \--docker-email=DOCKER_EMAIL If you already have a Docker credentials file then, rather than using the above command, you can import the credentials file as a Kubernetes Secrets.Create a Secret based on existing Docker credentials explains how to set this up.This is particularly useful if you are using multiple private container registries, as kubectl create secret docker-registry creates a Secret that only works with a single private registry.Note:Pods can only reference image pull secrets in their own namespace, so this process needs to be done one time per namespace.Referring to an imagePullSecrets on a PodNow, you can create pods which reference that secret by adding an imagePullSecrets section to a Pod definition. Each item in the imagePullSecrets array can only reference a Secret in the same namespace.For example:cat < pod.yaml apiVersion: v1 kind: Pod metadata: name: foo namespace: awesomeapps spec: containers: \- name: foo image: janedoe/awesomeapp:v1 imagePullSecrets: \- name: myregistrykey EOF cat <> ./kustomization.yaml resources: \- pod.yaml EOF This needs to be done for each pod that is using a private registry.However, setting of this field can be automated by setting the imagePullSecrets in a ServiceAccount resource.Check Add ImagePullSecrets to a Service Account for detailed instructions.You can use this in conjunction with a per-node .docker/config.json. The credentials will be merged.Use casesThere are a number of solutions for configuring private registries. Here are some common use cases and suggested solutions.Cluster running only non-proprietary (e.g. open-source) images. No need to hide images.Use public images from a public registryNo configuration required.Some cloud providers automatically cache or mirror public images, which improves availability and reduces the time to pull images.Cluster running some proprietary images which should be hidden to those outside the company, but visible to all cluster users.Use a hosted private registryManual configuration may be required on the nodes that need to access to private registryOr, run an internal private registry behind your firewall with open read access.No Kubernetes configuration is required.Use a hosted container image registry service that controls image accessIt will work better with Node autoscaling than manual node configuration.Or, on a cluster where changing the node configuration is inconvenient, use imagePullSecrets.Cluster with proprietary images, a few of which require stricter access control.Ensure AlwaysPullImages admission controller is active. Otherwise, all Pods potentially have access to all images.Move sensitive data into a "Secret" resource, instead of packaging it in an image.A multi-tenant cluster where each tenant needs own private registry.Ensure AlwaysPullImages admission controller is active. Otherwise, all Pods of all tenants potentially have access to all images.Run a private registry with authorization required.Generate registry credential for each tenant, put into secret, and populate secret to each tenant namespace.The tenant adds that secret to imagePullSecrets of each namespace.If you need access to multiple registries, you can create one secret for each registry.Legacy built-in kubelet credential providerIn older versions of Kubernetes, the kubelet had a direct integration with cloud provider credentials. This gave it the ability to dynamically fetch credentials for image registries.There were three built-in implementations of the kubelet credential provider integration: ACR (Azure Container Registry), ECR (Elastic Container Registry), and GCR (Google Container Registry).For more information on the legacy mechanism, read the documentation for the version of Kubernetes that you are using. Kubernetes v1.26 through to v1.33 do not include the legacy mechanism, so you would need to either:configure a kubelet image credential provider on each nodespecify image pull credentials using imagePullSecrets and at least one SecretWhat's nextRead the OCI Image Manifest Specification.Learn about container image garbage collection.Learn more about pulling an Image from a Private Registry.3.3.2 - Container EnvironmentThis page describes the resources available to Containers in the Container environment.Container environmentThe Kubernetes Container environment provides several important resources to Containers:A filesystem, which is a combination of an image and one or more volumes.Information about the Container itself.Information about other objects in the cluster.Container informationThe hostname of a Container is the name of the Pod in which the Container is running. It is available through the hostname command or the gethostname function call in libc.The Pod name and namespace are available as environment variables through the downward API.User defined environment variables from the Pod definition are also available to the Container, as are any environment variables specified statically in the container image.Cluster informationA list of all services that were running when a Container was created is available to that Container as environment variables. This list is limited to services within the same namespace as the new Container's Pod and Kubernetes control plane services.For a service named foo that maps to a Container named bar, the following variables are defined:FOO_SERVICE_HOST= FOO_SERVICE_PORT= Services have dedicated IP addresses and are available to the Container via DNS, if DNS addon is enabled. What's nextLearn more about Container lifecycle hooks.Get hands-on experience attaching handlers to Container lifecycle events.3.3.3 - Runtime ClassFEATURE STATE: Kubernetes v1.20 [stable]This page describes the RuntimeClass resource and runtime selection mechanism.RuntimeClass is a feature for selecting the container runtime configuration. The container runtime configuration is used to run a Pod's containers.MotivationYou can set a different RuntimeClass between different Pods to provide a balance of performance versus security. For example, if part of your workload deserves a high level of information security assurance, you might choose to schedule those Pods so that they run in a container runtime that uses hardware virtualization. You'd then benefit from the extra isolation of the alternative runtime, at the expense of some additional overhead.You can also use RuntimeClass to run different Pods with the same container runtime but with different settings.SetupConfigure the CRI implementation on nodes (runtime dependent)Create the corresponding RuntimeClass resources1. Configure the CRI implementation on nodesThe configurations available through RuntimeClass are Container Runtime Interface (CRI) implementation dependent. See the corresponding documentation (below) for your CRI implementation for how to configure.Note:RuntimeClass assumes a homogeneous node configuration across the cluster by default (which means that all nodes are configured the same way with respect to container runtimes). To support heterogeneous node configurations, see Scheduling below.The configurations have a corresponding handler name, referenced by the RuntimeClass. The handler must be a valid DNS label name.2. Create the corresponding RuntimeClass resourcesThe configurations setup in step 1 should each have an associated handler name, which identifies the configuration. For each handler, create a corresponding RuntimeClass object.The RuntimeClass resource currently only has 2 significant fields: the RuntimeClass name (metadata.name) and the handler (handler). The object definition looks like this:# RuntimeClass is defined in the node.k8s.io API group apiVersion: node.k8s.io/v1 kind: RuntimeClass metadata: # The name the RuntimeClass will be referenced by. # RuntimeClass is a non-namespaced resource. name: myclass # The name of the corresponding CRI configuration handler: myconfiguration The name of a RuntimeClass object must be a valid DNS subdomain name.Note:It is recommended that RuntimeClass write operations (create/update/patch/delete) be restricted to the cluster administrator. This is typically the default. See Authorization Overview for more details.UsageOnce RuntimeClasses are configured for the cluster, you can specify a runtimeClassName in the Pod spec to use it. For example:apiVersion: v1 kind: Pod metadata: name: mypod spec: runtimeClassName: myclass # ... This will instruct the kubelet to use the named RuntimeClass to run this pod. If the named RuntimeClass does not exist, or the CRI cannot run the corresponding handler, the pod will enter the Failed terminal phase. Look for a corresponding event for an error message.If no runtimeClassName is specified, the default RuntimeHandler will be used, which is equivalent to the behavior when the RuntimeClass feature is disabled.CRI ConfigurationFor more details on setting up CRI runtimes, see CRI installation.containerdRuntime handlers are configured through containerd's configuration at /etc/containerd/config.toml. Valid handlers are configured under the runtimes section:[plugins."io.containerd.grpc.v1.cri".containerd.runtimes.${HANDLER_NAME}] See containerd's config documentation for more details:CRI-ORuntime handlers are configured through CRI-O's configuration at /etc/crio/crio.conf. Valid handlers are configured under the crio.runtime table:[crio.runtime.runtimes.${HANDLER_NAME}] runtime_path = "${PATH_TO_BINARY}" See CRI-O's config documentation for more details.SchedulingFEATURE STATE: Kubernetes v1.16 [beta]By specifying the scheduling field for a RuntimeClass, you can set constraints to ensure that Pods running with this RuntimeClass are scheduled to nodes that support it. If scheduling is not set, this RuntimeClass is assumed to be supported by all nodes.To ensure pods land on nodes supporting a specific RuntimeClass, that set of nodes should have a common label which is then selected by the runtimeclass.scheduling.nodeSelector field. The RuntimeClass's nodeSelector is merged with the pod's nodeSelector in admission, effectively taking the intersection of the set of nodes selected by each. If there is a conflict, the pod will be rejected.If the supported nodes are tainted to prevent other RuntimeClass pods from running on the node, you can add tolerations to the RuntimeClass. As with the nodeSelector, the tolerations are merged with the pod's tolerations in admission, effectively taking the union of the set of nodes tolerated by each.To learn more about configuring the node selector and tolerations, see Assigning Pods to Nodes.Pod OverheadFEATURE STATE: Kubernetes v1.24 [stable]You can specify overhead resources that are associated with running a Pod. Declaring overhead allows the cluster (including the scheduler) to account for it when making decisions about Pods and resources.Pod overhead is defined in RuntimeClass through the overhead field. Through the use of this field, you can specify the overhead of running pods utilizing this RuntimeClass and ensure these overheads are accounted for in Kubernetes.What's nextRuntimeClass DesignRuntimeClass Scheduling DesignRead about the Pod Overhead conceptPodOverhead Feature Design3.3.4 - Container Lifecycle HooksThis page describes how kubelet managed Containers can use the Container lifecycle hook framework to run code triggered by events during their management lifecycle.OverviewAnalogous to many programming language frameworks that have component lifecycle hooks, such as Angular, Kubernetes provides Containers with lifecycle hooks. The hooks enable Containers to be aware of events in their management lifecycle and run code implemented in a handler when the corresponding lifecycle hook is executed.Container hooksThere are two hooks that are exposed to Containers:PostStartThis hook is executed immediately after a container is created. However, there is no guarantee that the hook will execute before the container ENTRYPOINT. No parameters are passed to the handler.PreStopThis hook is called immediately before a container is terminated due to an API request or management event such as a liveness/startup probe failure, preemption, resource contention and others. A call to the PreStop hook fails if the container is already in a terminated or completed state and the hook must complete before the TERM signal to stop the container can be sent. The Pod's termination grace period countdown begins before the PreStop hook is executed, so regardless of the outcome of the handler, the container will eventually terminate within the Pod's termination grace period. No parameters are passed to the handler.A more detailed description of the termination behavior can be found in Termination of Pods.StopSignalThe StopSignal lifecycle can be used to define a stop signal which would be sent to the container when it is stopped. If you set this, it overrides any STOPSIGNAL instruction defined within the container image.A more detailed description of termination behaviour with custom stop signals can be found in Stop Signals.Hook handler implementationsContainers can access a hook by implementing and registering a handler for that hook. There are three types of hook handlers that can be implemented for Containers:Exec - Executes a specific command, such as pre-stop.sh, inside the cgroups and namespaces of the Container. Resources consumed by the command are counted against the Container.HTTP - Executes an HTTP request against a specific endpoint on the Container.Sleep - Pauses the container for a specified duration. This is a beta-level feature default enabled by the PodLifecycleSleepAction feature gate.Note:The beta level PodLifecycleSleepActionAllowZero feature gate which is enabled by default from v1.33. It allows you to set a sleep duration of zero seconds (effectively a no-op) for your Sleep lifecycle hooks.Hook handler executionWhen a Container lifecycle management hook is called, the Kubernetes management system executes the handler according to the hook action, httpGet, tcpSocket (deprecated) and sleep are executed by the kubelet process, and exec is executed in the container.The PostStart hook handler call is initiated when a container is created, meaning the container ENTRYPOINT and the PostStart hook are triggered simultaneously. However, if the PostStart hook takes too long to execute or if it hangs, it can prevent the container from transitioning to a running state.PreStop hooks are not executed asynchronously from the signal to stop the Container; the hook must complete its execution before the TERM signal can be sent. If a PreStop hook hangs during execution, the Pod's phase will be Terminating and remain there until the Pod is killed after its terminationGracePeriodSeconds expires. This grace period applies to the total time it takes for both the PreStop hook to execute and for the Container to stop normally. If, for example, terminationGracePeriodSeconds is 60, and the hook takes 55 seconds to complete, and the Container takes 10 seconds to stop normally after receiving the signal, then the Container will be killed before it can stop normally, since terminationGracePeriodSeconds is less than the total time (55+10) it takes for these two things to happen.If either a PostStart or PreStop hook fails, it kills the Container.Users should make their hook handlers as lightweight as possible. There are cases, however, when long running commands make sense, such as when saving state prior to stopping a Container.Hook delivery guaranteesHook delivery is intended to be at least once, which means that a hook may be called multiple times for any given event, such as for PostStart or PreStop. It is up to the hook implementation to handle this correctly.Generally, only single deliveries are made. If, for example, an HTTP hook receiver is down and is unable to take traffic, there is no attempt to resend. In some rare cases, however, double delivery may occur. For instance, if a kubelet restarts in the middle of sending a hook, the hook might be resent after the kubelet comes back up.Debugging Hook handlersThe logs for a Hook handler are not exposed in Pod events. If a handler fails for some reason, it broadcasts an event. For PostStart, this is the FailedPostStartHook event, and for PreStop, this is the FailedPreStopHook event. To generate a failed FailedPostStartHook event yourself, modify the lifecycle-events.yaml file to change the postStart command to "badcommand" and apply it. Here is some example output of the resulting events you see from running kubectl describe pod lifecycle-demo:Events: Type Reason Age From Message \---- ------ ---- ---- ------- Normal Scheduled 7s default-scheduler Successfully assigned default/lifecycle-demo to ip-XXX-XXX-XX-XX.us-east-2... Normal Pulled 6s kubelet Successfully pulled image "nginx" in 229.604315ms Normal Pulling 4s (x2 over 6s) kubelet Pulling image "nginx" Normal Created 4s (x2 over 5s) kubelet Created container lifecycle-demo-container Normal Started 4s (x2 over 5s) kubelet Started container lifecycle-demo-container Warning FailedPostStartHook 4s (x2 over 5s) kubelet Exec lifecycle hook ([badcommand]) for Container "lifecycle-demo-container" in Pod "lifecycle-demo_default(30229739-9651-4e5a-9a32-a8f1688862db)" failed - error: command 'badcommand' exited with 126: , message: "OCI runtime exec failed: exec failed: container_linux.go:380: starting container process caused: exec: \"badcommand\": executable file not found in $PATH: unknown\r\n" Normal Killing 4s (x2 over 5s) kubelet FailedPostStartHook Normal Pulled 4s kubelet Successfully pulled image "nginx" in 215.66395ms Warning BackOff 2s (x2 over 3s) kubelet Back-off restarting failed container What's nextLearn more about the Container environment.Get hands-on experience attaching handlers to Container lifecycle events.3.4 - WorkloadsUnderstand Pods, the smallest deployable compute object in Kubernetes, and the higher-level abstractions that help you to run them.A workload is an application running on Kubernetes. Whether your workload is a single component or several that work together, on Kubernetes you run it inside a set of pods. In Kubernetes, a Pod represents a set of running containers on your cluster.Kubernetes pods have a defined lifecycle. For example, once a pod is running in your cluster then a critical fault on the node where that pod is running means that all the pods on that node fail. Kubernetes treats that level of failure as final: you would need to create a new Pod to recover, even if the node later becomes healthy.However, to make life considerably easier, you don't need to manage each Pod directly. Instead, you can use workload resources that manage a set of pods on your behalf. These resources configure controllers that make sure the right number of the right kind of pod are running, to match the state you specified.Kubernetes provides several built-in workload resources:Deployment and ReplicaSet (replacing the legacy resource ReplicationController). Deployment is a good fit for managing a stateless application workload on your cluster, where any Pod in the Deployment is interchangeable and can be replaced if needed.StatefulSet lets you run one or more related Pods that do track state somehow. For example, if your workload records data persistently, you can run a StatefulSet that matches each Pod with a PersistentVolume. Your code, running in the Pods for that StatefulSet, can replicate data to other Pods in the same StatefulSet to improve overall resilience.DaemonSet defines Pods that provide facilities that are local to nodes. Every time you add a node to your cluster that matches the specification in a DaemonSet, the control plane schedules a Pod for that DaemonSet onto the new node. Each pod in a DaemonSet performs a job similar to a system daemon on a classic Unix / POSIX server. A DaemonSet might be fundamental to the operation of your cluster, such as a plugin to run cluster networking, it might help you to manage the node, or it could provide optional behavior that enhances the container platform you are running.Job and CronJob provide different ways to define tasks that run to completion and then stop. You can use a Job to define a task that runs to completion, just once. You can use a CronJob to run the same Job multiple times according a schedule.In the wider Kubernetes ecosystem, you can find third-party workload resources that provide additional behaviors. Using a custom resource definition, you can add in a third-party workload resource if you want a specific behavior that's not part of Kubernetes' core. For example, if you wanted to run a group of Pods for your application but stop work unless all the Pods are available (perhaps for some high-throughput distributed task), then you can implement or install an extension that does provide that feature.What's nextAs well as reading about each API kind for workload management, you can read how to do specific tasks:Run a stateless application using a DeploymentRun a stateful application either as a single instance or as a replicated setRun automated tasks with a CronJobTo learn about Kubernetes' mechanisms for separating code from configuration, visit Configuration.There are two supporting concepts that provide backgrounds about how Kubernetes manages pods for applications:Garbage collection tidies up objects from your cluster after their owning resource has been removed.The time-to-live after finished controller removes Jobs once a defined time has passed since they completed.Once your application is running, you might want to make it available on the internet as a Service or, for web application only, using an Ingress.3.4.1 - PodsPods are the smallest deployable units of computing that you can create and manage in Kubernetes.A Pod (as in a pod of whales or pea pod) is a group of one or more containers, with shared storage and network resources, and a specification for how to run the containers. A Pod's contents are always co-located and co-scheduled, and run in a shared context. A Pod models an application-specific "logical host": it contains one or more application containers which are relatively tightly coupled. In non-cloud contexts, applications executed on the same physical or virtual machine are analogous to cloud applications executed on the same logical host.As well as application containers, a Pod can contain init containers that run during Pod startup. You can also inject ephemeral containers for debugging a running Pod.What is a Pod?Note:You need to install a container runtime into each node in the cluster so that Pods can run there.The shared context of a Pod is a set of Linux namespaces, cgroups, and potentially other facets of isolation - the same things that isolate a container. Within a Pod's context, the individual applications may have further sub-isolations applied.A Pod is similar to a set of containers with shared namespaces and shared filesystem volumes.Pods in a Kubernetes cluster are used in two main ways:Pods that run a single container. The "one-container-per-Pod" model is the most common Kubernetes use case; in this case, you can think of a Pod as a wrapper around a single container; Kubernetes manages Pods rather than managing the containers directly.Pods that run multiple containers that need to work together. A Pod can encapsulate an application composed of multiple co-located containers that are tightly coupled and need to share resources. These co-located containers form a single cohesive unit.Grouping multiple co-located and co-managed containers in a single Pod is a relatively advanced use case. You should use this pattern only in specific instances in which your containers are tightly coupled.You don't need to run multiple containers to provide replication (for resilience or capacity); if you need multiple replicas, see Workload management.Using PodsThe following is an example of a Pod which consists of a container running the image nginx:1.14.2.pods/simple-pod.yaml apiVersion: v1 kind: Pod metadata: name: nginx spec: containers: \- name: nginx image: nginx:1.14.2 ports: \- containerPort: 80 To create the Pod shown above, run the following command:kubectl apply -f https://k8s.io/examples/pods/simple-pod.yaml Pods are generally not created directly and are created using workload resources. See Working with Pods for more information on how Pods are used with workload resources.Workload resources for managing podsUsually you don't need to create Pods directly, even singleton Pods. Instead, create them using workload resources such as Deployment or Job. If your Pods need to track state, consider the StatefulSet resource.Each Pod is meant to run a single instance of a given application. If you want to scale your application horizontally (to provide more overall resources by running more instances), you should use multiple Pods, one for each instance. In Kubernetes, this is typically referred to as replication. Replicated Pods are usually created and managed as a group by a workload resource and its controller.See Pods and controllers for more information on how Kubernetes uses workload resources, and their controllers, to implement application scaling and auto-healing.Pods natively provide two kinds of shared resources for their constituent containers: networking and storage.Working with PodsYou'll rarely create individual Pods directly in Kubernetes—even singleton Pods. This is because Pods are designed as relatively ephemeral, disposable entities. When a Pod gets created (directly by you, or indirectly by a controller), the new Pod is scheduled to run on a Node in your cluster. The Pod remains on that node until the Pod finishes execution, the Pod object is deleted, the Pod is evicted for lack of resources, or the node fails.Note:Restarting a container in a Pod should not be confused with restarting a Pod. A Pod is not a process, but an environment for running container(s). A Pod persists until it is deleted.The name of a Pod must be a valid DNS subdomain value, but this can produce unexpected results for the Pod hostname. For best compatibility, the name should follow the more restrictive rules for a DNS label.Pod OSFEATURE STATE: Kubernetes v1.25 [stable]You should set the .spec.os.name field to either windows or linux to indicate the OS on which you want the pod to run. These two are the only operating systems supported for now by Kubernetes. In the future, this list may be expanded.In Kubernetes v1.33, the value of .spec.os.name does not affect how the kube-scheduler picks a node for the Pod to run on. In any cluster where there is more than one operating system for running nodes, you should set the kubernetes.io/os label correctly on each node, and define pods with a nodeSelector based on the operating system label. The kube-scheduler assigns your pod to a node based on other criteria and may or may not succeed in picking a suitable node placement where the node OS is right for the containers in that Pod. The Pod security standards also use this field to avoid enforcing policies that aren't relevant to the operating system.Pods and controllersYou can use workload resources to create and manage multiple Pods for you. A controller for the resource handles replication and rollout and automatic healing in case of Pod failure. For example, if a Node fails, a controller notices that Pods on that Node have stopped working and creates a replacement Pod. The scheduler places the replacement Pod onto a healthy Node.Here are some examples of workload resources that manage one or more Pods:DeploymentStatefulSetDaemonSetPod templatesControllers for workload resources create Pods from a pod template and manage those Pods on your behalf.PodTemplates are specifications for creating Pods, and are included in workload resources such as Deployments, Jobs, and DaemonSets.Each controller for a workload resource uses the PodTemplate inside the workload object to make actual Pods. The PodTemplate is part of the desired state of whatever workload resource you used to run your app.When you create a Pod, you can include environment variables in the Pod template for the containers that run in the Pod.The sample below is a manifest for a simple Job with a template that starts one container. The container in that Pod prints a message then pauses.apiVersion: batch/v1 kind: Job metadata: name: hello spec: template: # This is the pod template spec: containers: \- name: hello image: busybox:1.28 command: ['sh', '-c', 'echo "Hello, Kubernetes!" && sleep 3600'] restartPolicy: OnFailure # The pod template ends here Modifying the pod template or switching to a new pod template has no direct effect on the Pods that already exist. If you change the pod template for a workload resource, that resource needs to create replacement Pods that use the updated template.For example, the StatefulSet controller ensures that the running Pods match the current pod template for each StatefulSet object. If you edit the StatefulSet to change its pod template, the StatefulSet starts to create new Pods based on the updated template. Eventually, all of the old Pods are replaced with new Pods, and the update is complete.Each workload resource implements its own rules for handling changes to the Pod template. If you want to read more about StatefulSet specifically, read Update strategy in the StatefulSet Basics tutorial.On Nodes, the kubelet does not directly observe or manage any of the details around pod templates and updates; those details are abstracted away. That abstraction and separation of concerns simplifies system semantics, and makes it feasible to extend the cluster's behavior without changing existing code.Pod update and replacementAs mentioned in the previous section, when the Pod template for a workload resource is changed, the controller creates new Pods based on the updated template instead of updating or patching the existing Pods.Kubernetes doesn't prevent you from managing Pods directly. It is possible to update some fields of a running Pod, in place. However, Pod update operations like patch, and replace have some limitations:Most of the metadata about a Pod is immutable. For example, you cannot change the namespace, name, uid, or creationTimestamp fields.The generation field is unique. It will be automatically set by the system such that new pods have a generation of 1, and every update to mutable fields in the pod's spec will increment the generation by 1. If the alpha feature gate PodObservedGenerationTracking is set, the pod's status.observedGeneration will reflect the metadata.generation of the pod at the point that the pod status is being reported.If the metadata.deletionTimestamp is set, no new entry can be added to the metadata.finalizers list.Pod updates may not change fields other than spec.containers[*].image, spec.initContainers[*].image, spec.activeDeadlineSeconds or spec.tolerations. For spec.tolerations, you can only add new entries.When updating the spec.activeDeadlineSeconds field, two types of updates are allowed:setting the unassigned field to a positive number;updating the field from a positive number to a smaller, non-negative number.Pod subresourcesThe above update rules apply to regular pod updates, but other pod fields can be updated through subresources.Resize: The resize subresource allows container resources (spec.containers[*].resources) to be updated. See Resize Container Resources for more details.Ephemeral Containers: The ephemeralContainers subresource allows ephemeral containers to be added to a Pod. See Ephemeral Containers for more details.Status: The status subresource allows the pod status to be updated. This is typically only used by the Kubelet and other system controllers.Binding: The binding subresource allows setting the pod's spec.nodeName via a Binding request. This is typically only used by the scheduler.Resource sharing and communicationPods enable data sharing and communication among their constituent containers.Storage in PodsA Pod can specify a set of shared storage volumes. All containers in the Pod can access the shared volumes, allowing those containers to share data. Volumes also allow persistent data in a Pod to survive in case one of the containers within needs to be restarted. See Storage for more information on how Kubernetes implements shared storage and makes it available to Pods.Pod networkingEach Pod is assigned a unique IP address for each address family. Every container in a Pod shares the network namespace, including the IP address and network ports. Inside a Pod (and only then), the containers that belong to the Pod can communicate with one another using localhost. When containers in a Pod communicate with entities outside the Pod, they must coordinate how they use the shared network resources (such as ports). Within a Pod, containers share an IP address and port space, and can find each other via localhost. The containers in a Pod can also communicate with each other using standard inter-process communications like SystemV semaphores or POSIX shared memory. Containers in different Pods have distinct IP addresses and can not communicate by OS-level IPC without special configuration. Containers that want to interact with a container running in a different Pod can use IP networking to communicate.Containers within the Pod see the system hostname as being the same as the configured name for the Pod. There's more about this in the networking section.Pod security settingsTo set security constraints on Pods and containers, you use the securityContext field in the Pod specification. This field gives you granular control over what a Pod or individual containers can do. For example:Drop specific Linux capabilities to avoid the impact of a CVE.Force all processes in the Pod to run as a non-root user or as a specific user or group ID.Set a specific seccomp profile.Set Windows security options, such as whether containers run as HostProcess.Caution:You can also use the Pod securityContext to enable privileged mode in Linux containers. Privileged mode overrides many of the other security settings in the securityContext. Avoid using this setting unless you can't grant the equivalent permissions by using other fields in the securityContext. In Kubernetes 1.26 and later, you can run Windows containers in a similarly privileged mode by setting the windowsOptions.hostProcess flag on the security context of the Pod spec. For details and instructions, see Create a Windows HostProcess Pod.To learn about kernel-level security constraints that you can use, see Linux kernel security constraints for Pods and containers.To learn more about the Pod security context, see Configure a Security Context for a Pod or Container.Static PodsStatic Pods are managed directly by the kubelet daemon on a specific node, without the API server observing them. Whereas most Pods are managed by the control plane (for example, a Deployment), for static Pods, the kubelet directly supervises each static Pod (and restarts it if it fails).Static Pods are always bound to one Kubelet on a specific node. The main use for static Pods is to run a self-hosted control plane: in other words, using the kubelet to supervise the individual control plane components.The kubelet automatically tries to create a mirror Pod on the Kubernetes API server for each static Pod. This means that the Pods running on a node are visible on the API server, but cannot be controlled from there. See the guide Create static Pods for more information.Note:The spec of a static Pod cannot refer to other API objects (e.g., ServiceAccount, ConfigMap, Secret, etc).Pods with multiple containersPods are designed to support multiple cooperating processes (as containers) that form a cohesive unit of service. The containers in a Pod are automatically co-located and co-scheduled on the same physical or virtual machine in the cluster. The containers can share resources and dependencies, communicate with one another, and coordinate when and how they are terminated.Pods in a Kubernetes cluster are used in two main ways:Pods that run a single container. The "one-container-per-Pod" model is the most common Kubernetes use case; in this case, you can think of a Pod as a wrapper around a single container; Kubernetes manages Pods rather than managing the containers directly.Pods that run multiple containers that need to work together. A Pod can encapsulate an application composed of multiple co-located containers that are tightly coupled and need to share resources. These co-located containers form a single cohesive unit of service—for example, one container serving data stored in a shared volume to the public, while a separate sidecar container refreshes or updates those files. The Pod wraps these containers, storage resources, and an ephemeral network identity together as a single unit.For example, you might have a container that acts as a web server for files in a shared volume, and a separate sidecar container that updates those files from a remote source, as in the following diagram:Some Pods have init containers as well as app containers. By default, init containers run and complete before the app containers are started.You can also have sidecar containers that provide auxiliary services to the main application Pod (for example: a service mesh).FEATURE STATE: Kubernetes v1.33 [stable] (enabled by default: true)Enabled by default, the SidecarContainers feature gate allows you to specify restartPolicy: Always for init containers. Setting the Always restart policy ensures that the containers where you set it are treated as sidecars that are kept running during the entire lifetime of the Pod. Containers that you explicitly define as sidecar containers start up before the main application Pod and remain running until the Pod is shut down.Container probesA probe is a diagnostic performed periodically by the kubelet on a container. To perform a diagnostic, the kubelet can invoke different actions:ExecAction (performed with the help of the container runtime)TCPSocketAction (checked directly by the kubelet)HTTPGetAction (checked directly by the kubelet)You can read more about probes in the Pod Lifecycle documentation.What's nextLearn about the lifecycle of a Pod.Learn about RuntimeClass and how you can use it to configure different Pods with different container runtime configurations.Read about PodDisruptionBudget and how you can use it to manage application availability during disruptions.Pod is a top-level resource in the Kubernetes REST API. The Pod object definition describes the object in detail.The Distributed System Toolkit: Patterns for Composite Containers explains common layouts for Pods with more than one container.Read about Pod topology spread constraintsTo understand the context for why Kubernetes wraps a common Pod API in other resources (such as StatefulSets or Deployments), you can read about the prior art, including:AuroraBorgMarathonOmegaTupperware.3.4.1.1 - Pod LifecycleThis page describes the lifecycle of a Pod. Pods follow a defined lifecycle, starting in the Pending phase, moving through Running if at least one of its primary containers starts OK, and then through either the Succeeded or Failed phases depending on whether any container in the Pod terminated in failure.Like individual application containers, Pods are considered to be relatively ephemeral (rather than durable) entities. Pods are created, assigned a unique ID (UID), and scheduled to run on nodes where they remain until termination (according to restart policy) or deletion. If a Node dies, the Pods running on (or scheduled to run on) that node are marked for deletion. The control plane marks the Pods for removal after a timeout period.Pod lifetimeWhilst a Pod is running, the kubelet is able to restart containers to handle some kind of faults. Within a Pod, Kubernetes tracks different container states and determines what action to take to make the Pod healthy again.In the Kubernetes API, Pods have both a specification and an actual status. The status for a Pod object consists of a set of Pod conditions. You can also inject custom readiness information into the condition data for a Pod, if that is useful to your application.Pods are only scheduled once in their lifetime; assigning a Pod to a specific node is called binding, and the process of selecting which node to use is called scheduling. Once a Pod has been scheduled and is bound to a node, Kubernetes tries to run that Pod on the node. The Pod runs on that node until it stops, or until the Pod is terminated; if Kubernetes isn't able to start the Pod on the selected node (for example, if the node crashes before the Pod starts), then that particular Pod never starts.You can use Pod Scheduling Readiness to delay scheduling for a Pod until all its scheduling gates are removed. For example, you might want to define a set of Pods but only trigger scheduling once all the Pods have been created.Pods and fault recoveryIf one of the containers in the Pod fails, then Kubernetes may try to restart that specific container. Read How Pods handle problems with containers to learn more.Pods can however fail in a way that the cluster cannot recover from, and in that case Kubernetes does not attempt to heal the Pod further; instead, Kubernetes deletes the Pod and relies on other components to provide automatic healing.If a Pod is scheduled to a node and that node then fails, the Pod is treated as unhealthy and Kubernetes eventually deletes the Pod. A Pod won't survive an eviction due to a lack of resources or Node maintenance.Kubernetes uses a higher-level abstraction, called a controller, that handles the work of managing the relatively disposable Pod instances.A given Pod (as defined by a UID) is never "rescheduled" to a different node; instead, that Pod can be replaced by a new, near-identical Pod. If you make a replacement Pod, it can even have same name (as in .metadata.name) that the old Pod had, but the replacement would have a different .metadata.uid from the old Pod.Kubernetes does not guarantee that a replacement for an existing Pod would be scheduled to the same node as the old Pod that was being replaced.Associated lifetimesWhen something is said to have the same lifetime as a Pod, such as a volume, that means that the thing exists as long as that specific Pod (with that exact UID) exists. If that Pod is deleted for any reason, and even if an identical replacement is created, the related thing (a volume, in this example) is also destroyed and created anew.Figure 1.A multi-container Pod that contains a file puller sidecar and a web server. The Pod uses an ephemeral emptyDir volume for shared storage between the containers.Pod phaseA Pod's status field is a PodStatus object, which has a phase field.The phase of a Pod is a simple, high-level summary of where the Pod is in its lifecycle. The phase is not intended to be a comprehensive rollup of observations of container or Pod state, nor is it intended to be a comprehensive state machine.The number and meanings of Pod phase values are tightly guarded. Other than what is documented here, nothing should be assumed about Pods that have a given phase value.Here are the possible values for phase:ValueDescriptionPendingThe Pod has been accepted by the Kubernetes cluster, but one or more of the containers has not been set up and made ready to run. This includes time a Pod spends waiting to be scheduled as well as the time spent downloading container images over the network.RunningThe Pod has been bound to a node, and all of the containers have been created. At least one container is still running, or is in the process of starting or restarting.SucceededAll containers in the Pod have terminated in success, and will not be restarted.FailedAll containers in the Pod have terminated, and at least one container has terminated in failure. That is, the container either exited with non-zero status or was terminated by the system, and is not set for automatic restarting.UnknownFor some reason the state of the Pod could not be obtained. This phase typically occurs due to an error in communicating with the node where the Pod should be running.Note:When a pod is failing to start repeatedly, CrashLoopBackOff may appear in the Status field of some kubectl commands. Similarly, when a pod is being deleted, Terminating may appear in the Status field of some kubectl commands.Make sure not to confuse Status, a kubectl display field for user intuition, with the pod's phase. Pod phase is an explicit part of the Kubernetes data model and of the Pod API. NAMESPACE NAME READY STATUS RESTARTS AGE alessandras-namespace alessandras-pod 0/1 CrashLoopBackOff 200 2d9h A Pod is granted a term to terminate gracefully, which defaults to 30 seconds. You can use the flag --force to terminate a Pod by force.Since Kubernetes 1.27, the kubelet transitions deleted Pods, except for static Pods and force-deleted Pods without a finalizer, to a terminal phase (Failed or Succeeded depending on the exit statuses of the pod containers) before their deletion from the API server.If a node dies or is disconnected from the rest of the cluster, Kubernetes applies a policy for setting the phase of all Pods on the lost node to Failed.Container statesAs well as the phase of the Pod overall, Kubernetes tracks the state of each container inside a Pod. You can use container lifecycle hooks to trigger events to run at certain points in a container's lifecycle.Once the scheduler assigns a Pod to a Node, the kubelet starts creating containers for that Pod using a container runtime. There are three possible container states: Waiting, Running, and Terminated.To check the state of a Pod's containers, you can use kubectl describe pod . The output shows the state for each container within that Pod.Each state has a specific meaning:WaitingIf a container is not in either the Running or Terminated state, it is Waiting. A container in the Waiting state is still running the operations it requires in order to complete start up: for example, pulling the container image from a container image registry, or applying Secret data. When you use kubectl to query a Pod with a container that is Waiting, you also see a Reason field to summarize why the container is in that state.RunningThe Running status indicates that a container is executing without issues. If there was a postStart hook configured, it has already executed and finished. When you use kubectl to query a Pod with a container that is Running, you also see information about when the container entered the Running state.TerminatedA container in the Terminated state began execution and then either ran to completion or failed for some reason. When you use kubectl to query a Pod with a container that is Terminated, you see a reason, an exit code, and the start and finish time for that container's period of execution.If a container has a preStop hook configured, this hook runs before the container enters the Terminated state.How Pods handle problems with containersKubernetes manages container failures within Pods using a restartPolicy defined in the Pod spec. This policy determines how Kubernetes reacts to containers exiting due to errors or other reasons, which falls in the following sequence:Initial crash: Kubernetes attempts an immediate restart based on the Pod restartPolicy.Repeated crashes: After the initial crash Kubernetes applies an exponential backoff delay for subsequent restarts, described in restartPolicy. This prevents rapid, repeated restart attempts from overloading the system.CrashLoopBackOff state: This indicates that the backoff delay mechanism is currently in effect for a given container that is in a crash loop, failing and restarting repeatedly.Backoff reset: If a container runs successfully for a certain duration (e.g., 10 minutes), Kubernetes resets the backoff delay, treating any new crash as the first one.In practice, a CrashLoopBackOff is a condition or event that might be seen as output from the kubectl command, while describing or listing Pods, when a container in the Pod fails to start properly and then continually tries and fails in a loop.In other words, when a container enters the crash loop, Kubernetes applies the exponential backoff delay mentioned in the Container restart policy. This mechanism prevents a faulty container from overwhelming the system with continuous failed start attempts.The CrashLoopBackOff can be caused by issues like the following:Application errors that cause the container to exit.Configuration errors, such as incorrect environment variables or missing configuration files.Resource constraints, where the container might not have enough memory or CPU to start properly.Health checks failing if the application doesn't start serving within the expected time.Container liveness probes or startup probes returning a Failure result as mentioned in the probes section.To investigate the root cause of a CrashLoopBackOff issue, a user can:Check logs: Use kubectl logs  to check the logs of the container. This is often the most direct way to diagnose the issue causing the crashes.Inspect events: Use kubectl describe pod  to see events for the Pod, which can provide hints about configuration or resource issues.Review configuration: Ensure that the Pod configuration, including environment variables and mounted volumes, is correct and that all required external resources are available.Check resource limits: Make sure that the container has enough CPU and memory allocated. Sometimes, increasing the resources in the Pod definition can resolve the issue.Debug application: There might exist bugs or misconfigurations in the application code. Running this container image locally or in a development environment can help diagnose application specific issues.Container restart policyThe spec of a Pod has a restartPolicy field with possible values Always, OnFailure, and Never. The default value is Always.The restartPolicy for a Pod applies to app containers in the Pod and to regular init containers. Sidecar containers ignore the Pod-level restartPolicy field: in Kubernetes, a sidecar is defined as an entry inside initContainers that has its container-level restartPolicy set to Always. For init containers that exit with an error, the kubelet restarts the init container if the Pod level restartPolicy is either OnFailure or Always:Always: Automatically restarts the container after any termination.OnFailure: Only restarts the container if it exits with an error (non-zero exit status).Never: Does not automatically restart the terminated container.When the kubelet is handling container restarts according to the configured restart policy, that only applies to restarts that make replacement containers inside the same Pod and running on the same node. After containers in a Pod exit, the kubelet restarts them with an exponential backoff delay (10s, 20s, 40s, …), that is capped at 300 seconds (5 minutes). Once a container has executed for 10 minutes without any problems, the kubelet resets the restart backoff timer for that container. Sidecar containers and Pod lifecycle explains the behaviour of init containers when specify restartpolicy field on it.Reduced container restart delayFEATURE STATE: Kubernetes v1.33 [alpha] (enabled by default: false)With the alpha feature gate ReduceDefaultCrashLoopBackOffDecay enabled, container start retries across your cluster will be reduced to begin at 1s (instead of 10s) and increase exponentially by 2x each restart until a maximum delay of 60s (instead of 300s which is 5 minutes).If you use this feature along with the alpha feature KubeletCrashLoopBackOffMax (described below), individual nodes may have different maximum delays.Configurable container restart delayFEATURE STATE: Kubernetes v1.32 [alpha] (enabled by default: false)With the alpha feature gate KubeletCrashLoopBackOffMax enabled, you can reconfigure the maximum delay between container start retries from the default of 300s (5 minutes). This configuration is set per node using kubelet configuration. In your kubelet configuration, under crashLoopBackOff set the maxContainerRestartPeriod field between "1s" and "300s". As described above in Container restart policy, delays on that node will still start at 10s and increase exponentially by 2x each restart, but will now be capped at your configured maximum. If the maxContainerRestartPeriod you configure is less than the default initial value of 10s, the initial delay will instead be set to the configured maximum.See the following kubelet configuration examples:# container restart delays will start at 10s, increasing # 2x each time they are restarted, to a maximum of 100s kind: KubeletConfiguration crashLoopBackOff: maxContainerRestartPeriod: "100s" # delays between container restarts will always be 2s kind: KubeletConfiguration crashLoopBackOff: maxContainerRestartPeriod: "2s" If you use this feature along with the alpha feature ReduceDefaultCrashLoopBackOffDecay (described above), your cluster defaults for initial backoff and maximum backoff will no longer be 10s and 300s, but 1s and 60s. Per node configuration takes precedence over the defaults set by ReduceDefaultCrashLoopBackOffDecay, even if this would result in a node having a longer maximum backoff than other nodes in the cluster.Pod conditionsA Pod has a PodStatus, which has an array of PodConditions through which the Pod has or has not passed. Kubelet manages the following PodConditions:PodScheduled: the Pod has been scheduled to a node.PodReadyToStartContainers: (beta feature; enabled by default) the Pod sandbox has been successfully created and networking configured.ContainersReady: all containers in the Pod are ready.Initialized: all init containers have completed successfully.Ready: the Pod is able to serve requests and should be added to the load balancing pools of all matching Services.Field nameDescriptiontypeName of this Pod condition.statusIndicates whether that condition is applicable, with possible values "True", "False", or "Unknown".lastProbeTimeTimestamp of when the Pod condition was last probed.lastTransitionTimeTimestamp for when the Pod last transitioned from one status to another.reasonMachine-readable, UpperCamelCase text indicating the reason for the condition's last transition.messageHuman-readable message indicating details about the last status transition.Pod readinessFEATURE STATE: Kubernetes v1.14 [stable]Your application can inject extra feedback or signals into PodStatus: Pod readiness. To use this, set readinessGates in the Pod's spec to specify a list of additional conditions that the kubelet evaluates for Pod readiness.Readiness gates are determined by the current state of status.condition fields for the Pod. If Kubernetes cannot find such a condition in the status.conditions field of a Pod, the status of the condition is defaulted to "False".Here is an example:kind: Pod ... spec: readinessGates: \- conditionType: "www.example.com/feature-1" status: conditions: \- type: Ready # a built in PodCondition status: "False" lastProbeTime: null lastTransitionTime: 2018-01-01T00:00:00Z \- type: "www.example.com/feature-1" # an extra PodCondition status: "False" lastProbeTime: null lastTransitionTime: 2018-01-01T00:00:00Z containerStatuses: \- containerID: docker://abcd... ready: true ... The Pod conditions you add must have names that meet the Kubernetes label key format.Status for Pod readinessThe kubectl patch command does not support patching object status. To set these status.conditions for the Pod, applications and operators should use the PATCH action. You can use a Kubernetes client library to write code that sets custom Pod conditions for Pod readiness.For a Pod that uses custom conditions, that Pod is evaluated to be ready only when both the following statements apply:All containers in the Pod are ready.All conditions specified in readinessGates are True.When a Pod's containers are Ready but at least one custom condition is missing or False, the kubelet sets the Pod's condition to ContainersReady.Pod network readinessFEATURE STATE: Kubernetes v1.29 [beta]Note:During its early development, this condition was named PodHasNetwork.After a Pod gets scheduled on a node, it needs to be admitted by the kubelet and to have any required storage volumes mounted. Once these phases are complete, the kubelet works with a container runtime (using Container Runtime Interface (CRI)) to set up a runtime sandbox and configure networking for the Pod. If the PodReadyToStartContainersCondition feature gate is enabled (it is enabled by default for Kubernetes 1.33), the PodReadyToStartContainers condition will be added to the status.conditions field of a Pod.The PodReadyToStartContainers condition is set to False by the Kubelet when it detects a Pod does not have a runtime sandbox with networking configured. This occurs in the following scenarios:Early in the lifecycle of the Pod, when the kubelet has not yet begun to set up a sandbox for the Pod using the container runtime.Later in the lifecycle of the Pod, when the Pod sandbox has been destroyed due to either:the node rebooting, without the Pod getting evictedfor container runtimes that use virtual machines for isolation, the Pod sandbox virtual machine rebooting, which then requires creating a new sandbox and fresh container network configuration.The PodReadyToStartContainers condition is set to True by the kubelet after the successful completion of sandbox creation and network configuration for the Pod by the runtime plugin. The kubelet can start pulling container images and create containers after PodReadyToStartContainers condition has been set to True.For a Pod with init containers, the kubelet sets the Initialized condition to True after the init containers have successfully completed (which happens after successful sandbox creation and network configuration by the runtime plugin). For a Pod without init containers, the kubelet sets the Initialized condition to True before sandbox creation and network configuration starts.Container probesA probe is a diagnostic performed periodically by the kubelet on a container. To perform a diagnostic, the kubelet either executes code within the container, or makes a network request.Check mechanismsThere are four different ways to check a container using a probe. Each probe must define exactly one of these four mechanisms:execExecutes a specified command inside the container. The diagnostic is considered successful if the command exits with a status code of 0.grpcPerforms a remote procedure call using gRPC. The target should implement gRPC health checks. The diagnostic is considered successful if the status of the response is SERVING.httpGetPerforms an HTTP GET request against the Pod's IP address on a specified port and path. The diagnostic is considered successful if the response has a status code greater than or equal to 200 and less than 400.tcpSocketPerforms a TCP check against the Pod's IP address on a specified port. The diagnostic is considered successful if the port is open. If the remote system (the container) closes the connection immediately after it opens, this counts as healthy.Caution:Unlike the other mechanisms, exec probe's implementation involves the creation/forking of multiple processes each time when executed. As a result, in case of the clusters having higher pod densities, lower intervals of initialDelaySeconds, periodSeconds, configuring any probe with exec mechanism might introduce an overhead on the cpu usage of the node. In such scenarios, consider using the alternative probe mechanisms to avoid the overhead.Probe outcomeEach probe has one of three results:SuccessThe container passed the diagnostic.FailureThe container failed the diagnostic.UnknownThe diagnostic failed (no action should be taken, and the kubelet will make further checks).Types of probeThe kubelet can optionally perform and react to three kinds of probes on running containers:livenessProbeIndicates whether the container is running. If the liveness probe fails, the kubelet kills the container, and the container is subjected to its restart policy. If a container does not provide a liveness probe, the default state is Success.readinessProbeIndicates whether the container is ready to respond to requests. If the readiness probe fails, the EndpointSlice controller removes the Pod's IP address from the EndpointSlices of all Services that match the Pod. The default state of readiness before the initial delay is Failure. If a container does not provide a readiness probe, the default state is Success.startupProbeIndicates whether the application within the container is started. All other probes are disabled if a startup probe is provided, until it succeeds. If the startup probe fails, the kubelet kills the container, and the container is subjected to its restart policy. If a container does not provide a startup probe, the default state is Success.For more information about how to set up a liveness, readiness, or startup probe, see Configure Liveness, Readiness and Startup Probes.When should you use a liveness probe?If the process in your container is able to crash on its own whenever it encounters an issue or becomes unhealthy, you do not necessarily need a liveness probe; the kubelet will automatically perform the correct action in accordance with the Pod's restartPolicy.If you'd like your container to be killed and restarted if a probe fails, then specify a liveness probe, and specify a restartPolicy of Always or OnFailure.When should you use a readiness probe?If you'd like to start sending traffic to a Pod only when a probe succeeds, specify a readiness probe. In this case, the readiness probe might be the same as the liveness probe, but the existence of the readiness probe in the spec means that the Pod will start without receiving any traffic and only start receiving traffic after the probe starts succeeding.If you want your container to be able to take itself down for maintenance, you can specify a readiness probe that checks an endpoint specific to readiness that is different from the liveness probe.If your app has a strict dependency on back-end services, you can implement both a liveness and a readiness probe. The liveness probe passes when the app itself is healthy, but the readiness probe additionally checks that each required back-end service is available. This helps you avoid directing traffic to Pods that can only respond with error messages.If your container needs to work on loading large data, configuration files, or migrations during startup, you can use a startup probe. However, if you want to detect the difference between an app that has failed and an app that is still processing its startup data, you might prefer a readiness probe.Note:If you want to be able to drain requests when the Pod is deleted, you do not necessarily need a readiness probe; on deletion, the Pod automatically puts itself into an unready state regardless of whether the readiness probe exists. The Pod remains in the unready state while it waits for the containers in the Pod to stop.When should you use a startup probe?Startup probes are useful for Pods that have containers that take a long time to come into service. Rather than set a long liveness interval, you can configure a separate configuration for probing the container as it starts up, allowing a time longer than the liveness interval would allow.If your container usually starts in more than \\( initialDelaySeconds + failureThreshold \times periodSeconds \\), you should specify a startup probe that checks the same endpoint as the liveness probe. The default for periodSeconds is 10s. You should then set its failureThreshold high enough to allow the container to start, without changing the default values of the liveness probe. This helps to protect against deadlocks.Termination of PodsBecause Pods represent processes running on nodes in the cluster, it is important to allow those processes to gracefully terminate when they are no longer needed (rather than being abruptly stopped with a KILL signal and having no chance to clean up).The design aim is for you to be able to request deletion and know when processes terminate, but also be able to ensure that deletes eventually complete. When you request deletion of a Pod, the cluster records and tracks the intended grace period before the Pod is allowed to be forcefully killed. With that forceful shutdown tracking in place, the kubelet attempts graceful shutdown.Typically, with this graceful termination of the pod, kubelet makes requests to the container runtime to attempt to stop the containers in the pod by first sending a TERM (aka. SIGTERM) signal, with a grace period timeout, to the main process in each container. The requests to stop the containers are processed by the container runtime asynchronously. There is no guarantee to the order of processing for these requests. Many container runtimes respect the STOPSIGNAL value defined in the container image and, if different, send the container image configured STOPSIGNAL instead of TERM. Once the grace period has expired, the KILL signal is sent to any remaining processes, and the Pod is then deleted from the API Server. If the kubelet or the container runtime's management service is restarted while waiting for processes to terminate, the cluster retries from the start including the full original grace period.Stop SignalsThe stop signal used to kill the container can be defined in the container image with the STOPSIGNAL instruction. If no stop signal is defined in the image, the default signal of the container runtime (SIGTERM for both containerd and CRI-O) would be used to kill the container.Defining custom stop signalsFEATURE STATE: Kubernetes v1.33 [alpha] (enabled by default: false)If the ContainerStopSignals feature gate is enabled, you can configure a custom stop signal for your containers from the container Lifecycle. We require the Pod's spec.os.name field to be present as a requirement for defining stop signals in the container lifecycle. The list of signals that are valid depends on the OS the Pod is scheduled to. For Pods scheduled to Windows nodes, we only support SIGTERM and SIGKILL as valid signals.Here is an example Pod spec defining a custom stop signal:spec: os: name: linux containers: \- name: my-container image: container-image:latest lifecycle: stopSignal: SIGUSR1 If a stop signal is defined in the lifecycle, this will override the signal defined in the container image. If no stop signal is defined in the container spec, the container would fall back to the default behavior.Pod Termination FlowPod termination flow, illustrated with an example:You use the kubectl tool to manually delete a specific Pod, with the default grace period (30 seconds).The Pod in the API server is updated with the time beyond which the Pod is considered "dead" along with the grace period. If you use kubectl describe to check the Pod you're deleting, that Pod shows up as "Terminating". On the node where the Pod is running: as soon as the kubelet sees that a Pod has been marked as terminating (a graceful shutdown duration has been set), the kubelet begins the local Pod shutdown process.If one of the Pod's containers has defined a preStop hook and the terminationGracePeriodSeconds in the Pod spec is not set to 0, the kubelet runs that hook inside of the container. The default terminationGracePeriodSeconds setting is 30 seconds.If the preStop hook is still running after the grace period expires, the kubelet requests a small, one-off grace period extension of 2 seconds.Note:If the preStop hook needs longer to complete than the default grace period allows, you must modify terminationGracePeriodSeconds to suit this.The kubelet triggers the container runtime to send a TERM signal to process 1 inside each container.There is special ordering if the Pod has any sidecar containers defined. Otherwise, the containers in the Pod receive the TERM signal at different times and in an arbitrary order. If the order of shutdowns matters, consider using a preStop hook to synchronize (or switch to using sidecar containers).At the same time as the kubelet is starting graceful shutdown of the Pod, the control plane evaluates whether to remove that shutting-down Pod from EndpointSlice objects, where those objects represent a Service with a configured selector. ReplicaSets and other workload resources no longer treat the shutting-down Pod as a valid, in-service replica.Pods that shut down slowly should not continue to serve regular traffic and should start terminating and finish processing open connections. Some applications need to go beyond finishing open connections and need more graceful termination, for example, session draining and completion.Any endpoints that represent the terminating Pods are not immediately removed from EndpointSlices, and a status indicating terminating state is exposed from the EndpointSlice API. Terminating endpoints always have their ready status as false (for backward compatibility with versions before 1.26), so load balancers will not use it for regular traffic.If traffic draining on terminating Pod is needed, the actual readiness can be checked as a condition serving. You can find more details on how to implement connections draining in the tutorial Pods And Endpoints Termination FlowThe kubelet ensures the Pod is shut down and terminatedWhen the grace period expires, if there is still any container running in the Pod, the kubelet triggers forcible shutdown. The container runtime sends SIGKILL to any processes still running in any container in the Pod. The kubelet also cleans up a hidden pause container if that container runtime uses one.The kubelet transitions the Pod into a terminal phase (Failed or Succeeded depending on the end state of its containers).The kubelet triggers forcible removal of the Pod object from the API server, by setting grace period to 0 (immediate deletion).The API server deletes the Pod's API object, which is then no longer visible from any client.Forced Pod terminationCaution:Forced deletions can be potentially disruptive for some workloads and their Pods.By default, all deletes are graceful within 30 seconds. The kubectl delete command supports the --grace-period= option which allows you to override the default and specify your own value.Setting the grace period to 0 forcibly and immediately deletes the Pod from the API server. If the Pod was still running on a node, that forcible deletion triggers the kubelet to begin immediate cleanup.Using kubectl, You must specify an additional flag --force along with --grace-period=0 in order to perform force deletions.When a force deletion is performed, the API server does not wait for confirmation from the kubelet that the Pod has been terminated on the node it was running on. It removes the Pod in the API immediately so a new Pod can be created with the same name. On the node, Pods that are set to terminate immediately will still be given a small grace period before being force killed.Caution:Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.If you need to force-delete Pods that are part of a StatefulSet, refer to the task documentation for deleting Pods from a StatefulSet.Pod shutdown and sidecar containersIf your Pod includes one or more sidecar containers (init containers with an Always restart policy), the kubelet will delay sending the TERM signal to these sidecar containers until the last main container has fully terminated. The sidecar containers will be terminated in the reverse order they are defined in the Pod spec. This ensures that sidecar containers continue serving the other containers in the Pod until they are no longer needed.This means that slow termination of a main container will also delay the termination of the sidecar containers. If the grace period expires before the termination process is complete, the Pod may enter forced termination. In this case, all remaining containers in the Pod will be terminated simultaneously with a short grace period.Similarly, if the Pod has a preStop hook that exceeds the termination grace period, emergency termination may occur. In general, if you have used preStop hooks to control the termination order without sidecar containers, you can now remove them and allow the kubelet to manage sidecar termination automatically.Garbage collection of PodsFor failed Pods, the API objects remain in the cluster's API until a human or controller process explicitly removes them.The Pod garbage collector (PodGC), which is a controller in the control plane, cleans up terminated Pods (with a phase of Succeeded or Failed), when the number of Pods exceeds the configured threshold (determined by terminated-pod-gc-threshold in the kube-controller-manager). This avoids a resource leak as Pods are created and terminated over time.Additionally, PodGC cleans up any Pods which satisfy any of the following conditions:are orphan Pods - bound to a node which no longer exists,are unscheduled terminating Pods,are terminating Pods, bound to a non-ready node tainted with node.kubernetes.io/out-of-service.Along with cleaning up the Pods, PodGC will also mark them as failed if they are in a non-terminal phase. Also, PodGC adds a Pod disruption condition when cleaning up an orphan Pod. See Pod disruption conditions for more details.What's nextGet hands-on experience attaching handlers to container lifecycle events.Get hands-on experience configuring Liveness, Readiness and Startup Probes.Learn more about container lifecycle hooks.Learn more about sidecar containers.For detailed information about Pod and container status in the API, see the API reference documentation covering status for Pod.3.4.1.2 - Init ContainersThis page provides an overview of init containers: specialized containers that run before app containers in a Pod. Init containers can contain utilities or setup scripts not present in an app image.You can specify init containers in the Pod specification alongside the containers array (which describes app containers).In Kubernetes, a sidecar container is a container that starts before the main application container and continues to run. This document is about init containers: containers that run to completion during Pod initialization.Understanding init containersA Pod can have multiple containers running apps within it, but it can also have one or more init containers, which are run before the app containers are started.Init containers are exactly like regular containers, except:Init containers always run to completion.Each init container must complete successfully before the next one starts.If a Pod's init container fails, the kubelet repeatedly restarts that init container until it succeeds. However, if the Pod has a restartPolicy of Never, and an init container fails during startup of that Pod, Kubernetes treats the overall Pod as failed.To specify an init container for a Pod, add the initContainers field into the Pod specification, as an array of container items (similar to the app containers field and its contents). See Container in the API reference for more details.The status of the init containers is returned in .status.initContainerStatuses field as an array of the container statuses (similar to the .status.containerStatuses field).Differences from regular containersInit containers support all the fields and features of app containers, including resource limits, volumes, and security settings. However, the resource requests and limits for an init container are handled differently, as documented in Resource sharing within containers.Regular init containers (in other words: excluding sidecar containers) do not support the lifecycle, livenessProbe, readinessProbe, or startupProbe fields. Init containers must run to completion before the Pod can be ready; sidecar containers continue running during a Pod's lifetime, and do support some probes. See sidecar container for further details about sidecar containers.If you specify multiple init containers for a Pod, kubelet runs each init container sequentially. Each init container must succeed before the next can run. When all of the init containers have run to completion, kubelet initializes the application containers for the Pod and runs them as usual.Differences from sidecar containersInit containers run and complete their tasks before the main application container starts. Unlike sidecar containers, init containers are not continuously running alongside the main containers.Init containers run to completion sequentially, and the main container does not start until all the init containers have successfully completed.init containers do not support lifecycle, livenessProbe, readinessProbe, or startupProbe whereas sidecar containers support all these probes to control their lifecycle.Init containers share the same resources (CPU, memory, network) with the main application containers but do not interact directly with them. They can, however, use shared volumes for data exchange.Using init containersBecause init containers have separate images from app containers, they have some advantages for start-up related code:Init containers can contain utilities or custom code for setup that are not present in an app image. For example, there is no need to make an image FROM another image just to use a tool like sed, awk, python, or dig during setup.The application image builder and deployer roles can work independently without the need to jointly build a single app image.Init containers can run with a different view of the filesystem than app containers in the same Pod. Consequently, they can be given access to Secrets that app containers cannot access.Because init containers run to completion before any app containers start, init containers offer a mechanism to block or delay app container startup until a set of preconditions are met. Once preconditions are met, all of the app containers in a Pod can start in parallel.Init containers can securely run utilities or custom code that would otherwise make an app container image less secure. By keeping unnecessary tools separate you can limit the attack surface of your app container image.ExamplesHere are some ideas for how to use init containers:Wait for a Service to be created, using a shell one-line command like:for i in {1..100}; do sleep 1; if nslookup myservice; then exit 0; fi; done; exit 1 Register this Pod with a remote server from the downward API with a command like:curl -X POST http://$MANAGEMENT_SERVICE_HOST:$MANAGEMENT_SERVICE_PORT/register -d 'instance=$()&ip;=$()' Wait for some time before starting the app container with a command likesleep 60 Clone a Git repository into a VolumePlace values into a configuration file and run a template tool to dynamically generate a configuration file for the main app container. For example, place the POD_IP value in a configuration and generate the main app configuration file using Jinja.Init containers in useThis example defines a simple Pod that has two init containers. The first waits for myservice, and the second waits for mydb. Once both init containers complete, the Pod runs the app container from its spec section.apiVersion: v1 kind: Pod metadata: name: myapp-pod labels: app.kubernetes.io/name: MyApp spec: containers: \- name: myapp-container image: busybox:1.28 command: ['sh', '-c', 'echo The app is running! && sleep 3600'] initContainers: \- name: init-myservice image: busybox:1.28 command: ['sh', '-c', "until nslookup myservice.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for myservice; sleep 2; done"] \- name: init-mydb image: busybox:1.28 command: ['sh', '-c', "until nslookup mydb.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for mydb; sleep 2; done"] You can start this Pod by running:kubectl apply -f myapp.yaml The output is similar to this:pod/myapp-pod created And check on its status with:kubectl get -f myapp.yaml The output is similar to this:NAME READY STATUS RESTARTS AGE myapp-pod 0/1 Init:0/2 0 6m or for more details:kubectl describe -f myapp.yaml The output is similar to this:Name: myapp-pod Namespace: default [...] Labels: app.kubernetes.io/name=MyApp Status: Pending [...] Init Containers: init-myservice: [...] State: Running [...] init-mydb: [...] State: Waiting Reason: PodInitializing Ready: False [...] Containers: myapp-container: [...] State: Waiting Reason: PodInitializing Ready: False [...] Events: FirstSeen LastSeen Count From SubObjectPath Type Reason Message \--------- -------- ----- ---- ------------- -------- ------ ------- 16s 16s 1 {default-scheduler } Normal Scheduled Successfully assigned myapp-pod to 172.17.4.201 16s 16s 1 {kubelet 172.17.4.201} spec.initContainers{init-myservice} Normal Pulling pulling image "busybox" 13s 13s 1 {kubelet 172.17.4.201} spec.initContainers{init-myservice} Normal Pulled Successfully pulled image "busybox" 13s 13s 1 {kubelet 172.17.4.201} spec.initContainers{init-myservice} Normal Created Created container init-myservice 13s 13s 1 {kubelet 172.17.4.201} spec.initContainers{init-myservice} Normal Started Started container init-myservice To see logs for the init containers in this Pod, run:kubectl logs myapp-pod -c init-myservice # Inspect the first init container kubectl logs myapp-pod -c init-mydb # Inspect the second init container At this point, those init containers will be waiting to discover Services named mydb and myservice.Here's a configuration you can use to make those Services appear:--- apiVersion: v1 kind: Service metadata: name: myservice spec: ports: \- protocol: TCP port: 80 targetPort: 9376 \--- apiVersion: v1 kind: Service metadata: name: mydb spec: ports: \- protocol: TCP port: 80 targetPort: 9377 To create the mydb and myservice services:kubectl apply -f services.yaml The output is similar to this:service/myservice created service/mydb created You'll then see that those init containers complete, and that the myapp-pod Pod moves into the Running state:kubectl get -f myapp.yaml The output is similar to this:NAME READY STATUS RESTARTS AGE myapp-pod 1/1 Running 0 9m This simple example should provide some inspiration for you to create your own init containers. What's next contains a link to a more detailed example.Detailed behaviorDuring Pod startup, the kubelet delays running init containers until the networking and storage are ready. Then the kubelet runs the Pod's init containers in the order they appear in the Pod's spec.Each init container must exit successfully before the next container starts. If a container fails to start due to the runtime or exits with failure, it is retried according to the Pod restartPolicy. However, if the Pod restartPolicy is set to Always, the init containers use restartPolicy OnFailure.A Pod cannot be Ready until all init containers have succeeded. The ports on an init container are not aggregated under a Service. A Pod that is initializing is in the Pending state but should have a condition Initialized set to false.If the Pod restarts, or is restarted, all init containers must execute again.Changes to the init container spec are limited to the container image field. Directly altering the image field of an init container does not restart the Pod or trigger its recreation. If the Pod has yet to start, that change may have an effect on how the Pod boots up.For a pod template you can typically change any field for an init container; the impact of making that change depends on where the pod template is used.Because init containers can be restarted, retried, or re-executed, init container code should be idempotent. In particular, code that writes into any emptyDir volume should be prepared for the possibility that an output file already exists.Init containers have all of the fields of an app container. However, Kubernetes prohibits readinessProbe from being used because init containers cannot define readiness distinct from completion. This is enforced during validation.Use activeDeadlineSeconds on the Pod to prevent init containers from failing forever. The active deadline includes init containers. However it is recommended to use activeDeadlineSeconds only if teams deploy their application as a Job, because activeDeadlineSeconds has an effect even after initContainer finished. The Pod which is already running correctly would be killed by activeDeadlineSeconds if you set.The name of each app and init container in a Pod must be unique; a validation error is thrown for any container sharing a name with another.Resource sharing within containersGiven the order of execution for init, sidecar and app containers, the following rules for resource usage apply:The highest of any particular resource request or limit defined on all init containers is the effective init request/limit. If any resource has no resource limit specified this is considered as the highest limit.The Pod's effective request/limit for a resource is the higher of:the sum of all app containers request/limit for a resourcethe effective init request/limit for a resourceScheduling is done based on effective requests/limits, which means init containers can reserve resources for initialization that are not used during the life of the Pod.The QoS (quality of service) tier of the Pod's effective QoS tier is the QoS tier for init containers and app containers alike.Quota and limits are applied based on the effective Pod request and limit.Init containers and Linux cgroupsOn Linux, resource allocations for Pod level control groups (cgroups) are based on the effective Pod request and limit, the same as the scheduler.Pod restart reasonsA Pod can restart, causing re-execution of init containers, for the following reasons:The Pod infrastructure container is restarted. This is uncommon and would have to be done by someone with root access to nodes.All containers in a Pod are terminated while restartPolicy is set to Always, forcing a restart, and the init container completion record has been lost due to garbage collection.The Pod will not be restarted when the init container image is changed, or the init container completion record has been lost due to garbage collection. This applies for Kubernetes v1.20 and later. If you are using an earlier version of Kubernetes, consult the documentation for the version you are using.What's nextLearn more about the following:Creating a Pod that has an init container.Debug init containers.Overview of kubelet and kubectl.Types of probes: liveness, readiness, startup probe.Sidecar containers.3.4.1.3 - Sidecar ContainersFEATURE STATE: Kubernetes v1.29 [beta]Sidecar containers are the secondary containers that run along with the main application container within the same Pod. These containers are used to enhance or to extend the functionality of the primary app container by providing additional services, or functionality such as logging, monitoring, security, or data synchronization, without directly altering the primary application code.Typically, you only have one app container in a Pod. For example, if you have a web application that requires a local webserver, the local webserver is a sidecar and the web application itself is the app container.Sidecar containers in KubernetesKubernetes implements sidecar containers as a special case of init containers; sidecar containers remain running after Pod startup. This document uses the term regular init containers to clearly refer to containers that only run during Pod startup.Provided that your cluster has the SidecarContainers feature gate enabled (the feature is active by default since Kubernetes v1.29), you can specify a restartPolicy for containers listed in a Pod's initContainers field. These restartable sidecar containers are independent from other init containers and from the main application container(s) within the same pod. These can be started, stopped, or restarted without affecting the main application container and other init containers.You can also run a Pod with multiple containers that are not marked as init or sidecar containers. This is appropriate if the containers within the Pod are required for the Pod to work overall, but you don't need to control which containers start or stop first. You could also do this if you need to support older versions of Kubernetes that don't support a container-level restartPolicy field.Example applicationHere's an example of a Deployment with two containers, one of which is a sidecar:application/deployment-sidecar.yaml apiVersion: apps/v1 kind: Deployment metadata: name: myapp labels: app: myapp spec: replicas: 1 selector: matchLabels: app: myapp template: metadata: labels: app: myapp spec: containers: \- name: myapp image: alpine:latest command: ['sh', '-c', 'while true; do echo "logging" >> /opt/logs.txt; sleep 1; done'] volumeMounts: \- name: data mountPath: /opt initContainers: \- name: logshipper image: alpine:latest restartPolicy: Always command: ['sh', '-c', 'tail -F /opt/logs.txt'] volumeMounts: \- name: data mountPath: /opt volumes: \- name: data emptyDir: {}Sidecar containers and Pod lifecycleIf an init container is created with its restartPolicy set to Always, it will start and remain running during the entire life of the Pod. This can be helpful for running supporting services separated from the main application containers.If a readinessProbe is specified for this init container, its result will be used to determine the ready state of the Pod.Since these containers are defined as init containers, they benefit from the same ordering and sequential guarantees as regular init containers, allowing you to mix sidecar containers with regular init containers for complex Pod initialization flows.Compared to regular init containers, sidecars defined within initContainers continue to run after they have started. This is important when there is more than one entry inside .spec.initContainers for a Pod. After a sidecar-style init container is running (the kubelet has set the started status for that init container to true), the kubelet then starts the next init container from the ordered .spec.initContainers list. That status either becomes true because there is a process running in the container and no startup probe defined, or as a result of its startupProbe succeeding.Upon Pod termination, the kubelet postpones terminating sidecar containers until the main application container has fully stopped. The sidecar containers are then shut down in the opposite order of their appearance in the Pod specification. This approach ensures that the sidecars remain operational, supporting other containers within the Pod, until their service is no longer required.Jobs with sidecar containersIf you define a Job that uses sidecar using Kubernetes-style init containers, the sidecar container in each Pod does not prevent the Job from completing after the main container has finished.Here's an example of a Job with two containers, one of which is a sidecar:application/job/job-sidecar.yaml apiVersion: batch/v1 kind: Job metadata: name: myjob spec: template: spec: containers: \- name: myjob image: alpine:latest command: ['sh', '-c', 'echo "logging" > /opt/logs.txt'] volumeMounts: \- name: data mountPath: /opt initContainers: \- name: logshipper image: alpine:latest restartPolicy: Always command: ['sh', '-c', 'tail -F /opt/logs.txt'] volumeMounts: \- name: data mountPath: /opt restartPolicy: Never volumes: \- name: data emptyDir: {}Differences from application containersSidecar containers run alongside app containers in the same pod. However, they do not execute the primary application logic; instead, they provide supporting functionality to the main application.Sidecar containers have their own independent lifecycles. They can be started, stopped, and restarted independently of app containers. This means you can update, scale, or maintain sidecar containers without affecting the primary application.Sidecar containers share the same network and storage namespaces with the primary container. This co-location allows them to interact closely and share resources.From a Kubernetes perspective, the sidecar container's graceful termination is less important. When other containers take all allotted graceful termination time, the sidecar containers will receive the SIGTERM signal, followed by the SIGKILL signal, before they have time to terminate gracefully. So exit codes different from 0 (0 indicates successful exit), for sidecar containers are normal on Pod termination and should be generally ignored by the external tooling.Differences from init containersSidecar containers work alongside the main container, extending its functionality and providing additional services.Sidecar containers run concurrently with the main application container. They are active throughout the lifecycle of the pod and can be started and stopped independently of the main container. Unlike init containers, sidecar containers support probes to control their lifecycle.Sidecar containers can interact directly with the main application containers, because like init containers they always share the same network, and can optionally also share volumes (filesystems).Init containers stop before the main containers start up, so init containers cannot exchange messages with the app container in a Pod. Any data passing is one-way (for example, an init container can put information inside an emptyDir volume).Changing the image of a sidecar container will not cause the Pod to restart, but will trigger a container restart.Resource sharing within containersGiven the order of execution for init, sidecar and app containers, the following rules for resource usage apply:The highest of any particular resource request or limit defined on all init containers is the effective init request/limit. If any resource has no resource limit specified this is considered as the highest limit.The Pod's effective request/limit for a resource is the sum of pod overhead and the higher of:the sum of all non-init containers(app and sidecar containers) request/limit for a resourcethe effective init request/limit for a resourceScheduling is done based on effective requests/limits, which means init containers can reserve resources for initialization that are not used during the life of the Pod.The QoS (quality of service) tier of the Pod's effective QoS tier is the QoS tier for all init, sidecar and app containers alike.Quota and limits are applied based on the effective Pod request and limit.Sidecar containers and Linux cgroupsOn Linux, resource allocations for Pod level control groups (cgroups) are based on the effective Pod request and limit, the same as the scheduler.What's nextLearn how to Adopt Sidecar ContainersRead a blog post on native sidecar containers.Read about creating a Pod that has an init container.Learn about the types of probes: liveness, readiness, startup probe.Learn about pod overhead.3.4.1.4 - Ephemeral ContainersFEATURE STATE: Kubernetes v1.25 [stable]This page provides an overview of ephemeral containers: a special type of container that runs temporarily in an existing Pod to accomplish user-initiated actions such as troubleshooting. You use ephemeral containers to inspect services rather than to build applications.Understanding ephemeral containersPods are the fundamental building block of Kubernetes applications. Since Pods are intended to be disposable and replaceable, you cannot add a container to a Pod once it has been created. Instead, you usually delete and replace Pods in a controlled fashion using deployments.Sometimes it's necessary to inspect the state of an existing Pod, however, for example to troubleshoot a hard-to-reproduce bug. In these cases you can run an ephemeral container in an existing Pod to inspect its state and run arbitrary commands.What is an ephemeral container?Ephemeral containers differ from other containers in that they lack guarantees for resources or execution, and they will never be automatically restarted, so they are not appropriate for building applications. Ephemeral containers are described using the same ContainerSpec as regular containers, but many fields are incompatible and disallowed for ephemeral containers.Ephemeral containers may not have ports, so fields such as ports, livenessProbe, readinessProbe are disallowed.Pod resource allocations are immutable, so setting resources is disallowed.For a complete list of allowed fields, see the EphemeralContainer reference documentation.Ephemeral containers are created using a special ephemeralcontainers handler in the API rather than by adding them directly to pod.spec, so it's not possible to add an ephemeral container using kubectl edit.Like regular containers, you may not change or remove an ephemeral container after you have added it to a Pod.Note:Ephemeral containers are not supported by static pods.Uses for ephemeral containersEphemeral containers are useful for interactive troubleshooting when kubectl exec is insufficient because a container has crashed or a container image doesn't include debugging utilities.In particular, distroless images enable you to deploy minimal container images that reduce attack surface and exposure to bugs and vulnerabilities. Since distroless images do not include a shell or any debugging utilities, it's difficult to troubleshoot distroless images using kubectl exec alone.When using ephemeral containers, it's helpful to enable process namespace sharing so you can view processes in other containers.What's nextLearn how to debug pods using ephemeral containers.3.4.1.5 - DisruptionsThis guide is for application owners who want to build highly available applications, and thus need to understand what types of disruptions can happen to Pods.It is also for cluster administrators who want to perform automated cluster actions, like upgrading and autoscaling clusters.Voluntary and involuntary disruptionsPods do not disappear until someone (a person or a controller) destroys them, or there is an unavoidable hardware or system software error.We call these unavoidable cases involuntary disruptions to an application. Examples are:a hardware failure of the physical machine backing the nodecluster administrator deletes VM (instance) by mistakecloud provider or hypervisor failure makes VM disappeara kernel panicthe node disappears from the cluster due to cluster network partitioneviction of a pod due to the node being out-of-resources.Except for the out-of-resources condition, all these conditions should be familiar to most users; they are not specific to Kubernetes.We call other cases voluntary disruptions. These include both actions initiated by the application owner and those initiated by a Cluster Administrator. Typical application owner actions include:deleting the deployment or other controller that manages the podupdating a deployment's pod template causing a restartdirectly deleting a pod (e.g. by accident)Cluster administrator actions include:Draining a node for repair or upgrade.Draining a node from a cluster to scale the cluster down (learn about Node Autoscaling.Removing a pod from a node to permit something else to fit on that node.These actions might be taken directly by the cluster administrator, or by automation run by the cluster administrator, or by your cluster hosting provider.Ask your cluster administrator or consult your cloud provider or distribution documentation to determine if any sources of voluntary disruptions are enabled for your cluster. If none are enabled, you can skip creating Pod Disruption Budgets.Caution:Not all voluntary disruptions are constrained by Pod Disruption Budgets. For example, deleting deployments or pods bypasses Pod Disruption Budgets.Dealing with disruptionsHere are some ways to mitigate involuntary disruptions:Ensure your pod requests the resources it needs.Replicate your application if you need higher availability. (Learn about running replicated stateless and stateful applications.)For even higher availability when running replicated applications, spread applications across racks (using anti-affinity) or across zones (if using a multi-zone cluster.)The frequency of voluntary disruptions varies. On a basic Kubernetes cluster, there are no automated voluntary disruptions (only user-triggered ones). However, your cluster administrator or hosting provider may run some additional services which cause voluntary disruptions. For example, rolling out node software updates can cause voluntary disruptions. Also, some implementations of cluster (node) autoscaling may cause voluntary disruptions to defragment and compact nodes. Your cluster administrator or hosting provider should have documented what level of voluntary disruptions, if any, to expect. Certain configuration options, such as using PriorityClasses in your pod spec can also cause voluntary (and involuntary) disruptions.Pod disruption budgetsFEATURE STATE: Kubernetes v1.21 [stable]Kubernetes offers features to help you run highly available applications even when you introduce frequent voluntary disruptions.As an application owner, you can create a PodDisruptionBudget (PDB) for each application. A PDB limits the number of Pods of a replicated application that are down simultaneously from voluntary disruptions. For example, a quorum-based application would like to ensure that the number of replicas running is never brought below the number needed for a quorum. A web front end might want to ensure that the number of replicas serving load never falls below a certain percentage of the total.Cluster managers and hosting providers should use tools which respect PodDisruptionBudgets by calling the Eviction API instead of directly deleting pods or deployments.For example, the kubectl drain subcommand lets you mark a node as going out of service. When you run kubectl drain, the tool tries to evict all of the Pods on the Node you're taking out of service. The eviction request that kubectl submits on your behalf may be temporarily rejected, so the tool periodically retries all failed requests until all Pods on the target node are terminated, or until a configurable timeout is reached.A PDB specifies the number of replicas that an application can tolerate having, relative to how many it is intended to have. For example, a Deployment which has a .spec.replicas: 5 is supposed to have 5 pods at any given time. If its PDB allows for there to be 4 at a time, then the Eviction API will allow voluntary disruption of one (but not two) pods at a time.The group of pods that comprise the application is specified using a label selector, the same as the one used by the application's controller (deployment, stateful-set, etc).The "intended" number of pods is computed from the .spec.replicas of the workload resource that is managing those pods. The control plane discovers the owning workload resource by examining the .metadata.ownerReferences of the Pod.Involuntary disruptions cannot be prevented by PDBs; however they do count against the budget.Pods which are deleted or unavailable due to a rolling upgrade to an application do count against the disruption budget, but workload resources (such as Deployment and StatefulSet) are not limited by PDBs when doing rolling upgrades. Instead, the handling of failures during application updates is configured in the spec for the specific workload resource.It is recommended to set AlwaysAllow Unhealthy Pod Eviction Policy to your PodDisruptionBudgets to support eviction of misbehaving applications during a node drain. The default behavior is to wait for the application pods to become healthy before the drain can proceed.When a pod is evicted using the eviction API, it is gracefully terminated, honoring the terminationGracePeriodSeconds setting in its PodSpec.PodDisruptionBudget exampleConsider a cluster with 3 nodes, node-1 through node-3. The cluster is running several applications. One of them has 3 replicas initially called pod-a, pod-b, and pod-c. Another, unrelated pod without a PDB, called pod-x, is also shown. Initially, the pods are laid out as follows:node-1node-2node-3pod-a availablepod-b availablepod-c availablepod-x availableAll 3 pods are part of a deployment, and they collectively have a PDB which requires there be at least 2 of the 3 pods to be available at all times.For example, assume the cluster administrator wants to reboot into a new kernel version to fix a bug in the kernel. The cluster administrator first tries to drain node-1 using the kubectl drain command. That tool tries to evict pod-a and pod-x. This succeeds immediately. Both pods go into the terminating state at the same time. This puts the cluster in this state:node-1 drainingnode-2node-3pod-a terminatingpod-b availablepod-c availablepod-x terminatingThe deployment notices that one of the pods is terminating, so it creates a replacement called pod-d. Since node-1 is cordoned, it lands on another node. Something has also created pod-y as a replacement for pod-x.(Note: for a StatefulSet, pod-a, which would be called something like pod-0, would need to terminate completely before its replacement, which is also called pod-0 but has a different UID, could be created. Otherwise, the example applies to a StatefulSet as well.)Now the cluster is in this state:node-1 drainingnode-2node-3pod-a terminatingpod-b availablepod-c availablepod-x terminatingpod-d startingpod-yAt some point, the pods terminate, and the cluster looks like this:node-1 drainednode-2node-3pod-b availablepod-c availablepod-d startingpod-yAt this point, if an impatient cluster administrator tries to drain node-2 or node-3, the drain command will block, because there are only 2 available pods for the deployment, and its PDB requires at least 2. After some time passes, pod-d becomes available.The cluster state now looks like this:node-1 drainednode-2node-3pod-b availablepod-c availablepod-d availablepod-yNow, the cluster administrator tries to drain node-2. The drain command will try to evict the two pods in some order, say pod-b first and then pod-d. It will succeed at evicting pod-b. But, when it tries to evict pod-d, it will be refused because that would leave only one pod available for the deployment.The deployment creates a replacement for pod-b called pod-e. Because there are not enough resources in the cluster to schedule pod-e the drain will again block. The cluster may end up in this state:node-1 drainednode-2node-3no nodepod-b terminatingpod-c availablepod-e pendingpod-d availablepod-yAt this point, the cluster administrator needs to add a node back to the cluster to proceed with the upgrade.You can see how Kubernetes varies the rate at which disruptions can happen, according to:how many replicas an application needshow long it takes to gracefully shutdown an instancehow long it takes a new instance to start upthe type of controllerthe cluster's resource capacityPod disruption conditionsFEATURE STATE: Kubernetes v1.31 [stable] (enabled by default: true)A dedicated Pod DisruptionTarget condition is added to indicate that the Pod is about to be deleted due to a disruption. The reason field of the condition additionally indicates one of the following reasons for the Pod termination:PreemptionBySchedulerPod is due to be preempted by a scheduler in order to accommodate a new Pod with a higher priority. For more information, see Pod priority preemption.DeletionByTaintManagerPod is due to be deleted by Taint Manager (which is part of the node lifecycle controller within kube-controller-manager) due to a NoExecute taint that the Pod does not tolerate; see taint-based evictions.EvictionByEvictionAPIPod has been marked for eviction using the Kubernetes API .DeletionByPodGCPod, that is bound to a no longer existing Node, is due to be deleted by Pod garbage collection.TerminationByKubeletPod has been terminated by the kubelet, because of either node pressure eviction, the graceful node shutdown, or preemption for system critical pods.In all other disruption scenarios, like eviction due to exceeding Pod container limits, Pods don't receive the DisruptionTarget condition because the disruptions were probably caused by the Pod and would reoccur on retry.Note:A Pod disruption might be interrupted. The control plane might re-attempt to continue the disruption of the same Pod, but it is not guaranteed. As a result, the DisruptionTarget condition might be added to a Pod, but that Pod might then not actually be deleted. In such a situation, after some time, the Pod disruption condition will be cleared.Along with cleaning up the pods, the Pod garbage collector (PodGC) will also mark them as failed if they are in a non-terminal phase (see also Pod garbage collection).When using a Job (or CronJob), you may want to use these Pod disruption conditions as part of your Job's Pod failure policy.Separating Cluster Owner and Application Owner RolesOften, it is useful to think of the Cluster Manager and Application Owner as separate roles with limited knowledge of each other. This separation of responsibilities may make sense in these scenarios:when there are many application teams sharing a Kubernetes cluster, and there is natural specialization of roleswhen third-party tools or services are used to automate cluster managementPod Disruption Budgets support this separation of roles by providing an interface between the roles.If you do not have such a separation of responsibilities in your organization, you may not need to use Pod Disruption Budgets.How to perform Disruptive Actions on your ClusterIf you are a Cluster Administrator, and you need to perform a disruptive action on all the nodes in your cluster, such as a node or system software upgrade, here are some options:Accept downtime during the upgrade.Failover to another complete replica cluster.No downtime, but may be costly both for the duplicated nodes and for human effort to orchestrate the switchover.Write disruption tolerant applications and use PDBs.No downtime.Minimal resource duplication.Allows more automation of cluster administration.Writing disruption-tolerant applications is tricky, but the work to tolerate voluntary disruptions largely overlaps with work to support autoscaling and tolerating involuntary disruptions.What's nextFollow steps to protect your application by configuring a Pod Disruption Budget.Learn more about draining nodesLearn about updating a deployment including steps to maintain its availability during the rollout.3.4.1.6 - Pod Quality of Service ClassesThis page introduces Quality of Service (QoS) classes in Kubernetes, and explains how Kubernetes assigns a QoS class to each Pod as a consequence of the resource constraints that you specify for the containers in that Pod. Kubernetes relies on this classification to make decisions about which Pods to evict when there are not enough available resources on a Node.Quality of Service classesKubernetes classifies the Pods that you run and allocates each Pod into a specific quality of service (QoS) class. Kubernetes uses that classification to influence how different pods are handled. Kubernetes does this classification based on the resource requests of the Containers in that Pod, along with how those requests relate to resource limits. This is known as Quality of Service (QoS) class. Kubernetes assigns every Pod a QoS class based on the resource requests and limits of its component Containers. QoS classes are used by Kubernetes to decide which Pods to evict from a Node experiencing Node Pressure. The possible QoS classes are Guaranteed, Burstable, and BestEffort. When a Node runs out of resources, Kubernetes will first evict BestEffort Pods running on that Node, followed by Burstable and finally Guaranteed Pods. When this eviction is due to resource pressure, only Pods exceeding resource requests are candidates for eviction.GuaranteedPods that are Guaranteed have the strictest resource limits and are least likely to face eviction. They are guaranteed not to be killed until they exceed their limits or there are no lower-priority Pods that can be preempted from the Node. They may not acquire resources beyond their specified limits. These Pods can also make use of exclusive CPUs using the static CPU management policy.CriteriaFor a Pod to be given a QoS class of Guaranteed:Every Container in the Pod must have a memory limit and a memory request.For every Container in the Pod, the memory limit must equal the memory request.Every Container in the Pod must have a CPU limit and a CPU request.For every Container in the Pod, the CPU limit must equal the CPU request.BurstablePods that are Burstable have some lower-bound resource guarantees based on the request, but do not require a specific limit. If a limit is not specified, it defaults to a limit equivalent to the capacity of the Node, which allows the Pods to flexibly increase their resources if resources are available. In the event of Pod eviction due to Node resource pressure, these Pods are evicted only after all BestEffort Pods are evicted. Because a Burstable Pod can include a Container that has no resource limits or requests, a Pod that is Burstable can try to use any amount of node resources.CriteriaA Pod is given a QoS class of Burstable if:The Pod does not meet the criteria for QoS class Guaranteed.At least one Container in the Pod has a memory or CPU request or limit.BestEffortPods in the BestEffort QoS class can use node resources that aren't specifically assigned to Pods in other QoS classes. For example, if you have a node with 16 CPU cores available to the kubelet, and you assign 4 CPU cores to a Guaranteed Pod, then a Pod in the BestEffort QoS class can try to use any amount of the remaining 12 CPU cores.The kubelet prefers to evict BestEffort Pods if the node comes under resource pressure.CriteriaA Pod has a QoS class of BestEffort if it doesn't meet the criteria for either Guaranteed or Burstable. In other words, a Pod is BestEffort only if none of the Containers in the Pod have a memory limit or a memory request, and none of the Containers in the Pod have a CPU limit or a CPU request. Containers in a Pod can request other resources (not CPU or memory) and still be classified as BestEffort.Memory QoS with cgroup v2FEATURE STATE: Kubernetes v1.22 [alpha] (enabled by default: false)Memory QoS uses the memory controller of cgroup v2 to guarantee memory resources in Kubernetes. Memory requests and limits of containers in pod are used to set specific interfaces memory.min and memory.high provided by the memory controller. When memory.min is set to memory requests, memory resources are reserved and never reclaimed by the kernel; this is how Memory QoS ensures memory availability for Kubernetes pods. And if memory limits are set in the container, this means that the system needs to limit container memory usage; Memory QoS uses memory.high to throttle workload approaching its memory limit, ensuring that the system is not overwhelmed by instantaneous memory allocation.Memory QoS relies on QoS class to determine which settings to apply; however, these are different mechanisms that both provide controls over quality of service.Some behavior is independent of QoS classCertain behavior is independent of the QoS class assigned by Kubernetes. For example:Any Container exceeding a resource limit will be killed and restarted by the kubelet without affecting other Containers in that Pod.If a Container exceeds its resource request and the node it runs on faces resource pressure, the Pod it is in becomes a candidate for eviction. If this occurs, all Containers in the Pod will be terminated. Kubernetes may create a replacement Pod, usually on a different node.The resource request of a Pod is equal to the sum of the resource requests of its component Containers, and the resource limit of a Pod is equal to the sum of the resource limits of its component Containers.The kube-scheduler does not consider QoS class when selecting which Pods to preempt. Preemption can occur when a cluster does not have enough resources to run all the Pods you defined.What's nextLearn about resource management for Pods and Containers.Learn about Node-pressure eviction.Learn about Pod priority and preemption.Learn about Pod disruptions.Learn how to assign memory resources to containers and pods.Learn how to assign CPU resources to containers and pods.Learn how to configure Quality of Service for Pods.3.4.1.7 - User NamespacesFEATURE STATE: Kubernetes v1.30 [beta]This page explains how user namespaces are used in Kubernetes pods. A user namespace isolates the user running inside the container from the one in the host.A process running as root in a container can run as a different (non-root) user in the host; in other words, the process has full privileges for operations inside the user namespace, but is unprivileged for operations outside the namespace.You can use this feature to reduce the damage a compromised container can do to the host or other pods in the same node. There are several security vulnerabilities rated either HIGH or CRITICAL that were not exploitable when user namespaces is active. It is expected user namespace will mitigate some future vulnerabilities too.Before you beginNote: This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the content guide before submitting a change. More information.This is a Linux-only feature and support is needed in Linux for idmap mounts on the filesystems used. This means:On the node, the filesystem you use for /var/lib/kubelet/pods/, or the custom directory you configure for this, needs idmap mount support.All the filesystems used in the pod's volumes must support idmap mounts.In practice this means you need at least Linux 6.3, as tmpfs started supporting idmap mounts in that version. This is usually needed as several Kubernetes features use tmpfs (the service account token that is mounted by default uses a tmpfs, Secrets use a tmpfs, etc.)Some popular filesystems that support idmap mounts in Linux 6.3 are: btrfs, ext4, xfs, fat, tmpfs, overlayfs.In addition, the container runtime and its underlying OCI runtime must support user namespaces. The following OCI runtimes offer support:crun version 1.9 or greater (it's recommend version 1.13+).runc version 1.2 or greaterNote:Some OCI runtimes do not include the support needed for using user namespaces in Linux pods. If you use a managed Kubernetes, or have downloaded it from packages and set it up, it's possible that nodes in your cluster use a runtime that doesn't include this support.To use user namespaces with Kubernetes, you also need to use a CRI container runtime to use this feature with Kubernetes pods:containerd: version 2.0 (and later) supports user namespaces for containers.CRI-O: version 1.25 (and later) supports user namespaces for containers.You can see the status of user namespaces support in cri-dockerd tracked in an issue on GitHub.IntroductionUser namespaces is a Linux feature that allows to map users in the container to different users in the host. Furthermore, the capabilities granted to a pod in a user namespace are valid only in the namespace and void outside of it.A pod can opt-in to use user namespaces by setting the pod.spec.hostUsers field to false.The kubelet will pick host UIDs/GIDs a pod is mapped to, and will do so in a way to guarantee that no two pods on the same node use the same mapping.The runAsUser, runAsGroup, fsGroup, etc. fields in the pod.spec always refer to the user inside the container. These users will be used for volume mounts (specified in pod.spec.volumes) and therefore the host UID/GID will not have any effect on writes/reads from volumes the pod can mount. In other words, the inodes created/read in volumes mounted by the pod will be the same as if the pod wasn't using user namespaces.This way, a pod can easily enable and disable user namespaces (without affecting its volume's file ownerships) and can also share volumes with pods without user namespaces by just setting the appropriate users inside the container (RunAsUser, RunAsGroup, fsGroup, etc.). This applies to any volume the pod can mount, including hostPath (if the pod is allowed to mount hostPath volumes).By default, the valid UIDs/GIDs when this feature is enabled is the range 0-65535. This applies to files and processes (runAsUser, runAsGroup, etc.).Files using a UID/GID outside this range will be seen as belonging to the overflow ID, usually 65534 (configured in /proc/sys/kernel/overflowuid and /proc/sys/kernel/overflowgid). However, it is not possible to modify those files, even by running as the 65534 user/group.If the range 0-65535 is extended with a configuration knob, the aforementioned restrictions apply to the extended range.Most applications that need to run as root but don't access other host namespaces or resources, should continue to run fine without any changes needed if user namespaces is activated.Understanding user namespaces for podsSeveral container runtimes with their default configuration (like Docker Engine, containerd, CRI-O) use Linux namespaces for isolation. Other technologies exist and can be used with those runtimes too (e.g. Kata Containers uses VMs instead of Linux namespaces). This page is applicable for container runtimes using Linux namespaces for isolation.When creating a pod, by default, several new namespaces are used for isolation: a network namespace to isolate the network of the container, a PID namespace to isolate the view of processes, etc. If a user namespace is used, this will isolate the users in the container from the users in the node.This means containers can run as root and be mapped to a non-root user on the host. Inside the container the process will think it is running as root (and therefore tools like apt, yum, etc. work fine), while in reality the process doesn't have privileges on the host. You can verify this, for example, if you check which user the container process is running by executing ps aux from the host. The user ps shows is not the same as the user you see if you execute inside the container the command id.This abstraction limits what can happen, for example, if the container manages to escape to the host. Given that the container is running as a non-privileged user on the host, it is limited what it can do to the host.Furthermore, as users on each pod will be mapped to different non-overlapping users in the host, it is limited what they can do to other pods too.Capabilities granted to a pod are also limited to the pod user namespace and mostly invalid out of it, some are even completely void. Here are two examples:CAP_SYS_MODULE does not have any effect if granted to a pod using user namespaces, the pod isn't able to load kernel modules.CAP_SYS_ADMIN is limited to the pod's user namespace and invalid outside of it.Without using a user namespace a container running as root, in the case of a container breakout, has root privileges on the node. And if some capability were granted to the container, the capabilities are valid on the host too. None of this is true when we use user namespaces.If you want to know more details about what changes when user namespaces are in use, see man 7 user_namespaces.Set up a node to support user namespacesBy default, the kubelet assigns pods UIDs/GIDs above the range 0-65535, based on the assumption that the host's files and processes use UIDs/GIDs within this range, which is standard for most Linux distributions. This approach prevents any overlap between the UIDs/GIDs of the host and those of the pods.Avoiding the overlap is important to mitigate the impact of vulnerabilities such as CVE-2021-25741, where a pod can potentially read arbitrary files in the host. If the UIDs/GIDs of the pod and the host don't overlap, it is limited what a pod would be able to do: the pod UID/GID won't match the host's file owner/group.The kubelet can use a custom range for user IDs and group IDs for pods. To configure a custom range, the node needs to have:A user kubelet in the system (you cannot use any other username here)The binary getsubids installed (part of shadow-utils) and in the PATH for the kubelet binary.A configuration of subordinate UIDs/GIDs for the kubelet user (see man 5 subuid and man 5 subgid).This setting only gathers the UID/GID range configuration and does not change the user executing the kubelet.You must follow some constraints for the subordinate ID range that you assign to the kubelet user:The subordinate user ID, that starts the UID range for Pods, must be a multiple of 65536 and must also be greater than or equal to 65536. In other words, you cannot use any ID from the range 0-65535 for Pods; the kubelet imposes this restriction to make it difficult to create an accidentally insecure configuration.The subordinate ID count must be a multiple of 65536The subordinate ID count must be at least 65536 x  where  is the maximum number of pods that can run on the node.You must assign the same range for both user IDs and for group IDs, It doesn't matter if other users have user ID ranges that don't align with the group ID ranges.None of the assigned ranges should overlap with any other assignment.The subordinate configuration must be only one line. In other words, you can't have multiple ranges.For example, you could define /etc/subuid and /etc/subgid to both have these entries for the kubelet user:# The format is # name:firstID:count of IDs # where # - firstID is 65536 (the minimum value possible) # - count of IDs is 110 * 65536 # (110 is the default limit for number of pods on the node) kubelet:65536:7208960 ID count for each of PodsStarting with Kubernetes v1.33, the ID count for each of Pods can be set in KubeletConfiguration.apiVersion: kubelet.config.k8s.io/v1beta1 kind: KubeletConfiguration userNamespaces: idsPerPod: 1048576 The value of idsPerPod (uint32) must be a multiple of 65536. The default value is 65536. This value only applies to containers created after the kubelet was started with this KubeletConfiguration. Running containers are not affected by this config.In Kubernetes prior to v1.33, the ID count for each of Pods was hard-coded to 65536.Integration with Pod security admission checksFEATURE STATE: Kubernetes v1.29 [alpha]For Linux Pods that enable user namespaces, Kubernetes relaxes the application of Pod Security Standards in a controlled way. This behavior can be controlled by the feature gate UserNamespacesPodSecurityStandards, which allows an early opt-in for end users. Admins have to ensure that user namespaces are enabled by all nodes within the cluster if using the feature gate.If you enable the associated feature gate and create a Pod that uses user namespaces, the following fields won't be constrained even in contexts that enforce the Baseline or Restricted pod security standard. This behavior does not present a security concern because root inside a Pod with user namespaces actually refers to the user inside the container, that is never mapped to a privileged user on the host. Here's the list of fields that are not checks for Pods in those circumstances:spec.securityContext.runAsNonRootspec.containers[*].securityContext.runAsNonRootspec.initContainers[*].securityContext.runAsNonRootspec.ephemeralContainers[*].securityContext.runAsNonRootspec.securityContext.runAsUserspec.containers[*].securityContext.runAsUserspec.initContainers[*].securityContext.runAsUserspec.ephemeralContainers[*].securityContext.runAsUserLimitationsWhen using a user namespace for the pod, it is disallowed to use other host namespaces. In particular, if you set hostUsers: false then you are not allowed to set any of:hostNetwork: truehostIPC: truehostPID: trueWhat's nextTake a look at Use a User Namespace With a Pod3.4.1.8 - Downward APIThere are two ways to expose Pod and container fields to a running container: environment variables, and as files that are populated by a special volume type. Together, these two ways of exposing Pod and container fields are called the downward API.It is sometimes useful for a container to have information about itself, without being overly coupled to Kubernetes. The downward API allows containers to consume information about themselves or the cluster without using the Kubernetes client or API server.An example is an existing application that assumes a particular well-known environment variable holds a unique identifier. One possibility is to wrap the application, but that is tedious and error-prone, and it violates the goal of low coupling. A better option would be to use the Pod's name as an identifier, and inject the Pod's name into the well-known environment variable.In Kubernetes, there are two ways to expose Pod and container fields to a running container:as environment variablesas files in a downwardAPI volumeTogether, these two ways of exposing Pod and container fields are called the downward API.Available fieldsOnly some Kubernetes API fields are available through the downward API. This section lists which fields you can make available.You can pass information from available Pod-level fields using fieldRef. At the API level, the spec for a Pod always defines at least one Container. You can pass information from available Container-level fields using resourceFieldRef.Information available via fieldRefFor some Pod-level fields, you can provide them to a container either as an environment variable or using a downwardAPI volume. The fields available via either mechanism are:metadata.namethe pod's namemetadata.namespacethe pod's namespacemetadata.uidthe pod's unique IDmetadata.annotations['']the value of the pod's annotation named  (for example, metadata.annotations['myannotation'])metadata.labels['']the text value of the pod's label named  (for example, metadata.labels['mylabel'])The following information is available through environment variables but not as a downwardAPI volume fieldRef:spec.serviceAccountNamethe name of the pod's service accountspec.nodeNamethe name of the node where the Pod is executingstatus.hostIPthe primary IP address of the node to which the Pod is assignedstatus.hostIPsthe IP addresses is a dual-stack version of status.hostIP, the first is always the same as status.hostIP.status.podIPthe pod's primary IP address (usually, its IPv4 address)status.podIPsthe IP addresses is a dual-stack version of status.podIP, the first is always the same as status.podIPThe following information is available through a downwardAPI volume fieldRef, but not as environment variables:metadata.labelsall of the pod's labels, formatted as label-key="escaped-label-value" with one label per linemetadata.annotationsall of the pod's annotations, formatted as annotation-key="escaped-annotation-value" with one annotation per lineInformation available via resourceFieldRefThese container-level fields allow you to provide information about requests and limits for resources such as CPU and memory.Note:FEATURE STATE: Kubernetes v1.33 [beta] (enabled by default: true)Container CPU and memory resources can be resized while the container is running. If this happens, a downward API volume will be updated, but environment variables will not be updated unless the container restarts. See Resize CPU and Memory Resources assigned to Containers for more details.resource: limits.cpuA container's CPU limitresource: requests.cpuA container's CPU requestresource: limits.memoryA container's memory limitresource: requests.memoryA container's memory requestresource: limits.hugepages-*A container's hugepages limitresource: requests.hugepages-*A container's hugepages requestresource: limits.ephemeral-storageA container's ephemeral-storage limitresource: requests.ephemeral-storageA container's ephemeral-storage requestFallback information for resource limitsIf CPU and memory limits are not specified for a container, and you use the downward API to try to expose that information, then the kubelet defaults to exposing the maximum allocatable value for CPU and memory based on the node allocatable calculation.What's nextYou can read about downwardAPI volumes.You can try using the downward API to expose container- or Pod-level information:as environment variablesas files in downwardAPI volume3.4.2 - Workload ManagementKubernetes provides several built-in APIs for declarative management of your workloads and the components of those workloads.Ultimately, your applications run as containers inside Pods; however, managing individual Pods would be a lot of effort. For example, if a Pod fails, you probably want to run a new Pod to replace it. Kubernetes can do that for you.You use the Kubernetes API to create a workload object that represents a higher abstraction level than a Pod, and then the Kubernetes control plane automatically manages Pod objects on your behalf, based on the specification for the workload object you defined.The built-in APIs for managing workloads are:Deployment (and, indirectly, ReplicaSet), the most common way to run an application on your cluster. Deployment is a good fit for managing a stateless application workload on your cluster, where any Pod in the Deployment is interchangeable and can be replaced if needed. (Deployments are a replacement for the legacy ReplicationController API).A StatefulSet lets you manage one or more Pods – all running the same application code – where the Pods rely on having a distinct identity. This is different from a Deployment where the Pods are expected to be interchangeable. The most common use for a StatefulSet is to be able to make a link between its Pods and their persistent storage. For example, you can run a StatefulSet that associates each Pod with a PersistentVolume. If one of the Pods in the StatefulSet fails, Kubernetes makes a replacement Pod that is connected to the same PersistentVolume.A DaemonSet defines Pods that provide facilities that are local to a specific node; for example, a driver that lets containers on that node access a storage system. You use a DaemonSet when the driver, or other node-level service, has to run on the node where it's useful. Each Pod in a DaemonSet performs a role similar to a system daemon on a classic Unix / POSIX server. A DaemonSet might be fundamental to the operation of your cluster, such as a plugin to let that node access cluster networking, it might help you to manage the node, or it could provide less essential facilities that enhance the container platform you are running. You can run DaemonSets (and their pods) across every node in your cluster, or across just a subset (for example, only install the GPU accelerator driver on nodes that have a GPU installed).You can use a Job and / or a CronJob to define tasks that run to completion and then stop. A Job represents a one-off task, whereas each CronJob repeats according to a schedule.Other topics in this section:3.4.2.1 - DeploymentsA Deployment manages a set of Pods to run an application workload, usually one that doesn't maintain state.A Deployment provides declarative updates for Pods and ReplicaSets.You describe a desired state in a Deployment, and the Deployment Controller changes the actual state to the desired state at a controlled rate. You can define Deployments to create new ReplicaSets, or to remove existing Deployments and adopt all their resources with new Deployments.Note:Do not manage ReplicaSets owned by a Deployment. Consider opening an issue in the main Kubernetes repository if your use case is not covered below.Use CaseThe following are typical use cases for Deployments:Create a Deployment to rollout a ReplicaSet. The ReplicaSet creates Pods in the background. Check the status of the rollout to see if it succeeds or not.Declare the new state of the Pods by updating the PodTemplateSpec of the Deployment. A new ReplicaSet is created and the Deployment manages moving the Pods from the old ReplicaSet to the new one at a controlled rate. Each new ReplicaSet updates the revision of the Deployment.Rollback to an earlier Deployment revision if the current state of the Deployment is not stable. Each rollback updates the revision of the Deployment.Scale up the Deployment to facilitate more load.Pause the rollout of a Deployment to apply multiple fixes to its PodTemplateSpec and then resume it to start a new rollout.Use the status of the Deployment as an indicator that a rollout has stuck.Clean up older ReplicaSets that you don't need anymore.Creating a DeploymentThe following is an example of a Deployment. It creates a ReplicaSet to bring up three nginx Pods:controllers/nginx-deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment labels: app: nginx spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: \- name: nginx image: nginx:1.14.2 ports: \- containerPort: 80 In this example:A Deployment named nginx-deployment is created, indicated by the .metadata.name field. This name will become the basis for the ReplicaSets and Pods which are created later. See Writing a Deployment Spec for more details.The Deployment creates a ReplicaSet that creates three replicated Pods, indicated by the .spec.replicas field.The .spec.selector field defines how the created ReplicaSet finds which Pods to manage. In this case, you select a label that is defined in the Pod template (app: nginx). However, more sophisticated selection rules are possible, as long as the Pod template itself satisfies the rule.Note:The .spec.selector.matchLabels field is a map of {key,value} pairs. A single {key,value} in the matchLabels map is equivalent to an element of matchExpressions, whose key field is "key", the operator is "In", and the values array contains only "value". All of the requirements, from both matchLabels and matchExpressions, must be satisfied in order to match.The .spec.template field contains the following sub-fields:The Pods are labeled app: nginxusing the .metadata.labels field.The Pod template's specification, or .spec field, indicates that the Pods run one container, nginx, which runs the nginx Docker Hub image at version 1.14.2.Create one container and name it nginx using the .spec.containers[0].name field.Before you begin, make sure your Kubernetes cluster is up and running. Follow the steps given below to create the above Deployment:Create the Deployment by running the following command:kubectl apply -f https://k8s.io/examples/controllers/nginx-deployment.yaml Run kubectl get deployments to check if the Deployment was created.If the Deployment is still being created, the output is similar to the following:NAME READY UP-TO-DATE AVAILABLE AGE nginx-deployment 0/3 0 0 1s When you inspect the Deployments in your cluster, the following fields are displayed:NAME lists the names of the Deployments in the namespace.READY displays how many replicas of the application are available to your users. It follows the pattern ready/desired.UP-TO-DATE displays the number of replicas that have been updated to achieve the desired state.AVAILABLE displays how many replicas of the application are available to your users.AGE displays the amount of time that the application has been running.Notice how the number of desired replicas is 3 according to .spec.replicas field.To see the Deployment rollout status, run kubectl rollout status deployment/nginx-deployment.The output is similar to:Waiting for rollout to finish: 2 out of 3 new replicas have been updated... deployment "nginx-deployment" successfully rolled out Run the kubectl get deployments again a few seconds later. The output is similar to this:NAME READY UP-TO-DATE AVAILABLE AGE nginx-deployment 3/3 3 3 18s Notice that the Deployment has created all three replicas, and all replicas are up-to-date (they contain the latest Pod template) and available.To see the ReplicaSet (rs) created by the Deployment, run kubectl get rs. The output is similar to this:NAME DESIRED CURRENT READY AGE nginx-deployment-75675f5897 3 3 3 18s ReplicaSet output shows the following fields:NAME lists the names of the ReplicaSets in the namespace.DESIRED displays the desired number of replicas of the application, which you define when you create the Deployment. This is the desired state.CURRENT displays how many replicas are currently running.READY displays how many replicas of the application are available to your users.AGE displays the amount of time that the application has been running.Notice that the name of the ReplicaSet is always formatted as [DEPLOYMENT-NAME]-[HASH]. This name will become the basis for the Pods which are created.The HASH string is the same as the pod-template-hash label on the ReplicaSet.To see the labels automatically generated for each Pod, run kubectl get pods --show-labels. The output is similar to:NAME READY STATUS RESTARTS AGE LABELS nginx-deployment-75675f5897-7ci7o 1/1 Running 0 18s app=nginx,pod-template-hash=75675f5897 nginx-deployment-75675f5897-kzszj 1/1 Running 0 18s app=nginx,pod-template-hash=75675f5897 nginx-deployment-75675f5897-qqcnn 1/1 Running 0 18s app=nginx,pod-template-hash=75675f5897 The created ReplicaSet ensures that there are three nginx Pods.Note:You must specify an appropriate selector and Pod template labels in a Deployment (in this case, app: nginx).Do not overlap labels or selectors with other controllers (including other Deployments and StatefulSets). Kubernetes doesn't stop you from overlapping, and if multiple controllers have overlapping selectors those controllers might conflict and behave unexpectedly.Pod-template-hash labelCaution:Do not change this label.The pod-template-hash label is added by the Deployment controller to every ReplicaSet that a Deployment creates or adopts.This label ensures that child ReplicaSets of a Deployment do not overlap. It is generated by hashing the PodTemplate of the ReplicaSet and using the resulting hash as the label value that is added to the ReplicaSet selector, Pod template labels, and in any existing Pods that the ReplicaSet might have.Updating a DeploymentNote:A Deployment's rollout is triggered if and only if the Deployment's Pod template (that is, .spec.template) is changed, for example if the labels or container images of the template are updated. Other updates, such as scaling the Deployment, do not trigger a rollout.Follow the steps given below to update your Deployment:Let's update the nginx Pods to use the nginx:1.16.1 image instead of the nginx:1.14.2 image.kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.16.1 or use the following command:kubectl set image deployment/nginx-deployment nginx=nginx:1.16.1 where deployment/nginx-deployment indicates the Deployment, nginx indicates the Container the update will take place and nginx:1.16.1 indicates the new image and its tag.The output is similar to:deployment.apps/nginx-deployment image updated Alternatively, you can edit the Deployment and change .spec.template.spec.containers[0].image from nginx:1.14.2 to nginx:1.16.1:kubectl edit deployment/nginx-deployment The output is similar to:deployment.apps/nginx-deployment edited To see the rollout status, run:kubectl rollout status deployment/nginx-deployment The output is similar to this:Waiting for rollout to finish: 2 out of 3 new replicas have been updated... ordeployment "nginx-deployment" successfully rolled out Get more details on your updated Deployment:After the rollout succeeds, you can view the Deployment by running kubectl get deployments. The output is similar to this:NAME READY UP-TO-DATE AVAILABLE AGE nginx-deployment 3/3 3 3 36s Run kubectl get rs to see that the Deployment updated the Pods by creating a new ReplicaSet and scaling it up to 3 replicas, as well as scaling down the old ReplicaSet to 0 replicas.kubectl get rs The output is similar to this:NAME DESIRED CURRENT READY AGE nginx-deployment-1564180365 3 3 3 6s nginx-deployment-2035384211 0 0 0 36s Running get pods should now show only the new Pods:kubectl get pods The output is similar to this:NAME READY STATUS RESTARTS AGE nginx-deployment-1564180365-khku8 1/1 Running 0 14s nginx-deployment-1564180365-nacti 1/1 Running 0 14s nginx-deployment-1564180365-z9gth 1/1 Running 0 14s Next time you want to update these Pods, you only need to update the Deployment's Pod template again.Deployment ensures that only a certain number of Pods are down while they are being updated. By default, it ensures that at least 75% of the desired number of Pods are up (25% max unavailable).Deployment also ensures that only a certain number of Pods are created above the desired number of Pods. By default, it ensures that at most 125% of the desired number of Pods are up (25% max surge).For example, if you look at the above Deployment closely, you will see that it first creates a new Pod, then deletes an old Pod, and creates another new one. It does not kill old Pods until a sufficient number of new Pods have come up, and does not create new Pods until a sufficient number of old Pods have been killed. It makes sure that at least 3 Pods are available and that at max 4 Pods in total are available. In case of a Deployment with 4 replicas, the number of Pods would be between 3 and 5.Get details of your Deployment:kubectl describe deployments The output is similar to this:Name: nginx-deployment Namespace: default CreationTimestamp: Thu, 30 Nov 2017 10:56:25 +0000 Labels: app=nginx Annotations: deployment.kubernetes.io/revision=2 Selector: app=nginx Replicas: 3 desired | 3 updated | 3 total | 3 available | 0 unavailable StrategyType: RollingUpdate MinReadySeconds: 0 RollingUpdateStrategy: 25% max unavailable, 25% max surge Pod Template: Labels: app=nginx Containers: nginx: Image: nginx:1.16.1 Port: 80/TCP Environment:  Mounts:  Volumes:  Conditions: Type Status Reason \---- ------ ------ Available True MinimumReplicasAvailable Progressing True NewReplicaSetAvailable OldReplicaSets:  NewReplicaSet: nginx-deployment-1564180365 (3/3 replicas created) Events: Type Reason Age From Message \---- ------ ---- ---- ------- Normal ScalingReplicaSet 2m deployment-controller Scaled up replica set nginx-deployment-2035384211 to 3 Normal ScalingReplicaSet 24s deployment-controller Scaled up replica set nginx-deployment-1564180365 to 1 Normal ScalingReplicaSet 22s deployment-controller Scaled down replica set nginx-deployment-2035384211 to 2 Normal ScalingReplicaSet 22s deployment-controller Scaled up replica set nginx-deployment-1564180365 to 2 Normal ScalingReplicaSet 19s deployment-controller Scaled down replica set nginx-deployment-2035384211 to 1 Normal ScalingReplicaSet 19s deployment-controller Scaled up replica set nginx-deployment-1564180365 to 3 Normal ScalingReplicaSet 14s deployment-controller Scaled down replica set nginx-deployment-2035384211 to 0 Here you see that when you first created the Deployment, it created a ReplicaSet (nginx-deployment-2035384211) and scaled it up to 3 replicas directly. When you updated the Deployment, it created a new ReplicaSet (nginx-deployment-1564180365) and scaled it up to 1 and waited for it to come up. Then it scaled down the old ReplicaSet to 2 and scaled up the new ReplicaSet to 2 so that at least 3 Pods were available and at most 4 Pods were created at all times. It then continued scaling up and down the new and the old ReplicaSet, with the same rolling update strategy. Finally, you'll have 3 available replicas in the new ReplicaSet, and the old ReplicaSet is scaled down to 0.Note:Kubernetes doesn't count terminating Pods when calculating the number of availableReplicas, which must be between replicas - maxUnavailable and replicas + maxSurge. As a result, you might notice that there are more Pods than expected during a rollout, and that the total resources consumed by the Deployment is more than replicas + maxSurge until the terminationGracePeriodSeconds of the terminating Pods expires.Rollover (aka multiple updates in-flight)Each time a new Deployment is observed by the Deployment controller, a ReplicaSet is created to bring up the desired Pods. If the Deployment is updated, the existing ReplicaSet that controls Pods whose labels match .spec.selector but whose template does not match .spec.template are scaled down. Eventually, the new ReplicaSet is scaled to .spec.replicas and all old ReplicaSets is scaled to 0.If you update a Deployment while an existing rollout is in progress, the Deployment creates a new ReplicaSet as per the update and start scaling that up, and rolls over the ReplicaSet that it was scaling up previously \-- it will add it to its list of old ReplicaSets and start scaling it down.For example, suppose you create a Deployment to create 5 replicas of nginx:1.14.2, but then update the Deployment to create 5 replicas of nginx:1.16.1, when only 3 replicas of nginx:1.14.2 had been created. In that case, the Deployment immediately starts killing the 3 nginx:1.14.2 Pods that it had created, and starts creating nginx:1.16.1 Pods. It does not wait for the 5 replicas of nginx:1.14.2 to be created before changing course.Label selector updatesIt is generally discouraged to make label selector updates and it is suggested to plan your selectors up front. In any case, if you need to perform a label selector update, exercise great caution and make sure you have grasped all of the implications.Note:In API version apps/v1, a Deployment's label selector is immutable after it gets created.Selector additions require the Pod template labels in the Deployment spec to be updated with the new label too, otherwise a validation error is returned. This change is a non-overlapping one, meaning that the new selector does not select ReplicaSets and Pods created with the old selector, resulting in orphaning all old ReplicaSets and creating a new ReplicaSet.Selector updates changes the existing value in a selector key -- result in the same behavior as additions.Selector removals removes an existing key from the Deployment selector -- do not require any changes in the Pod template labels. Existing ReplicaSets are not orphaned, and a new ReplicaSet is not created, but note that the removed label still exists in any existing Pods and ReplicaSets.Rolling Back a DeploymentSometimes, you may want to rollback a Deployment; for example, when the Deployment is not stable, such as crash looping. By default, all of the Deployment's rollout history is kept in the system so that you can rollback anytime you want (you can change that by modifying revision history limit).Note:A Deployment's revision is created when a Deployment's rollout is triggered. This means that the new revision is created if and only if the Deployment's Pod template (.spec.template) is changed, for example if you update the labels or container images of the template. Other updates, such as scaling the Deployment, do not create a Deployment revision, so that you can facilitate simultaneous manual- or auto-scaling. This means that when you roll back to an earlier revision, only the Deployment's Pod template part is rolled back.Suppose that you made a typo while updating the Deployment, by putting the image name as nginx:1.161 instead of nginx:1.16.1:kubectl set image deployment/nginx-deployment nginx=nginx:1.161 The output is similar to this:deployment.apps/nginx-deployment image updated The rollout gets stuck. You can verify it by checking the rollout status:kubectl rollout status deployment/nginx-deployment The output is similar to this:Waiting for rollout to finish: 1 out of 3 new replicas have been updated... Press Ctrl-C to stop the above rollout status watch. For more information on stuck rollouts, read more here.You see that the number of old replicas (adding the replica count from nginx-deployment-1564180365 and nginx-deployment-2035384211) is 3, and the number of new replicas (from nginx-deployment-3066724191) is 1.kubectl get rs The output is similar to this:NAME DESIRED CURRENT READY AGE nginx-deployment-1564180365 3 3 3 25s nginx-deployment-2035384211 0 0 0 36s nginx-deployment-3066724191 1 1 0 6s Looking at the Pods created, you see that 1 Pod created by new ReplicaSet is stuck in an image pull loop.kubectl get pods The output is similar to this:NAME READY STATUS RESTARTS AGE nginx-deployment-1564180365-70iae 1/1 Running 0 25s nginx-deployment-1564180365-jbqqo 1/1 Running 0 25s nginx-deployment-1564180365-hysrc 1/1 Running 0 25s nginx-deployment-3066724191-08mng 0/1 ImagePullBackOff 0 6s Note:The Deployment controller stops the bad rollout automatically, and stops scaling up the new ReplicaSet. This depends on the rollingUpdate parameters (maxUnavailable specifically) that you have specified. Kubernetes by default sets the value to 25%.Get the description of the Deployment:kubectl describe deployment The output is similar to this:Name: nginx-deployment Namespace: default CreationTimestamp: Tue, 15 Mar 2016 14:48:04 -0700 Labels: app=nginx Selector: app=nginx Replicas: 3 desired | 1 updated | 4 total | 3 available | 1 unavailable StrategyType: RollingUpdate MinReadySeconds: 0 RollingUpdateStrategy: 25% max unavailable, 25% max surge Pod Template: Labels: app=nginx Containers: nginx: Image: nginx:1.161 Port: 80/TCP Host Port: 0/TCP Environment:  Mounts:  Volumes:  Conditions: Type Status Reason \---- ------ ------ Available True MinimumReplicasAvailable Progressing True ReplicaSetUpdated OldReplicaSets: nginx-deployment-1564180365 (3/3 replicas created) NewReplicaSet: nginx-deployment-3066724191 (1/1 replicas created) Events: FirstSeen LastSeen Count From SubObjectPath Type Reason Message \--------- -------- ----- ---- ------------- -------- ------ ------- 1m 1m 1 {deployment-controller } Normal ScalingReplicaSet Scaled up replica set nginx-deployment-2035384211 to 3 22s 22s 1 {deployment-controller } Normal ScalingReplicaSet Scaled up replica set nginx-deployment-1564180365 to 1 22s 22s 1 {deployment-controller } Normal ScalingReplicaSet Scaled down replica set nginx-deployment-2035384211 to 2 22s 22s 1 {deployment-controller } Normal ScalingReplicaSet Scaled up replica set nginx-deployment-1564180365 to 2 21s 21s 1 {deployment-controller } Normal ScalingReplicaSet Scaled down replica set nginx-deployment-2035384211 to 1 21s 21s 1 {deployment-controller } Normal ScalingReplicaSet Scaled up replica set nginx-deployment-1564180365 to 3 13s 13s 1 {deployment-controller } Normal ScalingReplicaSet Scaled down replica set nginx-deployment-2035384211 to 0 13s 13s 1 {deployment-controller } Normal ScalingReplicaSet Scaled up replica set nginx-deployment-3066724191 to 1 To fix this, you need to rollback to a previous revision of Deployment that is stable.Checking Rollout History of a DeploymentFollow the steps given below to check the rollout history:First, check the revisions of this Deployment:kubectl rollout history deployment/nginx-deployment The output is similar to this:deployments "nginx-deployment" REVISION CHANGE-CAUSE 1 kubectl apply --filename=https://k8s.io/examples/controllers/nginx-deployment.yaml 2 kubectl set image deployment/nginx-deployment nginx=nginx:1.16.1 3 kubectl set image deployment/nginx-deployment nginx=nginx:1.161 CHANGE-CAUSE is copied from the Deployment annotation kubernetes.io/change-cause to its revisions upon creation. You can specify theCHANGE-CAUSE message by:Annotating the Deployment with kubectl annotate deployment/nginx-deployment kubernetes.io/change-cause="image updated to 1.16.1"Manually editing the manifest of the resource.To see the details of each revision, run:kubectl rollout history deployment/nginx-deployment --revision=2 The output is similar to this:deployments "nginx-deployment" revision 2 Labels: app=nginx pod-template-hash=1159050644 Annotations: kubernetes.io/change-cause=kubectl set image deployment/nginx-deployment nginx=nginx:1.16.1 Containers: nginx: Image: nginx:1.16.1 Port: 80/TCP QoS Tier: cpu: BestEffort memory: BestEffort Environment Variables:  No volumes. Rolling Back to a Previous RevisionFollow the steps given below to rollback the Deployment from the current version to the previous version, which is version 2.Now you've decided to undo the current rollout and rollback to the previous revision:kubectl rollout undo deployment/nginx-deployment The output is similar to this:deployment.apps/nginx-deployment rolled back Alternatively, you can rollback to a specific revision by specifying it with --to-revision:kubectl rollout undo deployment/nginx-deployment --to-revision=2 The output is similar to this:deployment.apps/nginx-deployment rolled back For more details about rollout related commands, read kubectl rollout.The Deployment is now rolled back to a previous stable revision. As you can see, a DeploymentRollback event for rolling back to revision 2 is generated from Deployment controller.Check if the rollback was successful and the Deployment is running as expected, run:kubectl get deployment nginx-deployment The output is similar to this:NAME READY UP-TO-DATE AVAILABLE AGE nginx-deployment 3/3 3 3 30m Get the description of the Deployment:kubectl describe deployment nginx-deployment The output is similar to this:Name: nginx-deployment Namespace: default CreationTimestamp: Sun, 02 Sep 2018 18:17:55 -0500 Labels: app=nginx Annotations: deployment.kubernetes.io/revision=4 kubernetes.io/change-cause=kubectl set image deployment/nginx-deployment nginx=nginx:1.16.1 Selector: app=nginx Replicas: 3 desired | 3 updated | 3 total | 3 available | 0 unavailable StrategyType: RollingUpdate MinReadySeconds: 0 RollingUpdateStrategy: 25% max unavailable, 25% max surge Pod Template: Labels: app=nginx Containers: nginx: Image: nginx:1.16.1 Port: 80/TCP Host Port: 0/TCP Environment:  Mounts:  Volumes:  Conditions: Type Status Reason \---- ------ ------ Available True MinimumReplicasAvailable Progressing True NewReplicaSetAvailable OldReplicaSets:  NewReplicaSet: nginx-deployment-c4747d96c (3/3 replicas created) Events: Type Reason Age From Message \---- ------ ---- ---- ------- Normal ScalingReplicaSet 12m deployment-controller Scaled up replica set nginx-deployment-75675f5897 to 3 Normal ScalingReplicaSet 11m deployment-controller Scaled up replica set nginx-deployment-c4747d96c to 1 Normal ScalingReplicaSet 11m deployment-controller Scaled down replica set nginx-deployment-75675f5897 to 2 Normal ScalingReplicaSet 11m deployment-controller Scaled up replica set nginx-deployment-c4747d96c to 2 Normal ScalingReplicaSet 11m deployment-controller Scaled down replica set nginx-deployment-75675f5897 to 1 Normal ScalingReplicaSet 11m deployment-controller Scaled up replica set nginx-deployment-c4747d96c to 3 Normal ScalingReplicaSet 11m deployment-controller Scaled down replica set nginx-deployment-75675f5897 to 0 Normal ScalingReplicaSet 11m deployment-controller Scaled up replica set nginx-deployment-595696685f to 1 Normal DeploymentRollback 15s deployment-controller Rolled back deployment "nginx-deployment" to revision 2 Normal ScalingReplicaSet 15s deployment-controller Scaled down replica set nginx-deployment-595696685f to 0 Scaling a DeploymentYou can scale a Deployment by using the following command:kubectl scale deployment/nginx-deployment --replicas=10 The output is similar to this:deployment.apps/nginx-deployment scaled Assuming horizontal Pod autoscaling is enabled in your cluster, you can set up an autoscaler for your Deployment and choose the minimum and maximum number of Pods you want to run based on the CPU utilization of your existing Pods.kubectl autoscale deployment/nginx-deployment --min=10 --max=15 --cpu-percent=80 The output is similar to this:deployment.apps/nginx-deployment scaled Proportional scalingRollingUpdate Deployments support running multiple versions of an application at the same time. When you or an autoscaler scales a RollingUpdate Deployment that is in the middle of a rollout (either in progress or paused), the Deployment controller balances the additional replicas in the existing active ReplicaSets (ReplicaSets with Pods) in order to mitigate risk. This is called proportional scaling.For example, you are running a Deployment with 10 replicas, maxSurge=3, and maxUnavailable=2.Ensure that the 10 replicas in your Deployment are running.kubectl get deploy The output is similar to this:NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE nginx-deployment 10 10 10 10 50s You update to a new image which happens to be unresolvable from inside the cluster.kubectl set image deployment/nginx-deployment nginx=nginx:sometag The output is similar to this:deployment.apps/nginx-deployment image updated The image update starts a new rollout with ReplicaSet nginx-deployment-1989198191, but it's blocked due to the maxUnavailable requirement that you mentioned above. Check out the rollout status:kubectl get rs The output is similar to this:NAME DESIRED CURRENT READY AGE nginx-deployment-1989198191 5 5 0 9s nginx-deployment-618515232 8 8 8 1m Then a new scaling request for the Deployment comes along. The autoscaler increments the Deployment replicas to 15. The Deployment controller needs to decide where to add these new 5 replicas. If you weren't using proportional scaling, all 5 of them would be added in the new ReplicaSet. With proportional scaling, you spread the additional replicas across all ReplicaSets. Bigger proportions go to the ReplicaSets with the most replicas and lower proportions go to ReplicaSets with less replicas. Any leftovers are added to the ReplicaSet with the most replicas. ReplicaSets with zero replicas are not scaled up.In our example above, 3 replicas are added to the old ReplicaSet and 2 replicas are added to the new ReplicaSet. The rollout process should eventually move all replicas to the new ReplicaSet, assuming the new replicas become healthy. To confirm this, run:kubectl get deploy The output is similar to this:NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE nginx-deployment 15 18 7 8 7m The rollout status confirms how the replicas were added to each ReplicaSet.kubectl get rs The output is similar to this:NAME DESIRED CURRENT READY AGE nginx-deployment-1989198191 7 7 0 7m nginx-deployment-618515232 11 11 11 7m Pausing and Resuming a rollout of a DeploymentWhen you update a Deployment, or plan to, you can pause rollouts for that Deployment before you trigger one or more updates. When you're ready to apply those changes, you resume rollouts for the Deployment. This approach allows you to apply multiple fixes in between pausing and resuming without triggering unnecessary rollouts.For example, with a Deployment that was created:Get the Deployment details:kubectl get deploy The output is similar to this:NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE nginx 3 3 3 3 1m Get the rollout status:kubectl get rs The output is similar to this:NAME DESIRED CURRENT READY AGE nginx-2142116321 3 3 3 1m Pause by running the following command:kubectl rollout pause deployment/nginx-deployment The output is similar to this:deployment.apps/nginx-deployment paused Then update the image of the Deployment:kubectl set image deployment/nginx-deployment nginx=nginx:1.16.1 The output is similar to this:deployment.apps/nginx-deployment image updated Notice that no new rollout started:kubectl rollout history deployment/nginx-deployment The output is similar to this:deployments "nginx" REVISION CHANGE-CAUSE 1  Get the rollout status to verify that the existing ReplicaSet has not changed:kubectl get rs The output is similar to this:NAME DESIRED CURRENT READY AGE nginx-2142116321 3 3 3 2m You can make as many updates as you wish, for example, update the resources that will be used:kubectl set resources deployment/nginx-deployment -c=nginx --limits=cpu=200m,memory=512Mi The output is similar to this:deployment.apps/nginx-deployment resource requirements updated The initial state of the Deployment prior to pausing its rollout will continue its function, but new updates to the Deployment will not have any effect as long as the Deployment rollout is paused.Eventually, resume the Deployment rollout and observe a new ReplicaSet coming up with all the new updates:kubectl rollout resume deployment/nginx-deployment The output is similar to this:deployment.apps/nginx-deployment resumed Watch the status of the rollout until it's done.kubectl get rs --watch The output is similar to this:NAME DESIRED CURRENT READY AGE nginx-2142116321 2 2 2 2m nginx-3926361531 2 2 0 6s nginx-3926361531 2 2 1 18s nginx-2142116321 1 2 2 2m nginx-2142116321 1 2 2 2m nginx-3926361531 3 2 1 18s nginx-3926361531 3 2 1 18s nginx-2142116321 1 1 1 2m nginx-3926361531 3 3 1 18s nginx-3926361531 3 3 2 19s nginx-2142116321 0 1 1 2m nginx-2142116321 0 1 1 2m nginx-2142116321 0 0 0 2m nginx-3926361531 3 3 3 20s Get the status of the latest rollout:kubectl get rs The output is similar to this:NAME DESIRED CURRENT READY AGE nginx-2142116321 0 0 0 2m nginx-3926361531 3 3 3 28s Note:You cannot rollback a paused Deployment until you resume it.Deployment statusA Deployment enters various states during its lifecycle. It can be progressing while rolling out a new ReplicaSet, it can be complete, or it can fail to progress.Progressing DeploymentKubernetes marks a Deployment as progressing when one of the following tasks is performed:The Deployment creates a new ReplicaSet.The Deployment is scaling up its newest ReplicaSet.The Deployment is scaling down its older ReplicaSet(s).New Pods become ready or available (ready for at least MinReadySeconds).When the rollout becomes “progressing”, the Deployment controller adds a condition with the following attributes to the Deployment's .status.conditions:type: Progressingstatus: "True"reason: NewReplicaSetCreated | reason: FoundNewReplicaSet | reason: ReplicaSetUpdatedYou can monitor the progress for a Deployment by using kubectl rollout status.Complete DeploymentKubernetes marks a Deployment as complete when it has the following characteristics:All of the replicas associated with the Deployment have been updated to the latest version you've specified, meaning any updates you've requested have been completed.All of the replicas associated with the Deployment are available.No old replicas for the Deployment are running.When the rollout becomes “complete”, the Deployment controller sets a condition with the following attributes to the Deployment's .status.conditions:type: Progressingstatus: "True"reason: NewReplicaSetAvailableThis Progressing condition will retain a status value of "True" until a new rollout is initiated. The condition holds even when availability of replicas changes (which does instead affect the Available condition).You can check if a Deployment has completed by using kubectl rollout status. If the rollout completed successfully, kubectl rollout status returns a zero exit code.kubectl rollout status deployment/nginx-deployment The output is similar to this:Waiting for rollout to finish: 2 of 3 updated replicas are available... deployment "nginx-deployment" successfully rolled out and the exit status from kubectl rollout is 0 (success):echo $? 0 Failed DeploymentYour Deployment may get stuck trying to deploy its newest ReplicaSet without ever completing. This can occur due to some of the following factors:Insufficient quotaReadiness probe failuresImage pull errorsInsufficient permissionsLimit rangesApplication runtime misconfigurationOne way you can detect this condition is to specify a deadline parameter in your Deployment spec: (.spec.progressDeadlineSeconds). .spec.progressDeadlineSeconds denotes the number of seconds the Deployment controller waits before indicating (in the Deployment status) that the Deployment progress has stalled.The following kubectl command sets the spec with progressDeadlineSeconds to make the controller report lack of progress of a rollout for a Deployment after 10 minutes:kubectl patch deployment/nginx-deployment -p '{"spec":{"progressDeadlineSeconds":600}}' The output is similar to this:deployment.apps/nginx-deployment patched Once the deadline has been exceeded, the Deployment controller adds a DeploymentCondition with the following attributes to the Deployment's .status.conditions:type: Progressingstatus: "False"reason: ProgressDeadlineExceededThis condition can also fail early and is then set to status value of "False" due to reasons as ReplicaSetCreateError. Also, the deadline is not taken into account anymore once the Deployment rollout completes.See the Kubernetes API conventions for more information on status conditions.Note:Kubernetes takes no action on a stalled Deployment other than to report a status condition with reason: ProgressDeadlineExceeded. Higher level orchestrators can take advantage of it and act accordingly, for example, rollback the Deployment to its previous version.Note:If you pause a Deployment rollout, Kubernetes does not check progress against your specified deadline. You can safely pause a Deployment rollout in the middle of a rollout and resume without triggering the condition for exceeding the deadline.You may experience transient errors with your Deployments, either due to a low timeout that you have set or due to any other kind of error that can be treated as transient. For example, let's suppose you have insufficient quota. If you describe the Deployment you will notice the following section:kubectl describe deployment nginx-deployment The output is similar to this:<...> Conditions: Type Status Reason \---- ------ ------ Available True MinimumReplicasAvailable Progressing True ReplicaSetUpdated ReplicaFailure True FailedCreate <...> If you run kubectl get deployment nginx-deployment -o yaml, the Deployment status is similar to this:status: availableReplicas: 2 conditions: \- lastTransitionTime: 2016-10-04T12:25:39Z lastUpdateTime: 2016-10-04T12:25:39Z message: Replica set "nginx-deployment-4262182780" is progressing. reason: ReplicaSetUpdated status: "True" type: Progressing \- lastTransitionTime: 2016-10-04T12:25:42Z lastUpdateTime: 2016-10-04T12:25:42Z message: Deployment has minimum availability. reason: MinimumReplicasAvailable status: "True" type: Available \- lastTransitionTime: 2016-10-04T12:25:39Z lastUpdateTime: 2016-10-04T12:25:39Z message: 'Error creating: pods "nginx-deployment-4262182780-" is forbidden: exceeded quota: object-counts, requested: pods=1, used: pods=3, limited: pods=2' reason: FailedCreate status: "True" type: ReplicaFailure observedGeneration: 3 replicas: 2 unavailableReplicas: 2 Eventually, once the Deployment progress deadline is exceeded, Kubernetes updates the status and the reason for the Progressing condition:Conditions: Type Status Reason \---- ------ ------ Available True MinimumReplicasAvailable Progressing False ProgressDeadlineExceeded ReplicaFailure True FailedCreate You can address an issue of insufficient quota by scaling down your Deployment, by scaling down other controllers you may be running, or by increasing quota in your namespace. If you satisfy the quota conditions and the Deployment controller then completes the Deployment rollout, you'll see the Deployment's status update with a successful condition (status: "True" and reason: NewReplicaSetAvailable).Conditions: Type Status Reason \---- ------ ------ Available True MinimumReplicasAvailable Progressing True NewReplicaSetAvailable type: Available with status: "True" means that your Deployment has minimum availability. Minimum availability is dictated by the parameters specified in the deployment strategy. type: Progressing with status: "True" means that your Deployment is either in the middle of a rollout and it is progressing or that it has successfully completed its progress and the minimum required new replicas are available (see the Reason of the condition for the particulars - in our case reason: NewReplicaSetAvailable means that the Deployment is complete).You can check if a Deployment has failed to progress by using kubectl rollout status. kubectl rollout status returns a non-zero exit code if the Deployment has exceeded the progression deadline.kubectl rollout status deployment/nginx-deployment The output is similar to this:Waiting for rollout to finish: 2 out of 3 new replicas have been updated... error: deployment "nginx" exceeded its progress deadline and the exit status from kubectl rollout is 1 (indicating an error):echo $? 1 Operating on a failed deploymentAll actions that apply to a complete Deployment also apply to a failed Deployment. You can scale it up/down, roll back to a previous revision, or even pause it if you need to apply multiple tweaks in the Deployment Pod template.Clean up PolicyYou can set .spec.revisionHistoryLimit field in a Deployment to specify how many old ReplicaSets for this Deployment you want to retain. The rest will be garbage-collected in the background. By default, it is 10.Note:Explicitly setting this field to 0, will result in cleaning up all the history of your Deployment thus that Deployment will not be able to roll back.The cleanup only starts after a Deployment reaches a complete state. If you set .spec.revisionHistoryLimit to 0, any rollout nonetheless triggers creation of a new ReplicaSet before Kubernetes removes the old one.Even with a non-zero revision history limit, you can have more ReplicaSets than the limit you configure. For example, if pods are crash looping, and there are multiple rolling updates events triggered over time, you might end up with more ReplicaSets than the .spec.revisionHistoryLimit because the Deployment never reaches a complete state.Canary DeploymentIf you want to roll out releases to a subset of users or servers using the Deployment, you can create multiple Deployments, one for each release, following the canary pattern described in managing resources.Writing a Deployment SpecAs with all other Kubernetes configs, a Deployment needs .apiVersion, .kind, and .metadata fields. For general information about working with config files, see deploying applications, configuring containers, and using kubectl to manage resources documents.When the control plane creates new Pods for a Deployment, the .metadata.name of the Deployment is part of the basis for naming those Pods. The name of a Deployment must be a valid DNS subdomain value, but this can produce unexpected results for the Pod hostnames. For best compatibility, the name should follow the more restrictive rules for a DNS label.A Deployment also needs a .spec section.Pod TemplateThe .spec.template and .spec.selector are the only required fields of the .spec.The .spec.template is a Pod template. It has exactly the same schema as a Pod, except it is nested and does not have an apiVersion or kind.In addition to required fields for a Pod, a Pod template in a Deployment must specify appropriate labels and an appropriate restart policy. For labels, make sure not to overlap with other controllers. See selector.Only a .spec.template.spec.restartPolicy equal to Always is allowed, which is the default if not specified.Replicas.spec.replicas is an optional field that specifies the number of desired Pods. It defaults to 1.Should you manually scale a Deployment, example via kubectl scale deployment deployment --replicas=X, and then you update that Deployment based on a manifest (for example: by running kubectl apply -f deployment.yaml), then applying that manifest overwrites the manual scaling that you previously did.If a HorizontalPodAutoscaler (or any similar API for horizontal scaling) is managing scaling for a Deployment, don't set .spec.replicas.Instead, allow the Kubernetes control plane to manage the .spec.replicas field automatically.Selector.spec.selector is a required field that specifies a label selector for the Pods targeted by this Deployment..spec.selector must match .spec.template.metadata.labels, or it will be rejected by the API.In API version apps/v1, .spec.selector and .metadata.labels do not default to .spec.template.metadata.labels if not set. So they must be set explicitly. Also note that .spec.selector is immutable after creation of the Deployment in apps/v1.A Deployment may terminate Pods whose labels match the selector if their template is different from .spec.template or if the total number of such Pods exceeds .spec.replicas. It brings up new Pods with .spec.template if the number of Pods is less than the desired number.Note:You should not create other Pods whose labels match this selector, either directly, by creating another Deployment, or by creating another controller such as a ReplicaSet or a ReplicationController. If you do so, the first Deployment thinks that it created these other Pods. Kubernetes does not stop you from doing this.If you have multiple controllers that have overlapping selectors, the controllers will fight with each other and won't behave correctly.Strategy.spec.strategy specifies the strategy used to replace old Pods by new ones. .spec.strategy.type can be "Recreate" or "RollingUpdate". "RollingUpdate" is the default value.Recreate DeploymentAll existing Pods are killed before new ones are created when .spec.strategy.type==Recreate.Note:This will only guarantee Pod termination previous to creation for upgrades. If you upgrade a Deployment, all Pods of the old revision will be terminated immediately. Successful removal is awaited before any Pod of the new revision is created. If you manually delete a Pod, the lifecycle is controlled by the ReplicaSet and the replacement will be created immediately (even if the old Pod is still in a Terminating state). If you need an "at most" guarantee for your Pods, you should consider using a StatefulSet.Rolling Update DeploymentThe Deployment updates Pods in a rolling update fashion when .spec.strategy.type==RollingUpdate. You can specify maxUnavailable and maxSurge to control the rolling update process.Max Unavailable.spec.strategy.rollingUpdate.maxUnavailable is an optional field that specifies the maximum number of Pods that can be unavailable during the update process. The value can be an absolute number (for example, 5) or a percentage of desired Pods (for example, 10%). The absolute number is calculated from percentage by rounding down. The value cannot be 0 if .spec.strategy.rollingUpdate.maxSurge is 0. The default value is 25%.For example, when this value is set to 30%, the old ReplicaSet can be scaled down to 70% of desired Pods immediately when the rolling update starts. Once new Pods are ready, old ReplicaSet can be scaled down further, followed by scaling up the new ReplicaSet, ensuring that the total number of Pods available at all times during the update is at least 70% of the desired Pods.Max Surge.spec.strategy.rollingUpdate.maxSurge is an optional field that specifies the maximum number of Pods that can be created over the desired number of Pods. The value can be an absolute number (for example, 5) or a percentage of desired Pods (for example, 10%). The value cannot be 0 if MaxUnavailable is 0. The absolute number is calculated from the percentage by rounding up. The default value is 25%.For example, when this value is set to 30%, the new ReplicaSet can be scaled up immediately when the rolling update starts, such that the total number of old and new Pods does not exceed 130% of desired Pods. Once old Pods have been killed, the new ReplicaSet can be scaled up further, ensuring that the total number of Pods running at any time during the update is at most 130% of desired Pods.Here are some Rolling Update Deployment examples that use the maxUnavailable and maxSurge:Max UnavailableMax SurgeHybridapiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment labels: app: nginx spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: \- name: nginx image: nginx:1.14.2 ports: \- containerPort: 80 strategy: type: RollingUpdate rollingUpdate: maxUnavailable: 1 apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment labels: app: nginx spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: \- name: nginx image: nginx:1.14.2 ports: \- containerPort: 80 strategy: type: RollingUpdate rollingUpdate: maxSurge: 1 apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment labels: app: nginx spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: \- name: nginx image: nginx:1.14.2 ports: \- containerPort: 80 strategy: type: RollingUpdate rollingUpdate: maxSurge: 1 maxUnavailable: 1 Progress Deadline Seconds.spec.progressDeadlineSeconds is an optional field that specifies the number of seconds you want to wait for your Deployment to progress before the system reports back that the Deployment has failed progressing - surfaced as a condition with type: Progressing, status: "False". and reason: ProgressDeadlineExceeded in the status of the resource. The Deployment controller will keep retrying the Deployment. This defaults to 600. In the future, once automatic rollback will be implemented, the Deployment controller will roll back a Deployment as soon as it observes such a condition.If specified, this field needs to be greater than .spec.minReadySeconds.Min Ready Seconds.spec.minReadySeconds is an optional field that specifies the minimum number of seconds for which a newly created Pod should be ready without any of its containers crashing, for it to be considered available. This defaults to 0 (the Pod will be considered available as soon as it is ready). To learn more about when a Pod is considered ready, see Container Probes.Terminating PodsFEATURE STATE: Kubernetes v1.33 [alpha] (enabled by default: false)You can enable this feature it by setting the DeploymentReplicaSetTerminatingReplicas feature gate on the API server and on the kube-controller-managerPods that become terminating due to deletion or scale down may take a long time to terminate, and may consume additional resources during that period. As a result, the total number of all pods can temporarily exceed .spec.replicas. Terminating pods can be tracked using the .status.terminatingReplicas field of the Deployment.Revision History LimitA Deployment's revision history is stored in the ReplicaSets it controls..spec.revisionHistoryLimit is an optional field that specifies the number of old ReplicaSets to retain to allow rollback. These old ReplicaSets consume resources in etcd and crowd the output of kubectl get rs. The configuration of each Deployment revision is stored in its ReplicaSets; therefore, once an old ReplicaSet is deleted, you lose the ability to rollback to that revision of Deployment. By default, 10 old ReplicaSets will be kept, however its ideal value depends on the frequency and stability of new Deployments.More specifically, setting this field to zero means that all old ReplicaSets with 0 replicas will be cleaned up. In this case, a new Deployment rollout cannot be undone, since its revision history is cleaned up.Paused.spec.paused is an optional boolean field for pausing and resuming a Deployment. The only difference between a paused Deployment and one that is not paused, is that any changes into the PodTemplateSpec of the paused Deployment will not trigger new rollouts as long as it is paused. A Deployment is not paused by default when it is created.What's nextLearn more about Pods.Run a stateless application using a Deployment.Read the Deployment to understand the Deployment API.Read about PodDisruptionBudget and how you can use it to manage application availability during disruptions.Use kubectl to create a Deployment.3.4.2.2 - ReplicaSetA ReplicaSet's purpose is to maintain a stable set of replica Pods running at any given time. Usually, you define a Deployment and let that Deployment manage ReplicaSets automatically.A ReplicaSet's purpose is to maintain a stable set of replica Pods running at any given time. As such, it is often used to guarantee the availability of a specified number of identical Pods.How a ReplicaSet worksA ReplicaSet is defined with fields, including a selector that specifies how to identify Pods it can acquire, a number of replicas indicating how many Pods it should be maintaining, and a pod template specifying the data of new Pods it should create to meet the number of replicas criteria. A ReplicaSet then fulfills its purpose by creating and deleting Pods as needed to reach the desired number. When a ReplicaSet needs to create new Pods, it uses its Pod template.A ReplicaSet is linked to its Pods via the Pods' metadata.ownerReferences field, which specifies what resource the current object is owned by. All Pods acquired by a ReplicaSet have their owning ReplicaSet's identifying information within their ownerReferences field. It's through this link that the ReplicaSet knows of the state of the Pods it is maintaining and plans accordingly.A ReplicaSet identifies new Pods to acquire by using its selector. If there is a Pod that has no OwnerReference or the OwnerReference is not a Controller and it matches a ReplicaSet's selector, it will be immediately acquired by said ReplicaSet.When to use a ReplicaSetA ReplicaSet ensures that a specified number of pod replicas are running at any given time. However, a Deployment is a higher-level concept that manages ReplicaSets and provides declarative updates to Pods along with a lot of other useful features. Therefore, we recommend using Deployments instead of directly using ReplicaSets, unless you require custom update orchestration or don't require updates at all.This actually means that you may never need to manipulate ReplicaSet objects: use a Deployment instead, and define your application in the spec section.Examplecontrollers/frontend.yaml apiVersion: apps/v1 kind: ReplicaSet metadata: name: frontend labels: app: guestbook tier: frontend spec: # modify replicas according to your case replicas: 3 selector: matchLabels: tier: frontend template: metadata: labels: tier: frontend spec: containers: \- name: php-redis image: us-docker.pkg.dev/google-samples/containers/gke/gb-frontend:v5 Saving this manifest into frontend.yaml and submitting it to a Kubernetes cluster will create the defined ReplicaSet and the Pods that it manages.kubectl apply -f https://kubernetes.io/examples/controllers/frontend.yaml You can then get the current ReplicaSets deployed:kubectl get rs And see the frontend one you created:NAME DESIRED CURRENT READY AGE frontend 3 3 3 6s You can also check on the state of the ReplicaSet:kubectl describe rs/frontend And you will see output similar to:Name: frontend Namespace: default Selector: tier=frontend Labels: app=guestbook tier=frontend Annotations:  Replicas: 3 current / 3 desired Pods Status: 3 Running / 0 Waiting / 0 Succeeded / 0 Failed Pod Template: Labels: tier=frontend Containers: php-redis: Image: us-docker.pkg.dev/google-samples/containers/gke/gb-frontend:v5 Port:  Host Port:  Environment:  Mounts:  Volumes:  Events: Type Reason Age From Message \---- ------ ---- ---- ------- Normal SuccessfulCreate 13s replicaset-controller Created pod: frontend-gbgfx Normal SuccessfulCreate 13s replicaset-controller Created pod: frontend-rwz57 Normal SuccessfulCreate 13s replicaset-controller Created pod: frontend-wkl7w And lastly you can check for the Pods brought up:kubectl get pods You should see Pod information similar to:NAME READY STATUS RESTARTS AGE frontend-gbgfx 1/1 Running 0 10m frontend-rwz57 1/1 Running 0 10m frontend-wkl7w 1/1 Running 0 10m You can also verify that the owner reference of these pods is set to the frontend ReplicaSet. To do this, get the yaml of one of the Pods running:kubectl get pods frontend-gbgfx -o yaml The output will look similar to this, with the frontend ReplicaSet's info set in the metadata's ownerReferences field:apiVersion: v1 kind: Pod metadata: creationTimestamp: "2024-02-28T22:30:44Z" generateName: frontend- labels: tier: frontend name: frontend-gbgfx namespace: default ownerReferences: \- apiVersion: apps/v1 blockOwnerDeletion: true controller: true kind: ReplicaSet name: frontend uid: e129deca-f864-481b-bb16-b27abfd92292 ... Non-Template Pod acquisitionsWhile you can create bare Pods with no problems, it is strongly recommended to make sure that the bare Pods do not have labels which match the selector of one of your ReplicaSets. The reason for this is because a ReplicaSet is not limited to owning Pods specified by its template-- it can acquire other Pods in the manner specified in the previous sections.Take the previous frontend ReplicaSet example, and the Pods specified in the following manifest:pods/pod-rs.yaml apiVersion: v1 kind: Pod metadata: name: pod1 labels: tier: frontend spec: containers: \- name: hello1 image: gcr.io/google-samples/hello-app:2.0 \--- apiVersion: v1 kind: Pod metadata: name: pod2 labels: tier: frontend spec: containers: \- name: hello2 image: gcr.io/google-samples/hello-app:1.0 As those Pods do not have a Controller (or any object) as their owner reference and match the selector of the frontend ReplicaSet, they will immediately be acquired by it.Suppose you create the Pods after the frontend ReplicaSet has been deployed and has set up its initial Pod replicas to fulfill its replica count requirement:kubectl apply -f https://kubernetes.io/examples/pods/pod-rs.yaml The new Pods will be acquired by the ReplicaSet, and then immediately terminated as the ReplicaSet would be over its desired count.Fetching the Pods:kubectl get pods The output shows that the new Pods are either already terminated, or in the process of being terminated:NAME READY STATUS RESTARTS AGE frontend-b2zdv 1/1 Running 0 10m frontend-vcmts 1/1 Running 0 10m frontend-wtsmm 1/1 Running 0 10m pod1 0/1 Terminating 0 1s pod2 0/1 Terminating 0 1s If you create the Pods first:kubectl apply -f https://kubernetes.io/examples/pods/pod-rs.yaml And then create the ReplicaSet however:kubectl apply -f https://kubernetes.io/examples/controllers/frontend.yaml You shall see that the ReplicaSet has acquired the Pods and has only created new ones according to its spec until the number of its new Pods and the original matches its desired count. As fetching the Pods:kubectl get pods Will reveal in its output:NAME READY STATUS RESTARTS AGE frontend-hmmj2 1/1 Running 0 9s pod1 1/1 Running 0 36s pod2 1/1 Running 0 36s In this manner, a ReplicaSet can own a non-homogeneous set of PodsWriting a ReplicaSet manifestAs with all other Kubernetes API objects, a ReplicaSet needs the apiVersion, kind, and metadata fields. For ReplicaSets, the kind is always a ReplicaSet.When the control plane creates new Pods for a ReplicaSet, the .metadata.name of the ReplicaSet is part of the basis for naming those Pods. The name of a ReplicaSet must be a valid DNS subdomain value, but this can produce unexpected results for the Pod hostnames. For best compatibility, the name should follow the more restrictive rules for a DNS label.A ReplicaSet also needs a .spec section.Pod TemplateThe .spec.template is a pod template which is also required to have labels in place. In our frontend.yaml example we had one label: tier: frontend. Be careful not to overlap with the selectors of other controllers, lest they try to adopt this Pod.For the template's restart policy field, .spec.template.spec.restartPolicy, the only allowed value is Always, which is the default.Pod SelectorThe .spec.selector field is a label selector. As discussed earlier these are the labels used to identify potential Pods to acquire. In our frontend.yaml example, the selector was:matchLabels: tier: frontend In the ReplicaSet, .spec.template.metadata.labels must match spec.selector, or it will be rejected by the API.Note:For 2 ReplicaSets specifying the same .spec.selector but different .spec.template.metadata.labels and .spec.template.spec fields, each ReplicaSet ignores the Pods created by the other ReplicaSet.ReplicasYou can specify how many Pods should run concurrently by setting .spec.replicas. The ReplicaSet will create/delete its Pods to match this number.If you do not specify .spec.replicas, then it defaults to 1.Working with ReplicaSetsDeleting a ReplicaSet and its PodsTo delete a ReplicaSet and all of its Pods, use kubectl delete. The Garbage collector automatically deletes all of the dependent Pods by default.When using the REST API or the client-go library, you must set propagationPolicy to Background or Foreground in the -d option. For example:kubectl proxy --port=8080 curl -X DELETE 'localhost:8080/apis/apps/v1/namespaces/default/replicasets/frontend' \ -d '{"kind":"DeleteOptions","apiVersion":"v1","propagationPolicy":"Foreground"}' \ -H "Content-Type: application/json" Deleting just a ReplicaSetYou can delete a ReplicaSet without affecting any of its Pods using kubectl delete with the --cascade=orphan option. When using the REST API or the client-go library, you must set propagationPolicy to Orphan. For example:kubectl proxy --port=8080 curl -X DELETE 'localhost:8080/apis/apps/v1/namespaces/default/replicasets/frontend' \ -d '{"kind":"DeleteOptions","apiVersion":"v1","propagationPolicy":"Orphan"}' \ -H "Content-Type: application/json" Once the original is deleted, you can create a new ReplicaSet to replace it. As long as the old and new .spec.selector are the same, then the new one will adopt the old Pods. However, it will not make any effort to make existing Pods match a new, different pod template. To update Pods to a new spec in a controlled way, use a Deployment, as ReplicaSets do not support a rolling update directly.Terminating PodsFEATURE STATE: Kubernetes v1.33 [alpha] (enabled by default: false)You can enable this feature it by setting the DeploymentReplicaSetTerminatingReplicas feature gate on the API server and on the kube-controller-managerPods that become terminating due to deletion or scale down may take a long time to terminate, and may consume additional resources during that period. As a result, the total number of all pods can temporarily exceed .spec.replicas. Terminating pods can be tracked using the .status.terminatingReplicas field of the ReplicaSet.Isolating Pods from a ReplicaSetYou can remove Pods from a ReplicaSet by changing their labels. This technique may be used to remove Pods from service for debugging, data recovery, etc. Pods that are removed in this way will be replaced automatically ( assuming that the number of replicas is not also changed).Scaling a ReplicaSetA ReplicaSet can be easily scaled up or down by simply updating the .spec.replicas field. The ReplicaSet controller ensures that a desired number of Pods with a matching label selector are available and operational.When scaling down, the ReplicaSet controller chooses which pods to delete by sorting the available pods to prioritize scaling down pods based on the following general algorithm:Pending (and unschedulable) pods are scaled down firstIf controller.kubernetes.io/pod-deletion-cost annotation is set, then the pod with the lower value will come first.Pods on nodes with more replicas come before pods on nodes with fewer replicas.If the pods' creation times differ, the pod that was created more recently comes before the older pod (the creation times are bucketed on an integer log scale).If all of the above match, then selection is random.Pod deletion costFEATURE STATE: Kubernetes v1.22 [beta]Using the controller.kubernetes.io/pod-deletion-cost annotation, users can set a preference regarding which pods to remove first when downscaling a ReplicaSet.The annotation should be set on the pod, the range is [-2147483648, 2147483647]. It represents the cost of deleting a pod compared to other pods belonging to the same ReplicaSet. Pods with lower deletion cost are preferred to be deleted before pods with higher deletion cost.The implicit value for this annotation for pods that don't set it is 0; negative values are permitted. Invalid values will be rejected by the API server.This feature is beta and enabled by default. You can disable it using the feature gate PodDeletionCost in both kube-apiserver and kube-controller-manager.Note:This is honored on a best-effort basis, so it does not offer any guarantees on pod deletion order.Users should avoid updating the annotation frequently, such as updating it based on a metric value, because doing so will generate a significant number of pod updates on the apiserver.Example Use CaseThe different pods of an application could have different utilization levels. On scale down, the application may prefer to remove the pods with lower utilization. To avoid frequently updating the pods, the application should update controller.kubernetes.io/pod-deletion-cost once before issuing a scale down (setting the annotation to a value proportional to pod utilization level). This works if the application itself controls the down scaling; for example, the driver pod of a Spark deployment.ReplicaSet as a Horizontal Pod Autoscaler TargetA ReplicaSet can also be a target for Horizontal Pod Autoscalers (HPA). That is, a ReplicaSet can be auto-scaled by an HPA. Here is an example HPA targeting the ReplicaSet we created in the previous example.controllers/hpa-rs.yaml apiVersion: autoscaling/v1 kind: HorizontalPodAutoscaler metadata: name: frontend-scaler spec: scaleTargetRef: kind: ReplicaSet name: frontend minReplicas: 3 maxReplicas: 10 targetCPUUtilizationPercentage: 50 Saving this manifest into hpa-rs.yaml and submitting it to a Kubernetes cluster should create the defined HPA that autoscales the target ReplicaSet depending on the CPU usage of the replicated Pods.kubectl apply -f https://k8s.io/examples/controllers/hpa-rs.yaml Alternatively, you can use the kubectl autoscale command to accomplish the same (and it's easier!)kubectl autoscale rs frontend --max=10 --min=3 --cpu-percent=50 Alternatives to ReplicaSetDeployment (recommended)Deployment is an object which can own ReplicaSets and update them and their Pods via declarative, server-side rolling updates. While ReplicaSets can be used independently, today they're mainly used by Deployments as a mechanism to orchestrate Pod creation, deletion and updates. When you use Deployments you don't have to worry about managing the ReplicaSets that they create. Deployments own and manage their ReplicaSets. As such, it is recommended to use Deployments when you want ReplicaSets.Bare PodsUnlike the case where a user directly created Pods, a ReplicaSet replaces Pods that are deleted or terminated for any reason, such as in the case of node failure or disruptive node maintenance, such as a kernel upgrade. For this reason, we recommend that you use a ReplicaSet even if your application requires only a single Pod. Think of it similarly to a process supervisor, only it supervises multiple Pods across multiple nodes instead of individual processes on a single node. A ReplicaSet delegates local container restarts to some agent on the node such as Kubelet.JobUse a Job instead of a ReplicaSet for Pods that are expected to terminate on their own (that is, batch jobs).DaemonSetUse a DaemonSet instead of a ReplicaSet for Pods that provide a machine-level function, such as machine monitoring or machine logging. These Pods have a lifetime that is tied to a machine lifetime: the Pod needs to be running on the machine before other Pods start, and are safe to terminate when the machine is otherwise ready to be rebooted/shutdown.ReplicationControllerReplicaSets are the successors to ReplicationControllers. The two serve the same purpose, and behave similarly, except that a ReplicationController does not support set-based selector requirements as described in the labels user guide. As such, ReplicaSets are preferred over ReplicationControllersWhat's nextLearn about Pods.Learn about Deployments.Run a Stateless Application Using a Deployment, which relies on ReplicaSets to work.ReplicaSet is a top-level resource in the Kubernetes REST API. Read the ReplicaSet object definition to understand the API for replica sets.Read about PodDisruptionBudget and how you can use it to manage application availability during disruptions.3.4.2.3 - StatefulSetsA StatefulSet runs a group of Pods, and maintains a sticky identity for each of those Pods. This is useful for managing applications that need persistent storage or a stable, unique network identity.StatefulSet is the workload API object used to manage stateful applications.Manages the deployment and scaling of a set of Pods, and provides guarantees about the ordering and uniqueness of these Pods.Like a Deployment, a StatefulSet manages Pods that are based on an identical container spec. Unlike a Deployment, a StatefulSet maintains a sticky identity for each of its Pods. These pods are created from the same spec, but are not interchangeable: each has a persistent identifier that it maintains across any rescheduling.If you want to use storage volumes to provide persistence for your workload, you can use a StatefulSet as part of the solution. Although individual Pods in a StatefulSet are susceptible to failure, the persistent Pod identifiers make it easier to match existing volumes to the new Pods that replace any that have failed.Using StatefulSetsStatefulSets are valuable for applications that require one or more of the following.Stable, unique network identifiers.Stable, persistent storage.Ordered, graceful deployment and scaling.Ordered, automated rolling updates.In the above, stable is synonymous with persistence across Pod (re)scheduling. If an application doesn't require any stable identifiers or ordered deployment, deletion, or scaling, you should deploy your application using a workload object that provides a set of stateless replicas. Deployment or ReplicaSet may be better suited to your stateless needs.LimitationsThe storage for a given Pod must either be provisioned by a PersistentVolume Provisioner (examples here) based on the requested storage class, or pre-provisioned by an admin.Deleting and/or scaling a StatefulSet down will not delete the volumes associated with the StatefulSet. This is done to ensure data safety, which is generally more valuable than an automatic purge of all related StatefulSet resources.StatefulSets currently require a Headless Service to be responsible for the network identity of the Pods. You are responsible for creating this Service.StatefulSets do not provide any guarantees on the termination of pods when a StatefulSet is deleted. To achieve ordered and graceful termination of the pods in the StatefulSet, it is possible to scale the StatefulSet down to 0 prior to deletion.When using Rolling Updates with the default Pod Management Policy (OrderedReady), it's possible to get into a broken state that requires manual intervention to repair.ComponentsThe example below demonstrates the components of a StatefulSet.apiVersion: v1 kind: Service metadata: name: nginx labels: app: nginx spec: ports: \- port: 80 name: web clusterIP: None selector: app: nginx \--- apiVersion: apps/v1 kind: StatefulSet metadata: name: web spec: selector: matchLabels: app: nginx # has to match .spec.template.metadata.labels serviceName: "nginx" replicas: 3 # by default is 1 minReadySeconds: 10 # by default is 0 template: metadata: labels: app: nginx # has to match .spec.selector.matchLabels spec: terminationGracePeriodSeconds: 10 containers: \- name: nginx image: registry.k8s.io/nginx-slim:0.24 ports: \- containerPort: 80 name: web volumeMounts: \- name: www mountPath: /usr/share/nginx/html volumeClaimTemplates: \- metadata: name: www spec: accessModes: [ "ReadWriteOnce" ] storageClassName: "my-storage-class" resources: requests: storage: 1Gi Note:This example uses the ReadWriteOnce access mode, for simplicity. For production use, the Kubernetes project recommends using the ReadWriteOncePod access mode instead.In the above example:A Headless Service, named nginx, is used to control the network domain.The StatefulSet, named web, has a Spec that indicates that 3 replicas of the nginx container will be launched in unique Pods.The volumeClaimTemplates will provide stable storage using PersistentVolumes provisioned by a PersistentVolume Provisioner.The name of a StatefulSet object must be a valid DNS label.Pod SelectorYou must set the .spec.selector field of a StatefulSet to match the labels of its .spec.template.metadata.labels. Failing to specify a matching Pod Selector will result in a validation error during StatefulSet creation.Volume Claim TemplatesYou can set the .spec.volumeClaimTemplates field to create a PersistentVolumeClaim. This will provide stable storage to the StatefulSet if eitherThe StorageClass specified for the volume claim is set up to use dynamic provisioning, orThe cluster already contains a PersistentVolume with the correct StorageClass and sufficient available storage space.Minimum ready secondsFEATURE STATE: Kubernetes v1.25 [stable].spec.minReadySeconds is an optional field that specifies the minimum number of seconds for which a newly created Pod should be running and ready without any of its containers crashing, for it to be considered available. This is used to check progression of a rollout when using a Rolling Update strategy. This field defaults to 0 (the Pod will be considered available as soon as it is ready). To learn more about when a Pod is considered ready, see Container Probes.Pod IdentityStatefulSet Pods have a unique identity that consists of an ordinal, a stable network identity, and stable storage. The identity sticks to the Pod, regardless of which node it's (re)scheduled on.Ordinal IndexFor a StatefulSet with N replicas, each Pod in the StatefulSet will be assigned an integer ordinal, that is unique over the Set. By default, pods will be assigned ordinals from 0 up through N-1. The StatefulSet controller will also add a pod label with this index: apps.kubernetes.io/pod-index.Start ordinalFEATURE STATE: Kubernetes v1.31 [stable] (enabled by default: true).spec.ordinals is an optional field that allows you to configure the integer ordinals assigned to each Pod. It defaults to nil. Within the field, you can configure the following options:.spec.ordinals.start: If the .spec.ordinals.start field is set, Pods will be assigned ordinals from .spec.ordinals.start up through .spec.ordinals.start + .spec.replicas - 1.Stable Network IDEach Pod in a StatefulSet derives its hostname from the name of the StatefulSet and the ordinal of the Pod. The pattern for the constructed hostname is $(statefulset name)-$(ordinal). The example above will create three Pods named web-0,web-1,web-2. A StatefulSet can use a Headless Service to control the domain of its Pods. The domain managed by this Service takes the form: $(service name).$(namespace).svc.cluster.local, where "cluster.local" is the cluster domain. As each Pod is created, it gets a matching DNS subdomain, taking the form: $(podname).$(governing service domain), where the governing service is defined by the serviceName field on the StatefulSet.Depending on how DNS is configured in your cluster, you may not be able to look up the DNS name for a newly-run Pod immediately. This behavior can occur when other clients in the cluster have already sent queries for the hostname of the Pod before it was created. Negative caching (normal in DNS) means that the results of previous failed lookups are remembered and reused, even after the Pod is running, for at least a few seconds.If you need to discover Pods promptly after they are created, you have a few options:Query the Kubernetes API directly (for example, using a watch) rather than relying on DNS lookups.Decrease the time of caching in your Kubernetes DNS provider (typically this means editing the config map for CoreDNS, which currently caches for 30 seconds).As mentioned in the limitations section, you are responsible for creating the Headless Service responsible for the network identity of the pods.Here are some examples of choices for Cluster Domain, Service name, StatefulSet name, and how that affects the DNS names for the StatefulSet's Pods.Cluster DomainService (ns/name)StatefulSet (ns/name)StatefulSet DomainPod DNSPod Hostnamecluster.localdefault/nginxdefault/webnginx.default.svc.cluster.localweb-{0..N-1}.nginx.default.svc.cluster.localweb-{0..N-1}cluster.localfoo/nginxfoo/webnginx.foo.svc.cluster.localweb-{0..N-1}.nginx.foo.svc.cluster.localweb-{0..N-1}kube.localfoo/nginxfoo/webnginx.foo.svc.kube.localweb-{0..N-1}.nginx.foo.svc.kube.localweb-{0..N-1}Note:Cluster Domain will be set to cluster.local unless otherwise configured.Stable StorageFor each VolumeClaimTemplate entry defined in a StatefulSet, each Pod receives one PersistentVolumeClaim. In the nginx example above, each Pod receives a single PersistentVolume with a StorageClass of my-storage-class and 1 GiB of provisioned storage. If no StorageClass is specified, then the default StorageClass will be used. When a Pod is (re)scheduled onto a node, its volumeMounts mount the PersistentVolumes associated with its PersistentVolume Claims. Note that, the PersistentVolumes associated with the Pods' PersistentVolume Claims are not deleted when the Pods, or StatefulSet are deleted. This must be done manually.Pod Name LabelWhen the StatefulSet controller creates a Pod, it adds a label, statefulset.kubernetes.io/pod-name, that is set to the name of the Pod. This label allows you to attach a Service to a specific Pod in the StatefulSet.Pod index labelFEATURE STATE: Kubernetes v1.32 [stable] (enabled by default: true)When the StatefulSet controller creates a Pod, the new Pod is labelled with apps.kubernetes.io/pod-index. The value of this label is the ordinal index of the Pod. This label allows you to route traffic to a particular pod index, filter logs/metrics using the pod index label, and more. Note the feature gate PodIndexLabel is enabled and locked by default for this feature, in order to disable it, users will have to use server emulated version v1.31.Deployment and Scaling GuaranteesFor a StatefulSet with N replicas, when Pods are being deployed, they are created sequentially, in order from {0..N-1}.When Pods are being deleted, they are terminated in reverse order, from {N-1..0}.Before a scaling operation is applied to a Pod, all of its predecessors must be Running and Ready.Before a Pod is terminated, all of its successors must be completely shutdown.The StatefulSet should not specify a pod.Spec.TerminationGracePeriodSeconds of 0. This practice is unsafe and strongly discouraged. For further explanation, please refer to force deleting StatefulSet Pods.When the nginx example above is created, three Pods will be deployed in the order web-0, web-1, web-2. web-1 will not be deployed before web-0 is Running and Ready, and web-2 will not be deployed until web-1 is Running and Ready. If web-0 should fail, after web-1 is Running and Ready, but before web-2 is launched, web-2 will not be launched until web-0 is successfully relaunched and becomes Running and Ready.If a user were to scale the deployed example by patching the StatefulSet such that replicas=1, web-2 would be terminated first. web-1 would not be terminated until web-2 is fully shutdown and deleted. If web-0 were to fail after web-2 has been terminated and is completely shutdown, but prior to web-1's termination, web-1 would not be terminated until web-0 is Running and Ready.Pod Management PoliciesStatefulSet allows you to relax its ordering guarantees while preserving its uniqueness and identity guarantees via its .spec.podManagementPolicy field.OrderedReady Pod ManagementOrderedReady pod management is the default for StatefulSets. It implements the behavior described above.Parallel Pod ManagementParallel pod management tells the StatefulSet controller to launch or terminate all Pods in parallel, and to not wait for Pods to become Running and Ready or completely terminated prior to launching or terminating another Pod. This option only affects the behavior for scaling operations. Updates are not affected.Update strategiesA StatefulSet's .spec.updateStrategy field allows you to configure and disable automated rolling updates for containers, labels, resource request/limits, and annotations for the Pods in a StatefulSet. There are two possible values:OnDeleteWhen a StatefulSet's .spec.updateStrategy.type is set to OnDelete, the StatefulSet controller will not automatically update the Pods in a StatefulSet. Users must manually delete Pods to cause the controller to create new Pods that reflect modifications made to a StatefulSet's .spec.template.RollingUpdateThe RollingUpdate update strategy implements automated, rolling updates for the Pods in a StatefulSet. This is the default update strategy.Rolling UpdatesWhen a StatefulSet's .spec.updateStrategy.type is set to RollingUpdate, the StatefulSet controller will delete and recreate each Pod in the StatefulSet. It will proceed in the same order as Pod termination (from the largest ordinal to the smallest), updating each Pod one at a time.The Kubernetes control plane waits until an updated Pod is Running and Ready prior to updating its predecessor. If you have set .spec.minReadySeconds (see Minimum Ready Seconds), the control plane additionally waits that amount of time after the Pod turns ready, before moving on.Partitioned rolling updatesThe RollingUpdate update strategy can be partitioned, by specifying a .spec.updateStrategy.rollingUpdate.partition. If a partition is specified, all Pods with an ordinal that is greater than or equal to the partition will be updated when the StatefulSet's .spec.template is updated. All Pods with an ordinal that is less than the partition will not be updated, and, even if they are deleted, they will be recreated at the previous version. If a StatefulSet's .spec.updateStrategy.rollingUpdate.partition is greater than its .spec.replicas, updates to its .spec.template will not be propagated to its Pods. In most cases you will not need to use a partition, but they are useful if you want to stage an update, roll out a canary, or perform a phased roll out.Maximum unavailable PodsFEATURE STATE: Kubernetes v1.24 [alpha]You can control the maximum number of Pods that can be unavailable during an update by specifying the .spec.updateStrategy.rollingUpdate.maxUnavailable field. The value can be an absolute number (for example, 5) or a percentage of desired Pods (for example, 10%). Absolute number is calculated from the percentage value by rounding it up. This field cannot be 0. The default setting is 1.This field applies to all Pods in the range 0 to replicas - 1. If there is any unavailable Pod in the range 0 to replicas - 1, it will be counted towards maxUnavailable.Note:The maxUnavailable field is in Alpha stage and it is honored only by API servers that are running with the MaxUnavailableStatefulSet feature gate enabled.Forced rollbackWhen using Rolling Updates with the default Pod Management Policy (OrderedReady), it's possible to get into a broken state that requires manual intervention to repair.If you update the Pod template to a configuration that never becomes Running and Ready (for example, due to a bad binary or application-level configuration error), StatefulSet will stop the rollout and wait.In this state, it's not enough to revert the Pod template to a good configuration. Due to a known issue, StatefulSet will continue to wait for the broken Pod to become Ready (which never happens) before it will attempt to revert it back to the working configuration.After reverting the template, you must also delete any Pods that StatefulSet had already attempted to run with the bad configuration. StatefulSet will then begin to recreate the Pods using the reverted template.PersistentVolumeClaim retentionFEATURE STATE: Kubernetes v1.32 [stable] (enabled by default: true)The optional .spec.persistentVolumeClaimRetentionPolicy field controls if and how PVCs are deleted during the lifecycle of a StatefulSet. You must enable the StatefulSetAutoDeletePVC feature gate on the API server and the controller manager to use this field. Once enabled, there are two policies you can configure for each StatefulSet:whenDeletedconfigures the volume retention behavior that applies when the StatefulSet is deletedwhenScaledconfigures the volume retention behavior that applies when the replica count of the StatefulSet is reduced; for example, when scaling down the set.For each policy that you can configure, you can set the value to either Delete or Retain.DeleteThe PVCs created from the StatefulSet volumeClaimTemplate are deleted for each Pod affected by the policy. With the whenDeleted policy all PVCs from the volumeClaimTemplate are deleted after their Pods have been deleted. With the whenScaled policy, only PVCs corresponding to Pod replicas being scaled down are deleted, after their Pods have been deleted.Retain (default)PVCs from the volumeClaimTemplate are not affected when their Pod is deleted. This is the behavior before this new feature.Bear in mind that these policies only apply when Pods are being removed due to the StatefulSet being deleted or scaled down. For example, if a Pod associated with a StatefulSet fails due to node failure, and the control plane creates a replacement Pod, the StatefulSet retains the existing PVC. The existing volume is unaffected, and the cluster will attach it to the node where the new Pod is about to launch.The default for policies is Retain, matching the StatefulSet behavior before this new feature.Here is an example policy.apiVersion: apps/v1 kind: StatefulSet ... spec: persistentVolumeClaimRetentionPolicy: whenDeleted: Retain whenScaled: Delete ... The StatefulSet controller adds owner references to its PVCs, which are then deleted by the garbage collector after the Pod is terminated. This enables the Pod to cleanly unmount all volumes before the PVCs are deleted (and before the backing PV and volume are deleted, depending on the retain policy). When you set the whenDeleted policy to Delete, an owner reference to the StatefulSet instance is placed on all PVCs associated with that StatefulSet.The whenScaled policy must delete PVCs only when a Pod is scaled down, and not when a Pod is deleted for another reason. When reconciling, the StatefulSet controller compares its desired replica count to the actual Pods present on the cluster. Any StatefulSet Pod whose id greater than the replica count is condemned and marked for deletion. If the whenScaled policy is Delete, the condemned Pods are first set as owners to the associated StatefulSet template PVCs, before the Pod is deleted. This causes the PVCs to be garbage collected after only the condemned Pods have terminated.This means that if the controller crashes and restarts, no Pod will be deleted before its owner reference has been updated appropriate to the policy. If a condemned Pod is force-deleted while the controller is down, the owner reference may or may not have been set up, depending on when the controller crashed. It may take several reconcile loops to update the owner references, so some condemned Pods may have set up owner references and others may not. For this reason we recommend waiting for the controller to come back up, which will verify owner references before terminating Pods. If that is not possible, the operator should verify the owner references on PVCs to ensure the expected objects are deleted when Pods are force-deleted.Replicas.spec.replicas is an optional field that specifies the number of desired Pods. It defaults to 1.Should you manually scale a deployment, example via kubectl scale statefulset statefulset --replicas=X, and then you update that StatefulSet based on a manifest (for example: by running kubectl apply -f statefulset.yaml), then applying that manifest overwrites the manual scaling that you previously did.If a HorizontalPodAutoscaler (or any similar API for horizontal scaling) is managing scaling for a Statefulset, don't set .spec.replicas. Instead, allow the Kubernetes control plane to manage the .spec.replicas field automatically.What's nextLearn about Pods.Find out how to use StatefulSetsFollow an example of deploying a stateful application.Follow an example of deploying Cassandra with Stateful Sets.Follow an example of running a replicated stateful application.Learn how to scale a StatefulSet.Learn what's involved when you delete a StatefulSet.Learn how to configure a Pod to use a volume for storage.Learn how to configure a Pod to use a PersistentVolume for storage.StatefulSet is a top-level resource in the Kubernetes REST API. Read the StatefulSet object definition to understand the API for stateful sets.Read about PodDisruptionBudget and how you can use it to manage application availability during disruptions.3.4.2.4 - DaemonSetA DaemonSet defines Pods that provide node-local facilities. These might be fundamental to the operation of your cluster, such as a networking helper tool, or be part of an add-on.A DaemonSet ensures that all (or some) Nodes run a copy of a Pod. As nodes are added to the cluster, Pods are added to them. As nodes are removed from the cluster, those Pods are garbage collected. Deleting a DaemonSet will clean up the Pods it created.Some typical uses of a DaemonSet are:running a cluster storage daemon on every noderunning a logs collection daemon on every noderunning a node monitoring daemon on every nodeIn a simple case, one DaemonSet, covering all nodes, would be used for each type of daemon. A more complex setup might use multiple DaemonSets for a single type of daemon, but with different flags and/or different memory and cpu requests for different hardware types.Writing a DaemonSet SpecCreate a DaemonSetYou can describe a DaemonSet in a YAML file. For example, the daemonset.yaml file below describes a DaemonSet that runs the fluentd-elasticsearch Docker image:controllers/daemonset.yaml apiVersion: apps/v1 kind: DaemonSet metadata: name: fluentd-elasticsearch namespace: kube-system labels: k8s-app: fluentd-logging spec: selector: matchLabels: name: fluentd-elasticsearch template: metadata: labels: name: fluentd-elasticsearch spec: tolerations: # these tolerations are to have the daemonset runnable on control plane nodes # remove them if your control plane nodes should not run pods \- key: node-role.kubernetes.io/control-plane operator: Exists effect: NoSchedule \- key: node-role.kubernetes.io/master operator: Exists effect: NoSchedule containers: \- name: fluentd-elasticsearch image: quay.io/fluentd_elasticsearch/fluentd:v2.5.2 resources: limits: memory: 200Mi requests: cpu: 100m memory: 200Mi volumeMounts: \- name: varlog mountPath: /var/log # it may be desirable to set a high priority class to ensure that a DaemonSet Pod # preempts running Pods # priorityClassName: important terminationGracePeriodSeconds: 30 volumes: \- name: varlog hostPath: path: /var/log Create a DaemonSet based on the YAML file:kubectl apply -f https://k8s.io/examples/controllers/daemonset.yaml Required FieldsAs with all other Kubernetes config, a DaemonSet needs apiVersion, kind, and metadata fields. For general information about working with config files, see running stateless applications and object management using kubectl.The name of a DaemonSet object must be a valid DNS subdomain name.A DaemonSet also needs a .spec section.Pod TemplateThe .spec.template is one of the required fields in .spec.The .spec.template is a pod template. It has exactly the same schema as a Pod, except it is nested and does not have an apiVersion or kind.In addition to required fields for a Pod, a Pod template in a DaemonSet has to specify appropriate labels (see pod selector).A Pod Template in a DaemonSet must have a RestartPolicy equal to Always, or be unspecified, which defaults to Always.Pod SelectorThe .spec.selector field is a pod selector. It works the same as the .spec.selector of a Job.You must specify a pod selector that matches the labels of the .spec.template. Also, once a DaemonSet is created, its .spec.selector can not be mutated. Mutating the pod selector can lead to the unintentional orphaning of Pods, and it was found to be confusing to users.The .spec.selector is an object consisting of two fields:matchLabels - works the same as the .spec.selector of a ReplicationController.matchExpressions - allows to build more sophisticated selectors by specifying key, list of values and an operator that relates the key and values.When the two are specified the result is ANDed.The .spec.selector must match the .spec.template.metadata.labels. Config with these two not matching will be rejected by the API.Running Pods on select NodesIf you specify a .spec.template.spec.nodeSelector, then the DaemonSet controller will create Pods on nodes which match that node selector. Likewise if you specify a .spec.template.spec.affinity, then DaemonSet controller will create Pods on nodes which match that node affinity. If you do not specify either, then the DaemonSet controller will create Pods on all nodes.How Daemon Pods are scheduledA DaemonSet can be used to ensure that all eligible nodes run a copy of a Pod. The DaemonSet controller creates a Pod for each eligible node and adds the spec.affinity.nodeAffinity field of the Pod to match the target host. After the Pod is created, the default scheduler typically takes over and then binds the Pod to the target host by setting the .spec.nodeName field. If the new Pod cannot fit on the node, the default scheduler may preempt (evict) some of the existing Pods based on the priority of the new Pod.Note:If it's important that the DaemonSet pod run on each node, it's often desirable to set the .spec.template.spec.priorityClassName of the DaemonSet to a PriorityClass with a higher priority to ensure that this eviction occurs.The user can specify a different scheduler for the Pods of the DaemonSet, by setting the .spec.template.spec.schedulerName field of the DaemonSet.The original node affinity specified at the .spec.template.spec.affinity.nodeAffinity field (if specified) is taken into consideration by the DaemonSet controller when evaluating the eligible nodes, but is replaced on the created Pod with the node affinity that matches the name of the eligible node.nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: \- matchFields: \- key: metadata.name operator: In values: \- target-host-name Taints and tolerationsThe DaemonSet controller automatically adds a set of tolerations to DaemonSet Pods:Tolerations for DaemonSet podsToleration keyEffectDetailsnode.kubernetes.io/not-readyNoExecuteDaemonSet Pods can be scheduled onto nodes that are not healthy or ready to accept Pods. Any DaemonSet Pods running on such nodes will not be evicted.node.kubernetes.io/unreachableNoExecuteDaemonSet Pods can be scheduled onto nodes that are unreachable from the node controller. Any DaemonSet Pods running on such nodes will not be evicted.node.kubernetes.io/disk-pressureNoScheduleDaemonSet Pods can be scheduled onto nodes with disk pressure issues.node.kubernetes.io/memory-pressureNoScheduleDaemonSet Pods can be scheduled onto nodes with memory pressure issues.node.kubernetes.io/pid-pressureNoScheduleDaemonSet Pods can be scheduled onto nodes with process pressure issues.node.kubernetes.io/unschedulableNoScheduleDaemonSet Pods can be scheduled onto nodes that are unschedulable.node.kubernetes.io/network-unavailableNoScheduleOnly added for DaemonSet Pods that request host networking, i.e., Pods having spec.hostNetwork: true. Such DaemonSet Pods can be scheduled onto nodes with unavailable network.You can add your own tolerations to the Pods of a DaemonSet as well, by defining these in the Pod template of the DaemonSet.Because the DaemonSet controller sets the node.kubernetes.io/unschedulable:NoSchedule toleration automatically, Kubernetes can run DaemonSet Pods on nodes that are marked as unschedulable.If you use a DaemonSet to provide an important node-level function, such as cluster networking, it is helpful that Kubernetes places DaemonSet Pods on nodes before they are ready. For example, without that special toleration, you could end up in a deadlock situation where the node is not marked as ready because the network plugin is not running there, and at the same time the network plugin is not running on that node because the node is not yet ready.Communicating with Daemon PodsSome possible patterns for communicating with Pods in a DaemonSet are:Push: Pods in the DaemonSet are configured to send updates to another service, such as a stats database. They do not have clients.NodeIP and Known Port: Pods in the DaemonSet can use a hostPort, so that the pods are reachable via the node IPs. Clients know the list of node IPs somehow, and know the port by convention.DNS: Create a headless service with the same pod selector, and then discover DaemonSets using the endpoints resource or retrieve multiple A records from DNS.Service: Create a service with the same Pod selector, and use the service to reach a daemon on a random node. (No way to reach specific node.)Updating a DaemonSetIf node labels are changed, the DaemonSet will promptly add Pods to newly matching nodes and delete Pods from newly not-matching nodes.You can modify the Pods that a DaemonSet creates. However, Pods do not allow all fields to be updated. Also, the DaemonSet controller will use the original template the next time a node (even with the same name) is created.You can delete a DaemonSet. If you specify --cascade=orphan with kubectl, then the Pods will be left on the nodes. If you subsequently create a new DaemonSet with the same selector, the new DaemonSet adopts the existing Pods. If any Pods need replacing the DaemonSet replaces them according to its updateStrategy.You can perform a rolling update on a DaemonSet.Alternatives to DaemonSetInit scriptsIt is certainly possible to run daemon processes by directly starting them on a node (e.g. using init, upstartd, or systemd). This is perfectly fine. However, there are several advantages to running such processes via a DaemonSet:Ability to monitor and manage logs for daemons in the same way as applications.Same config language and tools (e.g. Pod templates, kubectl) for daemons and applications.Running daemons in containers with resource limits increases isolation between daemons from app containers. However, this can also be accomplished by running the daemons in a container but not in a Pod.Bare PodsIt is possible to create Pods directly which specify a particular node to run on. However, a DaemonSet replaces Pods that are deleted or terminated for any reason, such as in the case of node failure or disruptive node maintenance, such as a kernel upgrade. For this reason, you should use a DaemonSet rather than creating individual Pods.Static PodsIt is possible to create Pods by writing a file to a certain directory watched by Kubelet. These are called static pods. Unlike DaemonSet, static Pods cannot be managed with kubectl or other Kubernetes API clients. Static Pods do not depend on the apiserver, making them useful in cluster bootstrapping cases. Also, static Pods may be deprecated in the future.DeploymentsDaemonSets are similar to Deployments in that they both create Pods, and those Pods have processes which are not expected to terminate (e.g. web servers, storage servers).Use a Deployment for stateless services, like frontends, where scaling up and down the number of replicas and rolling out updates are more important than controlling exactly which host the Pod runs on. Use a DaemonSet when it is important that a copy of a Pod always run on all or certain hosts, if the DaemonSet provides node-level functionality that allows other Pods to run correctly on that particular node.For example, network plugins often include a component that runs as a DaemonSet. The DaemonSet component makes sure that the node where it's running has working cluster networking.What's nextLearn about Pods.Learn about static Pods, which are useful for running Kubernetes control plane components.Find out how to use DaemonSetsPerform a rolling update on a DaemonSetPerform a rollback on a DaemonSet (for example, if a roll out didn't work how you expected).Understand how Kubernetes assigns Pods to Nodes.Learn about device plugins and add ons, which often run as DaemonSets.DaemonSet is a top-level resource in the Kubernetes REST API. Read the DaemonSet object definition to understand the API for daemon sets.3.4.2.5 - JobsJobs represent one-off tasks that run to completion and then stop.A Job creates one or more Pods and will continue to retry execution of the Pods until a specified number of them successfully terminate. As pods successfully complete, the Job tracks the successful completions. When a specified number of successful completions is reached, the task (ie, Job) is complete. Deleting a Job will clean up the Pods it created. Suspending a Job will delete its active Pods until the Job is resumed again.A simple case is to create one Job object in order to reliably run one Pod to completion. The Job object will start a new Pod if the first Pod fails or is deleted (for example due to a node hardware failure or a node reboot).You can also use a Job to run multiple Pods in parallel.If you want to run a Job (either a single task, or several in parallel) on a schedule, see CronJob.Running an example JobHere is an example Job config. It computes π to 2000 places and prints it out. It takes around 10s to complete.controllers/job.yaml apiVersion: batch/v1 kind: Job metadata: name: pi spec: template: spec: containers: \- name: pi image: perl:5.34.0 command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"] restartPolicy: Never backoffLimit: 4 You can run the example with this command:kubectl apply -f https://kubernetes.io/examples/controllers/job.yaml The output is similar to this:job.batch/pi created Check on the status of the Job with kubectl:kubectl describe job pikubectl get job pi -o yaml Name: pi Namespace: default Selector: batch.kubernetes.io/controller-uid=c9948307-e56d-4b5d-8302-ae2d7b7da67c Labels: batch.kubernetes.io/controller-uid=c9948307-e56d-4b5d-8302-ae2d7b7da67c batch.kubernetes.io/job-name=pi ... Annotations: batch.kubernetes.io/job-tracking: "" Parallelism: 1 Completions: 1 Start Time: Mon, 02 Dec 2019 15:20:11 +0200 Completed At: Mon, 02 Dec 2019 15:21:16 +0200 Duration: 65s Pods Statuses: 0 Running / 1 Succeeded / 0 Failed Pod Template: Labels: batch.kubernetes.io/controller-uid=c9948307-e56d-4b5d-8302-ae2d7b7da67c batch.kubernetes.io/job-name=pi Containers: pi: Image: perl:5.34.0 Port:  Host Port:  Command: perl -Mbignum=bpi -wle print bpi(2000) Environment:  Mounts:  Volumes:  Events: Type Reason Age From Message \---- ------ ---- ---- ------- Normal SuccessfulCreate 21s job-controller Created pod: pi-xf9p4 Normal Completed 18s job-controller Job completed apiVersion: batch/v1 kind: Job metadata: annotations: batch.kubernetes.io/job-tracking: "" ... creationTimestamp: "2022-11-10T17:53:53Z" generation: 1 labels: batch.kubernetes.io/controller-uid: 863452e6-270d-420e-9b94-53a54146c223 batch.kubernetes.io/job-name: pi name: pi namespace: default resourceVersion: "4751" uid: 204fb678-040b-497f-9266-35ffa8716d14 spec: backoffLimit: 4 completionMode: NonIndexed completions: 1 parallelism: 1 selector: matchLabels: batch.kubernetes.io/controller-uid: 863452e6-270d-420e-9b94-53a54146c223 suspend: false template: metadata: creationTimestamp: null labels: batch.kubernetes.io/controller-uid: 863452e6-270d-420e-9b94-53a54146c223 batch.kubernetes.io/job-name: pi spec: containers: \- command: \- perl \- -Mbignum=bpi \- -wle \- print bpi(2000) image: perl:5.34.0 imagePullPolicy: IfNotPresent name: pi resources: {} terminationMessagePath: /dev/termination-log terminationMessagePolicy: File dnsPolicy: ClusterFirst restartPolicy: Never schedulerName: default-scheduler securityContext: {} terminationGracePeriodSeconds: 30 status: active: 1 ready: 0 startTime: "2022-11-10T17:53:57Z" uncountedTerminatedPods: {} To view completed Pods of a Job, use kubectl get pods.To list all the Pods that belong to a Job in a machine readable form, you can use a command like this:pods=$(kubectl get pods --selector=batch.kubernetes.io/job-name=pi --output=jsonpath='{.items[*].metadata.name}') echo $pods The output is similar to this:pi-5rwd7 Here, the selector is the same as the selector for the Job. The --output=jsonpath option specifies an expression with the name from each Pod in the returned list.View the standard output of one of the pods:kubectl logs $pods Another way to view the logs of a Job:kubectl logs jobs/pi The output is similar to this:3.1415926535897932384626433832795028841971693993751058209749445923078164062862089986280348253421170679821480865132823066470938446095505822317253594081284811174502841027019385211055596446229489549303819644288109756659334461284756482337867831652712019091456485669234603486104543266482133936072602491412737245870066063155881748815209209628292540917153643678925903600113305305488204665213841469519415116094330572703657595919530921861173819326117931051185480744623799627495673518857527248912279381830119491298336733624406566430860213949463952247371907021798609437027705392171762931767523846748184676694051320005681271452635608277857713427577896091736371787214684409012249534301465495853710507922796892589235420199561121290219608640344181598136297747713099605187072113499999983729780499510597317328160963185950244594553469083026425223082533446850352619311881710100031378387528865875332083814206171776691473035982534904287554687311595628638823537875937519577818577805321712268066130019278766111959092164201989380952572010654858632788659361533818279682303019520353018529689957736225994138912497217752834791315155748572424541506959508295331168617278558890750983817546374649393192550604009277016711390098488240128583616035637076601047101819429555961989467678374494482553797747268471040475346462080466842590694912933136770289891521047521620569660240580381501935112533824300355876402474964732639141992726042699227967823547816360093417216412199245863150302861829745557067498385054945885869269956909272107975093029553211653449872027559602364806654991198818347977535663698074265425278625518184175746728909777727938000816470600161452491921732172147723501414419735685481613611573525521334757418494684385233239073941433345477624168625189835694855620992192221842725502542568876717904946016534668049886272327917860857843838279679766814541009538837863609506800642251252051173929848960841284886269456042419652850222106611863067442786220391949450471237137869609563643719172874677646575739624138908658326459958133904780275901 Writing a Job specAs with all other Kubernetes config, a Job needs apiVersion, kind, and metadata fields.When the control plane creates new Pods for a Job, the .metadata.name of the Job is part of the basis for naming those Pods. The name of a Job must be a valid DNS subdomain value, but this can produce unexpected results for the Pod hostnames. For best compatibility, the name should follow the more restrictive rules for a DNS label. Even when the name is a DNS subdomain, the name must be no longer than 63 characters.A Job also needs a .spec section.Job LabelsJob labels will have batch.kubernetes.io/ prefix for job-name and controller-uid.Pod TemplateThe .spec.template is the only required field of the .spec.The .spec.template is a pod template. It has exactly the same schema as a Pod, except it is nested and does not have an apiVersion or kind.In addition to required fields for a Pod, a pod template in a Job must specify appropriate labels (see pod selector) and an appropriate restart policy.Only a RestartPolicy equal to Never or OnFailure is allowed.Pod selectorThe .spec.selector field is optional. In almost all cases you should not specify it. See section specifying your own pod selector.Parallel execution for JobsThere are three main types of task suitable to run as a Job:Non-parallel Jobsnormally, only one Pod is started, unless the Pod fails.the Job is complete as soon as its Pod terminates successfully.Parallel Jobs with a fixed completion count:specify a non-zero positive value for .spec.completions.the Job represents the overall task, and is complete when there are .spec.completions successful Pods.when using .spec.completionMode="Indexed", each Pod gets a different index in the range 0 to .spec.completions-1.Parallel Jobs with a work queue:do not specify .spec.completions, default to .spec.parallelism.the Pods must coordinate amongst themselves or an external service to determine what each should work on. For example, a Pod might fetch a batch of up to N items from the work queue.each Pod is independently capable of determining whether or not all its peers are done, and thus that the entire Job is done.when any Pod from the Job terminates with success, no new Pods are created.once at least one Pod has terminated with success and all Pods are terminated, then the Job is completed with success.once any Pod has exited with success, no other Pod should still be doing any work for this task or writing any output. They should all be in the process of exiting.For a non-parallel Job, you can leave both .spec.completions and .spec.parallelism unset. When both are unset, both are defaulted to 1.For a fixed completion count Job, you should set .spec.completions to the number of completions needed. You can set .spec.parallelism, or leave it unset and it will default to 1.For a work queue Job, you must leave .spec.completions unset, and set .spec.parallelism to a non-negative integer.For more information about how to make use of the different types of job, see the job patterns section.Controlling parallelismThe requested parallelism (.spec.parallelism) can be set to any non-negative value. If it is unspecified, it defaults to 1. If it is specified as 0, then the Job is effectively paused until it is increased.Actual parallelism (number of pods running at any instant) may be more or less than requested parallelism, for a variety of reasons:For fixed completion count Jobs, the actual number of pods running in parallel will not exceed the number of remaining completions. Higher values of .spec.parallelism are effectively ignored.For work queue Jobs, no new Pods are started after any Pod has succeeded -- remaining Pods are allowed to complete, however.If the Job Controller has not had time to react.If the Job controller failed to create Pods for any reason (lack of ResourceQuota, lack of permission, etc.), then there may be fewer pods than requested.The Job controller may throttle new Pod creation due to excessive previous pod failures in the same Job.When a Pod is gracefully shut down, it takes time to stop.Completion modeFEATURE STATE: Kubernetes v1.24 [stable]Jobs with fixed completion count - that is, jobs that have non null .spec.completions - can have a completion mode that is specified in .spec.completionMode:NonIndexed (default): the Job is considered complete when there have been .spec.completions successfully completed Pods. In other words, each Pod completion is homologous to each other. Note that Jobs that have null .spec.completions are implicitly NonIndexed.Indexed: the Pods of a Job get an associated completion index from 0 to .spec.completions-1. The index is available through four mechanisms:The Pod annotation batch.kubernetes.io/job-completion-index.The Pod label batch.kubernetes.io/job-completion-index (for v1.28 and later). Note the feature gate PodIndexLabel must be enabled to use this label, and it is enabled by default.As part of the Pod hostname, following the pattern $(job-name)-$(index). When you use an Indexed Job in combination with a Service, Pods within the Job can use the deterministic hostnames to address each other via DNS. For more information about how to configure this, see Job with Pod-to-Pod Communication.From the containerized task, in the environment variable JOB_COMPLETION_INDEX.The Job is considered complete when there is one successfully completed Pod for each index. For more information about how to use this mode, see Indexed Job for Parallel Processing with Static Work Assignment.Note:Although rare, more than one Pod could be started for the same index (due to various reasons such as node failures, kubelet restarts, or Pod evictions). In this case, only the first Pod that completes successfully will count towards the completion count and update the status of the Job. The other Pods that are running or completed for the same index will be deleted by the Job controller once they are detected.Handling Pod and container failuresA container in a Pod may fail for a number of reasons, such as because the process in it exited with a non-zero exit code, or the container was killed for exceeding a memory limit, etc. If this happens, and the .spec.template.spec.restartPolicy = "OnFailure", then the Pod stays on the node, but the container is re-run. Therefore, your program needs to handle the case when it is restarted locally, or else specify .spec.template.spec.restartPolicy = "Never". See pod lifecycle for more information on restartPolicy.An entire Pod can also fail, for a number of reasons, such as when the pod is kicked off the node (node is upgraded, rebooted, deleted, etc.), or if a container of the Pod fails and the .spec.template.spec.restartPolicy = "Never". When a Pod fails, then the Job controller starts a new Pod. This means that your application needs to handle the case when it is restarted in a new pod. In particular, it needs to handle temporary files, locks, incomplete output and the like caused by previous runs.By default, each pod failure is counted towards the .spec.backoffLimit limit, see pod backoff failure policy. However, you can customize handling of pod failures by setting the Job's pod failure policy.Additionally, you can choose to count the pod failures independently for each index of an Indexed Job by setting the .spec.backoffLimitPerIndex field (for more information, see backoff limit per index).Note that even if you specify .spec.parallelism = 1 and .spec.completions = 1 and .spec.template.spec.restartPolicy = "Never", the same program may sometimes be started twice.If you do specify .spec.parallelism and .spec.completions both greater than 1, then there may be multiple pods running at once. Therefore, your pods must also be tolerant of concurrency.If you specify the .spec.podFailurePolicy field, the Job controller does not consider a terminating Pod (a pod that has a .metadata.deletionTimestamp field set) as a failure until that Pod is terminal (its .status.phase is Failed or Succeeded). However, the Job controller creates a replacement Pod as soon as the termination becomes apparent. Once the pod terminates, the Job controller evaluates .backoffLimit and .podFailurePolicy for the relevant Job, taking this now-terminated Pod into consideration.If either of these requirements is not satisfied, the Job controller counts a terminating Pod as an immediate failure, even if that Pod later terminates with phase: "Succeeded".Pod backoff failure policyThere are situations where you want to fail a Job after some amount of retries due to a logical error in configuration etc. To do so, set .spec.backoffLimit to specify the number of retries before considering a Job as failed.The .spec.backoffLimit is set by default to 6, unless the backoff limit per index (only Indexed Job) is specified. When .spec.backoffLimitPerIndex is specified, then .spec.backoffLimit defaults to 2147483647 (MaxInt32).Failed Pods associated with the Job are recreated by the Job controller with an exponential back-off delay (10s, 20s, 40s ...) capped at six minutes.The number of retries is calculated in two ways:The number of Pods with .status.phase = "Failed".When using restartPolicy = "OnFailure", the number of retries in all the containers of Pods with .status.phase equal to Pending or Running.If either of the calculations reaches the .spec.backoffLimit, the Job is considered failed.Note:If your job has restartPolicy = "OnFailure", keep in mind that your Pod running the Job will be terminated once the job backoff limit has been reached. This can make debugging the Job's executable more difficult. We suggest setting restartPolicy = "Never" when debugging the Job or using a logging system to ensure output from failed Jobs is not lost inadvertently.Backoff limit per indexFEATURE STATE: Kubernetes v1.33 [stable] (enabled by default: true)When you run an indexed Job, you can choose to handle retries for pod failures independently for each index. To do so, set the .spec.backoffLimitPerIndex to specify the maximal number of pod failures per index.When the per-index backoff limit is exceeded for an index, Kubernetes considers the index as failed and adds it to the .status.failedIndexes field. The succeeded indexes, those with a successfully executed pods, are recorded in the .status.completedIndexes field, regardless of whether you set the backoffLimitPerIndex field.Note that a failing index does not interrupt execution of other indexes. Once all indexes finish for a Job where you specified a backoff limit per index, if at least one of those indexes did fail, the Job controller marks the overall Job as failed, by setting the Failed condition in the status. The Job gets marked as failed even if some, potentially nearly all, of the indexes were processed successfully.You can additionally limit the maximal number of indexes marked failed by setting the .spec.maxFailedIndexes field. When the number of failed indexes exceeds the maxFailedIndexes field, the Job controller triggers termination of all remaining running Pods for that Job. Once all pods are terminated, the entire Job is marked failed by the Job controller, by setting the Failed condition in the Job status.Here is an example manifest for a Job that defines a backoffLimitPerIndex:/controllers/job-backoff-limit-per-index-example.yaml apiVersion: batch/v1 kind: Job metadata: name: job-backoff-limit-per-index-example spec: completions: 10 parallelism: 3 completionMode: Indexed # required for the feature backoffLimitPerIndex: 1 # maximal number of failures per index maxFailedIndexes: 5 # maximal number of failed indexes before terminating the Job execution template: spec: restartPolicy: Never # required for the feature containers: \- name: example image: python command: # The jobs fails as there is at least one failed index # (all even indexes fail in here), yet all indexes # are executed as maxFailedIndexes is not exceeded. \- python3 \- -c \- | import os, sys print("Hello world") if int(os.environ.get("JOB_COMPLETION_INDEX")) % 2 == 0: sys.exit(1) In the example above, the Job controller allows for one restart for each of the indexes. When the total number of failed indexes exceeds 5, then the entire Job is terminated.Once the job is finished, the Job status looks as follows:kubectl get -o yaml job job-backoff-limit-per-index-example status: completedIndexes: 1,3,5,7,9 failedIndexes: 0,2,4,6,8 succeeded: 5 # 1 succeeded pod for each of 5 succeeded indexes failed: 10 # 2 failed pods (1 retry) for each of 5 failed indexes conditions: \- message: Job has failed indexes reason: FailedIndexes status: "True" type: FailureTarget \- message: Job has failed indexes reason: FailedIndexes status: "True" type: Failed The Job controller adds the FailureTarget Job condition to trigger Job termination and cleanup. When all of the Job Pods are terminated, the Job controller adds the Failed condition with the same values for reason and message as the FailureTarget Job condition. For details, see Termination of Job Pods.Additionally, you may want to use the per-index backoff along with a pod failure policy. When using per-index backoff, there is a new FailIndex action available which allows you to avoid unnecessary retries within an index.Pod failure policyFEATURE STATE: Kubernetes v1.31 [stable] (enabled by default: true)A Pod failure policy, defined with the .spec.podFailurePolicy field, enables your cluster to handle Pod failures based on the container exit codes and the Pod conditions.In some situations, you may want to have a better control when handling Pod failures than the control provided by the Pod backoff failure policy, which is based on the Job's .spec.backoffLimit. These are some examples of use cases:To optimize costs of running workloads by avoiding unnecessary Pod restarts, you can terminate a Job as soon as one of its Pods fails with an exit code indicating a software bug.To guarantee that your Job finishes even if there are disruptions, you can ignore Pod failures caused by disruptions (such as preemption, API-initiated eviction or taint-based eviction) so that they don't count towards the .spec.backoffLimit limit of retries.You can configure a Pod failure policy, in the .spec.podFailurePolicy field, to meet the above use cases. This policy can handle Pod failures based on the container exit codes and the Pod conditions.Here is a manifest for a Job that defines a podFailurePolicy:/controllers/job-pod-failure-policy-example.yaml apiVersion: batch/v1 kind: Job metadata: name: job-pod-failure-policy-example spec: completions: 12 parallelism: 3 template: spec: restartPolicy: Never containers: \- name: main image: docker.io/library/bash:5 command: ["bash"] # example command simulating a bug which triggers the FailJob action args: \- -c \- echo "Hello world!" && sleep 5 && exit 42 backoffLimit: 6 podFailurePolicy: rules: \- action: FailJob onExitCodes: containerName: main # optional operator: In # one of: In, NotIn values: [42] \- action: Ignore # one of: Ignore, FailJob, Count onPodConditions: \- type: DisruptionTarget # indicates Pod disruption In the example above, the first rule of the Pod failure policy specifies that the Job should be marked failed if the main container fails with the 42 exit code. The following are the rules for the main container specifically:an exit code of 0 means that the container succeededan exit code of 42 means that the entire Job failedany other exit code represents that the container failed, and hence the entire Pod. The Pod will be re-created if the total number of restarts is below backoffLimit. If the backoffLimit is reached the entire Job failed.Note:Because the Pod template specifies a restartPolicy: Never, the kubelet does not restart the main container in that particular Pod.The second rule of the Pod failure policy, specifying the Ignore action for failed Pods with condition DisruptionTarget excludes Pod disruptions from being counted towards the .spec.backoffLimit limit of retries.Note:If the Job failed, either by the Pod failure policy or Pod backoff failure policy, and the Job is running multiple Pods, Kubernetes terminates all the Pods in that Job that are still Pending or Running.These are some requirements and semantics of the API:if you want to use a .spec.podFailurePolicy field for a Job, you must also define that Job's pod template with .spec.restartPolicy set to Never.the Pod failure policy rules you specify under spec.podFailurePolicy.rules are evaluated in order. Once a rule matches a Pod failure, the remaining rules are ignored. When no rule matches the Pod failure, the default handling applies.you may want to restrict a rule to a specific container by specifying its name inspec.podFailurePolicy.rules[*].onExitCodes.containerName. When not specified the rule applies to all containers. When specified, it should match one the container or initContainer names in the Pod template.you may specify the action taken when a Pod failure policy is matched by spec.podFailurePolicy.rules[*].action. Possible values are:FailJob: use to indicate that the Pod's job should be marked as Failed and all running Pods should be terminated.Ignore: use to indicate that the counter towards the .spec.backoffLimit should not be incremented and a replacement Pod should be created.Count: use to indicate that the Pod should be handled in the default way. The counter towards the .spec.backoffLimit should be incremented.FailIndex: use this action along with backoff limit per index to avoid unnecessary retries within the index of a failed pod.Note:When you use a podFailurePolicy, the job controller only matches Pods in the Failed phase. Pods with a deletion timestamp that are not in a terminal phase (Failed or Succeeded) are considered still terminating. This implies that terminating pods retain a tracking finalizer until they reach a terminal phase. Since Kubernetes 1.27, Kubelet transitions deleted pods to a terminal phase (see: Pod Phase). This ensures that deleted pods have their finalizers removed by the Job controller.Note:Starting with Kubernetes v1.28, when Pod failure policy is used, the Job controller recreates terminating Pods only once these Pods reach the terminal Failed phase. This behavior is similar to podReplacementPolicy: Failed. For more information, see Pod replacement policy.When you use the podFailurePolicy, and the Job fails due to the pod matching the rule with the FailJob action, then the Job controller triggers the Job termination process by adding the FailureTarget condition. For more details, see Job termination and cleanup.Success policyWhen creating an Indexed Job, you can define when a Job can be declared as succeeded using a .spec.successPolicy, based on the pods that succeeded.By default, a Job succeeds when the number of succeeded Pods equals .spec.completions. These are some situations where you might want additional control for declaring a Job succeeded:When running simulations with different parameters, you might not need all the simulations to succeed for the overall Job to be successful.When following a leader-worker pattern, only the success of the leader determines the success or failure of a Job. Examples of this are frameworks like MPI and PyTorch etc.You can configure a success policy, in the .spec.successPolicy field, to meet the above use cases. This policy can handle Job success based on the succeeded pods. After the Job meets the success policy, the job controller terminates the lingering Pods. A success policy is defined by rules. Each rule can take one of the following forms:When you specify the succeededIndexes only, once all indexes specified in the succeededIndexes succeed, the job controller marks the Job as succeeded. The succeededIndexes must be a list of intervals between 0 and .spec.completions-1.When you specify the succeededCount only, once the number of succeeded indexes reaches the succeededCount, the job controller marks the Job as succeeded.When you specify both succeededIndexes and succeededCount, once the number of succeeded indexes from the subset of indexes specified in the succeededIndexes reaches the succeededCount, the job controller marks the Job as succeeded.Note that when you specify multiple rules in the .spec.successPolicy.rules, the job controller evaluates the rules in order. Once the Job meets a rule, the job controller ignores remaining rules.Here is a manifest for a Job with successPolicy:/controllers/job-success-policy.yaml apiVersion: batch/v1 kind: Job metadata: name: job-success spec: parallelism: 10 completions: 10 completionMode: Indexed # Required for the success policy successPolicy: rules: \- succeededIndexes: 0,2-3 succeededCount: 1 template: spec: containers: \- name: main image: python command: # Provided that at least one of the Pods with 0, 2, and 3 indexes has succeeded, # the overall Job is a success. \- python3 \- -c \- | import os, sys if os.environ.get("JOB_COMPLETION_INDEX") == "2": sys.exit(0) else: sys.exit(1) restartPolicy: Never In the example above, both succeededIndexes and succeededCount have been specified. Therefore, the job controller will mark the Job as succeeded and terminate the lingering Pods when either of the specified indexes, 0, 2, or 3, succeed. The Job that meets the success policy gets the SuccessCriteriaMet condition with a SuccessPolicy reason. After the removal of the lingering Pods is issued, the Job gets the Complete condition.Note that the succeededIndexes is represented as intervals separated by a hyphen. The number are listed in represented by the first and last element of the series, separated by a hyphen.Note:When you specify both a success policy and some terminating policies such as .spec.backoffLimit and .spec.podFailurePolicy, once the Job meets either policy, the job controller respects the terminating policy and ignores the success policy.Job termination and cleanupWhen a Job completes, no more Pods are created, but the Pods are usually not deleted either. Keeping them around allows you to still view the logs of completed pods to check for errors, warnings, or other diagnostic output. The job object also remains after it is completed so that you can view its status. It is up to the user to delete old jobs after noting their status. Delete the job with kubectl (e.g. kubectl delete jobs/pi or kubectl delete -f ./job.yaml). When you delete the job using kubectl, all the pods it created are deleted too.By default, a Job will run uninterrupted unless a Pod fails (restartPolicy=Never) or a Container exits in error (restartPolicy=OnFailure), at which point the Job defers to the .spec.backoffLimit described above. Once .spec.backoffLimit has been reached the Job will be marked as failed and any running Pods will be terminated.Another way to terminate a Job is by setting an active deadline. Do this by setting the .spec.activeDeadlineSeconds field of the Job to a number of seconds. The activeDeadlineSeconds applies to the duration of the job, no matter how many Pods are created. Once a Job reaches activeDeadlineSeconds, all of its running Pods are terminated and the Job status will become type: Failed with reason: DeadlineExceeded.Note that a Job's .spec.activeDeadlineSeconds takes precedence over its .spec.backoffLimit. Therefore, a Job that is retrying one or more failed Pods will not deploy additional Pods once it reaches the time limit specified by activeDeadlineSeconds, even if the backoffLimit is not yet reached.Example:apiVersion: batch/v1 kind: Job metadata: name: pi-with-timeout spec: backoffLimit: 5 activeDeadlineSeconds: 100 template: spec: containers: \- name: pi image: perl:5.34.0 command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"] restartPolicy: Never Note that both the Job spec and the Pod template spec within the Job have an activeDeadlineSeconds field. Ensure that you set this field at the proper level.Keep in mind that the restartPolicy applies to the Pod, and not to the Job itself: there is no automatic Job restart once the Job status is type: Failed. That is, the Job termination mechanisms activated with .spec.activeDeadlineSeconds and .spec.backoffLimit result in a permanent Job failure that requires manual intervention to resolve.Terminal Job conditionsA Job has two possible terminal states, each of which has a corresponding Job condition:Succeeded: Job condition CompleteFailed: Job condition FailedJobs fail for the following reasons:The number of Pod failures exceeded the specified .spec.backoffLimit in the Job specification. For details, see Pod backoff failure policy.The Job runtime exceeded the specified .spec.activeDeadlineSecondsAn indexed Job that used .spec.backoffLimitPerIndex has failed indexes. For details, see Backoff limit per index.The number of failed indexes in the Job exceeded the specified spec.maxFailedIndexes. For details, see Backoff limit per indexA failed Pod matches a rule in .spec.podFailurePolicy that has the FailJob action. For details about how Pod failure policy rules might affect failure evaluation, see Pod failure policy.Jobs succeed for the following reasons:The number of succeeded Pods reached the specified .spec.completionsThe criteria specified in .spec.successPolicy are met. For details, see Success policy.In Kubernetes v1.31 and later the Job controller delays the addition of the terminal conditions,Failed or Complete, until all of the Job Pods are terminated.In Kubernetes v1.30 and earlier, the Job controller added the Complete or the Failed Job terminal conditions as soon as the Job termination process was triggered and all Pod finalizers were removed. However, some Pods would still be running or terminating at the moment that the terminal condition was added.In Kubernetes v1.31 and later, the controller only adds the Job terminal conditions after all of the Pods are terminated. You can control this behavior by using the JobManagedBy and the JobPodReplacementPolicy (both enabled by default) feature gates.Termination of Job podsThe Job controller adds the FailureTarget condition or the SuccessCriteriaMet condition to the Job to trigger Pod termination after a Job meets either the success or failure criteria.Factors like terminationGracePeriodSeconds might increase the amount of time from the moment that the Job controller adds the FailureTarget condition or the SuccessCriteriaMet condition to the moment that all of the Job Pods terminate and the Job controller adds a terminal condition (Failed or Complete).You can use the FailureTarget or the SuccessCriteriaMet condition to evaluate whether the Job has failed or succeeded without having to wait for the controller to add a terminal condition.For example, you might want to decide when to create a replacement Job that replaces a failed Job. If you replace the failed Job when the FailureTarget condition appears, your replacement Job runs sooner, but could result in Pods from the failed and the replacement Job running at the same time, using extra compute resources.Alternatively, if your cluster has limited resource capacity, you could choose to wait until the Failed condition appears on the Job, which would delay your replacement Job but would ensure that you conserve resources by waiting until all of the failed Pods are removed.Clean up finished jobs automaticallyFinished Jobs are usually no longer needed in the system. Keeping them around in the system will put pressure on the API server. If the Jobs are managed directly by a higher level controller, such as CronJobs, the Jobs can be cleaned up by CronJobs based on the specified capacity-based cleanup policy.TTL mechanism for finished JobsFEATURE STATE: Kubernetes v1.23 [stable]Another way to clean up finished Jobs (either Complete or Failed) automatically is to use a TTL mechanism provided by a TTL controller for finished resources, by specifying the .spec.ttlSecondsAfterFinished field of the Job.When the TTL controller cleans up the Job, it will delete the Job cascadingly, i.e. delete its dependent objects, such as Pods, together with the Job. Note that when the Job is deleted, its lifecycle guarantees, such as finalizers, will be honored.For example:apiVersion: batch/v1 kind: Job metadata: name: pi-with-ttl spec: ttlSecondsAfterFinished: 100 template: spec: containers: \- name: pi image: perl:5.34.0 command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"] restartPolicy: Never The Job pi-with-ttl will be eligible to be automatically deleted, 100 seconds after it finishes.If the field is set to 0, the Job will be eligible to be automatically deleted immediately after it finishes. If the field is unset, this Job won't be cleaned up by the TTL controller after it finishes.Note:It is recommended to set ttlSecondsAfterFinished field because unmanaged jobs (Jobs that you created directly, and not indirectly through other workload APIs such as CronJob) have a default deletion policy of orphanDependents causing Pods created by an unmanaged Job to be left around after that Job is fully deleted. Even though the control plane eventually garbage collects the Pods from a deleted Job after they either fail or complete, sometimes those lingering pods may cause cluster performance degradation or in worst case cause the cluster to go offline due to this degradation.You can use LimitRanges and ResourceQuotas to place a cap on the amount of resources that a particular namespace can consume.Job patternsThe Job object can be used to process a set of independent but related work items. These might be emails to be sent, frames to be rendered, files to be transcoded, ranges of keys in a NoSQL database to scan, and so on.In a complex system, there may be multiple different sets of work items. Here we are just considering one set of work items that the user wants to manage together — a batch job.There are several different patterns for parallel computation, each with strengths and weaknesses. The tradeoffs are:One Job object for each work item, versus a single Job object for all work items. One Job per work item creates some overhead for the user and for the system to manage large numbers of Job objects. A single Job for all work items is better for large numbers of items.Number of Pods created equals number of work items, versus each Pod can process multiple work items. When the number of Pods equals the number of work items, the Pods typically requires less modification to existing code and containers. Having each Pod process multiple work items is better for large numbers of items.Several approaches use a work queue. This requires running a queue service, and modifications to the existing program or container to make it use the work queue. Other approaches are easier to adapt to an existing containerised application.When the Job is associated with a headless Service, you can enable the Pods within a Job to communicate with each other to collaborate in a computation.The tradeoffs are summarized here, with columns 2 to 4 corresponding to the above tradeoffs. The pattern names are also links to examples and more detailed description.PatternSingle Job objectFewer pods than work items?Use app unmodified?Queue with Pod Per Work Item✓sometimesQueue with Variable Pod Count✓✓Indexed Job with Static Work Assignment✓✓Job with Pod-to-Pod Communication✓sometimessometimesJob Template Expansion✓When you specify completions with .spec.completions, each Pod created by the Job controller has an identical spec. This means that all pods for a task will have the same command line and the same image, the same volumes, and (almost) the same environment variables. These patterns are different ways to arrange for pods to work on different things.This table shows the required settings for .spec.parallelism and .spec.completions for each of the patterns. Here, W is the number of work items.Pattern.spec.completions.spec.parallelismQueue with Pod Per Work ItemWanyQueue with Variable Pod CountnullanyIndexed Job with Static Work AssignmentWanyJob with Pod-to-Pod CommunicationWWJob Template Expansion1should be 1Advanced usageSuspending a JobFEATURE STATE: Kubernetes v1.24 [stable]When a Job is created, the Job controller will immediately begin creating Pods to satisfy the Job's requirements and will continue to do so until the Job is complete. However, you may want to temporarily suspend a Job's execution and resume it later, or start Jobs in suspended state and have a custom controller decide later when to start them.To suspend a Job, you can update the .spec.suspend field of the Job to true; later, when you want to resume it again, update it to false. Creating a Job with .spec.suspend set to true will create it in the suspended state.When a Job is resumed from suspension, its .status.startTime field will be reset to the current time. This means that the .spec.activeDeadlineSeconds timer will be stopped and reset when a Job is suspended and resumed.When you suspend a Job, any running Pods that don't have a status of Completed will be terminated with a SIGTERM signal. The Pod's graceful termination period will be honored and your Pod must handle this signal in this period. This may involve saving progress for later or undoing changes. Pods terminated this way will not count towards the Job's completions count.An example Job definition in the suspended state can be like so:kubectl get job myjob -o yaml apiVersion: batch/v1 kind: Job metadata: name: myjob spec: suspend: true parallelism: 1 completions: 5 template: spec: ... You can also toggle Job suspension by patching the Job using the command line.Suspend an active Job:kubectl patch job/myjob --type=strategic --patch '{"spec":{"suspend":true}}' Resume a suspended Job:kubectl patch job/myjob --type=strategic --patch '{"spec":{"suspend":false}}' The Job's status can be used to determine if a Job is suspended or has been suspended in the past:kubectl get jobs/myjob -o yaml apiVersion: batch/v1 kind: Job # .metadata and .spec omitted status: conditions: \- lastProbeTime: "2021-02-05T13:14:33Z" lastTransitionTime: "2021-02-05T13:14:33Z" status: "True" type: Suspended startTime: "2021-02-05T13:13:48Z" The Job condition of type "Suspended" with status "True" means the Job is suspended; the lastTransitionTime field can be used to determine how long the Job has been suspended for. If the status of that condition is "False", then the Job was previously suspended and is now running. If such a condition does not exist in the Job's status, the Job has never been stopped.Events are also created when the Job is suspended and resumed:kubectl describe jobs/myjob Name: myjob ... Events: Type Reason Age From Message \---- ------ ---- ---- ------- Normal SuccessfulCreate 12m job-controller Created pod: myjob-hlrpl Normal SuccessfulDelete 11m job-controller Deleted pod: myjob-hlrpl Normal Suspended 11m job-controller Job suspended Normal SuccessfulCreate 3s job-controller Created pod: myjob-jvb44 Normal Resumed 3s job-controller Job resumed The last four events, particularly the "Suspended" and "Resumed" events, are directly a result of toggling the .spec.suspend field. In the time between these two events, we see that no Pods were created, but Pod creation restarted as soon as the Job was resumed.Mutable Scheduling DirectivesFEATURE STATE: Kubernetes v1.27 [stable]In most cases, a parallel job will want the pods to run with constraints, like all in the same zone, or all either on GPU model x or y but not a mix of both.The suspend field is the first step towards achieving those semantics. Suspend allows a custom queue controller to decide when a job should start; However, once a job is unsuspended, a custom queue controller has no influence on where the pods of a job will actually land.This feature allows updating a Job's scheduling directives before it starts, which gives custom queue controllers the ability to influence pod placement while at the same time offloading actual pod-to-node assignment to kube-scheduler. This is allowed only for suspended Jobs that have never been unsuspended before.The fields in a Job's pod template that can be updated are node affinity, node selector, tolerations, labels, annotations and scheduling gates.Specifying your own Pod selectorNormally, when you create a Job object, you do not specify .spec.selector. The system defaulting logic adds this field when the Job is created. It picks a selector value that will not overlap with any other jobs.However, in some cases, you might need to override this automatically set selector. To do this, you can specify the .spec.selector of the Job.Be very careful when doing this. If you specify a label selector which is not unique to the pods of that Job, and which matches unrelated Pods, then pods of the unrelated job may be deleted, or this Job may count other Pods as completing it, or one or both Jobs may refuse to create Pods or run to completion. If a non-unique selector is chosen, then other controllers (e.g. ReplicationController) and their Pods may behave in unpredictable ways too. Kubernetes will not stop you from making a mistake when specifying .spec.selector.Here is an example of a case when you might want to use this feature.Say Job old is already running. You want existing Pods to keep running, but you want the rest of the Pods it creates to use a different pod template and for the Job to have a new name. You cannot update the Job because these fields are not updatable. Therefore, you delete Job old but leave its pods running, using kubectl delete jobs/old --cascade=orphan. Before deleting it, you make a note of what selector it uses:kubectl get job old -o yaml The output is similar to this:kind: Job metadata: name: old ... spec: selector: matchLabels: batch.kubernetes.io/controller-uid: a8f3d00d-c6d2-11e5-9f87-42010af00002 ... Then you create a new Job with name new and you explicitly specify the same selector. Since the existing Pods have label batch.kubernetes.io/controller-uid=a8f3d00d-c6d2-11e5-9f87-42010af00002, they are controlled by Job new as well.You need to specify manualSelector: true in the new Job since you are not using the selector that the system normally generates for you automatically.kind: Job metadata: name: new ... spec: manualSelector: true selector: matchLabels: batch.kubernetes.io/controller-uid: a8f3d00d-c6d2-11e5-9f87-42010af00002 ... The new Job itself will have a different uid from a8f3d00d-c6d2-11e5-9f87-42010af00002. Setting manualSelector: true tells the system that you know what you are doing and to allow this mismatch.Job tracking with finalizersFEATURE STATE: Kubernetes v1.26 [stable]The control plane keeps track of the Pods that belong to any Job and notices if any such Pod is removed from the API server. To do that, the Job controller creates Pods with the finalizer batch.kubernetes.io/job-tracking. The controller removes the finalizer only after the Pod has been accounted for in the Job status, allowing the Pod to be removed by other controllers or users.Note:See My pod stays terminating if you observe that pods from a Job are stuck with the tracking finalizer.Elastic Indexed JobsFEATURE STATE: Kubernetes v1.31 [stable] (enabled by default: true)You can scale Indexed Jobs up or down by mutating both .spec.parallelism and .spec.completions together such that .spec.parallelism == .spec.completions. When scaling down, Kubernetes removes the Pods with higher indexes.Use cases for elastic Indexed Jobs include batch workloads which require scaling an indexed Job, such as MPI, Horovod, Ray, and PyTorch training jobs.Delayed creation of replacement podsFEATURE STATE: Kubernetes v1.29 [beta]Note:You can only set podReplacementPolicy on Jobs if you enable the JobPodReplacementPolicy feature gate (enabled by default).By default, the Job controller recreates Pods as soon they either fail or are terminating (have a deletion timestamp). This means that, at a given time, when some of the Pods are terminating, the number of running Pods for a Job can be greater than parallelism or greater than one Pod per index (if you are using an Indexed Job).You may choose to create replacement Pods only when the terminating Pod is fully terminal (has status.phase: Failed). To do this, set the .spec.podReplacementPolicy: Failed. The default replacement policy depends on whether the Job has a podFailurePolicy set. With no Pod failure policy defined for a Job, omitting the podReplacementPolicy field selects the TerminatingOrFailed replacement policy: the control plane creates replacement Pods immediately upon Pod deletion (as soon as the control plane sees that a Pod for this Job has deletionTimestamp set). For Jobs with a Pod failure policy set, the default podReplacementPolicy is Failed, and no other value is permitted. See Pod failure policy to learn more about Pod failure policies for Jobs.kind: Job metadata: name: new ... spec: podReplacementPolicy: Failed ... Provided your cluster has the feature gate enabled, you can inspect the .status.terminating field of a Job. The value of the field is the number of Pods owned by the Job that are currently terminating.kubectl get jobs/myjob -o yaml apiVersion: batch/v1 kind: Job # .metadata and .spec omitted status: terminating: 3 # three Pods are terminating and have not yet reached the Failed phase Delegation of managing a Job object to external controllerFEATURE STATE: Kubernetes v1.32 [beta] (enabled by default: true)Note:You can only set the managedBy field on Jobs if you enable the JobManagedBy feature gate (enabled by default).This feature allows you to disable the built-in Job controller, for a specific Job, and delegate reconciliation of the Job to an external controller.You indicate the controller that reconciles the Job by setting a custom value for the spec.managedBy field - any value other than kubernetes.io/job-controller. The value of the field is immutable.Note:When using this feature, make sure the controller indicated by the field is installed, otherwise the Job may not be reconciled at all.Note:When developing an external Job controller be aware that your controller needs to operate in a fashion conformant with the definitions of the API spec and status fields of the Job object.Please review these in detail in the Job API. We also recommend that you run the e2e conformance tests for the Job object to verify your implementation.Finally, when developing an external Job controller make sure it does not use the batch.kubernetes.io/job-tracking finalizer, reserved for the built-in controller.Warning:If you are considering to disable the JobManagedBy feature gate, or to downgrade the cluster to a version without the feature gate enabled, check if there are jobs with a custom value of the spec.managedBy field. If there are such jobs, there is a risk that they might be reconciled by two controllers after the operation: the built-in Job controller and the external controller indicated by the field value.AlternativesBare PodsWhen the node that a Pod is running on reboots or fails, the pod is terminated and will not be restarted. However, a Job will create new Pods to replace terminated ones. For this reason, we recommend that you use a Job rather than a bare Pod, even if your application requires only a single Pod.Replication ControllerJobs are complementary to Replication Controllers. A Replication Controller manages Pods which are not expected to terminate (e.g. web servers), and a Job manages Pods that are expected to terminate (e.g. batch tasks).As discussed in Pod Lifecycle, Job is only appropriate for pods with RestartPolicy equal to OnFailure or Never. (Note: If RestartPolicy is not set, the default value is Always.)Single Job starts controller PodAnother pattern is for a single Job to create a Pod which then creates other Pods, acting as a sort of custom controller for those Pods. This allows the most flexibility, but may be somewhat complicated to get started with and offers less integration with Kubernetes.One example of this pattern would be a Job which starts a Pod which runs a script that in turn starts a Spark master controller (see spark example), runs a spark driver, and then cleans up.An advantage of this approach is that the overall process gets the completion guarantee of a Job object, but maintains complete control over what Pods are created and how work is assigned to them.What's nextLearn about Pods.Read about different ways of running Jobs:Coarse Parallel Processing Using a Work QueueFine Parallel Processing Using a Work QueueUse an indexed Job for parallel processing with static work assignmentCreate multiple Jobs based on a template: Parallel Processing using ExpansionsFollow the links within Clean up finished jobs automatically to learn more about how your cluster can clean up completed and / or failed tasks.Job is part of the Kubernetes REST API. Read the Job object definition to understand the API for jobs.Read about CronJob, which you can use to define a series of Jobs that will run based on a schedule, similar to the UNIX tool cron.Practice how to configure handling of retriable and non-retriable pod failures using podFailurePolicy, based on the step-by-step examples.3.4.2.6 - Automatic Cleanup for Finished JobsA time-to-live mechanism to clean up old Jobs that have finished execution.FEATURE STATE: Kubernetes v1.23 [stable]When your Job has finished, it's useful to keep that Job in the API (and not immediately delete the Job) so that you can tell whether the Job succeeded or failed.Kubernetes' TTL-after-finished controller provides a TTL (time to live) mechanism to limit the lifetime of Job objects that have finished execution.Cleanup for finished JobsThe TTL-after-finished controller is only supported for Jobs. You can use this mechanism to clean up finished Jobs (either Complete or Failed) automatically by specifying the .spec.ttlSecondsAfterFinished field of a Job, as in this example.The TTL-after-finished controller assumes that a Job is eligible to be cleaned up TTL seconds after the Job has finished. The timer starts once the status condition of the Job changes to show that the Job is either Complete or Failed; once the TTL has expired, that Job becomes eligible for cascading removal. When the TTL-after-finished controller cleans up a job, it will delete it cascadingly, that is to say it will delete its dependent objects together with it.Kubernetes honors object lifecycle guarantees on the Job, such as waiting for finalizers.You can set the TTL seconds at any time. Here are some examples for setting the .spec.ttlSecondsAfterFinished field of a Job:Specify this field in the Job manifest, so that a Job can be cleaned up automatically some time after it finishes.Manually set this field of existing, already finished Jobs, so that they become eligible for cleanup.Use a mutating admission webhook to set this field dynamically at Job creation time. Cluster administrators can use this to enforce a TTL policy for finished jobs.Use a mutating admission webhook to set this field dynamically after the Job has finished, and choose different TTL values based on job status, labels. For this case, the webhook needs to detect changes to the .status of the Job and only set a TTL when the Job is being marked as completed.Write your own controller to manage the cleanup TTL for Jobs that match a particular selector.CaveatsUpdating TTL for finished JobsYou can modify the TTL period, e.g. .spec.ttlSecondsAfterFinished field of Jobs, after the job is created or has finished. If you extend the TTL period after the existing ttlSecondsAfterFinished period has expired, Kubernetes doesn't guarantee to retain that Job, even if an update to extend the TTL returns a successful API response.Time skewBecause the TTL-after-finished controller uses timestamps stored in the Kubernetes jobs to determine whether the TTL has expired or not, this feature is sensitive to time skew in your cluster, which may cause the control plane to clean up Job objects at the wrong time.Clocks aren't always correct, but the difference should be very small. Please be aware of this risk when setting a non-zero TTL.What's nextRead Clean up Jobs automaticallyRefer to the Kubernetes Enhancement Proposal (KEP) for adding this mechanism.3.4.2.7 - CronJobA CronJob starts one-time Jobs on a repeating schedule.FEATURE STATE: Kubernetes v1.21 [stable]A CronJob creates Jobs on a repeating schedule.CronJob is meant for performing regular scheduled actions such as backups, report generation, and so on. One CronJob object is like one line of a crontab (cron table) file on a Unix system. It runs a Job periodically on a given schedule, written in Cron format.CronJobs have limitations and idiosyncrasies. For example, in certain circumstances, a single CronJob can create multiple concurrent Jobs. See the limitations below.When the control plane creates new Jobs and (indirectly) Pods for a CronJob, the .metadata.name of the CronJob is part of the basis for naming those Pods. The name of a CronJob must be a valid DNS subdomain value, but this can produce unexpected results for the Pod hostnames. For best compatibility, the name should follow the more restrictive rules for a DNS label. Even when the name is a DNS subdomain, the name must be no longer than 52 characters. This is because the CronJob controller will automatically append 11 characters to the name you provide and there is a constraint that the length of a Job name is no more than 63 characters.ExampleThis example CronJob manifest prints the current time and a hello message every minute:application/job/cronjob.yaml apiVersion: batch/v1 kind: CronJob metadata: name: hello spec: schedule: "* * * * *" jobTemplate: spec: template: spec: containers: \- name: hello image: busybox:1.28 imagePullPolicy: IfNotPresent command: \- /bin/sh \- -c \- date; echo Hello from the Kubernetes cluster restartPolicy: OnFailure (Running Automated Tasks with a CronJob takes you through this example in more detail).Writing a CronJob specSchedule syntaxThe .spec.schedule field is required. The value of that field follows the Cron syntax:# ┌───────────── minute (0 - 59) # │ ┌───────────── hour (0 - 23) # │ │ ┌───────────── day of the month (1 - 31) # │ │ │ ┌───────────── month (1 - 12) # │ │ │ │ ┌───────────── day of the week (0 - 6) (Sunday to Saturday) # │ │ │ │ │ OR sun, mon, tue, wed, thu, fri, sat # │ │ │ │ │ # │ │ │ │ │ # * * * * * For example, 0 3 * * 1 means this task is scheduled to run weekly on a Monday at 3 AM.The format also includes extended "Vixie cron" step values. As explained in the FreeBSD manual:Step values can be used in conjunction with ranges. Following a range with / specifies skips of the number's value through the range. For example, 0-23/2 can be used in the hours field to specify command execution every other hour (the alternative in the V7 standard is 0,2,4,6,8,10,12,14,16,18,20,22). Steps are also permitted after an asterisk, so if you want to say "every two hours", just use */2.Note:A question mark (?) in the schedule has the same meaning as an asterisk *, that is, it stands for any of available value for a given field.Other than the standard syntax, some macros like @monthly can also be used:EntryDescriptionEquivalent to@yearly (or @annually)Run once a year at midnight of 1 January0 0 1 1 *@monthlyRun once a month at midnight of the first day of the month0 0 1 * *@weeklyRun once a week at midnight on Sunday morning0 0 * * 0@daily (or @midnight)Run once a day at midnight0 0 * * *@hourlyRun once an hour at the beginning of the hour0 * * * *To generate CronJob schedule expressions, you can also use web tools like crontab.guru.Job templateThe .spec.jobTemplate defines a template for the Jobs that the CronJob creates, and it is required. It has exactly the same schema as a Job, except that it is nested and does not have an apiVersion or kind. You can specify common metadata for the templated Jobs, such as labels or annotations. For information about writing a Job .spec, see Writing a Job Spec.Deadline for delayed Job startThe .spec.startingDeadlineSeconds field is optional. This field defines a deadline (in whole seconds) for starting the Job, if that Job misses its scheduled time for any reason.After missing the deadline, the CronJob skips that instance of the Job (future occurrences are still scheduled). For example, if you have a backup Job that runs twice a day, you might allow it to start up to 8 hours late, but no later, because a backup taken any later wouldn't be useful: you would instead prefer to wait for the next scheduled run.For Jobs that miss their configured deadline, Kubernetes treats them as failed Jobs. If you don't specify startingDeadlineSeconds for a CronJob, the Job occurrences have no deadline.If the .spec.startingDeadlineSeconds field is set (not null), the CronJob controller measures the time between when a Job is expected to be created and now. If the difference is higher than that limit, it will skip this execution.For example, if it is set to 200, it allows a Job to be created for up to 200 seconds after the actual schedule.Concurrency policyThe .spec.concurrencyPolicy field is also optional. It specifies how to treat concurrent executions of a Job that is created by this CronJob. The spec may specify only one of the following concurrency policies:Allow (default): The CronJob allows concurrently running JobsForbid: The CronJob does not allow concurrent runs; if it is time for a new Job run and the previous Job run hasn't finished yet, the CronJob skips the new Job run. Also note that when the previous Job run finishes, .spec.startingDeadlineSeconds is still taken into account and may result in a new Job run.Replace: If it is time for a new Job run and the previous Job run hasn't finished yet, the CronJob replaces the currently running Job run with a new Job runNote that concurrency policy only applies to the Jobs created by the same CronJob. If there are multiple CronJobs, their respective Jobs are always allowed to run concurrently.Schedule suspensionYou can suspend execution of Jobs for a CronJob, by setting the optional .spec.suspend field to true. The field defaults to false.This setting does not affect Jobs that the CronJob has already started.If you do set that field to true, all subsequent executions are suspended (they remain scheduled, but the CronJob controller does not start the Jobs to run the tasks) until you unsuspend the CronJob.Caution:Executions that are suspended during their scheduled time count as missed Jobs. When .spec.suspend changes from true to false on an existing CronJob without a starting deadline, the missed Jobs are scheduled immediately.Jobs history limitsThe .spec.successfulJobsHistoryLimit and .spec.failedJobsHistoryLimit fields specify how many completed and failed Jobs should be kept. Both fields are optional..spec.successfulJobsHistoryLimit: This field specifies the number of successful finished jobs to keep. The default value is 3. Setting this field to 0 will not keep any successful jobs..spec.failedJobsHistoryLimit: This field specifies the number of failed finished jobs to keep. The default value is 1. Setting this field to 0 will not keep any failed jobs.For another way to clean up Jobs automatically, see Clean up finished Jobs automatically.Time zonesFEATURE STATE: Kubernetes v1.27 [stable]For CronJobs with no time zone specified, the kube-controller-manager interprets schedules relative to its local time zone.You can specify a time zone for a CronJob by setting .spec.timeZone to the name of a valid time zone. For example, setting .spec.timeZone: "Etc/UTC" instructs Kubernetes to interpret the schedule relative to Coordinated Universal Time.A time zone database from the Go standard library is included in the binaries and used as a fallback in case an external database is not available on the system.CronJob limitationsUnsupported TimeZone specificationSpecifying a timezone using CRON_TZ or TZ variables inside .spec.schedule is not officially supported (and never has been). If you try to set a schedule that includes TZ or CRON_TZ timezone specification, Kubernetes will fail to create or update the resource with a validation error. You should specify time zones using the time zone field, instead.Modifying a CronJobBy design, a CronJob contains a template for new Jobs. If you modify an existing CronJob, the changes you make will apply to new Jobs that start to run after your modification is complete. Jobs (and their Pods) that have already started continue to run without changes. That is, the CronJob does not update existing Jobs, even if those remain running.Job creationA CronJob creates a Job object approximately once per execution time of its schedule. The scheduling is approximate because there are certain circumstances where two Jobs might be created, or no Job might be created. Kubernetes tries to avoid those situations, but does not completely prevent them. Therefore, the Jobs that you define should be idempotent.Starting with Kubernetes v1.32, CronJobs apply an annotation batch.kubernetes.io/cronjob-scheduled-timestamp to their created Jobs. This annotation indicates the originally scheduled creation time for the Job and is formatted in RFC3339.If startingDeadlineSeconds is set to a large value or left unset (the default) and if concurrencyPolicy is set to Allow, the Jobs will always run at least once.Caution:If startingDeadlineSeconds is set to a value less than 10 seconds, the CronJob may not be scheduled. This is because the CronJob controller checks things every 10 seconds.For every CronJob, the CronJob Controller checks how many schedules it missed in the duration from its last scheduled time until now. If there are more than 100 missed schedules, then it does not start the Job and logs the error.Cannot determine if job needs to be started. Too many missed start time (> 100). Set or decrease .spec.startingDeadlineSeconds or check clock skew. It is important to note that if the startingDeadlineSeconds field is set (not nil), the controller counts how many missed Jobs occurred from the value of startingDeadlineSeconds until now rather than from the last scheduled time until now. For example, if startingDeadlineSeconds is 200, the controller counts how many missed Jobs occurred in the last 200 seconds.A CronJob is counted as missed if it has failed to be created at its scheduled time. For example, if concurrencyPolicy is set to Forbid and a CronJob was attempted to be scheduled when there was a previous schedule still running, then it would count as missed.For example, suppose a CronJob is set to schedule a new Job every one minute beginning at 08:30:00, and its startingDeadlineSeconds field is not set. If the CronJob controller happens to be down from 08:29:00 to 10:21:00, the Job will not start as the number of missed Jobs which missed their schedule is greater than 100.To illustrate this concept further, suppose a CronJob is set to schedule a new Job every one minute beginning at 08:30:00, and its startingDeadlineSeconds is set to 200 seconds. If the CronJob controller happens to be down for the same period as the previous example (08:29:00 to 10:21:00,) the Job will still start at 10:22:00. This happens as the controller now checks how many missed schedules happened in the last 200 seconds (i.e., 3 missed schedules), rather than from the last scheduled time until now.The CronJob is only responsible for creating Jobs that match its schedule, and the Job in turn is responsible for the management of the Pods it represents.What's nextLearn about Pods and Jobs, two concepts that CronJobs rely upon.Read about the detailed format of CronJob .spec.schedule fields.For instructions on creating and working with CronJobs, and for an example of a CronJob manifest, see Running automated tasks with CronJobs.CronJob is part of the Kubernetes REST API. Read the CronJob API reference for more details.3.4.2.8 - ReplicationControllerLegacy API for managing workloads that can scale horizontally. Superseded by the Deployment and ReplicaSet APIs.Note:A Deployment that configures a ReplicaSet is now the recommended way to set up replication.A ReplicationController ensures that a specified number of pod replicas are running at any one time. In other words, a ReplicationController makes sure that a pod or a homogeneous set of pods is always up and available.How a ReplicationController worksIf there are too many pods, the ReplicationController terminates the extra pods. If there are too few, the ReplicationController starts more pods. Unlike manually created pods, the pods maintained by a ReplicationController are automatically replaced if they fail, are deleted, or are terminated. For example, your pods are re-created on a node after disruptive maintenance such as a kernel upgrade. For this reason, you should use a ReplicationController even if your application requires only a single pod. A ReplicationController is similar to a process supervisor, but instead of supervising individual processes on a single node, the ReplicationController supervises multiple pods across multiple nodes.ReplicationController is often abbreviated to "rc" in discussion, and as a shortcut in kubectl commands.A simple case is to create one ReplicationController object to reliably run one instance of a Pod indefinitely. A more complex use case is to run several identical replicas of a replicated service, such as web servers.Running an example ReplicationControllerThis example ReplicationController config runs three copies of the nginx web server.controllers/replication.yaml apiVersion: v1 kind: ReplicationController metadata: name: nginx spec: replicas: 3 selector: app: nginx template: metadata: name: nginx labels: app: nginx spec: containers: \- name: nginx image: nginx ports: \- containerPort: 80 Run the example job by downloading the example file and then running this command:kubectl apply -f https://k8s.io/examples/controllers/replication.yaml The output is similar to this:replicationcontroller/nginx created Check on the status of the ReplicationController using this command:kubectl describe replicationcontrollers/nginx The output is similar to this:Name: nginx Namespace: default Selector: app=nginx Labels: app=nginx Annotations:  Replicas: 3 current / 3 desired Pods Status: 0 Running / 3 Waiting / 0 Succeeded / 0 Failed Pod Template: Labels: app=nginx Containers: nginx: Image: nginx Port: 80/TCP Environment:  Mounts:  Volumes:  Events: FirstSeen LastSeen Count From SubobjectPath Type Reason Message \--------- -------- ----- ---- ------------- ---- ------ ------- 20s 20s 1 {replication-controller } Normal SuccessfulCreate Created pod: nginx-qrm3m 20s 20s 1 {replication-controller } Normal SuccessfulCreate Created pod: nginx-3ntk0 20s 20s 1 {replication-controller } Normal SuccessfulCreate Created pod: nginx-4ok8v Here, three pods are created, but none is running yet, perhaps because the image is being pulled. A little later, the same command may show:Pods Status: 3 Running / 0 Waiting / 0 Succeeded / 0 Failed To list all the pods that belong to the ReplicationController in a machine readable form, you can use a command like this:pods=$(kubectl get pods --selector=app=nginx --output=jsonpath={.items..metadata.name}) echo $pods The output is similar to this:nginx-3ntk0 nginx-4ok8v nginx-qrm3m Here, the selector is the same as the selector for the ReplicationController (seen in the kubectl describe output), and in a different form in replication.yaml. The --output=jsonpath option specifies an expression with the name from each pod in the returned list.Writing a ReplicationController ManifestAs with all other Kubernetes config, a ReplicationController needs apiVersion, kind, and metadata fields.When the control plane creates new Pods for a ReplicationController, the .metadata.name of the ReplicationController is part of the basis for naming those Pods. The name of a ReplicationController must be a valid DNS subdomain value, but this can produce unexpected results for the Pod hostnames. For best compatibility, the name should follow the more restrictive rules for a DNS label.For general information about working with configuration files, see object management.A ReplicationController also needs a .spec section.Pod TemplateThe .spec.template is the only required field of the .spec.The .spec.template is a pod template. It has exactly the same schema as a Pod, except it is nested and does not have an apiVersion or kind.In addition to required fields for a Pod, a pod template in a ReplicationController must specify appropriate labels and an appropriate restart policy. For labels, make sure not to overlap with other controllers. See pod selector.Only a .spec.template.spec.restartPolicy equal to Always is allowed, which is the default if not specified.For local container restarts, ReplicationControllers delegate to an agent on the node, for example the Kubelet.Labels on the ReplicationControllerThe ReplicationController can itself have labels (.metadata.labels). Typically, you would set these the same as the .spec.template.metadata.labels; if .metadata.labels is not specified then it defaults to .spec.template.metadata.labels. However, they are allowed to be different, and the .metadata.labels do not affect the behavior of the ReplicationController.Pod SelectorThe .spec.selector field is a label selector. A ReplicationController manages all the pods with labels that match the selector. It does not distinguish between pods that it created or deleted and pods that another person or process created or deleted. This allows the ReplicationController to be replaced without affecting the running pods.If specified, the .spec.template.metadata.labels must be equal to the .spec.selector, or it will be rejected by the API. If .spec.selector is unspecified, it will be defaulted to .spec.template.metadata.labels.Also you should not normally create any pods whose labels match this selector, either directly, with another ReplicationController, or with another controller such as Job. If you do so, the ReplicationController thinks that it created the other pods. Kubernetes does not stop you from doing this.If you do end up with multiple controllers that have overlapping selectors, you will have to manage the deletion yourself (see below).Multiple ReplicasYou can specify how many pods should run concurrently by setting .spec.replicas to the number of pods you would like to have running concurrently. The number running at any time may be higher or lower, such as if the replicas were just increased or decreased, or if a pod is gracefully shutdown, and a replacement starts early.If you do not specify .spec.replicas, then it defaults to 1.Working with ReplicationControllersDeleting a ReplicationController and its PodsTo delete a ReplicationController and all its pods, use kubectl delete. Kubectl will scale the ReplicationController to zero and wait for it to delete each pod before deleting the ReplicationController itself. If this kubectl command is interrupted, it can be restarted.When using the REST API or client library, you need to do the steps explicitly (scale replicas to 0, wait for pod deletions, then delete the ReplicationController).Deleting only a ReplicationControllerYou can delete a ReplicationController without affecting any of its pods.Using kubectl, specify the --cascade=orphan option to kubectl delete.When using the REST API or client library, you can delete the ReplicationController object.Once the original is deleted, you can create a new ReplicationController to replace it. As long as the old and new .spec.selector are the same, then the new one will adopt the old pods. However, it will not make any effort to make existing pods match a new, different pod template. To update pods to a new spec in a controlled way, use a rolling update.Isolating pods from a ReplicationControllerPods may be removed from a ReplicationController's target set by changing their labels. This technique may be used to remove pods from service for debugging and data recovery. Pods that are removed in this way will be replaced automatically (assuming that the number of replicas is not also changed).Common usage patternsReschedulingAs mentioned above, whether you have 1 pod you want to keep running, or 1000, a ReplicationController will ensure that the specified number of pods exists, even in the event of node failure or pod termination (for example, due to an action by another control agent).ScalingThe ReplicationController enables scaling the number of replicas up or down, either manually or by an auto-scaling control agent, by updating the replicas field.Rolling updatesThe ReplicationController is designed to facilitate rolling updates to a service by replacing pods one-by-one.As explained in #1353, the recommended approach is to create a new ReplicationController with 1 replica, scale the new (+1) and old (-1) controllers one by one, and then delete the old controller after it reaches 0 replicas. This predictably updates the set of pods regardless of unexpected failures.Ideally, the rolling update controller would take application readiness into account, and would ensure that a sufficient number of pods were productively serving at any given time.The two ReplicationControllers would need to create pods with at least one differentiating label, such as the image tag of the primary container of the pod, since it is typically image updates that motivate rolling updates.Multiple release tracksIn addition to running multiple releases of an application while a rolling update is in progress, it's common to run multiple releases for an extended period of time, or even continuously, using multiple release tracks. The tracks would be differentiated by labels.For instance, a service might target all pods with tier in (frontend), environment in (prod). Now say you have 10 replicated pods that make up this tier. But you want to be able to 'canary' a new version of this component. You could set up a ReplicationController with replicas set to 9 for the bulk of the replicas, with labels tier=frontend, environment=prod, track=stable, and another ReplicationController with replicas set to 1 for the canary, with labels tier=frontend, environment=prod, track=canary. Now the service is covering both the canary and non-canary pods. But you can mess with the ReplicationControllers separately to test things out, monitor the results, etc.Using ReplicationControllers with ServicesMultiple ReplicationControllers can sit behind a single service, so that, for example, some traffic goes to the old version, and some goes to the new version.A ReplicationController will never terminate on its own, but it isn't expected to be as long-lived as services. Services may be composed of pods controlled by multiple ReplicationControllers, and it is expected that many ReplicationControllers may be created and destroyed over the lifetime of a service (for instance, to perform an update of pods that run the service). Both services themselves and their clients should remain oblivious to the ReplicationControllers that maintain the pods of the services.Writing programs for ReplicationPods created by a ReplicationController are intended to be fungible and semantically identical, though their configurations may become heterogeneous over time. This is an obvious fit for replicated stateless servers, but ReplicationControllers can also be used to maintain availability of master-elected, sharded, and worker-pool applications. Such applications should use dynamic work assignment mechanisms, such as the RabbitMQ work queues, as opposed to static/one-time customization of the configuration of each pod, which is considered an anti-pattern. Any pod customization performed, such as vertical auto-sizing of resources (for example, cpu or memory), should be performed by another online controller process, not unlike the ReplicationController itself.Responsibilities of the ReplicationControllerThe ReplicationController ensures that the desired number of pods matches its label selector and are operational. Currently, only terminated pods are excluded from its count. In the future, readiness and other information available from the system may be taken into account, we may add more controls over the replacement policy, and we plan to emit events that could be used by external clients to implement arbitrarily sophisticated replacement and/or scale-down policies.The ReplicationController is forever constrained to this narrow responsibility. It itself will not perform readiness nor liveness probes. Rather than performing auto-scaling, it is intended to be controlled by an external auto-scaler (as discussed in #492), which would change its replicas field. We will not add scheduling policies (for example, spreading) to the ReplicationController. Nor should it verify that the pods controlled match the currently specified template, as that would obstruct auto-sizing and other automated processes. Similarly, completion deadlines, ordering dependencies, configuration expansion, and other features belong elsewhere. We even plan to factor out the mechanism for bulk pod creation (#170).The ReplicationController is intended to be a composable building-block primitive. We expect higher-level APIs and/or tools to be built on top of it and other complementary primitives for user convenience in the future. The "macro" operations currently supported by kubectl (run, scale) are proof-of-concept examples of this. For instance, we could imagine something like Asgard managing ReplicationControllers, auto-scalers, services, scheduling policies, canaries, etc.API ObjectReplication controller is a top-level resource in the Kubernetes REST API. More details about the API object can be found at: ReplicationController API object.Alternatives to ReplicationControllerReplicaSetReplicaSet is the next-generation ReplicationController that supports the new set-based label selector. It's mainly used by Deployment as a mechanism to orchestrate pod creation, deletion and updates. Note that we recommend using Deployments instead of directly using Replica Sets, unless you require custom update orchestration or don't require updates at all.Deployment (Recommended)Deployment is a higher-level API object that updates its underlying Replica Sets and their Pods. Deployments are recommended if you want the rolling update functionality, because they are declarative, server-side, and have additional features.Bare PodsUnlike in the case where a user directly created pods, a ReplicationController replaces pods that are deleted or terminated for any reason, such as in the case of node failure or disruptive node maintenance, such as a kernel upgrade. For this reason, we recommend that you use a ReplicationController even if your application requires only a single pod. Think of it similarly to a process supervisor, only it supervises multiple pods across multiple nodes instead of individual processes on a single node. A ReplicationController delegates local container restarts to some agent on the node, such as the kubelet.JobUse a Job instead of a ReplicationController for pods that are expected to terminate on their own (that is, batch jobs).DaemonSetUse a DaemonSet instead of a ReplicationController for pods that provide a machine-level function, such as machine monitoring or machine logging. These pods have a lifetime that is tied to a machine lifetime: the pod needs to be running on the machine before other pods start, and are safe to terminate when the machine is otherwise ready to be rebooted/shutdown.What's nextLearn about Pods.Learn about Deployment, the replacement for ReplicationController.ReplicationController is part of the Kubernetes REST API. Read the ReplicationController object definition to understand the API for replication controllers.3.4.3 - Autoscaling WorkloadsWith autoscaling, you can automatically update your workloads in one way or another. This allows your cluster to react to changes in resource demand more elastically and efficiently.In Kubernetes, you can scale a workload depending on the current demand of resources. This allows your cluster to react to changes in resource demand more elastically and efficiently.When you scale a workload, you can either increase or decrease the number of replicas managed by the workload, or adjust the resources available to the replicas in-place.The first approach is referred to as horizontal scaling, while the second is referred to as vertical scaling.There are manual and automatic ways to scale your workloads, depending on your use case.Scaling workloads manuallyKubernetes supports manual scaling of workloads. Horizontal scaling can be done using the kubectl CLI. For vertical scaling, you need to patch the resource definition of your workload.See below for examples of both strategies.Horizontal scaling: Running multiple instances of your appVertical scaling: Resizing CPU and memory resources assigned to containersScaling workloads automaticallyKubernetes also supports automatic scaling of workloads, which is the focus of this page.The concept of Autoscaling in Kubernetes refers to the ability to automatically update an object that manages a set of Pods (for example a Deployment).Scaling workloads horizontallyIn Kubernetes, you can automatically scale a workload horizontally using a HorizontalPodAutoscaler (HPA).It is implemented as a Kubernetes API resource and a controller and periodically adjusts the number of replicas in a workload to match observed resource utilization such as CPU or memory usage.There is a walkthrough tutorial of configuring a HorizontalPodAutoscaler for a Deployment.Scaling workloads verticallyFEATURE STATE: Kubernetes v1.25 [stable]You can automatically scale a workload vertically using a VerticalPodAutoscaler (VPA). Unlike the HPA, the VPA doesn't come with Kubernetes by default, but is a separate project that can be found on GitHub.Once installed, it allows you to create CustomResourceDefinitions (CRDs) for your workloads which define how and when to scale the resources of the managed replicas.Note:You will need to have the Metrics Server installed to your cluster for the VPA to work.At the moment, the VPA can operate in four different modes:Different modes of the VPAModeDescriptionAutoCurrently Recreate. This might change to in-place updates in the future.RecreateThe VPA assigns resource requests on pod creation as well as updates them on existing pods by evicting them when the requested resources differ significantly from the new recommendationInitialThe VPA only assigns resource requests on pod creation and never changes them later.OffThe VPA does not automatically change the resource requirements of the pods. The recommendations are calculated and can be inspected in the VPA object.In-place pod vertical scalingFEATURE STATE: Kubernetes v1.33 [beta] (enabled by default: true)As of Kubernetes 1.33, VPA does not support resizing pods in-place, but this integration is being worked on. For manually resizing pods in-place, see Resize Container Resources In-Place.Autoscaling based on cluster sizeFor workloads that need to be scaled based on the size of the cluster (for example cluster-dns or other system components), you can use the Cluster Proportional Autoscaler. Just like the VPA, it is not part of the Kubernetes core, but hosted as its own project on GitHub.The Cluster Proportional Autoscaler watches the number of schedulable nodes and cores and scales the number of replicas of the target workload accordingly.If the number of replicas should stay the same, you can scale your workloads vertically according to the cluster size using the Cluster Proportional Vertical Autoscaler. The project is currently in beta and can be found on GitHub.While the Cluster Proportional Autoscaler scales the number of replicas of a workload, the Cluster Proportional Vertical Autoscaler adjusts the resource requests for a workload (for example a Deployment or DaemonSet) based on the number of nodes and/or cores in the cluster.Event driven AutoscalingIt is also possible to scale workloads based on events, for example using the Kubernetes Event Driven Autoscaler (KEDA).KEDA is a CNCF-graduated project enabling you to scale your workloads based on the number of events to be processed, for example the amount of messages in a queue. There exists a wide range of adapters for different event sources to choose from.Autoscaling based on schedulesAnother strategy for scaling your workloads is to schedule the scaling operations, for example in order to reduce resource consumption during off-peak hours.Similar to event driven autoscaling, such behavior can be achieved using KEDA in conjunction with its Cron scaler. The Cron scaler allows you to define schedules (and time zones) for scaling your workloads in or out.Scaling cluster infrastructureIf scaling workloads isn't enough to meet your needs, you can also scale your cluster infrastructure itself.Scaling the cluster infrastructure normally means adding or removing nodes. Read Node autoscaling for more information.What's nextLearn more about scaling horizontallyScale a StatefulSetHorizontalPodAutoscaler WalkthroughResize Container Resources In-PlaceAutoscale the DNS Service in a ClusterLearn about Node autoscaling3.4.4 - Managing WorkloadsYou've deployed your application and exposed it via a Service. Now what? Kubernetes provides a number of tools to help you manage your application deployment, including scaling and updating.Organizing resource configurationsMany applications require multiple resources to be created, such as a Deployment along with a Service. Management of multiple resources can be simplified by grouping them together in the same file (separated by --- in YAML). For example:application/nginx-app.yaml apiVersion: v1 kind: Service metadata: name: my-nginx-svc labels: app: nginx spec: type: LoadBalancer ports: \- port: 80 selector: app: nginx \--- apiVersion: apps/v1 kind: Deployment metadata: name: my-nginx labels: app: nginx spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: \- name: nginx image: nginx:1.14.2 ports: \- containerPort: 80 Multiple resources can be created the same way as a single resource:kubectl apply -f https://k8s.io/examples/application/nginx-app.yaml service/my-nginx-svc created deployment.apps/my-nginx created The resources will be created in the order they appear in the manifest. Therefore, it's best to specify the Service first, since that will ensure the scheduler can spread the pods associated with the Service as they are created by the controller(s), such as Deployment.kubectl apply also accepts multiple -f arguments:kubectl apply -f https://k8s.io/examples/application/nginx/nginx-svc.yaml \ -f https://k8s.io/examples/application/nginx/nginx-deployment.yaml It is a recommended practice to put resources related to the same microservice or application tier into the same file, and to group all of the files associated with your application in the same directory. If the tiers of your application bind to each other using DNS, you can deploy all of the components of your stack together.A URL can also be specified as a configuration source, which is handy for deploying directly from manifests in your source control system:kubectl apply -f https://k8s.io/examples/application/nginx/nginx-deployment.yaml deployment.apps/my-nginx created If you need to define more manifests, such as adding a ConfigMap, you can do that too.External toolsThis section lists only the most common tools used for managing workloads on Kubernetes. To see a larger list, view Application definition and image build in the CNCF Landscape.Helm🛇 This item links to a third party project or product that is not part of Kubernetes itself. More informationHelm is a tool for managing packages of pre-configured Kubernetes resources. These packages are known as Helm charts.KustomizeKustomize traverses a Kubernetes manifest to add, remove or update configuration options. It is available both as a standalone binary and as a native feature of kubectl.Bulk operations in kubectlResource creation isn't the only operation that kubectl can perform in bulk. It can also extract resource names from configuration files in order to perform other operations, in particular to delete the same resources you created:kubectl delete -f https://k8s.io/examples/application/nginx-app.yaml deployment.apps "my-nginx" deleted service "my-nginx-svc" deleted In the case of two resources, you can specify both resources on the command line using the resource/name syntax:kubectl delete deployments/my-nginx services/my-nginx-svc For larger numbers of resources, you'll find it easier to specify the selector (label query) specified using -l or --selector, to filter resources by their labels:kubectl delete deployment,services -l app=nginx deployment.apps "my-nginx" deleted service "my-nginx-svc" deleted Chaining and filteringBecause kubectl outputs resource names in the same syntax it accepts, you can chain operations using $() or xargs:kubectl get $(kubectl create -f docs/concepts/cluster-administration/nginx/ -o name | grep service/ ) kubectl create -f docs/concepts/cluster-administration/nginx/ -o name | grep service/ | xargs -i kubectl get '{}' The output might be similar to:NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE my-nginx-svc LoadBalancer 10.0.0.208  80/TCP 0s With the above commands, first you create resources under examples/application/nginx/ and print the resources created with -o name output format (print each resource as resource/name). Then you grep only the Service, and then print it with kubectl get.Recursive operations on local filesIf you happen to organize your resources across several subdirectories within a particular directory, you can recursively perform the operations on the subdirectories also, by specifying \--recursive or -R alongside the --filename/-f argument.For instance, assume there is a directory project/k8s/development that holds all of the manifests needed for the development environment, organized by resource type:project/k8s/development ├── configmap │ └── my-configmap.yaml ├── deployment │ └── my-deployment.yaml └── pvc └── my-pvc.yaml By default, performing a bulk operation on project/k8s/development will stop at the first level of the directory, not processing any subdirectories. If you had tried to create the resources in this directory using the following command, we would have encountered an error:kubectl apply -f project/k8s/development error: you must provide one or more resources by argument or filename (.json|.yaml|.yml|stdin) Instead, specify the --recursive or -R command line argument along with the --filename/-f argument:kubectl apply -f project/k8s/development --recursive configmap/my-config created deployment.apps/my-deployment created persistentvolumeclaim/my-pvc created The --recursive argument works with any operation that accepts the --filename/-f argument such as: kubectl create, kubectl get, kubectl delete, kubectl describe, or even kubectl rollout.The --recursive argument also works when multiple -f arguments are provided:kubectl apply -f project/k8s/namespaces -f project/k8s/development --recursive namespace/development created namespace/staging created configmap/my-config created deployment.apps/my-deployment created persistentvolumeclaim/my-pvc created If you're interested in learning more about kubectl, go ahead and read Command line tool (kubectl).Updating your application without an outageAt some point, you'll eventually need to update your deployed application, typically by specifying a new image or image tag. kubectl supports several update operations, each of which is applicable to different scenarios.You can run multiple copies of your app, and use a rollout to gradually shift the traffic to new healthy Pods. Eventually, all the running Pods would have the new software.This section of the page guides you through how to create and update applications with Deployments.Let's say you were running version 1.14.2 of nginx:kubectl create deployment my-nginx --image=nginx:1.14.2 deployment.apps/my-nginx created Ensure that there is 1 replica:kubectl scale --replicas 1 deployments/my-nginx --subresource='scale' --type='merge' -p '{"spec":{"replicas": 1}}' deployment.apps/my-nginx scaled and allow Kubernetes to add more temporary replicas during a rollout, by setting a surge maximum of 100%:kubectl patch --type='merge' -p '{"spec":{"strategy":{"rollingUpdate":{"maxSurge": "100%" }}}}' deployment.apps/my-nginx patched To update to version 1.16.1, change .spec.template.spec.containers[0].image from nginx:1.14.2 to nginx:1.16.1 using kubectl edit:kubectl edit deployment/my-nginx # Change the manifest to use the newer container image, then save your changes That's it! The Deployment will declaratively update the deployed nginx application progressively behind the scene. It ensures that only a certain number of old replicas may be down while they are being updated, and only a certain number of new replicas may be created above the desired number of pods. To learn more details about how this happens, visit Deployment.You can use rollouts with DaemonSets, Deployments, or StatefulSets.Managing rolloutsYou can use kubectl rollout to manage a progressive update of an existing application.For example:kubectl apply -f my-deployment.yaml # wait for rollout to finish kubectl rollout status deployment/my-deployment --timeout 10m # 10 minute timeout orkubectl apply -f backing-stateful-component.yaml # don't wait for rollout to finish, just check the status kubectl rollout status statefulsets/backing-stateful-component --watch=false You can also pause, resume or cancel a rollout. Visit kubectl rollout to learn more.Canary deploymentsAnother scenario where multiple labels are needed is to distinguish deployments of different releases or configurations of the same component. It is common practice to deploy a canary of a new application release (specified via image tag in the pod template) side by side with the previous release so that the new release can receive live production traffic before fully rolling it out.For instance, you can use a track label to differentiate different releases.The primary, stable release would have a track label with value as stable:name: frontend replicas: 3 ... labels: app: guestbook tier: frontend track: stable ... image: gb-frontend:v3 and then you can create a new release of the guestbook frontend that carries the track label with different value (i.e. canary), so that two sets of pods would not overlap:name: frontend-canary replicas: 1 ... labels: app: guestbook tier: frontend track: canary ... image: gb-frontend:v4 The frontend service would span both sets of replicas by selecting the common subset of their labels (i.e. omitting the track label), so that the traffic will be redirected to both applications:selector: app: guestbook tier: frontend You can tweak the number of replicas of the stable and canary releases to determine the ratio of each release that will receive live production traffic (in this case, 3:1). Once you're confident, you can update the stable track to the new application release and remove the canary one.Updating annotationsSometimes you would want to attach annotations to resources. Annotations are arbitrary non-identifying metadata for retrieval by API clients such as tools or libraries. This can be done with kubectl annotate. For example:kubectl annotate pods my-nginx-v4-9gw19 description='my frontend running nginx' kubectl get pods my-nginx-v4-9gw19 -o yaml apiVersion: v1 kind: pod metadata: annotations: description: my frontend running nginx ... For more information, see annotations and kubectl annotate.Scaling your applicationWhen load on your application grows or shrinks, use kubectl to scale your application. For instance, to decrease the number of nginx replicas from 3 to 1, do:kubectl scale deployment/my-nginx --replicas=1 deployment.apps/my-nginx scaled Now you only have one pod managed by the deployment.kubectl get pods -l app=nginx NAME READY STATUS RESTARTS AGE my-nginx-2035384211-j5fhi 1/1 Running 0 30m To have the system automatically choose the number of nginx replicas as needed, ranging from 1 to 3, do:# This requires an existing source of container and Pod metrics kubectl autoscale deployment/my-nginx --min=1 --max=3 horizontalpodautoscaler.autoscaling/my-nginx autoscaled Now your nginx replicas will be scaled up and down as needed, automatically.For more information, please see kubectl scale, kubectl autoscale and horizontal pod autoscaler document.In-place updates of resourcesSometimes it's necessary to make narrow, non-disruptive updates to resources you've created.kubectl applyIt is suggested to maintain a set of configuration files in source control (see configuration as code), so that they can be maintained and versioned along with the code for the resources they configure. Then, you can use kubectl apply to push your configuration changes to the cluster.This command will compare the version of the configuration that you're pushing with the previous version and apply the changes you've made, without overwriting any automated changes to properties you haven't specified.kubectl apply -f https://k8s.io/examples/application/nginx/nginx-deployment.yaml deployment.apps/my-nginx configured To learn more about the underlying mechanism, read server-side apply.kubectl editAlternatively, you may also update resources with kubectl edit:kubectl edit deployment/my-nginx This is equivalent to first get the resource, edit it in text editor, and then apply the resource with the updated version:kubectl get deployment my-nginx -o yaml > /tmp/nginx.yaml vi /tmp/nginx.yaml # do some edit, and then save the file kubectl apply -f /tmp/nginx.yaml deployment.apps/my-nginx configured rm /tmp/nginx.yaml This allows you to do more significant changes more easily. Note that you can specify the editor with your EDITOR or KUBE_EDITOR environment variables.For more information, please see kubectl edit.kubectl patchYou can use kubectl patch to update API objects in place. This subcommand supports JSON patch, JSON merge patch, and strategic merge patch.See Update API Objects in Place Using kubectl patch for more details.Disruptive updatesIn some cases, you may need to update resource fields that cannot be updated once initialized, or you may want to make a recursive change immediately, such as to fix broken pods created by a Deployment. To change such fields, use replace --force, which deletes and re-creates the resource. In this case, you can modify your original configuration file:kubectl replace -f https://k8s.io/examples/application/nginx/nginx-deployment.yaml --force deployment.apps/my-nginx deleted deployment.apps/my-nginx replaced What's nextLearn about how to use kubectl for application introspection and debugging.3.5 - Services, Load Balancing, and NetworkingConcepts and resources behind networking in Kubernetes.The Kubernetes network modelThe Kubernetes network model is built out of several pieces:Each pod in a cluster gets its own unique cluster-wide IP address.A pod has its own private network namespace which is shared by all of the containers within the pod. Processes running in different containers in the same pod can communicate with each other over localhost.The pod network (also called a cluster network) handles communication between pods. It ensures that (barring intentional network segmentation):All pods can communicate with all other pods, whether they are on the same node or on different nodes. Pods can communicate with each other directly, without the use of proxies or address translation (NAT).On Windows, this rule does not apply to host-network pods.Agents on a node (such as system daemons, or kubelet) can communicate with all pods on that node.The Service API lets you provide a stable (long lived) IP address or hostname for a service implemented by one or more backend pods, where the individual pods making up the service can change over time.Kubernetes automatically manages EndpointSlice objects to provide information about the pods currently backing a Service.A service proxy implementation monitors the set of Service and EndpointSlice objects, and programs the data plane to route service traffic to its backends, by using operating system or cloud provider APIs to intercept or rewrite packets.The Gateway API (or its predecessor, Ingress) allows you to make Services accessible to clients that are outside the cluster.A simpler, but less-configurable, mechanism for cluster ingress is available via the Service API's type: LoadBalancer, when using a supported Cloud Provider.NetworkPolicy is a built-in Kubernetes API that allows you to control traffic between pods, or between pods and the outside world.In older container systems, there was no automatic connectivity between containers on different hosts, and so it was often necessary to explicitly create links between containers, or to map container ports to host ports to make them reachable by containers on other hosts. This is not needed in Kubernetes; Kubernetes's model is that pods can be treated much like VMs or physical hosts from the perspectives of port allocation, naming, service discovery, load balancing, application configuration, and migration.Only a few parts of this model are implemented by Kubernetes itself. For the other parts, Kubernetes defines the APIs, but the corresponding functionality is provided by external components, some of which are optional:Pod network namespace setup is handled by system-level software implementing the Container Runtime Interface.The pod network itself is managed by a pod network implementation. On Linux, most container runtimes use the Container Networking Interface (CNI) to interact with the pod network implementation, so these implementations are often called CNI plugins.Kubernetes provides a default implementation of service proxying, called kube-proxy, but some pod network implementations instead use their own service proxy that is more tightly integrated with the rest of the implementation.NetworkPolicy is generally also implemented by the pod network implementation. (Some simpler pod network implementations don't implement NetworkPolicy, or an administrator may choose to configure the pod network without NetworkPolicy support. In these cases, the API will still be present, but it will have no effect.)There are many implementations of the Gateway API, some of which are specific to particular cloud environments, some more focused on "bare metal" environments, and others more generic.What's nextThe Connecting Applications with Services tutorial lets you learn about Services and Kubernetes networking with a hands-on example.Cluster Networking explains how to set up networking for your cluster, and also provides an overview of the technologies involved.3.5.1 - ServiceExpose an application running in your cluster behind a single outward-facing endpoint, even when the workload is split across multiple backends.In Kubernetes, a Service is a method for exposing a network application that is running as one or more Pods in your cluster.A key aim of Services in Kubernetes is that you don't need to modify your existing application to use an unfamiliar service discovery mechanism. You can run code in Pods, whether this is a code designed for a cloud-native world, or an older app you've containerized. You use a Service to make that set of Pods available on the network so that clients can interact with it.If you use a Deployment to run your app, that Deployment can create and destroy Pods dynamically. From one moment to the next, you don't know how many of those Pods are working and healthy; you might not even know what those healthy Pods are named. Kubernetes Pods are created and destroyed to match the desired state of your cluster. Pods are ephemeral resources (you should not expect that an individual Pod is reliable and durable).Each Pod gets its own IP address (Kubernetes expects network plugins to ensure this). For a given Deployment in your cluster, the set of Pods running in one moment in time could be different from the set of Pods running that application a moment later.This leads to a problem: if some set of Pods (call them "backends") provides functionality to other Pods (call them "frontends") inside your cluster, how do the frontends find out and keep track of which IP address to connect to, so that the frontend can use the backend part of the workload?Enter Services.Services in KubernetesThe Service API, part of Kubernetes, is an abstraction to help you expose groups of Pods over a network. Each Service object defines a logical set of endpoints (usually these endpoints are Pods) along with a policy about how to make those pods accessible.For example, consider a stateless image-processing backend which is running with 3 replicas. Those replicas are fungible—frontends do not care which backend they use. While the actual Pods that compose the backend set may change, the frontend clients should not need to be aware of that, nor should they need to keep track of the set of backends themselves.The Service abstraction enables this decoupling.The set of Pods targeted by a Service is usually determined by a selector that you define. To learn about other ways to define Service endpoints, see Services without selectors.If your workload speaks HTTP, you might choose to use an Ingress to control how web traffic reaches that workload. Ingress is not a Service type, but it acts as the entry point for your cluster. An Ingress lets you consolidate your routing rules into a single resource, so that you can expose multiple components of your workload, running separately in your cluster, behind a single listener.The Gateway API for Kubernetes provides extra capabilities beyond Ingress and Service. You can add Gateway to your cluster - it is a family of extension APIs, implemented using CustomResourceDefinitions - and then use these to configure access to network services that are running in your cluster.Cloud-native service discoveryIf you're able to use Kubernetes APIs for service discovery in your application, you can query the API server for matching EndpointSlices. Kubernetes updates the EndpointSlices for a Service whenever the set of Pods in a Service changes.For non-native applications, Kubernetes offers ways to place a network port or load balancer in between your application and the backend Pods.Either way, your workload can use these service discovery mechanisms to find the target it wants to connect to.Defining a ServiceA Service is an object (the same way that a Pod or a ConfigMap is an object). You can create, view or modify Service definitions using the Kubernetes API. Usually you use a tool such as kubectl to make those API calls for you.For example, suppose you have a set of Pods that each listen on TCP port 9376 and are labelled as app.kubernetes.io/name=MyApp. You can define a Service to publish that TCP listener:service/simple-service.yaml apiVersion: v1 kind: Service metadata: name: my-service spec: selector: app.kubernetes.io/name: MyApp ports: \- protocol: TCP port: 80 targetPort: 9376 Applying this manifest creates a new Service named "my-service" with the default ClusterIP service type. The Service targets TCP port 9376 on any Pod with the app.kubernetes.io/name: MyApp label.Kubernetes assigns this Service an IP address (the cluster IP), that is used by the virtual IP address mechanism. For more details on that mechanism, read Virtual IPs and Service Proxies.The controller for that Service continuously scans for Pods that match its selector, and then makes any necessary updates to the set of EndpointSlices for the Service.The name of a Service object must be a valid RFC 1035 label name.Note:A Service can map any incoming port to a targetPort. By default and for convenience, the targetPort is set to the same value as the port field.Port definitionsPort definitions in Pods have names, and you can reference these names in the targetPort attribute of a Service. For example, we can bind the targetPort of the Service to the Pod port in the following way:apiVersion: v1 kind: Pod metadata: name: nginx labels: app.kubernetes.io/name: proxy spec: containers: \- name: nginx image: nginx:stable ports: \- containerPort: 80 name: http-web-svc \--- apiVersion: v1 kind: Service metadata: name: nginx-service spec: selector: app.kubernetes.io/name: proxy ports: \- name: name-of-service-port protocol: TCP port: 80 targetPort: http-web-svc This works even if there is a mixture of Pods in the Service using a single configured name, with the same network protocol available via different port numbers. This offers a lot of flexibility for deploying and evolving your Services. For example, you can change the port numbers that Pods expose in the next version of your backend software, without breaking clients.The default protocol for Services is TCP; you can also use any other supported protocol.Because many Services need to expose more than one port, Kubernetes supports multiple port definitions for a single Service. Each port definition can have the same protocol, or a different one.Services without selectorsServices most commonly abstract access to Kubernetes Pods thanks to the selector, but when used with a corresponding set of EndpointSlices objects and without a selector, the Service can abstract other kinds of backends, including ones that run outside the cluster.For example:You want to have an external database cluster in production, but in your test environment you use your own databases.You want to point your Service to a Service in a different Namespace or on another cluster.You are migrating a workload to Kubernetes. While evaluating the approach, you run only a portion of your backends in Kubernetes.In any of these scenarios you can define a Service without specifying a selector to match Pods. For example:apiVersion: v1 kind: Service metadata: name: my-service spec: ports: \- name: http protocol: TCP port: 80 targetPort: 9376 Because this Service has no selector, the corresponding EndpointSlice objects are not created automatically. You can map the Service to the network address and port where it's running, by adding an EndpointSlice object manually. For example:apiVersion: discovery.k8s.io/v1 kind: EndpointSlice metadata: name: my-service-1 # by convention, use the name of the Service # as a prefix for the name of the EndpointSlice labels: # You should set the "kubernetes.io/service-name" label. # Set its value to match the name of the Service kubernetes.io/service-name: my-service addressType: IPv4 ports: \- name: http # should match with the name of the service port defined above appProtocol: http protocol: TCP port: 9376 endpoints: \- addresses: \- "10.4.5.6" \- addresses: \- "10.1.2.3" Custom EndpointSlicesWhen you create an EndpointSlice object for a Service, you can use any name for the EndpointSlice. Each EndpointSlice in a namespace must have a unique name. You link an EndpointSlice to a Service by setting the kubernetes.io/service-name label on that EndpointSlice.Note:The endpoint IPs must not be: loopback (127.0.0.0/8 for IPv4, ::1/128 for IPv6), or link-local (169.254.0.0/16 and 224.0.0.0/24 for IPv4, fe80::/64 for IPv6).The endpoint IP addresses cannot be the cluster IPs of other Kubernetes Services, because kube-proxy doesn't support virtual IPs as a destination.For an EndpointSlice that you create yourself, or in your own code, you should also pick a value to use for the label endpointslice.kubernetes.io/managed-by. If you create your own controller code to manage EndpointSlices, consider using a value similar to "my-domain.example/name-of-controller". If you are using a third party tool, use the name of the tool in all-lowercase and change spaces and other punctuation to dashes (-). If people are directly using a tool such as kubectl to manage EndpointSlices, use a name that describes this manual management, such as "staff" or "cluster-admins". You should avoid using the reserved value "controller", which identifies EndpointSlices managed by Kubernetes' own control plane.Accessing a Service without a selectorAccessing a Service without a selector works the same as if it had a selector. In the example for a Service without a selector, traffic is routed to one of the two endpoints defined in the EndpointSlice manifest: a TCP connection to 10.1.2.3 or 10.4.5.6, on port 9376.Note:The Kubernetes API server does not allow proxying to endpoints that are not mapped to pods. Actions such as kubectl port-forward service/ forwardedPort:servicePort where the service has no selector will fail due to this constraint. This prevents the Kubernetes API server from being used as a proxy to endpoints the caller may not be authorized to access.An ExternalName Service is a special case of Service that does not have selectors and uses DNS names instead. For more information, see the ExternalName section.EndpointSlicesFEATURE STATE: Kubernetes v1.21 [stable]EndpointSlices are objects that represent a subset (a slice) of the backing network endpoints for a Service.Your Kubernetes cluster tracks how many endpoints each EndpointSlice represents. If there are so many endpoints for a Service that a threshold is reached, then Kubernetes adds another empty EndpointSlice and stores new endpoint information there. By default, Kubernetes makes a new EndpointSlice once the existing EndpointSlices all contain at least 100 endpoints. Kubernetes does not make the new EndpointSlice until an extra endpoint needs to be added.See EndpointSlices for more information about this API.Endpoints (deprecated)FEATURE STATE: Kubernetes v1.33 [deprecated]The EndpointSlice API is the evolution of the older Endpoints API. The deprecated Endpoints API has several problems relative to EndpointSlice:It does not support dual-stack clusters.It does not contain information needed to support newer features, such as trafficDistribution.It will truncate the list of endpoints if it is too long to fit in a single object.Because of this, it is recommended that all clients use the EndpointSlice API rather than Endpoints.Over-capacity endpointsKubernetes limits the number of endpoints that can fit in a single Endpoints object. When there are over 1000 backing endpoints for a Service, Kubernetes truncates the data in the Endpoints object. Because a Service can be linked with more than one EndpointSlice, the 1000 backing endpoint limit only affects the legacy Endpoints API.In that case, Kubernetes selects at most 1000 possible backend endpoints to store into the Endpoints object, and sets an annotation on the Endpoints: endpoints.kubernetes.io/over-capacity: truncated. The control plane also removes that annotation if the number of backend Pods drops below 1000.Traffic is still sent to backends, but any load balancing mechanism that relies on the legacy Endpoints API only sends traffic to at most 1000 of the available backing endpoints.The same API limit means that you cannot manually update an Endpoints to have more than 1000 endpoints.Application protocolFEATURE STATE: Kubernetes v1.20 [stable]The appProtocol field provides a way to specify an application protocol for each Service port. This is used as a hint for implementations to offer richer behavior for protocols that they understand. The value of this field is mirrored by the corresponding Endpoints and EndpointSlice objects.This field follows standard Kubernetes label syntax. Valid values are one of:IANA standard service names.Implementation-defined prefixed names such as mycompany.com/my-custom-protocol.Kubernetes-defined prefixed names:ProtocolDescriptionkubernetes.io/h2cHTTP/2 over cleartext as described in RFC 7540kubernetes.io/wsWebSocket over cleartext as described in RFC 6455kubernetes.io/wssWebSocket over TLS as described in RFC 6455Multi-port ServicesFor some Services, you need to expose more than one port. Kubernetes lets you configure multiple port definitions on a Service object. When using multiple ports for a Service, you must give all of your ports names so that these are unambiguous. For example:apiVersion: v1 kind: Service metadata: name: my-service spec: selector: app.kubernetes.io/name: MyApp ports: \- name: http protocol: TCP port: 80 targetPort: 9376 \- name: https protocol: TCP port: 443 targetPort: 9377 Note:As with Kubernetes names in general, names for ports must only contain lowercase alphanumeric characters and -. Port names must also start and end with an alphanumeric character.For example, the names 123-abc and web are valid, but 123_abc and -web are not.Service typeFor some parts of your application (for example, frontends) you may want to expose a Service onto an external IP address, one that's accessible from outside of your cluster.Kubernetes Service types allow you to specify what kind of Service you want.The available type values and their behaviors are:ClusterIPExposes the Service on a cluster-internal IP. Choosing this value makes the Service only reachable from within the cluster. This is the default that is used if you don't explicitly specify a type for a Service. You can expose the Service to the public internet using an Ingress or a Gateway.NodePortExposes the Service on each Node's IP at a static port (the NodePort). To make the node port available, Kubernetes sets up a cluster IP address, the same as if you had requested a Service of type: ClusterIP.LoadBalancerExposes the Service externally using an external load balancer. Kubernetes does not directly offer a load balancing component; you must provide one, or you can integrate your Kubernetes cluster with a cloud provider.ExternalNameMaps the Service to the contents of the externalName field (for example, to the hostname api.foo.bar.example). The mapping configures your cluster's DNS server to return a CNAME record with that external hostname value. No proxying of any kind is set up.The type field in the Service API is designed as nested functionality - each level adds to the previous. However there is an exception to this nested design. You can define a LoadBalancer Service by disabling the load balancer NodePort allocation.type: ClusterIPThis default Service type assigns an IP address from a pool of IP addresses that your cluster has reserved for that purpose.Several of the other types for Service build on the ClusterIP type as a foundation.If you define a Service that has the .spec.clusterIP set to "None" then Kubernetes does not assign an IP address. See headless Services for more information.Choosing your own IP addressYou can specify your own cluster IP address as part of a Service creation request. To do this, set the .spec.clusterIP field. For example, if you already have an existing DNS entry that you wish to reuse, or legacy systems that are configured for a specific IP address and difficult to re-configure.The IP address that you choose must be a valid IPv4 or IPv6 address from within the service-cluster-ip-range CIDR range that is configured for the API server. If you try to create a Service with an invalid clusterIP address value, the API server will return a 422 HTTP status code to indicate that there's a problem.Read avoiding collisions to learn how Kubernetes helps reduce the risk and impact of two different Services both trying to use the same IP address.type: NodePortIf you set the type field to NodePort, the Kubernetes control plane allocates a port from a range specified by --service-node-port-range flag (default: 30000-32767). Each node proxies that port (the same port number on every Node) into your Service. Your Service reports the allocated port in its .spec.ports[*].nodePort field.Using a NodePort gives you the freedom to set up your own load balancing solution, to configure environments that are not fully supported by Kubernetes, or even to expose one or more nodes' IP addresses directly.For a node port Service, Kubernetes additionally allocates a port (TCP, UDP or SCTP to match the protocol of the Service). Every node in the cluster configures itself to listen on that assigned port and to forward traffic to one of the ready endpoints associated with that Service. You'll be able to contact the type: NodePort Service, from outside the cluster, by connecting to any node using the appropriate protocol (for example: TCP), and the appropriate port (as assigned to that Service).Choosing your own portIf you want a specific port number, you can specify a value in the nodePort field. The control plane will either allocate you that port or report that the API transaction failed. This means that you need to take care of possible port collisions yourself. You also have to use a valid port number, one that's inside the range configured for NodePort use.Here is an example manifest for a Service of type: NodePort that specifies a NodePort value (30007, in this example):apiVersion: v1 kind: Service metadata: name: my-service spec: type: NodePort selector: app.kubernetes.io/name: MyApp ports: \- port: 80 # By default and for convenience, the `targetPort` is set to # the same value as the `port` field. targetPort: 80 # Optional field # By default and for convenience, the Kubernetes control plane # will allocate a port from a range (default: 30000-32767) nodePort: 30007 Reserve Nodeport ranges to avoid collisionsThe policy for assigning ports to NodePort services applies to both the auto-assignment and the manual assignment scenarios. When a user wants to create a NodePort service that uses a specific port, the target port may conflict with another port that has already been assigned.To avoid this problem, the port range for NodePort services is divided into two bands. Dynamic port assignment uses the upper band by default, and it may use the lower band once the upper band has been exhausted. Users can then allocate from the lower band with a lower risk of port collision.Custom IP address configuration for type: NodePort ServicesYou can set up nodes in your cluster to use a particular IP address for serving node port services. You might want to do this if each node is connected to multiple networks (for example: one network for application traffic, and another network for traffic between nodes and the control plane).If you want to specify particular IP address(es) to proxy the port, you can set the \--nodeport-addresses flag for kube-proxy or the equivalent nodePortAddresses field of the kube-proxy configuration file to particular IP block(s).This flag takes a comma-delimited list of IP blocks (e.g. 10.0.0.0/8, 192.0.2.0/25) to specify IP address ranges that kube-proxy should consider as local to this node.For example, if you start kube-proxy with the --nodeport-addresses=127.0.0.0/8 flag, kube-proxy only selects the loopback interface for NodePort Services. The default for --nodeport-addresses is an empty list. This means that kube-proxy should consider all available network interfaces for NodePort. (That's also compatible with earlier Kubernetes releases.)Note:This Service is visible as :spec.ports[*].nodePort and .spec.clusterIP:spec.ports[*].port. If the --nodeport-addresses flag for kube-proxy or the equivalent field in the kube-proxy configuration file is set,  would be a filtered node IP address (or possibly IP addresses).type: LoadBalancerOn cloud providers which support external load balancers, setting the type field to LoadBalancer provisions a load balancer for your Service. The actual creation of the load balancer happens asynchronously, and information about the provisioned balancer is published in the Service's .status.loadBalancer field. For example:apiVersion: v1 kind: Service metadata: name: my-service spec: selector: app.kubernetes.io/name: MyApp ports: \- protocol: TCP port: 80 targetPort: 9376 clusterIP: 10.0.171.239 type: LoadBalancer status: loadBalancer: ingress: \- ip: 192.0.2.127 Traffic from the external load balancer is directed at the backend Pods. The cloud provider decides how it is load balanced.To implement a Service of type: LoadBalancer, Kubernetes typically starts off by making the changes that are equivalent to you requesting a Service of type: NodePort. The cloud-controller-manager component then configures the external load balancer to forward traffic to that assigned node port.You can configure a load balanced Service to omit assigning a node port, provided that the cloud provider implementation supports this.Some cloud providers allow you to specify the loadBalancerIP. In those cases, the load-balancer is created with the user-specified loadBalancerIP. If the loadBalancerIP field is not specified, the load balancer is set up with an ephemeral IP address. If you specify a loadBalancerIP but your cloud provider does not support the feature, the loadbalancerIP field that you set is ignored.Note:The.spec.loadBalancerIP field for a Service was deprecated in Kubernetes v1.24.This field was under-specified and its meaning varies across implementations. It also cannot support dual-stack networking. This field may be removed in a future API version.If you're integrating with a provider that supports specifying the load balancer IP address(es) for a Service via a (provider specific) annotation, you should switch to doing that.If you are writing code for a load balancer integration with Kubernetes, avoid using this field. You can integrate with Gateway rather than Service, or you can define your own (provider specific) annotations on the Service that specify the equivalent detail.Node liveness impact on load balancer trafficLoad balancer health checks are critical to modern applications. They are used to determine which server (virtual machine, or IP address) the load balancer should dispatch traffic to. The Kubernetes APIs do not define how health checks have to be implemented for Kubernetes managed load balancers, instead it's the cloud providers (and the people implementing integration code) who decide on the behavior. Load balancer health checks are extensively used within the context of supporting the externalTrafficPolicy field for Services.Load balancers with mixed protocol typesFEATURE STATE: Kubernetes v1.26 [stable] (enabled by default: true)By default, for LoadBalancer type of Services, when there is more than one port defined, all ports must have the same protocol, and the protocol must be one which is supported by the cloud provider.The feature gate MixedProtocolLBService (enabled by default for the kube-apiserver as of v1.24) allows the use of different protocols for LoadBalancer type of Services, when there is more than one port defined.Note:The set of protocols that can be used for load balanced Services is defined by your cloud provider; they may impose restrictions beyond what the Kubernetes API enforces.Disabling load balancer NodePort allocationFEATURE STATE: Kubernetes v1.24 [stable]You can optionally disable node port allocation for a Service of type: LoadBalancer, by setting the field spec.allocateLoadBalancerNodePorts to false. This should only be used for load balancer implementations that route traffic directly to pods as opposed to using node ports. By default, spec.allocateLoadBalancerNodePorts is true and type LoadBalancer Services will continue to allocate node ports. If spec.allocateLoadBalancerNodePorts is set to false on an existing Service with allocated node ports, those node ports will not be de-allocated automatically. You must explicitly remove the nodePorts entry in every Service port to de-allocate those node ports.Specifying class of load balancer implementationFEATURE STATE: Kubernetes v1.24 [stable]For a Service with type set to LoadBalancer, the .spec.loadBalancerClass field enables you to use a load balancer implementation other than the cloud provider default.By default, .spec.loadBalancerClass is not set and a LoadBalancer type of Service uses the cloud provider's default load balancer implementation if the cluster is configured with a cloud provider using the --cloud-provider component flag.If you specify .spec.loadBalancerClass, it is assumed that a load balancer implementation that matches the specified class is watching for Services. Any default load balancer implementation (for example, the one provided by the cloud provider) will ignore Services that have this field set. spec.loadBalancerClass can be set on a Service of type LoadBalancer only. Once set, it cannot be changed. The value of spec.loadBalancerClass must be a label-style identifier, with an optional prefix such as "internal-vip" or "example.com/internal-vip". Unprefixed names are reserved for end-users.Load balancer IP address modeFEATURE STATE: Kubernetes v1.32 [stable] (enabled by default: true)For a Service of type: LoadBalancer, a controller can set .status.loadBalancer.ingress.ipMode. The .status.loadBalancer.ingress.ipMode specifies how the load-balancer IP behaves. It may be specified only when the .status.loadBalancer.ingress.ip field is also specified.There are two possible values for .status.loadBalancer.ingress.ipMode: "VIP" and "Proxy". The default value is "VIP" meaning that traffic is delivered to the node with the destination set to the load-balancer's IP and port. There are two cases when setting this to "Proxy", depending on how the load-balancer from the cloud provider delivers the traffics:If the traffic is delivered to the node then DNATed to the pod, the destination would be set to the node's IP and node port;If the traffic is delivered directly to the pod, the destination would be set to the pod's IP and port.Service implementations may use this information to adjust traffic routing.Internal load balancerIn a mixed environment it is sometimes necessary to route traffic from Services inside the same (virtual) network address block.In a split-horizon DNS environment you would need two Services to be able to route both external and internal traffic to your endpoints.To set an internal load balancer, add one of the following annotations to your Service depending on the cloud service provider you're using:DefaultGCPAWSAzureIBM CloudOpenStackBaidu CloudTencent CloudAlibaba CloudOCISelect one of the tabs.metadata: name: my-service annotations: networking.gke.io/load-balancer-type: "Internal" metadata: name: my-service annotations: service.beta.kubernetes.io/aws-load-balancer-internal: "true" metadata: name: my-service annotations: service.beta.kubernetes.io/azure-load-balancer-internal: "true" metadata: name: my-service annotations: service.kubernetes.io/ibm-load-balancer-cloud-provider-ip-type: "private" metadata: name: my-service annotations: service.beta.kubernetes.io/openstack-internal-load-balancer: "true" metadata: name: my-service annotations: service.beta.kubernetes.io/cce-load-balancer-internal-vpc: "true" metadata: annotations: service.kubernetes.io/qcloud-loadbalancer-internal-subnetid: subnet-xxxxx metadata: annotations: service.beta.kubernetes.io/alibaba-cloud-loadbalancer-address-type: "intranet" metadata: name: my-service annotations: service.beta.kubernetes.io/oci-load-balancer-internal: true type: ExternalNameServices of type ExternalName map a Service to a DNS name, not to a typical selector such as my-service or cassandra. You specify these Services with the spec.externalName parameter.This Service definition, for example, maps the my-service Service in the prod namespace to my.database.example.com:apiVersion: v1 kind: Service metadata: name: my-service namespace: prod spec: type: ExternalName externalName: my.database.example.com Note:A Service of type: ExternalName accepts an IPv4 address string, but treats that string as a DNS name comprised of digits, not as an IP address (the internet does not however allow such names in DNS). Services with external names that resemble IPv4 addresses are not resolved by DNS servers.If you want to map a Service directly to a specific IP address, consider using headless Services.When looking up the host my-service.prod.svc.cluster.local, the cluster DNS Service returns a CNAME record with the value my.database.example.com. Accessing my-service works in the same way as other Services but with the crucial difference that redirection happens at the DNS level rather than via proxying or forwarding. Should you later decide to move your database into your cluster, you can start its Pods, add appropriate selectors or endpoints, and change the Service's type.Caution:You may have trouble using ExternalName for some common protocols, including HTTP and HTTPS. If you use ExternalName then the hostname used by clients inside your cluster is different from the name that the ExternalName references.For protocols that use hostnames this difference may lead to errors or unexpected responses. HTTP requests will have a Host: header that the origin server does not recognize; TLS servers will not be able to provide a certificate matching the hostname that the client connected to.Headless ServicesSometimes you don't need load-balancing and a single Service IP. In this case, you can create what are termed headless Services, by explicitly specifying "None" for the cluster IP address (.spec.clusterIP).You can use a headless Service to interface with other service discovery mechanisms, without being tied to Kubernetes' implementation.For headless Services, a cluster IP is not allocated, kube-proxy does not handle these Services, and there is no load balancing or proxying done by the platform for them.A headless Service allows a client to connect to whichever Pod it prefers, directly. Services that are headless don't configure routes and packet forwarding using virtual IP addresses and proxies; instead, headless Services report the endpoint IP addresses of the individual pods via internal DNS records, served through the cluster's DNS service. To define a headless Service, you make a Service with .spec.type set to ClusterIP (which is also the default for type), and you additionally set .spec.clusterIP to None.The string value None is a special case and is not the same as leaving the .spec.clusterIP field unset.How DNS is automatically configured depends on whether the Service has selectors defined:With selectorsFor headless Services that define selectors, the endpoints controller creates EndpointSlices in the Kubernetes API, and modifies the DNS configuration to return A or AAAA records (IPv4 or IPv6 addresses) that point directly to the Pods backing the Service.Without selectorsFor headless Services that do not define selectors, the control plane does not create EndpointSlice objects. However, the DNS system looks for and configures either:DNS CNAME records for type: ExternalName Services.DNS A / AAAA records for all IP addresses of the Service's ready endpoints, for all Service types other than ExternalName.For IPv4 endpoints, the DNS system creates A records.For IPv6 endpoints, the DNS system creates AAAA records.When you define a headless Service without a selector, the port must match the targetPort.Discovering servicesFor clients running inside your cluster, Kubernetes supports two primary modes of finding a Service: environment variables and DNS.Environment variablesWhen a Pod is run on a Node, the kubelet adds a set of environment variables for each active Service. It adds {SVCNAME}_SERVICE_HOST and {SVCNAME}_SERVICE_PORT variables, where the Service name is upper-cased and dashes are converted to underscores.For example, the Service redis-primary which exposes TCP port 6379 and has been allocated cluster IP address 10.0.0.11, produces the following environment variables:REDIS_PRIMARY_SERVICE_HOST=10.0.0.11 REDIS_PRIMARY_SERVICE_PORT=6379 REDIS_PRIMARY_PORT=tcp://10.0.0.11:6379 REDIS_PRIMARY_PORT_6379_TCP=tcp://10.0.0.11:6379 REDIS_PRIMARY_PORT_6379_TCP_PROTO=tcp REDIS_PRIMARY_PORT_6379_TCP_PORT=6379 REDIS_PRIMARY_PORT_6379_TCP_ADDR=10.0.0.11 Note:When you have a Pod that needs to access a Service, and you are using the environment variable method to publish the port and cluster IP to the client Pods, you must create the Service before the client Pods come into existence. Otherwise, those client Pods won't have their environment variables populated.If you only use DNS to discover the cluster IP for a Service, you don't need to worry about this ordering issue.Kubernetes also supports and provides variables that are compatible with Docker Engine's "legacy container links" feature. You can read makeLinkVariables to see how this is implemented in Kubernetes.DNSYou can (and almost always should) set up a DNS service for your Kubernetes cluster using an add-on.A cluster-aware DNS server, such as CoreDNS, watches the Kubernetes API for new Services and creates a set of DNS records for each one. If DNS has been enabled throughout your cluster then all Pods should automatically be able to resolve Services by their DNS name.For example, if you have a Service called my-service in a Kubernetes namespace my-ns, the control plane and the DNS Service acting together create a DNS record for my-service.my-ns. Pods in the my-ns namespace should be able to find the service by doing a name lookup for my-service (my-service.my-ns would also work).Pods in other namespaces must qualify the name as my-service.my-ns. These names will resolve to the cluster IP assigned for the Service.Kubernetes also supports DNS SRV (Service) records for named ports. If the my-service.my-ns Service has a port named http with the protocol set to TCP, you can do a DNS SRV query for _http._tcp.my-service.my-ns to discover the port number for http, as well as the IP address.The Kubernetes DNS server is the only way to access ExternalName Services. You can find more information about ExternalName resolution in DNS for Services and Pods.Virtual IP addressing mechanismRead Virtual IPs and Service Proxies explains the mechanism Kubernetes provides to expose a Service with a virtual IP address.Traffic policiesYou can set the .spec.internalTrafficPolicy and .spec.externalTrafficPolicy fields to control how Kubernetes routes traffic to healthy (“ready”) backends.See Traffic Policies for more details.Traffic distributionFEATURE STATE: Kubernetes v1.33 [stable] (enabled by default: true)The .spec.trafficDistribution field provides another way to influence traffic routing within a Kubernetes Service. While traffic policies focus on strict semantic guarantees, traffic distribution allows you to express preferences (such as routing to topologically closer endpoints). This can help optimize for performance, cost, or reliability. In Kubernetes 1.33, the following field value is supported:PreferCloseIndicates a preference for routing traffic to endpoints that are in the same zone as the client.FEATURE STATE: Kubernetes v1.33 [alpha] (enabled by default: false)Two additional values are available when the PreferSameTrafficDistribution feature gate is enabled:PreferSameZoneThis is an alias for PreferClose that is clearer about the intended semantics.PreferSameNodeIndicates a preference for routing traffic to endpoints that are on the same node as the client.If the field is not set, the implementation will apply its default routing strategy.See Traffic Distribution for more detailsSession stickinessIf you want to make sure that connections from a particular client are passed to the same Pod each time, you can configure session affinity based on the client's IP address. Read session affinity to learn more.External IPsIf there are external IPs that route to one or more cluster nodes, Kubernetes Services can be exposed on those externalIPs. When network traffic arrives into the cluster, with the external IP (as destination IP) and the port matching that Service, rules and routes that Kubernetes has configured ensure that the traffic is routed to one of the endpoints for that Service.When you define a Service, you can specify externalIPs for any service type. In the example below, the Service named "my-service" can be accessed by clients using TCP, on "198.51.100.32:80" (calculated from .spec.externalIPs[] and .spec.ports[].port).apiVersion: v1 kind: Service metadata: name: my-service spec: selector: app.kubernetes.io/name: MyApp ports: \- name: http protocol: TCP port: 80 targetPort: 49152 externalIPs: \- 198.51.100.32 Note:Kubernetes does not manage allocation of externalIPs; these are the responsibility of the cluster administrator.API ObjectService is a top-level resource in the Kubernetes REST API. You can find more details about the Service API object.What's nextLearn more about Services and how they fit into Kubernetes:Follow the Connecting Applications with Services tutorial.Read about Ingress, which exposes HTTP and HTTPS routes from outside the cluster to Services within your cluster.Read about Gateway, an extension to Kubernetes that provides more flexibility than Ingress.For more context, read the following:Virtual IPs and Service ProxiesEndpointSlicesService API referenceEndpointSlice API referenceEndpoint API reference (legacy)3.5.2 - IngressMake your HTTP (or HTTPS) network service available using a protocol-aware configuration mechanism, that understands web concepts like URIs, hostnames, paths, and more. The Ingress concept lets you map traffic to different backends based on rules you define via the Kubernetes API.FEATURE STATE: Kubernetes v1.19 [stable]An API object that manages external access to the services in a cluster, typically HTTP.Ingress may provide load balancing, SSL termination and name-based virtual hosting.Note:Ingress is frozen. New features are being added to the Gateway API.TerminologyFor clarity, this guide defines the following terms:Node: A worker machine in Kubernetes, part of a cluster.Cluster: A set of Nodes that run containerized applications managed by Kubernetes. For this example, and in most common Kubernetes deployments, nodes in the cluster are not part of the public internet.Edge router: A router that enforces the firewall policy for your cluster. This could be a gateway managed by a cloud provider or a physical piece of hardware.Cluster network: A set of links, logical or physical, that facilitate communication within a cluster according to the Kubernetes networking model.Service: A Kubernetes Service that identifies a set of Pods using label selectors. Unless mentioned otherwise, Services are assumed to have virtual IPs only routable within the cluster network.What is Ingress?Ingress exposes HTTP and HTTPS routes from outside the cluster to services within the cluster. Traffic routing is controlled by rules defined on the Ingress resource.Here is a simple example where an Ingress sends all its traffic to one Service:Figure. IngressAn Ingress may be configured to give Services externally-reachable URLs, load balance traffic, terminate SSL / TLS, and offer name-based virtual hosting. An Ingress controller is responsible for fulfilling the Ingress, usually with a load balancer, though it may also configure your edge router or additional frontends to help handle the traffic.An Ingress does not expose arbitrary ports or protocols. Exposing services other than HTTP and HTTPS to the internet typically uses a service of type Service.Type=NodePort or Service.Type=LoadBalancer.PrerequisitesYou must have an Ingress controller to satisfy an Ingress. Only creating an Ingress resource has no effect.You may need to deploy an Ingress controller such as ingress-nginx. You can choose from a number of Ingress controllers.Ideally, all Ingress controllers should fit the reference specification. In reality, the various Ingress controllers operate slightly differently.Note:Make sure you review your Ingress controller's documentation to understand the caveats of choosing it.The Ingress resourceA minimal Ingress resource example:service/networking/minimal-ingress.yaml apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: minimal-ingress annotations: nginx.ingress.kubernetes.io/rewrite-target: / spec: ingressClassName: nginx-example rules: \- http: paths: \- path: /testpath pathType: Prefix backend: service: name: test port: number: 80 An Ingress needs apiVersion, kind, metadata and spec fields. The name of an Ingress object must be a valid DNS subdomain name. For general information about working with config files, see deploying applications, configuring containers, managing resources. Ingress frequently uses annotations to configure some options depending on the Ingress controller, an example of which is the rewrite-target annotation. Different Ingress controllers support different annotations. Review the documentation for your choice of Ingress controller to learn which annotations are supported.The Ingress spec has all the information needed to configure a load balancer or proxy server. Most importantly, it contains a list of rules matched against all incoming requests. Ingress resource only supports rules for directing HTTP(S) traffic.If the ingressClassName is omitted, a default Ingress class should be defined.There are some ingress controllers, that work without the definition of a default IngressClass. For example, the Ingress-NGINX controller can be configured with a flag \--watch-ingress-without-class. It is recommended though, to specify the default IngressClass as shown below.Ingress rulesEach HTTP rule contains the following information:An optional host. In this example, no host is specified, so the rule applies to all inbound HTTP traffic through the IP address specified. If a host is provided (for example, foo.bar.com), the rules apply to that host.A list of paths (for example, /testpath), each of which has an associated backend defined with a service.name and a service.port.name or service.port.number. Both the host and path must match the content of an incoming request before the load balancer directs traffic to the referenced Service.A backend is a combination of Service and port names as described in the Service doc or a custom resource backend by way of a CRD. HTTP (and HTTPS) requests to the Ingress that match the host and path of the rule are sent to the listed backend.A defaultBackend is often configured in an Ingress controller to service any requests that do not match a path in the spec.DefaultBackendAn Ingress with no rules sends all traffic to a single default backend and .spec.defaultBackend is the backend that should handle requests in that case. The defaultBackend is conventionally a configuration option of the Ingress controller and is not specified in your Ingress resources. If no .spec.rules are specified, .spec.defaultBackend must be specified. If defaultBackend is not set, the handling of requests that do not match any of the rules will be up to the ingress controller (consult the documentation for your ingress controller to find out how it handles this case).If none of the hosts or paths match the HTTP request in the Ingress objects, the traffic is routed to your default backend.Resource backendsA Resource backend is an ObjectRef to another Kubernetes resource within the same namespace as the Ingress object. A Resource is a mutually exclusive setting with Service, and will fail validation if both are specified. A common usage for a Resource backend is to ingress data to an object storage backend with static assets.service/networking/ingress-resource-backend.yaml apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: ingress-resource-backend spec: defaultBackend: resource: apiGroup: k8s.example.com kind: StorageBucket name: static-assets rules: \- http: paths: \- path: /icons pathType: ImplementationSpecific backend: resource: apiGroup: k8s.example.com kind: StorageBucket name: icon-assets After creating the Ingress above, you can view it with the following command:kubectl describe ingress ingress-resource-backend Name: ingress-resource-backend Namespace: default Address: Default backend: APIGroup: k8s.example.com, Kind: StorageBucket, Name: static-assets Rules: Host Path Backends \---- ---- -------- * /icons APIGroup: k8s.example.com, Kind: StorageBucket, Name: icon-assets Annotations:  Events:  Path typesEach path in an Ingress is required to have a corresponding path type. Paths that do not include an explicit pathType will fail validation. There are three supported path types:ImplementationSpecific: With this path type, matching is up to the IngressClass. Implementations can treat this as a separate pathType or treat it identically to Prefix or Exact path types.Exact: Matches the URL path exactly and with case sensitivity.Prefix: Matches based on a URL path prefix split by /. Matching is case sensitive and done on a path element by element basis. A path element refers to the list of labels in the path split by the / separator. A request is a match for path p if every p is an element-wise prefix of p of the request path.Note:If the last element of the path is a substring of the last element in request path, it is not a match (for example: /foo/bar matches /foo/bar/baz, but does not match /foo/barbaz).ExamplesKindPath(s)Request path(s)Matches?Prefix/(all paths)YesExact/foo/fooYesExact/foo/barNoExact/foo/foo/NoExact/foo//fooNoPrefix/foo/foo, /foo/YesPrefix/foo//foo, /foo/YesPrefix/aaa/bb/aaa/bbbNoPrefix/aaa/bbb/aaa/bbbYesPrefix/aaa/bbb//aaa/bbbYes, ignores trailing slashPrefix/aaa/bbb/aaa/bbb/Yes, matches trailing slashPrefix/aaa/bbb/aaa/bbb/cccYes, matches subpathPrefix/aaa/bbb/aaa/bbbxyzNo, does not match string prefixPrefix/, /aaa/aaa/cccYes, matches /aaa prefixPrefix/, /aaa, /aaa/bbb/aaa/bbbYes, matches /aaa/bbb prefixPrefix/, /aaa, /aaa/bbb/cccYes, matches / prefixPrefix/aaa/cccNo, uses default backendMixed/foo (Prefix), /foo (Exact)/fooYes, prefers ExactMultiple matchesIn some cases, multiple paths within an Ingress will match a request. In those cases precedence will be given first to the longest matching path. If two paths are still equally matched, precedence will be given to paths with an exact path type over prefix path type.Hostname wildcardsHosts can be precise matches (for example “foo.bar.com”) or a wildcard (for example “*.foo.com”). Precise matches require that the HTTP host header matches the host field. Wildcard matches require the HTTP host header is equal to the suffix of the wildcard rule.HostHost headerMatch?*.foo.combar.foo.comMatches based on shared suffix*.foo.combaz.bar.foo.comNo match, wildcard only covers a single DNS label*.foo.comfoo.comNo match, wildcard only covers a single DNS labelservice/networking/ingress-wildcard-host.yaml apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: ingress-wildcard-host spec: rules: \- host: "foo.bar.com" http: paths: \- pathType: Prefix path: "/bar" backend: service: name: service1 port: number: 80 \- host: "*.foo.com" http: paths: \- pathType: Prefix path: "/foo" backend: service: name: service2 port: number: 80 Ingress classIngresses can be implemented by different controllers, often with different configuration. Each Ingress should specify a class, a reference to an IngressClass resource that contains additional configuration including the name of the controller that should implement the class.service/networking/external-lb.yaml apiVersion: networking.k8s.io/v1 kind: IngressClass metadata: name: external-lb spec: controller: example.com/ingress-controller parameters: apiGroup: k8s.example.com kind: IngressParameters name: external-lb The .spec.parameters field of an IngressClass lets you reference another resource that provides configuration related to that IngressClass.The specific type of parameters to use depends on the ingress controller that you specify in the .spec.controller field of the IngressClass.IngressClass scopeDepending on your ingress controller, you may be able to use parameters that you set cluster-wide, or just for one namespace.ClusterNamespacedThe default scope for IngressClass parameters is cluster-wide.If you set the .spec.parameters field and don't set .spec.parameters.scope, or if you set .spec.parameters.scope to Cluster, then the IngressClass refers to a cluster-scoped resource. The kind (in combination the apiGroup) of the parameters refers to a cluster-scoped API (possibly a custom resource), and the name of the parameters identifies a specific cluster scoped resource for that API.For example:--- apiVersion: networking.k8s.io/v1 kind: IngressClass metadata: name: external-lb-1 spec: controller: example.com/ingress-controller parameters: # The parameters for this IngressClass are specified in a # ClusterIngressParameter (API group k8s.example.net) named # "external-config-1". This definition tells Kubernetes to # look for a cluster-scoped parameter resource. scope: Cluster apiGroup: k8s.example.net kind: ClusterIngressParameter name: external-config-1 FEATURE STATE: Kubernetes v1.23 [stable]If you set the .spec.parameters field and set .spec.parameters.scope to Namespace, then the IngressClass refers to a namespaced-scoped resource. You must also set the namespace field within .spec.parameters to the namespace that contains the parameters you want to use.The kind (in combination the apiGroup) of the parameters refers to a namespaced API (for example: ConfigMap), and the name of the parameters identifies a specific resource in the namespace you specified in namespace.Namespace-scoped parameters help the cluster operator delegate control over the configuration (for example: load balancer settings, API gateway definition) that is used for a workload. If you used a cluster-scoped parameter then either:the cluster operator team needs to approve a different team's changes every time there's a new configuration change being applied.the cluster operator must define specific access controls, such as RBAC roles and bindings, that let the application team make changes to the cluster-scoped parameters resource.The IngressClass API itself is always cluster-scoped.Here is an example of an IngressClass that refers to parameters that are namespaced:--- apiVersion: networking.k8s.io/v1 kind: IngressClass metadata: name: external-lb-2 spec: controller: example.com/ingress-controller parameters: # The parameters for this IngressClass are specified in an # IngressParameter (API group k8s.example.com) named "external-config", # that's in the "external-configuration" namespace. scope: Namespace apiGroup: k8s.example.com kind: IngressParameter namespace: external-configuration name: external-config Deprecated annotationBefore the IngressClass resource and ingressClassName field were added in Kubernetes 1.18, Ingress classes were specified with a kubernetes.io/ingress.class annotation on the Ingress. This annotation was never formally defined, but was widely supported by Ingress controllers.The newer ingressClassName field on Ingresses is a replacement for that annotation, but is not a direct equivalent. While the annotation was generally used to reference the name of the Ingress controller that should implement the Ingress, the field is a reference to an IngressClass resource that contains additional Ingress configuration, including the name of the Ingress controller.Default IngressClassYou can mark a particular IngressClass as default for your cluster. Setting the ingressclass.kubernetes.io/is-default-class annotation to true on an IngressClass resource will ensure that new Ingresses without an ingressClassName field specified will be assigned this default IngressClass.Caution:If you have more than one IngressClass marked as the default for your cluster, the admission controller prevents creating new Ingress objects that don't have an ingressClassName specified. You can resolve this by ensuring that at most 1 IngressClass is marked as default in your cluster.There are some ingress controllers, that work without the definition of a default IngressClass. For example, the Ingress-NGINX controller can be configured with a flag \--watch-ingress-without-class. It is recommended though, to specify the default IngressClass:service/networking/default-ingressclass.yaml apiVersion: networking.k8s.io/v1 kind: IngressClass metadata: labels: app.kubernetes.io/component: controller name: nginx-example annotations: ingressclass.kubernetes.io/is-default-class: "true" spec: controller: k8s.io/ingress-nginx Types of IngressIngress backed by a single ServiceThere are existing Kubernetes concepts that allow you to expose a single Service (see alternatives). You can also do this with an Ingress by specifying a default backend with no rules.service/networking/test-ingress.yaml apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: test-ingress spec: defaultBackend: service: name: test port: number: 80 If you create it using kubectl apply -f you should be able to view the state of the Ingress you added:kubectl get ingress test-ingress NAME CLASS HOSTS ADDRESS PORTS AGE test-ingress external-lb * 203.0.113.123 80 59s Where 203.0.113.123 is the IP allocated by the Ingress controller to satisfy this Ingress.Note:Ingress controllers and load balancers may take a minute or two to allocate an IP address. Until that time, you often see the address listed as .Simple fanoutA fanout configuration routes traffic from a single IP address to more than one Service, based on the HTTP URI being requested. An Ingress allows you to keep the number of load balancers down to a minimum. For example, a setup like:Figure. Ingress Fan OutIt would require an Ingress such as:service/networking/simple-fanout-example.yaml apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: simple-fanout-example spec: rules: \- host: foo.bar.com http: paths: \- path: /foo pathType: Prefix backend: service: name: service1 port: number: 4200 \- path: /bar pathType: Prefix backend: service: name: service2 port: number: 8080 When you create the Ingress with kubectl apply -f:kubectl describe ingress simple-fanout-example Name: simple-fanout-example Namespace: default Address: 178.91.123.132 Default backend: default-http-backend:80 (10.8.2.3:8080) Rules: Host Path Backends \---- ---- -------- foo.bar.com /foo service1:4200 (10.8.0.90:4200) /bar service2:8080 (10.8.0.91:8080) Events: Type Reason Age From Message \---- ------ ---- ---- ------- Normal ADD 22s loadbalancer-controller default/test The Ingress controller provisions an implementation-specific load balancer that satisfies the Ingress, as long as the Services (service1, service2) exist. When it has done so, you can see the address of the load balancer at the Address field.Note:Depending on the Ingress controller you are using, you may need to create a default-http-backend Service.Name based virtual hostingName-based virtual hosts support routing HTTP traffic to multiple host names at the same IP address.Figure. Ingress Name Based Virtual hostingThe following Ingress tells the backing load balancer to route requests based on the Host header.service/networking/name-virtual-host-ingress.yaml apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: name-virtual-host-ingress spec: rules: \- host: foo.bar.com http: paths: \- pathType: Prefix path: "/" backend: service: name: service1 port: number: 80 \- host: bar.foo.com http: paths: \- pathType: Prefix path: "/" backend: service: name: service2 port: number: 80 If you create an Ingress resource without any hosts defined in the rules, then any web traffic to the IP address of your Ingress controller can be matched without a name based virtual host being required.For example, the following Ingress routes traffic requested for first.bar.com to service1, second.bar.com to service2, and any traffic whose request host header doesn't match first.bar.com and second.bar.com to service3.service/networking/name-virtual-host-ingress-no-third-host.yaml apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: name-virtual-host-ingress-no-third-host spec: rules: \- host: first.bar.com http: paths: \- pathType: Prefix path: "/" backend: service: name: service1 port: number: 80 \- host: second.bar.com http: paths: \- pathType: Prefix path: "/" backend: service: name: service2 port: number: 80 \- http: paths: \- pathType: Prefix path: "/" backend: service: name: service3 port: number: 80 TLSYou can secure an Ingress by specifying a Secret that contains a TLS private key and certificate. The Ingress resource only supports a single TLS port, 443, and assumes TLS termination at the ingress point (traffic to the Service and its Pods is in plaintext). If the TLS configuration section in an Ingress specifies different hosts, they are multiplexed on the same port according to the hostname specified through the SNI TLS extension (provided the Ingress controller supports SNI). The TLS secret must contain keys named tls.crt and tls.key that contain the certificate and private key to use for TLS. For example:apiVersion: v1 kind: Secret metadata: name: testsecret-tls namespace: default data: tls.crt: base64 encoded cert tls.key: base64 encoded key type: kubernetes.io/tls Referencing this secret in an Ingress tells the Ingress controller to secure the channel from the client to the load balancer using TLS. You need to make sure the TLS secret you created came from a certificate that contains a Common Name (CN), also known as a Fully Qualified Domain Name (FQDN) for https-example.foo.com.Note:Keep in mind that TLS will not work on the default rule because the certificates would have to be issued for all the possible sub-domains. Therefore, hosts in the tls section need to explicitly match the host in the rules section.service/networking/tls-example-ingress.yaml apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: tls-example-ingress spec: tls: \- hosts: \- https-example.foo.com secretName: testsecret-tls rules: \- host: https-example.foo.com http: paths: \- path: / pathType: Prefix backend: service: name: service1 port: number: 80 Note:There is a gap between TLS features supported by various Ingress controllers. Please refer to documentation on nginx, GCE, or any other platform specific Ingress controller to understand how TLS works in your environment.Load balancingAn Ingress controller is bootstrapped with some load balancing policy settings that it applies to all Ingress, such as the load balancing algorithm, backend weight scheme, and others. More advanced load balancing concepts (e.g. persistent sessions, dynamic weights) are not yet exposed through the Ingress. You can instead get these features through the load balancer used for a Service.It's also worth noting that even though health checks are not exposed directly through the Ingress, there exist parallel concepts in Kubernetes such as readiness probes that allow you to achieve the same end result. Please review the controller specific documentation to see how they handle health checks (for example: nginx, or GCE).Updating an IngressTo update an existing Ingress to add a new Host, you can update it by editing the resource:kubectl describe ingress test Name: test Namespace: default Address: 178.91.123.132 Default backend: default-http-backend:80 (10.8.2.3:8080) Rules: Host Path Backends \---- ---- -------- foo.bar.com /foo service1:80 (10.8.0.90:80) Annotations: nginx.ingress.kubernetes.io/rewrite-target: / Events: Type Reason Age From Message \---- ------ ---- ---- ------- Normal ADD 35s loadbalancer-controller default/test kubectl edit ingress test This pops up an editor with the existing configuration in YAML format. Modify it to include the new Host:spec: rules: \- host: foo.bar.com http: paths: \- backend: service: name: service1 port: number: 80 path: /foo pathType: Prefix \- host: bar.baz.com http: paths: \- backend: service: name: service2 port: number: 80 path: /foo pathType: Prefix .. After you save your changes, kubectl updates the resource in the API server, which tells the Ingress controller to reconfigure the load balancer.Verify this:kubectl describe ingress test Name: test Namespace: default Address: 178.91.123.132 Default backend: default-http-backend:80 (10.8.2.3:8080) Rules: Host Path Backends \---- ---- -------- foo.bar.com /foo service1:80 (10.8.0.90:80) bar.baz.com /foo service2:80 (10.8.0.91:80) Annotations: nginx.ingress.kubernetes.io/rewrite-target: / Events: Type Reason Age From Message \---- ------ ---- ---- ------- Normal ADD 45s loadbalancer-controller default/test You can achieve the same outcome by invoking kubectl replace -f on a modified Ingress YAML file.Failing across availability zonesTechniques for spreading traffic across failure domains differ between cloud providers. Please check the documentation of the relevant Ingress controller for details.AlternativesYou can expose a Service in multiple ways that don't directly involve the Ingress resource:Use Service.Type=LoadBalancerUse Service.Type=NodePortWhat's nextLearn about the Ingress APILearn about Ingress controllersSet up Ingress on Minikube with the NGINX Controller3.5.3 - Ingress ControllersIn order for an Ingress to work in your cluster, there must be an ingress controller running. You need to select at least one ingress controller and make sure it is set up in your cluster. This page lists common ingress controllers that you can deploy.In order for the Ingress resource to work, the cluster must have an ingress controller running.Unlike other types of controllers which run as part of the kube-controller-manager binary, Ingress controllers are not started automatically with a cluster. Use this page to choose the ingress controller implementation that best fits your cluster.Kubernetes as a project supports and maintains AWS, GCE, and nginx ingress controllers.Additional controllersNote: This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the content guide before submitting a change. More information.AKS Application Gateway Ingress Controller is an ingress controller that configures the Azure Application Gateway.Alibaba Cloud MSE Ingress is an ingress controller that configures the Alibaba Cloud Native Gateway, which is also the commercial version of Higress.Apache APISIX ingress controller is an Apache APISIX-based ingress controller.Avi Kubernetes Operator provides L4-L7 load-balancing using VMware NSX Advanced Load Balancer.BFE Ingress Controller is a BFE-based ingress controller.Cilium Ingress Controller is an ingress controller powered by Cilium.The Citrix ingress controller works with Citrix Application Delivery Controller.Contour is an Envoy based ingress controller.Emissary-Ingress API Gateway is an Envoy-based ingress controller.EnRoute is an Envoy based API gateway that can run as an ingress controller.Easegress IngressController is an Easegress based API gateway that can run as an ingress controller.F5 BIG-IP Container Ingress Services for Kubernetes lets you use an Ingress to configure F5 BIG-IP virtual servers.FortiADC Ingress Controller support the Kubernetes Ingress resources and allows you to manage FortiADC objects from KubernetesGloo is an open-source ingress controller based on Envoy, which offers API gateway functionality.HAProxy Ingress is an ingress controller for HAProxy.Higress is an Envoy based API gateway that can run as an ingress controller.The HAProxy Ingress Controller for Kubernetes is also an ingress controller for HAProxy.Istio Ingress is an Istio based ingress controller.The Kong Ingress Controller for Kubernetes is an ingress controller driving Kong Gateway.Kusk Gateway is an OpenAPI-driven ingress controller based on Envoy.The NGINX Ingress Controller for Kubernetes works with the NGINX webserver (as a proxy).The ngrok Kubernetes Ingress Controller is an open source controller for adding secure public access to your K8s services using the ngrok platform.The OCI Native Ingress Controller is an Ingress controller for Oracle Cloud Infrastructure which allows you to manage the OCI Load Balancer.OpenNJet Ingress Controller is a OpenNJet-based ingress controller.The Pomerium Ingress Controller is based on Pomerium, which offers context-aware access policy.Skipper HTTP router and reverse proxy for service composition, including use cases like Kubernetes Ingress, designed as a library to build your custom proxy.The Traefik Kubernetes Ingress provider is an ingress controller for the Traefik proxy.Tyk Operator extends Ingress with Custom Resources to bring API Management capabilities to Ingress. Tyk Operator works with the Open Source Tyk Gateway & Tyk Cloud control plane.Voyager is an ingress controller for HAProxy.Wallarm Ingress Controller is an Ingress Controller that provides WAAP (WAF) and API Security capabilities.Using multiple Ingress controllersYou may deploy any number of ingress controllers using ingress class within a cluster. Note the .metadata.name of your ingress class resource. When you create an ingress you would need that name to specify the ingressClassName field on your Ingress object (refer to IngressSpec v1 reference). ingressClassName is a replacement of the older annotation method.If you do not specify an IngressClass for an Ingress, and your cluster has exactly one IngressClass marked as default, then Kubernetes applies the cluster's default IngressClass to the Ingress. You mark an IngressClass as default by setting the ingressclass.kubernetes.io/is-default-class annotation on that IngressClass, with the string value "true".Ideally, all ingress controllers should fulfill this specification, but the various ingress controllers operate slightly differently.Note:Make sure you review your ingress controller's documentation to understand the caveats of choosing it.What's nextLearn more about Ingress.Set up Ingress on Minikube with the NGINX Controller.3.5.4 - Gateway APIGateway API is a family of API kinds that provide dynamic infrastructure provisioning and advanced traffic routing.Make network services available by using an extensible, role-oriented, protocol-aware configuration mechanism. Gateway API is an add-on containing API kinds that provide dynamic infrastructure provisioning and advanced traffic routing.Design principlesThe following principles shaped the design and architecture of Gateway API:Role-oriented: Gateway API kinds are modeled after organizational roles that are responsible for managing Kubernetes service networking:Infrastructure Provider: Manages infrastructure that allows multiple isolated clusters to serve multiple tenants, e.g. a cloud provider.Cluster Operator: Manages clusters and is typically concerned with policies, network access, application permissions, etc.Application Developer: Manages an application running in a cluster and is typically concerned with application-level configuration and Service composition.Portable: Gateway API specifications are defined as custom resources and are supported by many implementations.Expressive: Gateway API kinds support functionality for common traffic routing use cases such as header-based matching, traffic weighting, and others that were only possible in Ingress by using custom annotations.Extensible: Gateway allows for custom resources to be linked at various layers of the API. This makes granular customization possible at the appropriate places within the API structure.Resource modelGateway API has three stable API kinds:GatewayClass: Defines a set of gateways with common configuration and managed by a controller that implements the class.Gateway: Defines an instance of traffic handling infrastructure, such as cloud load balancer.HTTPRoute: Defines HTTP-specific rules for mapping traffic from a Gateway listener to a representation of backend network endpoints. These endpoints are often represented as a Service.Gateway API is organized into different API kinds that have interdependent relationships to support the role-oriented nature of organizations. A Gateway object is associated with exactly one GatewayClass; the GatewayClass describes the gateway controller responsible for managing Gateways of this class. One or more route kinds such as HTTPRoute, are then associated to Gateways. A Gateway can filter the routes that may be attached to its listeners, forming a bidirectional trust model with routes.The following figure illustrates the relationships of the three stable Gateway API kinds:GatewayClassGateways can be implemented by different controllers, often with different configurations. A Gateway must reference a GatewayClass that contains the name of the controller that implements the class.A minimal GatewayClass example:apiVersion: gateway.networking.k8s.io/v1 kind: GatewayClass metadata: name: example-class spec: controllerName: example.com/gateway-controller In this example, a controller that has implemented Gateway API is configured to manage GatewayClasses with the controller name example.com/gateway-controller. Gateways of this class will be managed by the implementation's controller.See the GatewayClass reference for a full definition of this API kind.GatewayA Gateway describes an instance of traffic handling infrastructure. It defines a network endpoint that can be used for processing traffic, i.e. filtering, balancing, splitting, etc. for backends such as a Service. For example, a Gateway may represent a cloud load balancer or an in-cluster proxy server that is configured to accept HTTP traffic.A minimal Gateway resource example:apiVersion: gateway.networking.k8s.io/v1 kind: Gateway metadata: name: example-gateway spec: gatewayClassName: example-class listeners: \- name: http protocol: HTTP port: 80 In this example, an instance of traffic handling infrastructure is programmed to listen for HTTP traffic on port 80. Since the addresses field is unspecified, an address or hostname is assigned to the Gateway by the implementation's controller. This address is used as a network endpoint for processing traffic of backend network endpoints defined in routes.See the Gateway reference for a full definition of this API kind.HTTPRouteThe HTTPRoute kind specifies routing behavior of HTTP requests from a Gateway listener to backend network endpoints. For a Service backend, an implementation may represent the backend network endpoint as a Service IP or the backing EndpointSlices of the Service. An HTTPRoute represents configuration that is applied to the underlying Gateway implementation. For example, defining a new HTTPRoute may result in configuring additional traffic routes in a cloud load balancer or in-cluster proxy server.A minimal HTTPRoute example:apiVersion: gateway.networking.k8s.io/v1 kind: HTTPRoute metadata: name: example-httproute spec: parentRefs: \- name: example-gateway hostnames: \- "www.example.com" rules: \- matches: \- path: type: PathPrefix value: /login backendRefs: \- name: example-svc port: 8080 In this example, HTTP traffic from Gateway example-gateway with the Host: header set to www.example.com and the request path specified as /login will be routed to Service example-svc on port 8080.See the HTTPRoute reference for a full definition of this API kind.Request flowHere is a simple example of HTTP traffic being routed to a Service by using a Gateway and an HTTPRoute:In this example, the request flow for a Gateway implemented as a reverse proxy is:The client starts to prepare an HTTP request for the URL http://www.example.comThe client's DNS resolver queries for the destination name and learns a mapping to one or more IP addresses associated with the Gateway.The client sends a request to the Gateway IP address; the reverse proxy receives the HTTP request and uses the Host: header to match a configuration that was derived from the Gateway and attached HTTPRoute.Optionally, the reverse proxy can perform request header and/or path matching based on match rules of the HTTPRoute.Optionally, the reverse proxy can modify the request; for example, to add or remove headers, based on filter rules of the HTTPRoute.Lastly, the reverse proxy forwards the request to one or more backends.ConformanceGateway API covers a broad set of features and is widely implemented. This combination requires clear conformance definitions and tests to ensure that the API provides a consistent experience wherever it is used.See the conformance documentation to understand details such as release channels, support levels, and running conformance tests.Migrating from IngressGateway API is the successor to the Ingress API. However, it does not include the Ingress kind. As a result, a one-time conversion from your existing Ingress resources to Gateway API resources is necessary.Refer to the ingress migration guide for details on migrating Ingress resources to Gateway API resources.What's nextInstead of Gateway API resources being natively implemented by Kubernetes, the specifications are defined as Custom Resources supported by a wide range of implementations. Install the Gateway API CRDs or follow the installation instructions of your selected implementation. After installing an implementation, use the Getting Started guide to help you quickly start working with Gateway API.Note:Make sure to review the documentation of your selected implementation to understand any caveats.Refer to the API specification for additional details of all Gateway API kinds.3.5.5 - EndpointSlicesThe EndpointSlice API is the mechanism that Kubernetes uses to let your Service scale to handle large numbers of backends, and allows the cluster to update its list of healthy backends efficiently.FEATURE STATE: Kubernetes v1.21 [stable]Kubernetes' EndpointSlice API provides a way to track network endpoints within a Kubernetes cluster.EndpointSlice APIIn Kubernetes, an EndpointSlice contains references to a set of network endpoints. The control plane automatically creates EndpointSlices for any Kubernetes Service that has a selector specified. These EndpointSlices include references to all the Pods that match the Service selector. EndpointSlices group network endpoints together by unique combinations of IP family, protocol, port number, and Service name. The name of a EndpointSlice object must be a valid DNS subdomain name.As an example, here's a sample EndpointSlice object, that's owned by the example Kubernetes Service.apiVersion: discovery.k8s.io/v1 kind: EndpointSlice metadata: name: example-abc labels: kubernetes.io/service-name: example addressType: IPv4 ports: \- name: http protocol: TCP port: 80 endpoints: \- addresses: \- "10.1.2.3" conditions: ready: true hostname: pod-1 nodeName: node-1 zone: us-west2-a By default, the control plane creates and manages EndpointSlices to have no more than 100 endpoints each. You can configure this with the \--max-endpoints-per-slice kube-controller-manager flag, up to a maximum of 1000.EndpointSlices act as the source of truth for kube-proxy when it comes to how to route internal traffic.Address typesEndpointSlices support two address types:IPv4IPv6Each EndpointSlice object represents a specific IP address type. If you have a Service that is available via IPv4 and IPv6, there will be at least two EndpointSlice objects (one for IPv4, and one for IPv6).ConditionsThe EndpointSlice API stores conditions about endpoints that may be useful for consumers. The three conditions are serving, terminating, and ready.ServingFEATURE STATE: Kubernetes v1.26 [stable]The serving condition indicates that the endpoint is currently serving responses, and so it should be used as a target for Service traffic. For endpoints backed by a Pod, this maps to the Pod's Ready condition.TerminatingFEATURE STATE: Kubernetes v1.26 [stable]The terminating condition indicates that the endpoint is terminating. For endpoints backed by a Pod, this condition is set when the Pod is first deleted (that is, when it receives a deletion timestamp, but most likely before the Pod's containers exit).Service proxies will normally ignore endpoints that are terminating, but they may route traffic to endpoints that are both serving and terminating if all available endpoints are terminating. (This helps to ensure that no Service traffic is lost during rolling updates of the underlying Pods.)ReadyThe ready condition is essentially a shortcut for checking "serving and not terminating" (though it will also always be true for Services with spec.publishNotReadyAddresses set to true).Topology informationEach endpoint within an EndpointSlice can contain relevant topology information. The topology information includes the location of the endpoint and information about the corresponding Node and zone. These are available in the following per endpoint fields on EndpointSlices:nodeName - The name of the Node this endpoint is on.zone - The zone this endpoint is in.ManagementMost often, the control plane (specifically, the endpoint slice controller) creates and manages EndpointSlice objects. There are a variety of other use cases for EndpointSlices, such as service mesh implementations, that could result in other entities or controllers managing additional sets of EndpointSlices.To ensure that multiple entities can manage EndpointSlices without interfering with each other, Kubernetes defines the label endpointslice.kubernetes.io/managed-by, which indicates the entity managing an EndpointSlice. The endpoint slice controller sets endpointslice-controller.k8s.io as the value for this label on all EndpointSlices it manages. Other entities managing EndpointSlices should also set a unique value for this label.OwnershipIn most use cases, EndpointSlices are owned by the Service that the endpoint slice object tracks endpoints for. This ownership is indicated by an owner reference on each EndpointSlice as well as a kubernetes.io/service-name label that enables simple lookups of all EndpointSlices belonging to a Service.Distribution of EndpointSlicesEach EndpointSlice has a set of ports that applies to all endpoints within the resource. When named ports are used for a Service, Pods may end up with different target port numbers for the same named port, requiring different EndpointSlices.The control plane tries to fill EndpointSlices as full as possible, but does not actively rebalance them. The logic is fairly straightforward:Iterate through existing EndpointSlices, remove endpoints that are no longer desired and update matching endpoints that have changed.Iterate through EndpointSlices that have been modified in the first step and fill them up with any new endpoints needed.If there's still new endpoints left to add, try to fit them into a previously unchanged slice and/or create new ones.Importantly, the third step prioritizes limiting EndpointSlice updates over a perfectly full distribution of EndpointSlices. As an example, if there are 10 new endpoints to add and 2 EndpointSlices with room for 5 more endpoints each, this approach will create a new EndpointSlice instead of filling up the 2 existing EndpointSlices. In other words, a single EndpointSlice creation is preferable to multiple EndpointSlice updates.With kube-proxy running on each Node and watching EndpointSlices, every change to an EndpointSlice becomes relatively expensive since it will be transmitted to every Node in the cluster. This approach is intended to limit the number of changes that need to be sent to every Node, even if it may result with multiple EndpointSlices that are not full.In practice, this less than ideal distribution should be rare. Most changes processed by the EndpointSlice controller will be small enough to fit in an existing EndpointSlice, and if not, a new EndpointSlice is likely going to be necessary soon anyway. Rolling updates of Deployments also provide a natural repacking of EndpointSlices with all Pods and their corresponding endpoints getting replaced.Duplicate endpointsDue to the nature of EndpointSlice changes, endpoints may be represented in more than one EndpointSlice at the same time. This naturally occurs as changes to different EndpointSlice objects can arrive at the Kubernetes client watch / cache at different times.Note:Clients of the EndpointSlice API must iterate through all the existing EndpointSlices associated to a Service and build a complete list of unique network endpoints. It is important to mention that endpoints may be duplicated in different EndpointSlices.You can find a reference implementation for how to perform this endpoint aggregation and deduplication as part of the EndpointSliceCache code within kube-proxy.EndpointSlice mirroringFEATURE STATE: Kubernetes v1.33 [deprecated]The EndpointSlice API is a replacement for the older Endpoints API. To preserve compatibility with older controllers and user workloads that expect kube-proxy to route traffic based on Endpoints resources, the cluster's control plane mirrors most user-created Endpoints resources to corresponding EndpointSlices.(However, this feature, like the rest of the Endpoints API, is deprecated. Users who manually specify endpoints for selectorless Services should do so by creating EndpointSlice resources directly, rather than by creating Endpoints resources and allowing them to be mirrored.)The control plane mirrors Endpoints resources unless:the Endpoints resource has a endpointslice.kubernetes.io/skip-mirror label set to true.the Endpoints resource has a control-plane.alpha.kubernetes.io/leader annotation.the corresponding Service resource does not exist.the corresponding Service resource has a non-nil selector.Individual Endpoints resources may translate into multiple EndpointSlices. This will occur if an Endpoints resource has multiple subsets or includes endpoints with multiple IP families (IPv4 and IPv6). A maximum of 1000 addresses per subset will be mirrored to EndpointSlices.What's nextFollow the Connecting Applications with Services tutorialRead the API reference for the EndpointSlice APIRead the API reference for the Endpoints API3.5.6 - Network PoliciesIf you want to control traffic flow at the IP address or port level (OSI layer 3 or 4), NetworkPolicies allow you to specify rules for traffic flow within your cluster, and also between Pods and the outside world. Your cluster must use a network plugin that supports NetworkPolicy enforcement.If you want to control traffic flow at the IP address or port level for TCP, UDP, and SCTP protocols, then you might consider using Kubernetes NetworkPolicies for particular applications in your cluster. NetworkPolicies are an application-centric construct which allow you to specify how a pod is allowed to communicate with various network "entities" (we use the word "entity" here to avoid overloading the more common terms such as "endpoints" and "services", which have specific Kubernetes connotations) over the network. NetworkPolicies apply to a connection with a pod on one or both ends, and are not relevant to other connections.The entities that a Pod can communicate with are identified through a combination of the following three identifiers:Other pods that are allowed (exception: a pod cannot block access to itself)Namespaces that are allowedIP blocks (exception: traffic to and from the node where a Pod is running is always allowed, regardless of the IP address of the Pod or the node)When defining a pod- or namespace-based NetworkPolicy, you use a selector to specify what traffic is allowed to and from the Pod(s) that match the selector.Meanwhile, when IP-based NetworkPolicies are created, we define policies based on IP blocks (CIDR ranges).PrerequisitesNetwork policies are implemented by the network plugin. To use network policies, you must be using a networking solution which supports NetworkPolicy. Creating a NetworkPolicy resource without a controller that implements it will have no effect.The two sorts of pod isolationThere are two sorts of isolation for a pod: isolation for egress, and isolation for ingress. They concern what connections may be established. "Isolation" here is not absolute, rather it means "some restrictions apply". The alternative, "non-isolated for $direction", means that no restrictions apply in the stated direction. The two sorts of isolation (or not) are declared independently, and are both relevant for a connection from one pod to another.By default, a pod is non-isolated for egress; all outbound connections are allowed. A pod is isolated for egress if there is any NetworkPolicy that both selects the pod and has "Egress" in its policyTypes; we say that such a policy applies to the pod for egress. When a pod is isolated for egress, the only allowed connections from the pod are those allowed by the egress list of some NetworkPolicy that applies to the pod for egress. Reply traffic for those allowed connections will also be implicitly allowed. The effects of those egress lists combine additively.By default, a pod is non-isolated for ingress; all inbound connections are allowed. A pod is isolated for ingress if there is any NetworkPolicy that both selects the pod and has "Ingress" in its policyTypes; we say that such a policy applies to the pod for ingress. When a pod is isolated for ingress, the only allowed connections into the pod are those from the pod's node and those allowed by the ingress list of some NetworkPolicy that applies to the pod for ingress. Reply traffic for those allowed connections will also be implicitly allowed. The effects of those ingress lists combine additively.Network policies do not conflict; they are additive. If any policy or policies apply to a given pod for a given direction, the connections allowed in that direction from that pod is the union of what the applicable policies allow. Thus, order of evaluation does not affect the policy result.For a connection from a source pod to a destination pod to be allowed, both the egress policy on the source pod and the ingress policy on the destination pod need to allow the connection. If either side does not allow the connection, it will not happen.The NetworkPolicy resourceSee the NetworkPolicy reference for a full definition of the resource.An example NetworkPolicy might look like this:service/networking/networkpolicy.yaml apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: test-network-policy namespace: default spec: podSelector: matchLabels: role: db policyTypes: \- Ingress \- Egress ingress: \- from: \- ipBlock: cidr: 172.17.0.0/16 except: \- 172.17.1.0/24 \- namespaceSelector: matchLabels: project: myproject \- podSelector: matchLabels: role: frontend ports: \- protocol: TCP port: 6379 egress: \- to: \- ipBlock: cidr: 10.0.0.0/24 ports: \- protocol: TCP port: 5978 Note:POSTing this to the API server for your cluster will have no effect unless your chosen networking solution supports network policy.Mandatory Fields: As with all other Kubernetes config, a NetworkPolicy needs apiVersion, kind, and metadata fields. For general information about working with config files, see Configure a Pod to Use a ConfigMap, and Object Management.spec: NetworkPolicy spec has all the information needed to define a particular network policy in the given namespace.podSelector: Each NetworkPolicy includes a podSelector which selects the grouping of pods to which the policy applies. The example policy selects pods with the label "role=db". An empty podSelector selects all pods in the namespace.policyTypes: Each NetworkPolicy includes a policyTypes list which may include either Ingress, Egress, or both. The policyTypes field indicates whether or not the given policy applies to ingress traffic to selected pod, egress traffic from selected pods, or both. If no policyTypes are specified on a NetworkPolicy then by default Ingress will always be set and Egress will be set if the NetworkPolicy has any egress rules.ingress: Each NetworkPolicy may include a list of allowed ingress rules. Each rule allows traffic which matches both the from and ports sections. The example policy contains a single rule, which matches traffic on a single port, from one of three sources, the first specified via an ipBlock, the second via a namespaceSelector and the third via a podSelector.egress: Each NetworkPolicy may include a list of allowed egress rules. Each rule allows traffic which matches both the to and ports sections. The example policy contains a single rule, which matches traffic on a single port to any destination in 10.0.0.0/24.So, the example NetworkPolicy:isolates role=db pods in the default namespace for both ingress and egress traffic (if they weren't already isolated)(Ingress rules) allows connections to all pods in the default namespace with the label role=db on TCP port 6379 from:any pod in the default namespace with the label role=frontendany pod in a namespace with the label project=myprojectIP addresses in the ranges 172.17.0.0–172.17.0.255 and 172.17.2.0–172.17.255.255 (ie, all of 172.17.0.0/16 except 172.17.1.0/24)(Egress rules) allows connections from any pod in the default namespace with the label role=db to CIDR 10.0.0.0/24 on TCP port 5978See the Declare Network Policy walkthrough for further examples.Behavior of to and from selectorsThere are four kinds of selectors that can be specified in an ingress from section or egress to section:podSelector: This selects particular Pods in the same namespace as the NetworkPolicy which should be allowed as ingress sources or egress destinations.namespaceSelector: This selects particular namespaces for which all Pods should be allowed as ingress sources or egress destinations.namespaceSelector and podSelector: A single to/from entry that specifies both namespaceSelector and podSelector selects particular Pods within particular namespaces. Be careful to use correct YAML syntax. For example: ... ingress: \- from: \- namespaceSelector: matchLabels: user: alice podSelector: matchLabels: role: client ... This policy contains a single from element allowing connections from Pods with the label role=client in namespaces with the label user=alice. But the following policy is different: ... ingress: \- from: \- namespaceSelector: matchLabels: user: alice \- podSelector: matchLabels: role: client ... It contains two elements in the from array, and allows connections from Pods in the local Namespace with the label role=client, or from any Pod in any namespace with the label user=alice.When in doubt, use kubectl describe to see how Kubernetes has interpreted the policy.ipBlock: This selects particular IP CIDR ranges to allow as ingress sources or egress destinations. These should be cluster-external IPs, since Pod IPs are ephemeral and unpredictable.Cluster ingress and egress mechanisms often require rewriting the source or destination IP of packets. In cases where this happens, it is not defined whether this happens before or after NetworkPolicy processing, and the behavior may be different for different combinations of network plugin, cloud provider, Service implementation, etc.In the case of ingress, this means that in some cases you may be able to filter incoming packets based on the actual original source IP, while in other cases, the "source IP" that the NetworkPolicy acts on may be the IP of a LoadBalancer or of the Pod's node, etc.For egress, this means that connections from pods to Service IPs that get rewritten to cluster-external IPs may or may not be subject to ipBlock-based policies.Default policiesBy default, if no policies exist in a namespace, then all ingress and egress traffic is allowed to and from pods in that namespace. The following examples let you change the default behavior in that namespace.Default deny all ingress trafficYou can create a "default" ingress isolation policy for a namespace by creating a NetworkPolicy that selects all pods but does not allow any ingress traffic to those pods.service/networking/network-policy-default-deny-ingress.yaml \--- apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: default-deny-ingress spec: podSelector: {} policyTypes: \- Ingress This ensures that even pods that aren't selected by any other NetworkPolicy will still be isolated for ingress. This policy does not affect isolation for egress from any pod.Allow all ingress trafficIf you want to allow all incoming connections to all pods in a namespace, you can create a policy that explicitly allows that.service/networking/network-policy-allow-all-ingress.yaml \--- apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: allow-all-ingress spec: podSelector: {} ingress: \- {} policyTypes: \- Ingress With this policy in place, no additional policy or policies can cause any incoming connection to those pods to be denied. This policy has no effect on isolation for egress from any pod.Default deny all egress trafficYou can create a "default" egress isolation policy for a namespace by creating a NetworkPolicy that selects all pods but does not allow any egress traffic from those pods.service/networking/network-policy-default-deny-egress.yaml \--- apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: default-deny-egress spec: podSelector: {} policyTypes: \- Egress This ensures that even pods that aren't selected by any other NetworkPolicy will not be allowed egress traffic. This policy does not change the ingress isolation behavior of any pod.Allow all egress trafficIf you want to allow all connections from all pods in a namespace, you can create a policy that explicitly allows all outgoing connections from pods in that namespace.service/networking/network-policy-allow-all-egress.yaml \--- apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: allow-all-egress spec: podSelector: {} egress: \- {} policyTypes: \- Egress With this policy in place, no additional policy or policies can cause any outgoing connection from those pods to be denied. This policy has no effect on isolation for ingress to any pod.Default deny all ingress and all egress trafficYou can create a "default" policy for a namespace which prevents all ingress AND egress traffic by creating the following NetworkPolicy in that namespace.service/networking/network-policy-default-deny-all.yaml \--- apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: default-deny-all spec: podSelector: {} policyTypes: \- Ingress \- Egress This ensures that even pods that aren't selected by any other NetworkPolicy will not be allowed ingress or egress traffic.Network traffic filteringNetworkPolicy is defined for layer 4 connections (TCP, UDP, and optionally SCTP). For all the other protocols, the behaviour may vary across network plugins.Note:You must be using a CNI plugin that supports SCTP protocol NetworkPolicies.When a deny all network policy is defined, it is only guaranteed to deny TCP, UDP and SCTP connections. For other protocols, such as ARP or ICMP, the behaviour is undefined. The same applies to allow rules: when a specific pod is allowed as ingress source or egress destination, it is undefined what happens with (for example) ICMP packets. Protocols such as ICMP may be allowed by some network plugins and denied by others.Targeting a range of portsFEATURE STATE: Kubernetes v1.25 [stable]When writing a NetworkPolicy, you can target a range of ports instead of a single port.This is achievable with the usage of the endPort field, as the following example:service/networking/networkpolicy-multiport-egress.yaml apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: multi-port-egress namespace: default spec: podSelector: matchLabels: role: db policyTypes: \- Egress egress: \- to: \- ipBlock: cidr: 10.0.0.0/24 ports: \- protocol: TCP port: 32000 endPort: 32768 The above rule allows any Pod with label role=db on the namespace default to communicate with any IP within the range 10.0.0.0/24 over TCP, provided that the target port is between the range 32000 and 32768.The following restrictions apply when using this field:The endPort field must be equal to or greater than the port field.endPort can only be defined if port is also defined.Both ports must be numeric.Note:Your cluster must be using a CNI plugin that supports the endPort field in NetworkPolicy specifications. If your network plugin does not support the endPort field and you specify a NetworkPolicy with that, the policy will be applied only for the single port field.Targeting multiple namespaces by labelIn this scenario, your Egress NetworkPolicy targets more than one namespace using their label names. For this to work, you need to label the target namespaces. For example:kubectl label namespace frontend namespace=frontend kubectl label namespace backend namespace=backend Add the labels under namespaceSelector in your NetworkPolicy document. For example:apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: egress-namespaces spec: podSelector: matchLabels: app: myapp policyTypes: \- Egress egress: \- to: \- namespaceSelector: matchExpressions: \- key: namespace operator: In values: ["frontend", "backend"] Note:It is not possible to directly specify the name of the namespaces in a NetworkPolicy. You must use a namespaceSelector with matchLabels or matchExpressions to select the namespaces based on their labels.Targeting a Namespace by its nameThe Kubernetes control plane sets an immutable label kubernetes.io/metadata.name on all namespaces, the value of the label is the namespace name.While NetworkPolicy cannot target a namespace by its name with some object field, you can use the standardized label to target a specific namespace.Pod lifecycleNote:The following applies to clusters with a conformant networking plugin and a conformant implementation of NetworkPolicy.When a new NetworkPolicy object is created, it may take some time for a network plugin to handle the new object. If a pod that is affected by a NetworkPolicy is created before the network plugin has completed NetworkPolicy handling, that pod may be started unprotected, and isolation rules will be applied when the NetworkPolicy handling is completed.Once the NetworkPolicy is handled by a network plugin,All newly created pods affected by a given NetworkPolicy will be isolated before they are started. Implementations of NetworkPolicy must ensure that filtering is effective throughout the Pod lifecycle, even from the very first instant that any container in that Pod is started. Because they are applied at Pod level, NetworkPolicies apply equally to init containers, sidecar containers, and regular containers.Allow rules will be applied eventually after the isolation rules (or may be applied at the same time). In the worst case, a newly created pod may have no network connectivity at all when it is first started, if isolation rules were already applied, but no allow rules were applied yet.Every created NetworkPolicy will be handled by a network plugin eventually, but there is no way to tell from the Kubernetes API when exactly that happens.Therefore, pods must be resilient against being started up with different network connectivity than expected. If you need to make sure the pod can reach certain destinations before being started, you can use an init container to wait for those destinations to be reachable before kubelet starts the app containers.Every NetworkPolicy will be applied to all selected pods eventually. Because the network plugin may implement NetworkPolicy in a distributed manner, it is possible that pods may see a slightly inconsistent view of network policies when the pod is first created, or when pods or policies change. For example, a newly-created pod that is supposed to be able to reach both Pod A on Node 1 and Pod B on Node 2 may find that it can reach Pod A immediately, but cannot reach Pod B until a few seconds later.NetworkPolicy and hostNetwork podsNetworkPolicy behaviour for hostNetwork pods is undefined, but it should be limited to 2 possibilities:The network plugin can distinguish hostNetwork pod traffic from all other traffic (including being able to distinguish traffic from different hostNetwork pods on the same node), and will apply NetworkPolicy to hostNetwork pods just like it does to pod-network pods.The network plugin cannot properly distinguish hostNetwork pod traffic, and so it ignores hostNetwork pods when matching podSelector and namespaceSelector. Traffic to/from hostNetwork pods is treated the same as all other traffic to/from the node IP. (This is the most common implementation.)This applies whena hostNetwork pod is selected by spec.podSelector. ... spec: podSelector: matchLabels: role: client ... a hostNetwork pod is selected by a podSelector or namespaceSelector in an ingress or egress rule. ... ingress: \- from: \- podSelector: matchLabels: role: client ... At the same time, since hostNetwork pods have the same IP addresses as the nodes they reside on, their connections will be treated as node connections. For example, you can allow traffic from a hostNetwork Pod using an ipBlock rule.What you can't do with network policies (at least, not yet)As of Kubernetes 1.33, the following functionality does not exist in the NetworkPolicy API, but you might be able to implement workarounds using Operating System components (such as SELinux, OpenVSwitch, IPTables, and so on) or Layer 7 technologies (Ingress controllers, Service Mesh implementations) or admission controllers. In case you are new to network security in Kubernetes, its worth noting that the following User Stories cannot (yet) be implemented using the NetworkPolicy API.Forcing internal cluster traffic to go through a common gateway (this might be best served with a service mesh or other proxy).Anything TLS related (use a service mesh or ingress controller for this).Node specific policies (you can use CIDR notation for these, but you cannot target nodes by their Kubernetes identities specifically).Targeting of services by name (you can, however, target pods or namespaces by their labels, which is often a viable workaround).Creation or management of "Policy requests" that are fulfilled by a third party.Default policies which are applied to all namespaces or pods (there are some third party Kubernetes distributions and projects which can do this).Advanced policy querying and reachability tooling.The ability to log network security events (for example connections that are blocked or accepted).The ability to explicitly deny policies (currently the model for NetworkPolicies are deny by default, with only the ability to add allow rules).The ability to prevent loopback or incoming host traffic (Pods cannot currently block localhost access, nor do they have the ability to block access from their resident node).NetworkPolicy's impact on existing connectionsWhen the set of NetworkPolicies that applies to an existing connection changes - this could happen either due to a change in NetworkPolicies or if the relevant labels of the namespaces/pods selected by the policy (both subject and peers) are changed in the middle of an existing connection - it is implementation defined as to whether the change will take effect for that existing connection or not. Example: A policy is created that leads to denying a previously allowed connection, the underlying network plugin implementation is responsible for defining if that new policy will close the existing connections or not. It is recommended not to modify policies/pods/namespaces in ways that might affect existing connections.What's nextSee the Declare Network Policy walkthrough for further examples.See more recipes for common scenarios enabled by the NetworkPolicy resource.3.5.7 - DNS for Services and PodsYour workload can discover Services within your cluster using DNS; this page explains how that works.Kubernetes creates DNS records for Services and Pods. You can contact Services with consistent DNS names instead of IP addresses.Kubernetes publishes information about Pods and Services which is used to program DNS. kubelet configures Pods' DNS so that running containers can look up Services by name rather than IP.Services defined in the cluster are assigned DNS names. By default, a client Pod's DNS search list includes the Pod's own namespace and the cluster's default domain.Namespaces of ServicesA DNS query may return different results based on the namespace of the Pod making it. DNS queries that don't specify a namespace are limited to the Pod's namespace. Access Services in other namespaces by specifying it in the DNS query.For example, consider a Pod in a test namespace. A data Service is in the prod namespace.A query for data returns no results, because it uses the Pod's test namespace.A query for data.prod returns the intended result, because it specifies the namespace.DNS queries may be expanded using the Pod's /etc/resolv.conf. kubelet configures this file for each Pod. For example, a query for just data may be expanded to data.test.svc.cluster.local. The values of the search option are used to expand queries. To learn more about DNS queries, see the resolv.conf manual page.nameserver 10.32.0.10 search .svc.cluster.local svc.cluster.local cluster.local options ndots:5 In summary, a Pod in the test namespace can successfully resolve either data.prod or data.prod.svc.cluster.local.DNS RecordsWhat objects get DNS records?ServicesPodsThe following sections detail the supported DNS record types and layout that is supported. Any other layout or names or queries that happen to work are considered implementation details and are subject to change without warning. For more up-to-date specification, see Kubernetes DNS-Based Service Discovery.ServicesA/AAAA records"Normal" (not headless) Services are assigned DNS A and/or AAAA records, depending on the IP family or families of the Service, with a name of the form my-svc.my-namespace.svc.cluster-domain.example. This resolves to the cluster IP of the Service.Headless Services (without a cluster IP) are also assigned DNS A and/or AAAA records, with a name of the form my-svc.my-namespace.svc.cluster-domain.example. Unlike normal Services, this resolves to the set of IPs of all of the Pods selected by the Service. Clients are expected to consume the set or else use standard round-robin selection from the set.SRV recordsSRV Records are created for named ports that are part of normal or headless services.For each named port, the SRV record has the form _port-name._port-protocol.my-svc.my-namespace.svc.cluster-domain.example.For a regular Service, this resolves to the port number and the domain name: my-svc.my-namespace.svc.cluster-domain.example.For a headless Service, this resolves to multiple answers, one for each Pod that is backing the Service, and contains the port number and the domain name of the Pod of the form hostname.my-svc.my-namespace.svc.cluster-domain.example.PodsA/AAAA recordsKube-DNS versions, prior to the implementation of the DNS specification, had the following DNS resolution:pod-ipv4-address.my-namespace.pod.cluster-domain.example For example, if a Pod in the default namespace has the IP address 172.17.0.3, and the domain name for your cluster is cluster.local, then the Pod has a DNS name:172-17-0-3.default.pod.cluster.local Some cluster DNS mechanisms, like CoreDNS, also provide A records for:...svc. Pod's hostname and subdomain fieldsCurrently when a Pod is created, its hostname (as observed from within the Pod) is the Pod's metadata.name value.The Pod spec has an optional hostname field, which can be used to specify a different hostname. When specified, it takes precedence over the Pod's name to be the hostname of the Pod (again, as observed from within the Pod). For example, given a Pod with spec.hostname set to "my-host", the Pod will have its hostname set to "my-host".The Pod spec also has an optional subdomain field which can be used to indicate that the pod is part of sub-group of the namespace. For example, a Pod with spec.hostname set to "foo", and spec.subdomain set to "bar", in namespace "my-namespace", will have its hostname set to "foo" and its fully qualified domain name (FQDN) set to "foo.bar.my-namespace.svc.cluster.local" (once more, as observed from within the Pod).If there exists a headless Service in the same namespace as the Pod, with the same name as the subdomain, the cluster's DNS Server also returns A and/or AAAA records for the Pod's fully qualified hostname.Example:apiVersion: v1 kind: Service metadata: name: busybox-subdomain spec: selector: name: busybox clusterIP: None ports: \- name: foo # name is not required for single-port Services port: 1234 \--- apiVersion: v1 kind: Pod metadata: name: busybox1 labels: name: busybox spec: hostname: busybox-1 subdomain: busybox-subdomain containers: \- image: busybox:1.28 command: \- sleep \- "3600" name: busybox \--- apiVersion: v1 kind: Pod metadata: name: busybox2 labels: name: busybox spec: hostname: busybox-2 subdomain: busybox-subdomain containers: \- image: busybox:1.28 command: \- sleep \- "3600" name: busybox Given the above Service "busybox-subdomain" and the Pods which set spec.subdomain to "busybox-subdomain", the first Pod will see its own FQDN as "busybox-1.busybox-subdomain.my-namespace.svc.cluster-domain.example". DNS serves A and/or AAAA records at that name, pointing to the Pod's IP. Both Pods "busybox1" and "busybox2" will have their own address records.An EndpointSlice can specify the DNS hostname for any endpoint addresses, along with its IP.Note:A and AAAA records are not created for Pod names since hostname is missing for the Pod. A Pod with no hostname but with subdomain will only create the A or AAAA record for the headless Service (busybox-subdomain.my-namespace.svc.cluster-domain.example), pointing to the Pods' IP addresses. Also, the Pod needs to be ready in order to have a record unless publishNotReadyAddresses=True is set on the Service.Pod's setHostnameAsFQDN fieldFEATURE STATE: Kubernetes v1.22 [stable]When a Pod is configured to have fully qualified domain name (FQDN), its hostname is the short hostname. For example, if you have a Pod with the fully qualified domain name busybox-1.busybox-subdomain.my-namespace.svc.cluster-domain.example, then by default the hostname command inside that Pod returns busybox-1 and the hostname --fqdn command returns the FQDN.When you set setHostnameAsFQDN: true in the Pod spec, the kubelet writes the Pod's FQDN into the hostname for that Pod's namespace. In this case, both hostname and hostname --fqdn return the Pod's FQDN.Note:In Linux, the hostname field of the kernel (the nodename field of struct utsname) is limited to 64 characters.If a Pod enables this feature and its FQDN is longer than 64 character, it will fail to start. The Pod will remain in Pending status (ContainerCreating as seen by kubectl) generating error events, such as Failed to construct FQDN from Pod hostname and cluster domain, FQDN long-FQDN is too long (64 characters is the max, 70 characters requested). One way of improving user experience for this scenario is to create an admission webhook controller to control FQDN size when users create top level objects, for example, Deployment.Pod's DNS PolicyDNS policies can be set on a per-Pod basis. Currently Kubernetes supports the following Pod-specific DNS policies. These policies are specified in the dnsPolicy field of a Pod Spec."Default": The Pod inherits the name resolution configuration from the node that the Pods run on. See related discussion for more details."ClusterFirst": Any DNS query that does not match the configured cluster domain suffix, such as "www.kubernetes.io", is forwarded to an upstream nameserver by the DNS server. Cluster administrators may have extra stub-domain and upstream DNS servers configured. See related discussion for details on how DNS queries are handled in those cases."ClusterFirstWithHostNet": For Pods running with hostNetwork, you should explicitly set its DNS policy to "ClusterFirstWithHostNet". Otherwise, Pods running with hostNetwork and "ClusterFirst" will fallback to the behavior of the "Default" policy.Note:This is not supported on Windows. See below for details."None": It allows a Pod to ignore DNS settings from the Kubernetes environment. All DNS settings are supposed to be provided using the dnsConfig field in the Pod Spec. See Pod's DNS config subsection below.Note:"Default" is not the default DNS policy. If dnsPolicy is not explicitly specified, then "ClusterFirst" is used.The example below shows a Pod with its DNS policy set to "ClusterFirstWithHostNet" because it has hostNetwork set to true.apiVersion: v1 kind: Pod metadata: name: busybox namespace: default spec: containers: \- image: busybox:1.28 command: \- sleep \- "3600" imagePullPolicy: IfNotPresent name: busybox restartPolicy: Always hostNetwork: true dnsPolicy: ClusterFirstWithHostNet Pod's DNS ConfigFEATURE STATE: Kubernetes v1.14 [stable]Pod's DNS Config allows users more control on the DNS settings for a Pod.The dnsConfig field is optional and it can work with any dnsPolicy settings. However, when a Pod's dnsPolicy is set to "None", the dnsConfig field has to be specified.Below are the properties a user can specify in the dnsConfig field:nameservers: a list of IP addresses that will be used as DNS servers for the Pod. There can be at most 3 IP addresses specified. When the Pod's dnsPolicy is set to "None", the list must contain at least one IP address, otherwise this property is optional. The servers listed will be combined to the base nameservers generated from the specified DNS policy with duplicate addresses removed.searches: a list of DNS search domains for hostname lookup in the Pod. This property is optional. When specified, the provided list will be merged into the base search domain names generated from the chosen DNS policy. Duplicate domain names are removed. Kubernetes allows up to 32 search domains.options: an optional list of objects where each object may have a name property (required) and a value property (optional). The contents in this property will be merged to the options generated from the specified DNS policy. Duplicate entries are removed.The following is an example Pod with custom DNS settings:service/networking/custom-dns.yaml apiVersion: v1 kind: Pod metadata: namespace: default name: dns-example spec: containers: \- name: test image: nginx dnsPolicy: "None" dnsConfig: nameservers: \- 192.0.2.1 # this is an example searches: \- ns1.svc.cluster-domain.example \- my.dns.search.suffix options: \- name: ndots value: "2" \- name: edns0 When the Pod above is created, the container test gets the following contents in its /etc/resolv.conf file:nameserver 192.0.2.1 search ns1.svc.cluster-domain.example my.dns.search.suffix options ndots:2 edns0 For IPv6 setup, search path and name server should be set up like this:kubectl exec -it dns-example -- cat /etc/resolv.conf The output is similar to this:nameserver 2001:db8:30::a search default.svc.cluster-domain.example svc.cluster-domain.example cluster-domain.example options ndots:5 DNS search domain list limitsFEATURE STATE: Kubernetes 1.28 [stable]Kubernetes itself does not limit the DNS Config until the length of the search domain list exceeds 32 or the total length of all search domains exceeds 2048. This limit applies to the node's resolver configuration file, the Pod's DNS Config, and the merged DNS Config respectively.Note:Some container runtimes of earlier versions may have their own restrictions on the number of DNS search domains. Depending on the container runtime environment, the pods with a large number of DNS search domains may get stuck in the pending state.It is known that containerd v1.5.5 or earlier and CRI-O v1.21 or earlier have this problem.DNS resolution on Windows nodesClusterFirstWithHostNet is not supported for Pods that run on Windows nodes. Windows treats all names with a . as a FQDN and skips FQDN resolution.On Windows, there are multiple DNS resolvers that can be used. As these come with slightly different behaviors, using the Resolve-DNSName powershell cmdlet for name query resolutions is recommended.On Linux, you have a DNS suffix list, which is used after resolution of a name as fully qualified has failed. On Windows, you can only have 1 DNS suffix, which is the DNS suffix associated with that Pod's namespace (example: mydns.svc.cluster.local). Windows can resolve FQDNs, Services, or network name which can be resolved with this single suffix. For example, a Pod spawned in the default namespace, will have the DNS suffix default.svc.cluster.local. Inside a Windows Pod, you can resolve both kubernetes.default.svc.cluster.local and kubernetes, but not the partially qualified names (kubernetes.default or kubernetes.default.svc).What's nextFor guidance on administering DNS configurations, check Configure DNS Service.3.5.8 - IPv4/IPv6 dual-stackKubernetes lets you configure single-stack IPv4 networking, single-stack IPv6 networking, or dual stack networking with both network families active. This page explains how.FEATURE STATE: Kubernetes v1.23 [stable]IPv4/IPv6 dual-stack networking enables the allocation of both IPv4 and IPv6 addresses to Pods and Services.IPv4/IPv6 dual-stack networking is enabled by default for your Kubernetes cluster starting in 1.21, allowing the simultaneous assignment of both IPv4 and IPv6 addresses.Supported FeaturesIPv4/IPv6 dual-stack on your Kubernetes cluster provides the following features:Dual-stack Pod networking (a single IPv4 and IPv6 address assignment per Pod)IPv4 and IPv6 enabled ServicesPod off-cluster egress routing (eg. the Internet) via both IPv4 and IPv6 interfacesPrerequisitesThe following prerequisites are needed in order to utilize IPv4/IPv6 dual-stack Kubernetes clusters:Kubernetes 1.20 or laterFor information about using dual-stack services with earlier Kubernetes versions, refer to the documentation for that version of Kubernetes.Provider support for dual-stack networking (Cloud provider or otherwise must be able to provide Kubernetes nodes with routable IPv4/IPv6 network interfaces)A network plugin that supports dual-stack networking.Configure IPv4/IPv6 dual-stackTo configure IPv4/IPv6 dual-stack, set dual-stack cluster network assignments:kube-apiserver:--service-cluster-ip-range=,kube-controller-manager:--cluster-cidr=,\--service-cluster-ip-range=,\--node-cidr-mask-size-ipv4|--node-cidr-mask-size-ipv6 defaults to /24 for IPv4 and /64 for IPv6kube-proxy:--cluster-cidr=,kubelet:--node-ip=,This option is required for bare metal dual-stack nodes (nodes that do not define a cloud provider with the --cloud-provider flag). If you are using a cloud provider and choose to override the node IPs chosen by the cloud provider, set the \--node-ip option.(The legacy built-in cloud providers do not support dual-stack --node-ip.)Note:An example of an IPv4 CIDR: 10.244.0.0/16 (though you would supply your own address range)An example of an IPv6 CIDR: fdXY:IJKL:MNOP:15::/64 (this shows the format but is not a valid address - see RFC 4193)ServicesYou can create Services which can use IPv4, IPv6, or both.The address family of a Service defaults to the address family of the first service cluster IP range (configured via the --service-cluster-ip-range flag to the kube-apiserver).When you define a Service you can optionally configure it as dual stack. To specify the behavior you want, you set the .spec.ipFamilyPolicy field to one of the following values:SingleStack: Single-stack service. The control plane allocates a cluster IP for the Service, using the first configured service cluster IP range.PreferDualStack: Allocates both IPv4 and IPv6 cluster IPs for the Service when dual-stack is enabled. If dual-stack is not enabled or supported, it falls back to single-stack behavior.RequireDualStack: Allocates Service .spec.clusterIPs from both IPv4 and IPv6 address ranges when dual-stack is enabled. If dual-stack is not enabled or supported, the Service API object creation fails.Selects the .spec.clusterIP from the list of .spec.clusterIPs based on the address family of the first element in the .spec.ipFamilies array.If you would like to define which IP family to use for single stack or define the order of IP families for dual-stack, you can choose the address families by setting an optional field, .spec.ipFamilies, on the Service.Note:The .spec.ipFamilies field is conditionally mutable: you can add or remove a secondary IP address family, but you cannot change the primary IP address family of an existing Service.You can set .spec.ipFamilies to any of the following array values:["IPv4"]["IPv6"]["IPv4","IPv6"] (dual stack)["IPv6","IPv4"] (dual stack)The first family you list is used for the legacy .spec.clusterIP field.Dual-stack Service configuration scenariosThese examples demonstrate the behavior of various dual-stack Service configuration scenarios.Dual-stack options on new ServicesThis Service specification does not explicitly define .spec.ipFamilyPolicy. When you create this Service, Kubernetes assigns a cluster IP for the Service from the first configured service-cluster-ip-range and sets the .spec.ipFamilyPolicy to SingleStack. (Services without selectors and headless Services with selectors will behave in this same way.)service/networking/dual-stack-default-svc.yaml apiVersion: v1 kind: Service metadata: name: my-service labels: app.kubernetes.io/name: MyApp spec: selector: app.kubernetes.io/name: MyApp ports: \- protocol: TCP port: 80 This Service specification explicitly defines PreferDualStack in .spec.ipFamilyPolicy. When you create this Service on a dual-stack cluster, Kubernetes assigns both IPv4 and IPv6 addresses for the service. The control plane updates the .spec for the Service to record the IP address assignments. The field .spec.clusterIPs is the primary field, and contains both assigned IP addresses; .spec.clusterIP is a secondary field with its value calculated from .spec.clusterIPs.For the .spec.clusterIP field, the control plane records the IP address that is from the same address family as the first service cluster IP range.On a single-stack cluster, the .spec.clusterIPs and .spec.clusterIP fields both only list one address.On a cluster with dual-stack enabled, specifying RequireDualStack in .spec.ipFamilyPolicy behaves the same as PreferDualStack.service/networking/dual-stack-preferred-svc.yaml apiVersion: v1 kind: Service metadata: name: my-service labels: app.kubernetes.io/name: MyApp spec: ipFamilyPolicy: PreferDualStack selector: app.kubernetes.io/name: MyApp ports: \- protocol: TCP port: 80 This Service specification explicitly defines IPv6 and IPv4 in .spec.ipFamilies as well as defining PreferDualStack in .spec.ipFamilyPolicy. When Kubernetes assigns an IPv6 and IPv4 address in .spec.clusterIPs, .spec.clusterIP is set to the IPv6 address because that is the first element in the .spec.clusterIPs array, overriding the default.service/networking/dual-stack-preferred-ipfamilies-svc.yaml apiVersion: v1 kind: Service metadata: name: my-service labels: app.kubernetes.io/name: MyApp spec: ipFamilyPolicy: PreferDualStack ipFamilies: \- IPv6 \- IPv4 selector: app.kubernetes.io/name: MyApp ports: \- protocol: TCP port: 80 Dual-stack defaults on existing ServicesThese examples demonstrate the default behavior when dual-stack is newly enabled on a cluster where Services already exist. (Upgrading an existing cluster to 1.21 or beyond will enable dual-stack.)When dual-stack is enabled on a cluster, existing Services (whether IPv4 or IPv6) are configured by the control plane to set .spec.ipFamilyPolicy to SingleStack and set .spec.ipFamilies to the address family of the existing Service. The existing Service cluster IP will be stored in .spec.clusterIPs.service/networking/dual-stack-default-svc.yaml apiVersion: v1 kind: Service metadata: name: my-service labels: app.kubernetes.io/name: MyApp spec: selector: app.kubernetes.io/name: MyApp ports: \- protocol: TCP port: 80 You can validate this behavior by using kubectl to inspect an existing service.kubectl get svc my-service -o yaml apiVersion: v1 kind: Service metadata: labels: app.kubernetes.io/name: MyApp name: my-service spec: clusterIP: 10.0.197.123 clusterIPs: \- 10.0.197.123 ipFamilies: \- IPv4 ipFamilyPolicy: SingleStack ports: \- port: 80 protocol: TCP targetPort: 80 selector: app.kubernetes.io/name: MyApp type: ClusterIP status: loadBalancer: {} When dual-stack is enabled on a cluster, existing headless Services with selectors are configured by the control plane to set .spec.ipFamilyPolicy to SingleStack and set .spec.ipFamilies to the address family of the first service cluster IP range (configured via the \--service-cluster-ip-range flag to the kube-apiserver) even though .spec.clusterIP is set to None.service/networking/dual-stack-default-svc.yaml apiVersion: v1 kind: Service metadata: name: my-service labels: app.kubernetes.io/name: MyApp spec: selector: app.kubernetes.io/name: MyApp ports: \- protocol: TCP port: 80 You can validate this behavior by using kubectl to inspect an existing headless service with selectors.kubectl get svc my-service -o yaml apiVersion: v1 kind: Service metadata: labels: app.kubernetes.io/name: MyApp name: my-service spec: clusterIP: None clusterIPs: \- None ipFamilies: \- IPv4 ipFamilyPolicy: SingleStack ports: \- port: 80 protocol: TCP targetPort: 80 selector: app.kubernetes.io/name: MyApp Switching Services between single-stack and dual-stackServices can be changed from single-stack to dual-stack and from dual-stack to single-stack.To change a Service from single-stack to dual-stack, change .spec.ipFamilyPolicy from SingleStack to PreferDualStack or RequireDualStack as desired. When you change this Service from single-stack to dual-stack, Kubernetes assigns the missing address family so that the Service now has IPv4 and IPv6 addresses.Edit the Service specification updating the .spec.ipFamilyPolicy from SingleStack to PreferDualStack.Before:spec: ipFamilyPolicy: SingleStack After:spec: ipFamilyPolicy: PreferDualStack To change a Service from dual-stack to single-stack, change .spec.ipFamilyPolicy from PreferDualStack or RequireDualStack to SingleStack. When you change this Service from dual-stack to single-stack, Kubernetes retains only the first element in the .spec.clusterIPs array, and sets .spec.clusterIP to that IP address and sets .spec.ipFamilies to the address family of .spec.clusterIPs.Headless Services without selectorFor Headless Services without selectors and without .spec.ipFamilyPolicy explicitly set, the .spec.ipFamilyPolicy field defaults to RequireDualStack.Service type LoadBalancerTo provision a dual-stack load balancer for your Service:Set the .spec.type field to LoadBalancerSet .spec.ipFamilyPolicy field to PreferDualStack or RequireDualStackNote:To use a dual-stack LoadBalancer type Service, your cloud provider must support IPv4 and IPv6 load balancers.Egress trafficIf you want to enable egress traffic in order to reach off-cluster destinations (eg. the public Internet) from a Pod that uses non-publicly routable IPv6 addresses, you need to enable the Pod to use a publicly routed IPv6 address via a mechanism such as transparent proxying or IP masquerading. The ip-masq-agent project supports IP masquerading on dual-stack clusters.Note:Ensure your CNI provider supports IPv6.Windows supportKubernetes on Windows does not support single-stack "IPv6-only" networking. However, dual-stack IPv4/IPv6 networking for pods and nodes with single-family services is supported.You can use IPv4/IPv6 dual-stack networking with l2bridge networks.Note:Overlay (VXLAN) networks on Windows do not support dual-stack networking.You can read more about the different network modes for Windows within the Networking on Windows topic.What's nextValidate IPv4/IPv6 dual-stack networkingEnable dual-stack networking using kubeadm3.5.9 - Topology Aware RoutingTopology Aware Routing provides a mechanism to help keep network traffic within the zone where it originated. Preferring same-zone traffic between Pods in your cluster can help with reliability, performance (network latency and throughput), or cost.FEATURE STATE: Kubernetes v1.23 [beta]Note:Prior to Kubernetes 1.27, this feature was known as Topology Aware Hints.Topology Aware Routing adjusts routing behavior to prefer keeping traffic in the zone it originated from. In some cases this can help reduce costs or improve network performance.MotivationKubernetes clusters are increasingly deployed in multi-zone environments. Topology Aware Routing provides a mechanism to help keep traffic within the zone it originated from. When calculating the endpoints for a Service, the EndpointSlice controller considers the topology (region and zone) of each endpoint and populates the hints field to allocate it to a zone. Cluster components such as kube-proxy can then consume those hints, and use them to influence how the traffic is routed (favoring topologically closer endpoints).Enabling Topology Aware RoutingNote:Prior to Kubernetes 1.27, this behavior was controlled using the service.kubernetes.io/topology-aware-hints annotation.You can enable Topology Aware Routing for a Service by setting the service.kubernetes.io/topology-mode annotation to Auto. When there are enough endpoints available in each zone, Topology Hints will be populated on EndpointSlices to allocate individual endpoints to specific zones, resulting in traffic being routed closer to where it originated from.When it works bestThis feature works best when:1. Incoming traffic is evenly distributedIf a large proportion of traffic is originating from a single zone, that traffic could overload the subset of endpoints that have been allocated to that zone. This feature is not recommended when incoming traffic is expected to originate from a single zone.2. The Service has 3 or more endpoints per zoneIn a three zone cluster, this means 9 or more endpoints. If there are fewer than 3 endpoints per zone, there is a high (≈50%) probability that the EndpointSlice controller will not be able to allocate endpoints evenly and instead will fall back to the default cluster-wide routing approach.How It WorksThe "Auto" heuristic attempts to proportionally allocate a number of endpoints to each zone. Note that this heuristic works best for Services that have a significant number of endpoints.EndpointSlice controllerThe EndpointSlice controller is responsible for setting hints on EndpointSlices when this heuristic is enabled. The controller allocates a proportional amount of endpoints to each zone. This proportion is based on the allocatable CPU cores for nodes running in that zone. For example, if one zone had 2 CPU cores and another zone only had 1 CPU core, the controller would allocate twice as many endpoints to the zone with 2 CPU cores.The following example shows what an EndpointSlice looks like when hints have been populated:apiVersion: discovery.k8s.io/v1 kind: EndpointSlice metadata: name: example-hints labels: kubernetes.io/service-name: example-svc addressType: IPv4 ports: \- name: http protocol: TCP port: 80 endpoints: \- addresses: \- "10.1.2.3" conditions: ready: true hostname: pod-1 zone: zone-a hints: forZones: \- name: "zone-a" kube-proxyThe kube-proxy component filters the endpoints it routes to based on the hints set by the EndpointSlice controller. In most cases, this means that the kube-proxy is able to route traffic to endpoints in the same zone. Sometimes the controller allocates endpoints from a different zone to ensure more even distribution of endpoints between zones. This would result in some traffic being routed to other zones.SafeguardsThe Kubernetes control plane and the kube-proxy on each node apply some safeguard rules before using Topology Aware Hints. If these don't check out, the kube-proxy selects endpoints from anywhere in your cluster, regardless of the zone.Insufficient number of endpoints: If there are less endpoints than zones in a cluster, the controller will not assign any hints.Impossible to achieve balanced allocation: In some cases, it will be impossible to achieve a balanced allocation of endpoints among zones. For example, if zone-a is twice as large as zone-b, but there are only 2 endpoints, an endpoint allocated to zone-a may receive twice as much traffic as zone-b. The controller does not assign hints if it can't get this "expected overload" value below an acceptable threshold for each zone. Importantly this is not based on real-time feedback. It is still possible for individual endpoints to become overloaded.One or more Nodes has insufficient information: If any node does not have a topology.kubernetes.io/zone label or is not reporting a value for allocatable CPU, the control plane does not set any topology-aware endpoint hints and so kube-proxy does not filter endpoints by zone.One or more endpoints does not have a zone hint: When this happens, the kube-proxy assumes that a transition from or to Topology Aware Hints is underway. Filtering endpoints for a Service in this state would be dangerous so the kube-proxy falls back to using all endpoints.A zone is not represented in hints: If the kube-proxy is unable to find at least one endpoint with a hint targeting the zone it is running in, it falls back to using endpoints from all zones. This is most likely to happen as you add a new zone into your existing cluster.ConstraintsTopology Aware Hints are not used when internalTrafficPolicy is set to Local on a Service. It is possible to use both features in the same cluster on different Services, just not on the same Service.This approach will not work well for Services that have a large proportion of traffic originating from a subset of zones. Instead this assumes that incoming traffic will be roughly proportional to the capacity of the Nodes in each zone.The EndpointSlice controller ignores unready nodes as it calculates the proportions of each zone. This could have unintended consequences if a large portion of nodes are unready.The EndpointSlice controller ignores nodes with the node-role.kubernetes.io/control-plane or node-role.kubernetes.io/master label set. This could be problematic if workloads are also running on those nodes.The EndpointSlice controller does not take into account tolerations when deploying or calculating the proportions of each zone. If the Pods backing a Service are limited to a subset of Nodes in the cluster, this will not be taken into account.This may not work well with autoscaling. For example, if a lot of traffic is originating from a single zone, only the endpoints allocated to that zone will be handling that traffic. That could result in Horizontal Pod Autoscaler either not picking up on this event, or newly added pods starting in a different zone.Custom heuristicsKubernetes is deployed in many different ways, there is no single heuristic for allocating endpoints to zones will work for every use case. A key goal of this feature is to enable custom heuristics to be developed if the built in heuristic does not work for your use case. The first steps to enable custom heuristics were included in the 1.27 release. This is a limited implementation that may not yet cover some relevant and plausible situations.What's nextFollow the Connecting Applications with Services tutorialLearn about the trafficDistribution field, which is closely related to the service.kubernetes.io/topology-mode annotation and provides flexible options for traffic routing within Kubernetes.3.5.10 - Networking on WindowsKubernetes supports running nodes on either Linux or Windows. You can mix both kinds of node within a single cluster. This page provides an overview to networking specific to the Windows operating system.Container networking on WindowsNetworking for Windows containers is exposed through CNI plugins. Windows containers function similarly to virtual machines in regards to networking. Each container has a virtual network adapter (vNIC) which is connected to a Hyper-V virtual switch (vSwitch). The Host Networking Service (HNS) and the Host Compute Service (HCS) work together to create containers and attach container vNICs to networks. HCS is responsible for the management of containers whereas HNS is responsible for the management of networking resources such as:Virtual networks (including creation of vSwitches)Endpoints / vNICsNamespacesPolicies including packet encapsulations, load-balancing rules, ACLs, and NAT rules.The Windows HNS and vSwitch implement namespacing and can create virtual NICs as needed for a pod or container. However, many configurations such as DNS, routes, and metrics are stored in the Windows registry database rather than as files inside /etc, which is how Linux stores those configurations. The Windows registry for the container is separate from that of the host, so concepts like mapping /etc/resolv.conf from the host into a container don't have the same effect they would on Linux. These must be configured using Windows APIs run in the context of that container. Therefore CNI implementations need to call the HNS instead of relying on file mappings to pass network details into the pod or container.Network modesWindows supports five different networking drivers/modes: L2bridge, L2tunnel, Overlay (Beta), Transparent, and NAT. In a heterogeneous cluster with Windows and Linux worker nodes, you need to select a networking solution that is compatible on both Windows and Linux. The following table lists the out-of-tree plugins are supported on Windows, with recommendations on when to use each CNI:Network DriverDescriptionContainer Packet ModificationsNetwork PluginsNetwork Plugin CharacteristicsL2bridgeContainers are attached to an external vSwitch. Containers are attached to the underlay network, although the physical network doesn't need to learn the container MACs because they are rewritten on ingress/egress.MAC is rewritten to host MAC, IP may be rewritten to host IP using HNS OutboundNAT policy.win-bridge, Azure-CNI, Flannel host-gateway uses win-bridgewin-bridge uses L2bridge network mode, connects containers to the underlay of hosts, offering best performance. Requires user-defined routes (UDR) for inter-node connectivity.L2TunnelThis is a special case of l2bridge, but only used on Azure. All packets are sent to the virtualization host where SDN policy is applied.MAC rewritten, IP visible on the underlay networkAzure-CNIAzure-CNI allows integration of containers with Azure vNET, and allows them to leverage the set of capabilities that Azure Virtual Network provides. For example, securely connect to Azure services or use Azure NSGs. See azure-cni for some examplesOverlayContainers are given a vNIC connected to an external vSwitch. Each overlay network gets its own IP subnet, defined by a custom IP prefix.The overlay network driver uses VXLAN encapsulation.Encapsulated with an outer header.win-overlay, Flannel VXLAN (uses win-overlay)win-overlay should be used when virtual container networks are desired to be isolated from underlay of hosts (e.g. for security reasons). Allows for IPs to be re-used for different overlay networks (which have different VNID tags) if you are restricted on IPs in your datacenter. This option requires KB4489899 on Windows Server 2019.Transparent (special use case for ovn-kubernetes)Requires an external vSwitch. Containers are attached to an external vSwitch which enables intra-pod communication via logical networks (logical switches and routers).Packet is encapsulated either via GENEVE or STT tunneling to reach pods which are not on the same host.Packets are forwarded or dropped via the tunnel metadata information supplied by the ovn network controller.NAT is done for north-south communication.ovn-kubernetesDeploy via ansible. Distributed ACLs can be applied via Kubernetes policies. IPAM support. Load-balancing can be achieved without kube-proxy. NATing is done without using iptables/netsh.NAT (not used in Kubernetes)Containers are given a vNIC connected to an internal vSwitch. DNS/DHCP is provided using an internal component called WinNATMAC and IP is rewritten to host MAC/IP.natIncluded here for completenessAs outlined above, the Flannel CNI plugin is also supported on Windows via the VXLAN network backend (Beta support ; delegates to win-overlay) and host-gateway network backend (stable support; delegates to win-bridge).This plugin supports delegating to one of the reference CNI plugins (win-overlay, win-bridge), to work in conjunction with Flannel daemon on Windows (Flanneld) for automatic node subnet lease assignment and HNS network creation. This plugin reads in its own configuration file (cni.conf), and aggregates it with the environment variables from the FlannelD generated subnet.env file. It then delegates to one of the reference CNI plugins for network plumbing, and sends the correct configuration containing the node-assigned subnet to the IPAM plugin (for example: host-local).For Node, Pod, and Service objects, the following network flows are supported for TCP/UDP traffic:Pod → Pod (IP)Pod → Pod (Name)Pod → Service (Cluster IP)Pod → Service (PQDN, but only if there are no ".")Pod → Service (FQDN)Pod → external (IP)Pod → external (DNS)Node → PodPod → NodeIP address management (IPAM)The following IPAM options are supported on Windows:host-localazure-vnet-ipam (for azure-cni only)Windows Server IPAM (fallback option if no IPAM is set)Direct Server Return (DSR)FEATURE STATE: Kubernetes v1.33 [beta]Load balancing mode where the IP address fixups and the LBNAT occurs at the container vSwitch port directly; service traffic arrives with the source IP set as the originating pod IP. This provides performance optimizations by allowing the return traffic routed through load balancers to bypass the load balancer and respond directly to the client; reducing load on the load balancer and also reducing overall latency. For more information, read Direct Server Return (DSR) in a nutshell.Load balancing and ServicesA Kubernetes Service is an abstraction that defines a logical set of Pods and a means to access them over a network. In a cluster that includes Windows nodes, you can use the following types of Service:NodePortClusterIPLoadBalancerExternalNameWindows container networking differs in some important ways from Linux networking. The Microsoft documentation for Windows Container Networking provides additional details and background.On Windows, you can use the following settings to configure Services and load balancing behavior:Windows Service SettingsFeatureDescriptionMinimum Supported Windows OS buildHow to enableSession affinityEnsures that connections from a particular client are passed to the same Pod each time.Windows Server 2022Set service.spec.sessionAffinity to "ClientIP"Direct Server Return (DSR)See DSR notes above.Windows Server 2019Set the following command line argument (assuming version 1.33): --enable-dsr=truePreserve-DestinationSkips DNAT of service traffic, thereby preserving the virtual IP of the target service in packets reaching the backend Pod. Also disables node-node forwarding.Windows Server, version 1903Set "preserve-destination": "true" in service annotations and enable DSR in kube-proxy.IPv4/IPv6 dual-stack networkingNative IPv4-to-IPv4 in parallel with IPv6-to-IPv6 communications to, from, and within a clusterWindows Server 2019See IPv4/IPv6 dual-stackClient IP preservationEnsures that source IP of incoming ingress traffic gets preserved. Also disables node-node forwarding.Windows Server 2019Set service.spec.externalTrafficPolicy to "Local" and enable DSR in kube-proxyLimitationsThe following networking functionality is not supported on Windows nodes:Host networking modeLocal NodePort access from the node itself (works for other nodes or external clients)More than 64 backend pods (or unique destination addresses) for a single ServiceIPv6 communication between Windows pods connected to overlay networksLocal Traffic Policy in non-DSR modeOutbound communication using the ICMP protocol via the win-overlay, win-bridge, or using the Azure-CNI plugin. Specifically, the Windows data plane (VFP) doesn't support ICMP packet transpositions, and this means:ICMP packets directed to destinations within the same network (such as pod to pod communication via ping) work as expected;TCP/UDP packets work as expected;ICMP packets directed to pass through a remote network (e.g. pod to external internet communication via ping) cannot be transposed and thus will not be routed back to their source;Since TCP/UDP packets can still be transposed, you can substitute ping  with curl  when debugging connectivity with the outside world.Other limitations:Windows reference network plugins win-bridge and win-overlay do not implement CNI spec v0.4.0, due to a missing CHECK implementation.The Flannel VXLAN CNI plugin has the following limitations on Windows:Node-pod connectivity is only possible for local pods with Flannel v0.12.0 (or higher).Flannel is restricted to using VNI 4096 and UDP port 4789. See the official Flannel VXLAN backend docs for more details on these parameters.3.5.11 - Service ClusterIP allocationIn Kubernetes, Services are an abstract way to expose an application running on a set of Pods. Services can have a cluster-scoped virtual IP address (using a Service of type: ClusterIP). Clients can connect using that virtual IP address, and Kubernetes then load-balances traffic to that Service across the different backing Pods.How Service ClusterIPs are allocated?When Kubernetes needs to assign a virtual IP address for a Service, that assignment happens one of two ways:dynamicallythe cluster's control plane automatically picks a free IP address from within the configured IP range for type: ClusterIP Services.staticallyyou specify an IP address of your choice, from within the configured IP range for Services.Across your whole cluster, every Service ClusterIP must be unique. Trying to create a Service with a specific ClusterIP that has already been allocated will return an error.Why do you need to reserve Service Cluster IPs?Sometimes you may want to have Services running in well-known IP addresses, so other components and users in the cluster can use them.The best example is the DNS Service for the cluster. As a soft convention, some Kubernetes installers assign the 10th IP address from the Service IP range to the DNS service. Assuming you configured your cluster with Service IP range 10.96.0.0/16 and you want your DNS Service IP to be 10.96.0.10, you'd have to create a Service like this:apiVersion: v1 kind: Service metadata: labels: k8s-app: kube-dns kubernetes.io/cluster-service: "true" kubernetes.io/name: CoreDNS name: kube-dns namespace: kube-system spec: clusterIP: 10.96.0.10 ports: \- name: dns port: 53 protocol: UDP targetPort: 53 \- name: dns-tcp port: 53 protocol: TCP targetPort: 53 selector: k8s-app: kube-dns type: ClusterIP But, as it was explained before, the IP address 10.96.0.10 has not been reserved. If other Services are created before or in parallel with dynamic allocation, there is a chance they can allocate this IP. Hence, you will not be able to create the DNS Service because it will fail with a conflict error.How can you avoid Service ClusterIP conflicts?The allocation strategy implemented in Kubernetes to allocate ClusterIPs to Services reduces the risk of collision.The ClusterIP range is divided, based on the formula min(max(16, cidrSize / 16), 256), described as never less than 16 or more than 256 with a graduated step between them.Dynamic IP assignment uses the upper band by default, once this has been exhausted it will use the lower range. This will allow users to use static allocations on the lower band with a low risk of collision.ExamplesExample 1This example uses the IP address range: 10.96.0.0/24 (CIDR notation) for the IP addresses of Services.Range Size: 28 - 2 = 254Band Offset: min(max(16, 256/16), 256) = min(16, 256) = 16Static band start: 10.96.0.1Static band end: 10.96.0.16Range end: 10.96.0.254pie showData title 10.96.0.0/24 "Static" : 16 "Dynamic" : 238JavaScript must be enabled to view this contentExample 2This example uses the IP address range: 10.96.0.0/20 (CIDR notation) for the IP addresses of Services.Range Size: 212 - 2 = 4094Band Offset: min(max(16, 4096/16), 256) = min(256, 256) = 256Static band start: 10.96.0.1Static band end: 10.96.1.0Range end: 10.96.15.254pie showData title 10.96.0.0/20 "Static" : 256 "Dynamic" : 3838JavaScript must be enabled to view this contentExample 3This example uses the IP address range: 10.96.0.0/16 (CIDR notation) for the IP addresses of Services.Range Size: 216 - 2 = 65534Band Offset: min(max(16, 65536/16), 256) = min(4096, 256) = 256Static band start: 10.96.0.1Static band ends: 10.96.1.0Range end: 10.96.255.254pie showData title 10.96.0.0/16 "Static" : 256 "Dynamic" : 65278JavaScript must be enabled to view this contentWhat's nextRead about Service External Traffic PolicyRead about Connecting Applications with ServicesRead about Services3.5.12 - Service Internal Traffic PolicyIf two Pods in your cluster want to communicate, and both Pods are actually running on the same node, use Service Internal Traffic Policy to keep network traffic within that node. Avoiding a round trip via the cluster network can help with reliability, performance (network latency and throughput), or cost.FEATURE STATE: Kubernetes v1.26 [stable]Service Internal Traffic Policy enables internal traffic restrictions to only route internal traffic to endpoints within the node the traffic originated from. The "internal" traffic here refers to traffic originated from Pods in the current cluster. This can help to reduce costs and improve performance.Using Service Internal Traffic PolicyYou can enable the internal-only traffic policy for a Service, by setting its .spec.internalTrafficPolicy to Local. This tells kube-proxy to only use node local endpoints for cluster internal traffic.Note:For pods on nodes with no endpoints for a given Service, the Service behaves as if it has zero endpoints (for Pods on this node) even if the service does have endpoints on other nodes.The following example shows what a Service looks like when you set .spec.internalTrafficPolicy to Local:apiVersion: v1 kind: Service metadata: name: my-service spec: selector: app.kubernetes.io/name: MyApp ports: \- protocol: TCP port: 80 targetPort: 9376 internalTrafficPolicy: Local How it worksThe kube-proxy filters the endpoints it routes to based on the spec.internalTrafficPolicy setting. When it's set to Local, only node local endpoints are considered. When it's Cluster (the default), or is not set, Kubernetes considers all endpoints.What's nextRead about Topology Aware RoutingRead about Service External Traffic PolicyFollow the Connecting Applications with Services tutorial3.6 - StorageWays to provide both long-term and temporary storage to Pods in your cluster.3.6.1 - VolumesKubernetes volumes provide a way for containers in a pod to access and share data via the filesystem. There are different kinds of volume that you can use for different purposes, such as:populating a configuration file based on a ConfigMap or a Secretproviding some temporary scratch space for a podsharing a filesystem between two different containers in the same podsharing a filesystem between two different pods (even if those Pods run on different nodes)durably storing data so that it stays available even if the Pod restarts or is replacedpassing configuration information to an app running in a container, based on details of the Pod the container is in (for example: telling a sidecar container what namespace the Pod is running in)providing read-only access to data in a different container imageData sharing can be between different local processes within a container, or between different containers, or between Pods.Why volumes are importantData persistence: On-disk files in a container are ephemeral, which presents some problems for non-trivial applications when running in containers. One problem occurs when a container crashes or is stopped, the container state is not saved so all of the files that were created or modified during the lifetime of the container are lost. After a crash, kubelet restarts the container with a clean state.Shared storage: Another problem occurs when multiple containers are running in a Pod and need to share files. It can be challenging to set up and access a shared filesystem across all of the containers.The Kubernetes volume abstraction can help you to solve both of these problems.Before you learn about volumes, PersistentVolumes and PersistentVolumeClaims, you should read up about Pods and make sure that you understand how Kubernetes uses Pods to run containers.How volumes workKubernetes supports many types of volumes. A Pod can use any number of volume types simultaneously. Ephemeral volume types have a lifetime linked to a specific Pod, but persistent volumes exist beyond the lifetime of any individual pod. When a pod ceases to exist, Kubernetes destroys ephemeral volumes; however, Kubernetes does not destroy persistent volumes. For any kind of volume in a given pod, data is preserved across container restarts.At its core, a volume is a directory, possibly with some data in it, which is accessible to the containers in a pod. How that directory comes to be, the medium that backs it, and the contents of it are determined by the particular volume type used.To use a volume, specify the volumes to provide for the Pod in .spec.volumes and declare where to mount those volumes into containers in .spec.containers[*].volumeMounts.When a pod is launched, a process in the container sees a filesystem view composed from the initial contents of the container image, plus volumes (if defined) mounted inside the container. The process sees a root filesystem that initially matches the contents of the container image. Any writes to within that filesystem hierarchy, if allowed, affect what that process views when it performs a subsequent filesystem access. Volumes are mounted at specified paths within the container filesystem. For each container defined within a Pod, you must independently specify where to mount each volume that the container uses.Volumes cannot mount within other volumes (but see Using subPath for a related mechanism). Also, a volume cannot contain a hard link to anything in a different volume.Types of volumesKubernetes supports several types of volumes.awsElasticBlockStore (deprecated)In Kubernetes 1.33, all operations for the in-tree awsElasticBlockStore type are redirected to the ebs.csi.aws.com CSI driver.The AWSElasticBlockStore in-tree storage driver was deprecated in the Kubernetes v1.19 release and then removed entirely in the v1.27 release.The Kubernetes project suggests that you use the AWS EBS third party storage driver instead.azureDisk (deprecated)In Kubernetes 1.33, all operations for the in-tree azureDisk type are redirected to the disk.csi.azure.com CSI driver.The AzureDisk in-tree storage driver was deprecated in the Kubernetes v1.19 release and then removed entirely in the v1.27 release.The Kubernetes project suggests that you use the Azure Disk third party storage driver instead.azureFile (deprecated)In Kubernetes 1.33, all operations for the in-tree azureFile type are redirected to the file.csi.azure.com CSI driver.The AzureFile in-tree storage driver was deprecated in the Kubernetes v1.21 release and then removed entirely in the v1.30 release.The Kubernetes project suggests that you use the Azure File third party storage driver instead.cephfs (removed)Kubernetes 1.33 does not include a cephfs volume type.The cephfs in-tree storage driver was deprecated in the Kubernetes v1.28 release and then removed entirely in the v1.31 release.cinder (deprecated)In Kubernetes 1.33, all operations for the in-tree cinder type are redirected to the cinder.csi.openstack.org CSI driver.The OpenStack Cinder in-tree storage driver was deprecated in the Kubernetes v1.11 release and then removed entirely in the v1.26 release.The Kubernetes project suggests that you use the OpenStack Cinder third party storage driver instead.configMapA ConfigMap provides a way to inject configuration data into pods. The data stored in a ConfigMap can be referenced in a volume of type configMap and then consumed by containerized applications running in a pod.When referencing a ConfigMap, you provide the name of the ConfigMap in the volume. You can customize the path to use for a specific entry in the ConfigMap. The following configuration shows how to mount the log-config ConfigMap onto a Pod called configmap-pod:apiVersion: v1 kind: Pod metadata: name: configmap-pod spec: containers: \- name: test image: busybox:1.28 command: ['sh', '-c', 'echo "The app is running!" && tail -f /dev/null'] volumeMounts: \- name: config-vol mountPath: /etc/config volumes: \- name: config-vol configMap: name: log-config items: \- key: log_level path: log_level.conf The log-config ConfigMap is mounted as a volume, and all contents stored in its log_level entry are mounted into the Pod at path /etc/config/log_level.conf. Note that this path is derived from the volume's mountPath and the path keyed with log_level.Note:You must create a ConfigMap before you can use it.A ConfigMap is always mounted as readOnly.A container using a ConfigMap as a subPath volume mount will not receive updates when the ConfigMap changes.Text data is exposed as files using the UTF-8 character encoding. For other character encodings, use binaryData.downwardAPIA downwardAPI volume makes downward API data available to applications. Within the volume, you can find the exposed data as read-only files in plain text format.Note:A container using the downward API as a subPath volume mount does not receive updates when field values change.See Expose Pod Information to Containers Through Files to learn more.emptyDirFor a Pod that defines an emptyDir volume, the volume is created when the Pod is assigned to a node. As the name says, the emptyDir volume is initially empty. All containers in the Pod can read and write the same files in the emptyDir volume, though that volume can be mounted at the same or different paths in each container. When a Pod is removed from a node for any reason, the data in the emptyDir is deleted permanently.Note:A container crashing does not remove a Pod from a node. The data in an emptyDir volume is safe across container crashes.Some uses for an emptyDir are:scratch space, such as for a disk-based merge sortcheckpointing a long computation for recovery from crashesholding files that a content-manager container fetches while a webserver container serves the dataThe emptyDir.medium field controls where emptyDir volumes are stored. By default emptyDir volumes are stored on whatever medium that backs the node such as disk, SSD, or network storage, depending on your environment. If you set the emptyDir.medium field to "Memory", Kubernetes mounts a tmpfs (RAM-backed filesystem) for you instead. While tmpfs is very fast, be aware that, unlike disks, files you write count against the memory limit of the container that wrote them.A size limit can be specified for the default medium, which limits the capacity of the emptyDir volume. The storage is allocated from node ephemeral storage. If that is filled up from another source (for example, log files or image overlays), the emptyDir may run out of capacity before this limit. If no size is specified, memory-backed volumes are sized to node allocatable memory.Caution:Please check here for points to note in terms of resource management when using memory-backed emptyDir.emptyDir configuration exampleapiVersion: v1 kind: Pod metadata: name: test-pd spec: containers: \- image: registry.k8s.io/test-webserver name: test-container volumeMounts: \- mountPath: /cache name: cache-volume volumes: \- name: cache-volume emptyDir: sizeLimit: 500Mi emptyDir memory configuration exampleapiVersion: v1 kind: Pod metadata: name: test-pd spec: containers: \- image: registry.k8s.io/test-webserver name: test-container volumeMounts: \- mountPath: /cache name: cache-volume volumes: \- name: cache-volume emptyDir: sizeLimit: 500Mi medium: Memory fc (fibre channel)An fc volume type allows an existing fibre channel block storage volume to be mounted in a Pod. You can specify single or multiple target world wide names (WWNs) using the parameter targetWWNs in your Volume configuration. If multiple WWNs are specified, targetWWNs expect that those WWNs are from multi-path connections.Note:You must configure FC SAN Zoning to allocate and mask those LUNs (volumes) to the target WWNs beforehand so that Kubernetes hosts can access them.See the fibre channel example for more details.gcePersistentDisk (deprecated)In Kubernetes 1.33, all operations for the in-tree gcePersistentDisk type are redirected to the pd.csi.storage.gke.io CSI driver.The gcePersistentDisk in-tree storage driver was deprecated in the Kubernetes v1.17 release and then removed entirely in the v1.28 release.The Kubernetes project suggests that you use the Google Compute Engine Persistent Disk CSI third party storage driver instead.gitRepo (deprecated)Warning:The gitRepo volume plugin is deprecated and is disabled by default.To provision a Pod that has a Git repository mounted, you can mount an emptyDir volume into an init container that clones the repo using Git, then mount the EmptyDir into the Pod's container.You can restrict the use of gitRepo volumes in your cluster using policies, such as ValidatingAdmissionPolicy. You can use the following Common Expression Language (CEL) expression as part of a policy to reject use of gitRepo volumes:!has(object.spec.volumes) || !object.spec.volumes.exists(v, has(v.gitRepo)) You can use this deprecated storage plugin in your cluster if you explicitly enable the GitRepoVolumeDriver feature gate.A gitRepo volume is an example of a volume plugin. This plugin mounts an empty directory and clones a git repository into this directory for your Pod to use.Here is an example of a gitRepo volume:apiVersion: v1 kind: Pod metadata: name: server spec: containers: \- image: nginx name: nginx volumeMounts: \- mountPath: /mypath name: git-volume volumes: \- name: git-volume gitRepo: repository: "git@somewhere:me/my-git-repository.git" revision: "22f1d8406d464b0c0874075539c1f2e96c253775" glusterfs (removed)Kubernetes 1.33 does not include a glusterfs volume type.The GlusterFS in-tree storage driver was deprecated in the Kubernetes v1.25 release and then removed entirely in the v1.26 release.hostPathA hostPath volume mounts a file or directory from the host node's filesystem into your Pod. This is not something that most Pods will need, but it offers a powerful escape hatch for some applications.Warning:Using the hostPath volume type presents many security risks. If you can avoid using a hostPath volume, you should. For example, define a local PersistentVolume, and use that instead.If you are restricting access to specific directories on the node using admission-time validation, that restriction is only effective when you additionally require that any mounts of that hostPath volume are read only. If you allow a read-write mount of any host path by an untrusted Pod, the containers in that Pod may be able to subvert the read-write host mount.Take care when using hostPath volumes, whether these are mounted as read-only or as read-write, because:Access to the host filesystem can expose privileged system credentials (such as for the kubelet) or privileged APIs (such as the container runtime socket) that can be used for container escape or to attack other parts of the cluster.Pods with identical configuration (such as created from a PodTemplate) may behave differently on different nodes due to different files on the nodes.hostPath volume usage is not treated as ephemeral storage usage. You need to monitor the disk usage by yourself because excessive hostPath disk usage will lead to disk pressure on the node.Some uses for a hostPath are:running a container that needs access to node-level system components (such as a container that transfers system logs to a central location, accessing those logs using a read-only mount of /var/log)making a configuration file stored on the host system available read-only to a static pod; unlike normal Pods, static Pods cannot access ConfigMapshostPath volume typesIn addition to the required path property, you can optionally specify a type for a hostPath volume.The available values for type are:ValueBehavior‌""Empty string (default) is for backward compatibility, which means that no checks will be performed before mounting the hostPath volume.DirectoryOrCreateIf nothing exists at the given path, an empty directory will be created there as needed with permission set to 0755, having the same group and ownership with Kubelet.DirectoryA directory must exist at the given pathFileOrCreateIf nothing exists at the given path, an empty file will be created there as needed with permission set to 0644, having the same group and ownership with Kubelet.FileA file must exist at the given pathSocketA UNIX socket must exist at the given pathCharDevice(Linux nodes only) A character device must exist at the given pathBlockDevice(Linux nodes only) A block device must exist at the given pathCaution:The FileOrCreate mode does not create the parent directory of the file. If the parent directory of the mounted file does not exist, the pod fails to start. To ensure that this mode works, you can try to mount directories and files separately, as shown in the FileOrCreate example for hostPath.Some files or directories created on the underlying hosts might only be accessible by root. You then either need to run your process as root in a privileged container or modify the file permissions on the host to read from or write to a hostPath volume.hostPath configuration exampleLinux nodeWindows node \--- # This manifest mounts /data/foo on the host as /foo inside the # single container that runs within the hostpath-example-linux Pod. # # The mount into the container is read-only. apiVersion: v1 kind: Pod metadata: name: hostpath-example-linux spec: os: { name: linux } nodeSelector: kubernetes.io/os: linux containers: \- name: example-container image: registry.k8s.io/test-webserver volumeMounts: \- mountPath: /foo name: example-volume readOnly: true volumes: \- name: example-volume # mount /data/foo, but only if that directory already exists hostPath: path: /data/foo # directory location on host type: Directory # this field is optional \--- # This manifest mounts C:\Data\foo on the host as C:\foo, inside the # single container that runs within the hostpath-example-windows Pod. # # The mount into the container is read-only. apiVersion: v1 kind: Pod metadata: name: hostpath-example-windows spec: os: { name: windows } nodeSelector: kubernetes.io/os: windows containers: \- name: example-container image: microsoft/windowsservercore:1709 volumeMounts: \- name: example-volume mountPath: "C:\\\foo" readOnly: true volumes: # mount C:\Data\foo from the host, but only if that directory already exists \- name: example-volume hostPath: path: "C:\\\Data\\\foo" # directory location on host type: Directory # this field is optional hostPath FileOrCreate configuration exampleThe following manifest defines a Pod that mounts /var/local/aaa inside the single container in the Pod. If the node does not already have a path /var/local/aaa, the kubelet creates it as a directory and then mounts it into the Pod.If /var/local/aaa already exists but is not a directory, the Pod fails. Additionally, the kubelet attempts to make a file named /var/local/aaa/1.txt inside that directory (as seen from the host); if something already exists at that path and isn't a regular file, the Pod fails.Here's the example manifest:apiVersion: v1 kind: Pod metadata: name: test-webserver spec: os: { name: linux } nodeSelector: kubernetes.io/os: linux containers: \- name: test-webserver image: registry.k8s.io/test-webserver:latest volumeMounts: \- mountPath: /var/local/aaa name: mydir \- mountPath: /var/local/aaa/1.txt name: myfile volumes: \- name: mydir hostPath: # Ensure the file directory is created. path: /var/local/aaa type: DirectoryOrCreate \- name: myfile hostPath: path: /var/local/aaa/1.txt type: FileOrCreate imageFEATURE STATE: Kubernetes v1.33 [beta] (enabled by default: false)An image volume source represents an OCI object (a container image or artifact) which is available on the kubelet's host machine.An example of using the image volume source is:pods/image-volumes.yaml apiVersion: v1 kind: Pod metadata: name: image-volume spec: containers: \- name: shell command: ["sleep", "infinity"] image: debian volumeMounts: \- name: volume mountPath: /volume volumes: \- name: volume image: reference: quay.io/crio/artifact:v2 pullPolicy: IfNotPresent The volume is resolved at pod startup depending on which pullPolicy value is provided:Alwaysthe kubelet always attempts to pull the reference. If the pull fails, the kubelet sets the Pod to Failed.Neverthe kubelet never pulls the reference and only uses a local image or artifact. The Pod becomes Failed if any layers of the image aren't already present locally, or if the manifest for that image isn't already cached.IfNotPresentthe kubelet pulls if the reference isn't already present on disk. The Pod becomes Failed if the reference isn't present and the pull fails.The volume gets re-resolved if the pod gets deleted and recreated, which means that new remote content will become available on pod recreation. A failure to resolve or pull the image during pod startup will block containers from starting and may add significant latency. Failures will be retried using normal volume backoff and will be reported on the pod reason and message.The types of objects that may be mounted by this volume are defined by the container runtime implementation on a host machine. At a minimum, they must include all valid types supported by the container image field. The OCI object gets mounted in a single directory (spec.containers[*].volumeMounts.mountPath) and will be mounted read-only. On Linux, the container runtime typically also mounts the volume with file execution blocked (noexec).Besides that:subPath or subPathExpr mounts for containers (spec.containers[*].volumeMounts.[subPath,subPathExpr]) are only supported from Kubernetes v1.33.The field spec.securityContext.fsGroupChangePolicy has no effect on this volume type.The AlwaysPullImages Admission Controller does also work for this volume source like for container images.The following fields are available for the image type:referenceArtifact reference to be used. For example, you could specify registry.k8s.io/conformance:v1.33.0 to load the files from the Kubernetes conformance test image. Behaves in the same way as pod.spec.containers[*].image. Pull secrets will be assembled in the same way as for the container image by looking up node credentials, service account image pull secrets, and pod spec image pull secrets. This field is optional to allow higher level config management to default or override container images in workload controllers like Deployments and StatefulSets. More info about container imagespullPolicyPolicy for pulling OCI objects. Possible values are: Always, Never or IfNotPresent. Defaults to Always if :latest tag is specified, or IfNotPresent otherwise.See the Use an Image Volume With a Pod example for more details on how to use the volume source.iscsiAn iscsi volume allows an existing iSCSI (SCSI over IP) volume to be mounted into your Pod. Unlike emptyDir, which is erased when a Pod is removed, the contents of an iscsi volume are preserved and the volume is merely unmounted. This means that an iscsi volume can be pre-populated with data, and that data can be shared between pods.Note:You must have your own iSCSI server running with the volume created before you can use it.A feature of iSCSI is that it can be mounted as read-only by multiple consumers simultaneously. This means that you can pre-populate a volume with your dataset and then serve it in parallel from as many Pods as you need. Unfortunately, iSCSI volumes can only be mounted by a single consumer in read-write mode. Simultaneous writers are not allowed.See the iSCSI example for more details.localA local volume represents a mounted local storage device such as a disk, partition or directory.Local volumes can only be used as a statically created PersistentVolume. Dynamic provisioning is not supported.Compared to hostPath volumes, local volumes are used in a durable and portable manner without manually scheduling pods to nodes. The system is aware of the volume's node constraints by looking at the node affinity on the PersistentVolume.However, local volumes are subject to the availability of the underlying node and are not suitable for all applications. If a node becomes unhealthy, then the local volume becomes inaccessible to the pod. The pod using this volume is unable to run. Applications using local volumes must be able to tolerate this reduced availability, as well as potential data loss, depending on the durability characteristics of the underlying disk.The following example shows a PersistentVolume using a local volume and nodeAffinity:apiVersion: v1 kind: PersistentVolume metadata: name: example-pv spec: capacity: storage: 100Gi volumeMode: Filesystem accessModes: \- ReadWriteOnce persistentVolumeReclaimPolicy: Delete storageClassName: local-storage local: path: /mnt/disks/ssd1 nodeAffinity: required: nodeSelectorTerms: \- matchExpressions: \- key: kubernetes.io/hostname operator: In values: \- example-node You must set a PersistentVolume nodeAffinity when using local volumes. The Kubernetes scheduler uses the PersistentVolume nodeAffinity to schedule these Pods to the correct node.PersistentVolume volumeMode can be set to "Block" (instead of the default value "Filesystem") to expose the local volume as a raw block device.When using local volumes, it is recommended to create a StorageClass with volumeBindingMode set to WaitForFirstConsumer. For more details, see the local StorageClass example. Delaying volume binding ensures that the PersistentVolumeClaim binding decision will also be evaluated with any other node constraints the Pod may have, such as node resource requirements, node selectors, Pod affinity, and Pod anti-affinity.An external static provisioner can be run separately for improved management of the local volume lifecycle. Note that this provisioner does not support dynamic provisioning yet. For an example on how to run an external local provisioner, see the local volume provisioner user guide.Note:The local PersistentVolume requires manual cleanup and deletion by the user if the external static provisioner is not used to manage the volume lifecycle.nfsAn nfs volume allows an existing NFS (Network File System) share to be mounted into a Pod. Unlike emptyDir, which is erased when a Pod is removed, the contents of an nfs volume are preserved and the volume is merely unmounted. This means that an NFS volume can be pre-populated with data, and that data can be shared between pods. NFS can be mounted by multiple writers simultaneously.apiVersion: v1 kind: Pod metadata: name: test-pd spec: containers: \- image: registry.k8s.io/test-webserver name: test-container volumeMounts: \- mountPath: /my-nfs-data name: test-volume volumes: \- name: test-volume nfs: server: my-nfs-server.example.com path: /my-nfs-volume readOnly: true Note:You must have your own NFS server running with the share exported before you can use it.Also note that you can't specify NFS mount options in a Pod spec. You can either set mount options server-side or use /etc/nfsmount.conf. You can also mount NFS volumes via PersistentVolumes which do allow you to set mount options.See the NFS example for an example of mounting NFS volumes with PersistentVolumes.persistentVolumeClaimA persistentVolumeClaim volume is used to mount a PersistentVolume into a Pod. PersistentVolumeClaims are a way for users to "claim" durable storage (such as an iSCSI volume) without knowing the details of the particular cloud environment.See the information about PersistentVolumes for more details.portworxVolume (deprecated)FEATURE STATE: Kubernetes v1.25 [deprecated]A portworxVolume is an elastic block storage layer that runs hyperconverged with Kubernetes. Portworx fingerprints storage in a server, tiers based on capabilities, and aggregates capacity across multiple servers. Portworx runs in-guest in virtual machines or on bare metal Linux nodes.A portworxVolume can be dynamically created through Kubernetes or it can also be pre-provisioned and referenced inside a Pod. Here is an example Pod referencing a pre-provisioned Portworx volume:apiVersion: v1 kind: Pod metadata: name: test-portworx-volume-pod spec: containers: \- image: registry.k8s.io/test-webserver name: test-container volumeMounts: \- mountPath: /mnt name: pxvol volumes: \- name: pxvol # This Portworx volume must already exist. portworxVolume: volumeID: "pxvol" fsType: "" Note:Make sure you have an existing PortworxVolume with name pxvol before using it in the Pod.For more details, see the Portworx volume examples.Portworx CSI migrationFEATURE STATE: Kubernetes v1.33 [stable] (enabled by default: true)In Kubernetes 1.33, all operations for the in-tree Portworx volumes are redirected to the pxd.portworx.com Container Storage Interface (CSI) Driver by default.Portworx CSI Driver must be installed on the cluster.projectedA projected volume maps several existing volume sources into the same directory. For more details, see projected volumes.rbd (removed)Kubernetes 1.33 does not include a rbd volume type.The Rados Block Device (RBD) in-tree storage driver and its csi migration support were deprecated in the Kubernetes v1.28 release and then removed entirely in the v1.31 release.secretA secret volume is used to pass sensitive information, such as passwords, to Pods. You can store secrets in the Kubernetes API and mount them as files for use by pods without coupling to Kubernetes directly. secret volumes are backed by tmpfs (a RAM-backed filesystem) so they are never written to non-volatile storage.Note:You must create a Secret in the Kubernetes API before you can use it.A Secret is always mounted as readOnly.A container using a Secret as a subPath volume mount will not receive Secret updates.For more details, see Configuring Secrets.vsphereVolume (deprecated)In Kubernetes 1.33, all operations for the in-tree vsphereVolume type are redirected to the csi.vsphere.vmware.com CSI driver.The vsphereVolume in-tree storage driver was deprecated in the Kubernetes v1.19 release and then removed entirely in the v1.30 release.The Kubernetes project suggests that you use the vSphere CSI third party storage driver instead.Using subPathSometimes, it is useful to share one volume for multiple uses in a single pod. The volumeMounts[*].subPath property specifies a sub-path inside the referenced volume instead of its root.The following example shows how to configure a Pod with a LAMP stack (Linux Apache MySQL PHP) using a single, shared volume. This sample subPath configuration is not recommended for production use.The PHP application's code and assets map to the volume's html folder and the MySQL database is stored in the volume's mysql folder. For example:apiVersion: v1 kind: Pod metadata: name: my-lamp-site spec: containers: \- name: mysql image: mysql env: \- name: MYSQL_ROOT_PASSWORD value: "rootpasswd" volumeMounts: \- mountPath: /var/lib/mysql name: site-data subPath: mysql \- name: php image: php:7.0-apache volumeMounts: \- mountPath: /var/www/html name: site-data subPath: html volumes: \- name: site-data persistentVolumeClaim: claimName: my-lamp-site-data Using subPath with expanded environment variablesFEATURE STATE: Kubernetes v1.17 [stable]Use the subPathExpr field to construct subPath directory names from downward API environment variables. The subPath and subPathExpr properties are mutually exclusive.In this example, a Pod uses subPathExpr to create a directory pod1 within the hostPath volume /var/log/pods. The hostPath volume takes the Pod name from the downwardAPI. The host directory /var/log/pods/pod1 is mounted at /logs in the container.apiVersion: v1 kind: Pod metadata: name: pod1 spec: containers: \- name: container1 env: \- name: POD_NAME valueFrom: fieldRef: apiVersion: v1 fieldPath: metadata.name image: busybox:1.28 command: [ "sh", "-c", "while [ true ]; do echo 'Hello'; sleep 10; done | tee -a /logs/hello.txt" ] volumeMounts: \- name: workdir1 mountPath: /logs # The variable expansion uses round brackets (not curly brackets). subPathExpr: $(POD_NAME) restartPolicy: Never volumes: \- name: workdir1 hostPath: path: /var/log/pods ResourcesThe storage media (such as Disk or SSD) of an emptyDir volume is determined by the medium of the filesystem holding the kubelet root dir (typically /var/lib/kubelet). There is no limit on how much space an emptyDir or hostPath volume can consume, and no isolation between containers or pods.To learn about requesting space using a resource specification, see how to manage resources.Out-of-tree volume pluginsThe out-of-tree volume plugins include Container Storage Interface (CSI), and also FlexVolume (which is deprecated). These plugins enable storage vendors to create custom storage plugins without adding their plugin source code to the Kubernetes repository.Previously, all volume plugins were "in-tree". The "in-tree" plugins were built, linked, compiled, and shipped with the core Kubernetes binaries. This meant that adding a new storage system to Kubernetes (a volume plugin) required checking code into the core Kubernetes code repository.Both CSI and FlexVolume allow volume plugins to be developed independently of the Kubernetes code base, and deployed (installed) on Kubernetes clusters as extensions.For storage vendors looking to create an out-of-tree volume plugin, please refer to the volume plugin FAQ.csiContainer Storage Interface (CSI) defines a standard interface for container orchestration systems (like Kubernetes) to expose arbitrary storage systems to their container workloads.Please read the CSI design proposal for more information.Note:Support for CSI spec versions 0.2 and 0.3 is deprecated in Kubernetes v1.13 and will be removed in a future release.Note:CSI drivers may not be compatible across all Kubernetes releases. Please check the specific CSI driver's documentation for supported deployments steps for each Kubernetes release and a compatibility matrix.Once a CSI-compatible volume driver is deployed on a Kubernetes cluster, users may use the csi volume type to attach or mount the volumes exposed by the CSI driver.A csi volume can be used in a Pod in three different ways:through a reference to a PersistentVolumeClaimwith a generic ephemeral volumewith a CSI ephemeral volume if the driver supports thatThe following fields are available to storage administrators to configure a CSI persistent volume:driver: A string value that specifies the name of the volume driver to use. This value must correspond to the value returned in the GetPluginInfoResponse by the CSI driver as defined in the CSI spec. It is used by Kubernetes to identify which CSI driver to call out to, and by CSI driver components to identify which PV objects belong to the CSI driver.volumeHandle: A string value that uniquely identifies the volume. This value must correspond to the value returned in the volume.id field of the CreateVolumeResponse by the CSI driver as defined in the CSI spec. The value is passed as volume_id in all calls to the CSI volume driver when referencing the volume.readOnly: An optional boolean value indicating whether the volume is to be "ControllerPublished" (attached) as read only. Default is false. This value is passed to the CSI driver via the readonly field in the ControllerPublishVolumeRequest.fsType: If the PV's VolumeMode is Filesystem, then this field may be used to specify the filesystem that should be used to mount the volume. If the volume has not been formatted and formatting is supported, this value will be used to format the volume. This value is passed to the CSI driver via the VolumeCapability field of ControllerPublishVolumeRequest, NodeStageVolumeRequest, and NodePublishVolumeRequest.volumeAttributes: A map of string to string that specifies static properties of a volume. This map must correspond to the map returned in the volume.attributes field of the CreateVolumeResponse by the CSI driver as defined in the CSI spec. The map is passed to the CSI driver via the volume_context field in the ControllerPublishVolumeRequest, NodeStageVolumeRequest, and NodePublishVolumeRequest.controllerPublishSecretRef: A reference to the secret object containing sensitive information to pass to the CSI driver to complete the CSI ControllerPublishVolume and ControllerUnpublishVolume calls. This field is optional, and may be empty if no secret is required. If the Secret contains more than one secret, all secrets are passed.nodeExpandSecretRef: A reference to the secret containing sensitive information to pass to the CSI driver to complete the CSI NodeExpandVolume call. This field is optional and may be empty if no secret is required. If the object contains more than one secret, all secrets are passed. When you have configured secret data for node-initiated volume expansion, the kubelet passes that data via the NodeExpandVolume() call to the CSI driver. All supported versions of Kubernetes offer the nodeExpandSecretRef field, and have it available by default. Kubernetes releases prior to v1.25 did not include this support.Enable the feature gate named CSINodeExpandSecret for each kube-apiserver and for the kubelet on every node. Since Kubernetes version 1.27, this feature has been enabled by default and no explicit enablement of the feature gate is required. You must also be using a CSI driver that supports or requires secret data during node-initiated storage resize operations.nodePublishSecretRef: A reference to the secret object containing sensitive information to pass to the CSI driver to complete the CSI NodePublishVolume call. This field is optional and may be empty if no secret is required. If the secret object contains more than one secret, all secrets are passed.nodeStageSecretRef: A reference to the secret object containing sensitive information to pass to the CSI driver to complete the CSI NodeStageVolume call. This field is optional and may be empty if no secret is required. If the Secret contains more than one secret, all secrets are passed.CSI raw block volume supportFEATURE STATE: Kubernetes v1.18 [stable]Vendors with external CSI drivers can implement raw block volume support in Kubernetes workloads.You can set up your PersistentVolume/PersistentVolumeClaim with raw block volume support as usual, without any CSI-specific changes.CSI ephemeral volumesFEATURE STATE: Kubernetes v1.25 [stable]You can directly configure CSI volumes within the Pod specification. Volumes specified in this way are ephemeral and do not persist across pod restarts. See Ephemeral Volumes for more information.For more information on how to develop a CSI driver, refer to the kubernetes-csi documentationWindows CSI proxyFEATURE STATE: Kubernetes v1.22 [stable]CSI node plugins need to perform various privileged operations like scanning of disk devices and mounting of file systems. These operations differ for each host operating system. For Linux worker nodes, containerized CSI node plugins are typically deployed as privileged containers. For Windows worker nodes, privileged operations for containerized CSI node plugins is supported using csi-proxy, a community-managed, stand-alone binary that needs to be pre-installed on each Windows node.For more details, refer to the deployment guide of the CSI plugin you wish to deploy.Migrating to CSI drivers from in-tree pluginsFEATURE STATE: Kubernetes v1.25 [stable]The CSIMigration feature directs operations against existing in-tree plugins to corresponding CSI plugins (which are expected to be installed and configured). As a result, operators do not have to make any configuration changes to existing Storage Classes, PersistentVolumes or PersistentVolumeClaims (referring to in-tree plugins) when transitioning to a CSI driver that supersedes an in-tree plugin.Note:Existing PVs created by an in-tree volume plugin can still be used in the future without any configuration changes, even after the migration to CSI is completed for that volume type, and even after you upgrade to a version of Kubernetes that doesn't have compiled-in support for that kind of storage.As part of that migration, you - or another cluster administrator - must have installed and configured the appropriate CSI driver for that storage. The core of Kubernetes does not install that software for you.After that migration, you can also define new PVCs and PVs that refer to the legacy, built-in storage integrations. Provided you have the appropriate CSI driver installed and configured, the PV creation continues to work, even for brand new volumes. The actual storage management now happens through the CSI driver.The operations and features that are supported include: provisioning/delete, attach/detach, mount/unmount and resizing of volumes.In-tree plugins that support CSIMigration and have a corresponding CSI driver implemented are listed in Types of Volumes.flexVolume (deprecated)FEATURE STATE: Kubernetes v1.23 [deprecated]FlexVolume is an out-of-tree plugin interface that uses an exec-based model to interface with storage drivers. The FlexVolume driver binaries must be installed in a pre-defined volume plugin path on each node and in some cases the control plane nodes as well.Pods interact with FlexVolume drivers through the flexVolume in-tree volume plugin.The following FlexVolume plugins, deployed as PowerShell scripts on the host, support Windows nodes:SMBiSCSINote:FlexVolume is deprecated. Using an out-of-tree CSI driver is the recommended way to integrate external storage with Kubernetes.Maintainers of FlexVolume driver should implement a CSI Driver and help to migrate users of FlexVolume drivers to CSI. Users of FlexVolume should move their workloads to use the equivalent CSI Driver.Mount propagationCaution:Mount propagation is a low-level feature that does not work consistently on all volume types. The Kubernetes project recommends only using mount propagation with hostPath or memory-backed emptyDir volumes. See Kubernetes issue #95049 for more context.Mount propagation allows for sharing volumes mounted by a container to other containers in the same pod, or even to other pods on the same node.Mount propagation of a volume is controlled by the mountPropagation field in containers[*].volumeMounts. Its values are:None - This volume mount will not receive any subsequent mounts that are mounted to this volume or any of its subdirectories by the host. In similar fashion, no mounts created by the container will be visible on the host. This is the default mode.This mode is equal to rprivate mount propagation as described in mount(8)However, the CRI runtime may choose rslave mount propagation (i.e., HostToContainer) instead, when rprivate propagation is not applicable. cri-dockerd (Docker) is known to choose rslave mount propagation when the mount source contains the Docker daemon's root directory (/var/lib/docker).HostToContainer - This volume mount will receive all subsequent mounts that are mounted to this volume or any of its subdirectories.In other words, if the host mounts anything inside the volume mount, the container will see it mounted there.Similarly, if any Pod with Bidirectional mount propagation to the same volume mounts anything there, the container with HostToContainer mount propagation will see it.This mode is equal to rslave mount propagation as described in the mount(8)Bidirectional - This volume mount behaves the same the HostToContainer mount. In addition, all volume mounts created by the container will be propagated back to the host and to all containers of all pods that use the same volume.A typical use case for this mode is a Pod with a FlexVolume or CSI driver or a Pod that needs to mount something on the host using a hostPath volume.This mode is equal to rshared mount propagation as described in the mount(8)Warning:Bidirectional mount propagation can be dangerous. It can damage the host operating system and therefore it is allowed only in privileged containers. Familiarity with Linux kernel behavior is strongly recommended. In addition, any volume mounts created by containers in pods must be destroyed (unmounted) by the containers on termination.Read-only mountsA mount can be made read-only by setting the .spec.containers[].volumeMounts[].readOnly field to true. This does not make the volume itself read-only, but that specific container will not be able to write to it. Other containers in the Pod may mount the same volume as read-write.On Linux, read-only mounts are not recursively read-only by default. For example, consider a Pod which mounts the hosts /mnt as a hostPath volume. If there is another filesystem mounted read-write on /mnt/ (such as tmpfs, NFS, or USB storage), the volume mounted into the container(s) will also have a writeable /mnt/, even if the mount itself was specified as read-only.Recursive read-only mountsFEATURE STATE: Kubernetes v1.33 [stable] (enabled by default: true)Recursive read-only mounts can be enabled by setting the RecursiveReadOnlyMounts feature gate for kubelet and kube-apiserver, and setting the .spec.containers[].volumeMounts[].recursiveReadOnly field for a pod.The allowed values are:Disabled (default): no effect.Enabled: makes the mount recursively read-only. Needs all the following requirements to be satisfied:readOnly is set to truemountPropagation is unset, or, set to NoneThe host is running with Linux kernel v5.12 or laterThe CRI-level container runtime supports recursive read-only mountsThe OCI-level container runtime supports recursive read-only mounts.It will fail if any of these is not true.IfPossible: attempts to apply Enabled, and falls back to Disabled if the feature is not supported by the kernel or the runtime class.Example:storage/rro.yaml apiVersion: v1 kind: Pod metadata: name: rro spec: volumes: \- name: mnt hostPath: # tmpfs is mounted on /mnt/tmpfs path: /mnt containers: \- name: busybox image: busybox args: ["sleep", "infinity"] volumeMounts: # /mnt-rro/tmpfs is not writable \- name: mnt mountPath: /mnt-rro readOnly: true mountPropagation: None recursiveReadOnly: Enabled # /mnt-ro/tmpfs is writable \- name: mnt mountPath: /mnt-ro readOnly: true # /mnt-rw/tmpfs is writable \- name: mnt mountPath: /mnt-rw When this property is recognized by kubelet and kube-apiserver, the .status.containerStatuses[].volumeMounts[].recursiveReadOnly field is set to either Enabled or Disabled.ImplementationsNote: This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the content guide before submitting a change. More information.The following container runtimes are known to support recursive read-only mounts.CRI-level:containerd, since v2.0CRI-O, since v1.30OCI-level:runc, since v1.1crun, since v1.8.6What's nextFollow an example of deploying WordPress and MySQL with Persistent Volumes.3.6.2 - Persistent VolumesThis document describes persistent volumes in Kubernetes. Familiarity with volumes, StorageClasses and VolumeAttributesClasses is suggested.IntroductionManaging storage is a distinct problem from managing compute instances. The PersistentVolume subsystem provides an API for users and administrators that abstracts details of how storage is provided from how it is consumed. To do this, we introduce two new API resources: PersistentVolume and PersistentVolumeClaim.A PersistentVolume (PV) is a piece of storage in the cluster that has been provisioned by an administrator or dynamically provisioned using Storage Classes. It is a resource in the cluster just like a node is a cluster resource. PVs are volume plugins like Volumes, but have a lifecycle independent of any individual Pod that uses the PV. This API object captures the details of the implementation of the storage, be that NFS, iSCSI, or a cloud-provider-specific storage system.A PersistentVolumeClaim (PVC) is a request for storage by a user. It is similar to a Pod. Pods consume node resources and PVCs consume PV resources. Pods can request specific levels of resources (CPU and Memory). Claims can request specific size and access modes (e.g., they can be mounted ReadWriteOnce, ReadOnlyMany, ReadWriteMany, or ReadWriteOncePod, see AccessModes).While PersistentVolumeClaims allow a user to consume abstract storage resources, it is common that users need PersistentVolumes with varying properties, such as performance, for different problems. Cluster administrators need to be able to offer a variety of PersistentVolumes that differ in more ways than size and access modes, without exposing users to the details of how those volumes are implemented. For these needs, there is the StorageClass resource.See the detailed walkthrough with working examples.Lifecycle of a volume and claimPVs are resources in the cluster. PVCs are requests for those resources and also act as claim checks to the resource. The interaction between PVs and PVCs follows this lifecycle:ProvisioningThere are two ways PVs may be provisioned: statically or dynamically.StaticA cluster administrator creates a number of PVs. They carry the details of the real storage, which is available for use by cluster users. They exist in the Kubernetes API and are available for consumption.DynamicWhen none of the static PVs the administrator created match a user's PersistentVolumeClaim, the cluster may try to dynamically provision a volume specially for the PVC. This provisioning is based on StorageClasses: the PVC must request a storage class and the administrator must have created and configured that class for dynamic provisioning to occur. Claims that request the class "" effectively disable dynamic provisioning for themselves.To enable dynamic storage provisioning based on storage class, the cluster administrator needs to enable the DefaultStorageClass admission controller on the API server. This can be done, for example, by ensuring that DefaultStorageClass is among the comma-delimited, ordered list of values for the --enable-admission-plugins flag of the API server component. For more information on API server command-line flags, check kube-apiserver documentation.BindingA user creates, or in the case of dynamic provisioning, has already created, a PersistentVolumeClaim with a specific amount of storage requested and with certain access modes. A control loop in the control plane watches for new PVCs, finds a matching PV (if possible), and binds them together. If a PV was dynamically provisioned for a new PVC, the loop will always bind that PV to the PVC. Otherwise, the user will always get at least what they asked for, but the volume may be in excess of what was requested. Once bound, PersistentVolumeClaim binds are exclusive, regardless of how they were bound. A PVC to PV binding is a one-to-one mapping, using a ClaimRef which is a bi-directional binding between the PersistentVolume and the PersistentVolumeClaim.Claims will remain unbound indefinitely if a matching volume does not exist. Claims will be bound as matching volumes become available. For example, a cluster provisioned with many 50Gi PVs would not match a PVC requesting 100Gi. The PVC can be bound when a 100Gi PV is added to the cluster.UsingPods use claims as volumes. The cluster inspects the claim to find the bound volume and mounts that volume for a Pod. For volumes that support multiple access modes, the user specifies which mode is desired when using their claim as a volume in a Pod.Once a user has a claim and that claim is bound, the bound PV belongs to the user for as long as they need it. Users schedule Pods and access their claimed PVs by including a persistentVolumeClaim section in a Pod's volumes block. See Claims As Volumes for more details on this.Storage Object in Use ProtectionThe purpose of the Storage Object in Use Protection feature is to ensure that PersistentVolumeClaims (PVCs) in active use by a Pod and PersistentVolume (PVs) that are bound to PVCs are not removed from the system, as this may result in data loss.Note:PVC is in active use by a Pod when a Pod object exists that is using the PVC.If a user deletes a PVC in active use by a Pod, the PVC is not removed immediately. PVC removal is postponed until the PVC is no longer actively used by any Pods. Also, if an admin deletes a PV that is bound to a PVC, the PV is not removed immediately. PV removal is postponed until the PV is no longer bound to a PVC.You can see that a PVC is protected when the PVC's status is Terminating and the Finalizers list includes kubernetes.io/pvc-protection:kubectl describe pvc hostpath Name: hostpath Namespace: default StorageClass: example-hostpath Status: Terminating Volume: Labels:  Annotations: volume.beta.kubernetes.io/storage-class=example-hostpath volume.beta.kubernetes.io/storage-provisioner=example.com/hostpath Finalizers: [kubernetes.io/pvc-protection] ... You can see that a PV is protected when the PV's status is Terminating and the Finalizers list includes kubernetes.io/pv-protection too:kubectl describe pv task-pv-volume Name: task-pv-volume Labels: type=local Annotations:  Finalizers: [kubernetes.io/pv-protection] StorageClass: standard Status: Terminating Claim: Reclaim Policy: Delete Access Modes: RWO Capacity: 1Gi Message: Source: Type: HostPath (bare host directory volume) Path: /tmp/data HostPathType: Events:  ReclaimingWhen a user is done with their volume, they can delete the PVC objects from the API that allows reclamation of the resource. The reclaim policy for a PersistentVolume tells the cluster what to do with the volume after it has been released of its claim. Currently, volumes can either be Retained, Recycled, or Deleted.RetainThe Retain reclaim policy allows for manual reclamation of the resource. When the PersistentVolumeClaim is deleted, the PersistentVolume still exists and the volume is considered "released". But it is not yet available for another claim because the previous claimant's data remains on the volume. An administrator can manually reclaim the volume with the following steps.Delete the PersistentVolume. The associated storage asset in external infrastructure still exists after the PV is deleted.Manually clean up the data on the associated storage asset accordingly.Manually delete the associated storage asset.If you want to reuse the same storage asset, create a new PersistentVolume with the same storage asset definition.DeleteFor volume plugins that support the Delete reclaim policy, deletion removes both the PersistentVolume object from Kubernetes, as well as the associated storage asset in the external infrastructure. Volumes that were dynamically provisioned inherit the reclaim policy of their StorageClass, which defaults to Delete. The administrator should configure the StorageClass according to users' expectations; otherwise, the PV must be edited or patched after it is created. See Change the Reclaim Policy of a PersistentVolume.RecycleWarning:The Recycle reclaim policy is deprecated. Instead, the recommended approach is to use dynamic provisioning.If supported by the underlying volume plugin, the Recycle reclaim policy performs a basic scrub (rm -rf /thevolume/*) on the volume and makes it available again for a new claim.However, an administrator can configure a custom recycler Pod template using the Kubernetes controller manager command line arguments as described in the reference. The custom recycler Pod template must contain a volumes specification, as shown in the example below:apiVersion: v1 kind: Pod metadata: name: pv-recycler namespace: default spec: restartPolicy: Never volumes: \- name: vol hostPath: path: /any/path/it/will/be/replaced containers: \- name: pv-recycler image: "registry.k8s.io/busybox" command: ["/bin/sh", "-c", "test -e /scrub && rm -rf /scrub/..?* /scrub/.[!.]* /scrub/* && test -z \"$(ls -A /scrub)\" || exit 1"] volumeMounts: \- name: vol mountPath: /scrub However, the particular path specified in the custom recycler Pod template in the volumes part is replaced with the particular path of the volume that is being recycled.PersistentVolume deletion protection finalizerFEATURE STATE: Kubernetes v1.33 [stable] (enabled by default: true)Finalizers can be added on a PersistentVolume to ensure that PersistentVolumes having Delete reclaim policy are deleted only after the backing storage are deleted.The finalizer external-provisioner.volume.kubernetes.io/finalizer(introduced in v1.31) is added to both dynamically provisioned and statically provisioned CSI volumes.The finalizer kubernetes.io/pv-controller(introduced in v1.31) is added to dynamically provisioned in-tree plugin volumes and skipped for statically provisioned in-tree plugin volumes.The following is an example of dynamically provisioned in-tree plugin volume:kubectl describe pv pvc-74a498d6-3929-47e8-8c02-078c1ece4d78 Name: pvc-74a498d6-3929-47e8-8c02-078c1ece4d78 Labels:  Annotations: kubernetes.io/createdby: vsphere-volume-dynamic-provisioner pv.kubernetes.io/bound-by-controller: yes pv.kubernetes.io/provisioned-by: kubernetes.io/vsphere-volume Finalizers: [kubernetes.io/pv-protection kubernetes.io/pv-controller] StorageClass: vcp-sc Status: Bound Claim: default/vcp-pvc-1 Reclaim Policy: Delete Access Modes: RWO VolumeMode: Filesystem Capacity: 1Gi Node Affinity:  Message: Source: Type: vSphereVolume (a Persistent Disk resource in vSphere) VolumePath: [vsanDatastore] d49c4a62-166f-ce12-c464-020077ba5d46/kubernetes-dynamic-pvc-74a498d6-3929-47e8-8c02-078c1ece4d78.vmdk FSType: ext4 StoragePolicyName: vSAN Default Storage Policy Events:  The finalizer external-provisioner.volume.kubernetes.io/finalizer is added for CSI volumes. The following is an example:Name: pvc-2f0bab97-85a8-4552-8044-eb8be45cf48d Labels:  Annotations: pv.kubernetes.io/provisioned-by: csi.vsphere.vmware.com Finalizers: [kubernetes.io/pv-protection external-provisioner.volume.kubernetes.io/finalizer] StorageClass: fast Status: Bound Claim: demo-app/nginx-logs Reclaim Policy: Delete Access Modes: RWO VolumeMode: Filesystem Capacity: 200Mi Node Affinity:  Message: Source: Type: CSI (a Container Storage Interface (CSI) volume source) Driver: csi.vsphere.vmware.com FSType: ext4 VolumeHandle: 44830fa8-79b4-406b-8b58-621ba25353fd ReadOnly: false VolumeAttributes: storage.kubernetes.io/csiProvisionerIdentity=1648442357185-8081-csi.vsphere.vmware.com type=vSphere CNS Block Volume Events:  When the CSIMigration{provider} feature flag is enabled for a specific in-tree volume plugin, the kubernetes.io/pv-controller finalizer is replaced by the external-provisioner.volume.kubernetes.io/finalizer finalizer.The finalizers ensure that the PV object is removed only after the volume is deleted from the storage backend provided the reclaim policy of the PV is Delete. This also ensures that the volume is deleted from storage backend irrespective of the order of deletion of PV and PVC.Reserving a PersistentVolumeThe control plane can bind PersistentVolumeClaims to matching PersistentVolumes in the cluster. However, if you want a PVC to bind to a specific PV, you need to pre-bind them.By specifying a PersistentVolume in a PersistentVolumeClaim, you declare a binding between that specific PV and PVC. If the PersistentVolume exists and has not reserved PersistentVolumeClaims through its claimRef field, then the PersistentVolume and PersistentVolumeClaim will be bound.The binding happens regardless of some volume matching criteria, including node affinity. The control plane still checks that storage class, access modes, and requested storage size are valid.apiVersion: v1 kind: PersistentVolumeClaim metadata: name: foo-pvc namespace: foo spec: storageClassName: "" # Empty string must be explicitly set otherwise default StorageClass will be set volumeName: foo-pv ... This method does not guarantee any binding privileges to the PersistentVolume. If other PersistentVolumeClaims could use the PV that you specify, you first need to reserve that storage volume. Specify the relevant PersistentVolumeClaim in the claimRef field of the PV so that other PVCs can not bind to it.apiVersion: v1 kind: PersistentVolume metadata: name: foo-pv spec: storageClassName: "" claimRef: name: foo-pvc namespace: foo ... This is useful if you want to consume PersistentVolumes that have their persistentVolumeReclaimPolicy set to Retain, including cases where you are reusing an existing PV.Expanding Persistent Volumes ClaimsFEATURE STATE: Kubernetes v1.24 [stable]Support for expanding PersistentVolumeClaims (PVCs) is enabled by default. You can expand the following types of volumes:csi (including some CSI migrated volme types)flexVolume (deprecated)portworxVolume (deprecated)You can only expand a PVC if its storage class's allowVolumeExpansion field is set to true.apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: example-vol-default provisioner: vendor-name.example/magicstorage parameters: resturl: "http://192.168.10.100:8080" restuser: "" secretNamespace: "" secretName: "" allowVolumeExpansion: true To request a larger volume for a PVC, edit the PVC object and specify a larger size. This triggers expansion of the volume that backs the underlying PersistentVolume. A new PersistentVolume is never created to satisfy the claim. Instead, an existing volume is resized.Warning:Directly editing the size of a PersistentVolume can prevent an automatic resize of that volume. If you edit the capacity of a PersistentVolume, and then edit the .spec of a matching PersistentVolumeClaim to make the size of the PersistentVolumeClaim match the PersistentVolume, then no storage resize happens. The Kubernetes control plane will see that the desired state of both resources matches, conclude that the backing volume size has been manually increased and that no resize is necessary.CSI Volume expansionFEATURE STATE: Kubernetes v1.24 [stable]Support for expanding CSI volumes is enabled by default but it also requires a specific CSI driver to support volume expansion. Refer to documentation of the specific CSI driver for more information.Resizing a volume containing a file systemYou can only resize volumes containing a file system if the file system is XFS, Ext3, or Ext4.When a volume contains a file system, the file system is only resized when a new Pod is using the PersistentVolumeClaim in ReadWrite mode. File system expansion is either done when a Pod is starting up or when a Pod is running and the underlying file system supports online expansion.FlexVolumes (deprecated since Kubernetes v1.23) allow resize if the driver is configured with the RequiresFSResize capability to true. The FlexVolume can be resized on Pod restart.Resizing an in-use PersistentVolumeClaimFEATURE STATE: Kubernetes v1.24 [stable]In this case, you don't need to delete and recreate a Pod or deployment that is using an existing PVC. Any in-use PVC automatically becomes available to its Pod as soon as its file system has been expanded. This feature has no effect on PVCs that are not in use by a Pod or deployment. You must create a Pod that uses the PVC before the expansion can complete.Similar to other volume types - FlexVolume volumes can also be expanded when in-use by a Pod.Note:FlexVolume resize is possible only when the underlying driver supports resize.Recovering from Failure when Expanding VolumesIf a user specifies a new size that is too big to be satisfied by underlying storage system, expansion of PVC will be continuously retried until user or cluster administrator takes some action. This can be undesirable and hence Kubernetes provides following methods of recovering from such failures.Manually with Cluster Administrator accessBy requesting expansion to smaller sizeIf expanding underlying storage fails, the cluster administrator can manually recover the Persistent Volume Claim (PVC) state and cancel the resize requests. Otherwise, the resize requests are continuously retried by the controller without administrator intervention.Mark the PersistentVolume(PV) that is bound to the PersistentVolumeClaim(PVC) with Retain reclaim policy.Delete the PVC. Since PV has Retain reclaim policy - we will not lose any data when we recreate the PVC.Delete the claimRef entry from PV specs, so as new PVC can bind to it. This should make the PV Available.Re-create the PVC with smaller size than PV and set volumeName field of the PVC to the name of the PV. This should bind new PVC to existing PV.Don't forget to restore the reclaim policy of the PV.FEATURE STATE: Kubernetes v1.32 [beta] (enabled by default: true)Note:Recover from failing PVC expansion by users (RecoverVolumeExpansionFailure) is available as an beta feature since Kubernetes 1.32 and should be enabled by default. Refer to the feature gate documentation for more information.When using RecoverVolumeExpansionFailure feature, if expansion has failed for a PVC, you can retry expansion with a smaller size than the previously requested value. To request a new expansion attempt with a smaller proposed size, edit .spec.resources for that PVC and choose a value that is less than the value you previously tried. This is useful if expansion to a higher value did not succeed because of capacity constraint. If that has happened, or you suspect that it might have, you can retry expansion by specifying a size that is within the capacity limits of underlying storage provider. You can monitor status of resize operation by watching .status.allocatedResourceStatuses and events on the PVC.Note that, although you can specify a lower amount of storage than what was requested previously, the new value must still be higher than .status.capacity. Kubernetes does not support shrinking a PVC to less than its current size.Types of Persistent VolumesPersistentVolume types are implemented as plugins. Kubernetes currently supports the following plugins:csi - Container Storage Interface (CSI)fc - Fibre Channel (FC) storagehostPath - HostPath volume (for single node testing only; WILL NOT WORK in a multi-node cluster; consider using local volume instead)iscsi - iSCSI (SCSI over IP) storagelocal - local storage devices mounted on nodes.nfs - Network File System (NFS) storageThe following types of PersistentVolume are deprecated but still available. If you are using these volume types except for flexVolume, cephfs and rbd, please install corresponding CSI drivers.awsElasticBlockStore - AWS Elastic Block Store (EBS) (migration on by default starting v1.23)azureDisk - Azure Disk (migration on by default starting v1.23)azureFile - Azure File (migration on by default starting v1.24)cinder - Cinder (OpenStack block storage) (migration on by default starting v1.21)flexVolume - FlexVolume (deprecated starting v1.23, no migration plan and no plan to remove support)gcePersistentDisk - GCE Persistent Disk (migration on by default starting v1.23)portworxVolume - Portworx volume (migration on by default starting v1.31)vsphereVolume - vSphere VMDK volume (migration on by default starting v1.25)Older versions of Kubernetes also supported the following in-tree PersistentVolume types:cephfs (not available starting v1.31)flocker - Flocker storage. (not available starting v1.25)glusterfs - GlusterFS storage. (not available starting v1.26)photonPersistentDisk - Photon controller persistent disk. (not available starting v1.15)quobyte - Quobyte volume. (not available starting v1.25)rbd - Rados Block Device (RBD) volume (not available starting v1.31)scaleIO - ScaleIO volume. (not available starting v1.21)storageos - StorageOS volume. (not available starting v1.25)Persistent VolumesEach PV contains a spec and status, which is the specification and status of the volume. The name of a PersistentVolume object must be a valid DNS subdomain name.apiVersion: v1 kind: PersistentVolume metadata: name: pv0003 spec: capacity: storage: 5Gi volumeMode: Filesystem accessModes: \- ReadWriteOnce persistentVolumeReclaimPolicy: Recycle storageClassName: slow mountOptions: \- hard \- nfsvers=4.1 nfs: path: /tmp server: 172.17.0.2 Note:Helper programs relating to the volume type may be required for consumption of a PersistentVolume within a cluster. In this example, the PersistentVolume is of type NFS and the helper program /sbin/mount.nfs is required to support the mounting of NFS filesystems.CapacityGenerally, a PV will have a specific storage capacity. This is set using the PV's capacity attribute which is a Quantity value.Currently, storage size is the only resource that can be set or requested. Future attributes may include IOPS, throughput, etc.Volume ModeFEATURE STATE: Kubernetes v1.18 [stable]Kubernetes supports two volumeModes of PersistentVolumes: Filesystem and Block.volumeMode is an optional API parameter. Filesystem is the default mode used when volumeMode parameter is omitted.A volume with volumeMode: Filesystem is mounted into Pods into a directory. If the volume is backed by a block device and the device is empty, Kubernetes creates a filesystem on the device before mounting it for the first time.You can set the value of volumeMode to Block to use a volume as a raw block device. Such volume is presented into a Pod as a block device, without any filesystem on it. This mode is useful to provide a Pod the fastest possible way to access a volume, without any filesystem layer between the Pod and the volume. On the other hand, the application running in the Pod must know how to handle a raw block device. See Raw Block Volume Support for an example on how to use a volume with volumeMode: Block in a Pod.Access ModesA PersistentVolume can be mounted on a host in any way supported by the resource provider. As shown in the table below, providers will have different capabilities and each PV's access modes are set to the specific modes supported by that particular volume. For example, NFS can support multiple read/write clients, but a specific NFS PV might be exported on the server as read-only. Each PV gets its own set of access modes describing that specific PV's capabilities.The access modes are:ReadWriteOncethe volume can be mounted as read-write by a single node. ReadWriteOnce access mode still can allow multiple pods to access (read from or write to) that volume when the pods are running on the same node. For single pod access, please see ReadWriteOncePod.ReadOnlyManythe volume can be mounted as read-only by many nodes.ReadWriteManythe volume can be mounted as read-write by many nodes.ReadWriteOncePodFEATURE STATE: Kubernetes v1.29 [stable]the volume can be mounted as read-write by a single Pod. Use ReadWriteOncePod access mode if you want to ensure that only one pod across the whole cluster can read that PVC or write to it.Note:The ReadWriteOncePod access mode is only supported for CSI volumes and Kubernetes version 1.22+. To use this feature you will need to update the following CSI sidecars to these versions or greater:csi-provisioner:v3.0.0+csi-attacher:v3.3.0+csi-resizer:v1.3.0+In the CLI, the access modes are abbreviated to:RWO - ReadWriteOnceROX - ReadOnlyManyRWX - ReadWriteManyRWOP - ReadWriteOncePodNote:Kubernetes uses volume access modes to match PersistentVolumeClaims and PersistentVolumes. In some cases, the volume access modes also constrain where the PersistentVolume can be mounted. Volume access modes do not enforce write protection once the storage has been mounted. Even if the access modes are specified as ReadWriteOnce, ReadOnlyMany, or ReadWriteMany, they don't set any constraints on the volume. For example, even if a PersistentVolume is created as ReadOnlyMany, it is no guarantee that it will be read-only. If the access modes are specified as ReadWriteOncePod, the volume is constrained and can be mounted on only a single Pod.Important! A volume can only be mounted using one access mode at a time, even if it supports many.Volume PluginReadWriteOnceReadOnlyManyReadWriteManyReadWriteOncePodAzureFile✓✓✓-CephFS✓✓✓-CSIdepends on the driverdepends on the driverdepends on the driverdepends on the driverFC✓✓--FlexVolume✓✓depends on the driver-HostPath✓---iSCSI✓✓--NFS✓✓✓-RBD✓✓--VsphereVolume✓-- (works when Pods are collocated)-PortworxVolume✓-✓-ClassA PV can have a class, which is specified by setting the storageClassName attribute to the name of a StorageClass. A PV of a particular class can only be bound to PVCs requesting that class. A PV with no storageClassName has no class and can only be bound to PVCs that request no particular class.In the past, the annotation volume.beta.kubernetes.io/storage-class was used instead of the storageClassName attribute. This annotation is still working; however, it will become fully deprecated in a future Kubernetes release.Reclaim PolicyCurrent reclaim policies are:Retain -- manual reclamationRecycle -- basic scrub (rm -rf /thevolume/*)Delete -- delete the volumeFor Kubernetes 1.33, only nfs and hostPath volume types support recycling.Mount OptionsA Kubernetes administrator can specify additional mount options for when a Persistent Volume is mounted on a node.Note:Not all Persistent Volume types support mount options.The following volume types support mount options:csi (including CSI migrated volume types)iscsinfsMount options are not validated. If a mount option is invalid, the mount fails.In the past, the annotation volume.beta.kubernetes.io/mount-options was used instead of the mountOptions attribute. This annotation is still working; however, it will become fully deprecated in a future Kubernetes release.Node AffinityNote:For most volume types, you do not need to set this field. You need to explicitly set this for local volumes.A PV can specify node affinity to define constraints that limit what nodes this volume can be accessed from. Pods that use a PV will only be scheduled to nodes that are selected by the node affinity. To specify node affinity, set nodeAffinity in the .spec of a PV. The PersistentVolume API reference has more details on this field.PhaseA PersistentVolume will be in one of the following phases:Availablea free resource that is not yet bound to a claimBoundthe volume is bound to a claimReleasedthe claim has been deleted, but the associated storage resource is not yet reclaimed by the clusterFailedthe volume has failed its (automated) reclamationYou can see the name of the PVC bound to the PV using kubectl describe persistentvolume .Phase transition timestampFEATURE STATE: Kubernetes v1.31 [stable] (enabled by default: true)The .status field for a PersistentVolume can include an alpha lastPhaseTransitionTime field. This field records the timestamp of when the volume last transitioned its phase. For newly created volumes the phase is set to Pending and lastPhaseTransitionTime is set to the current time.PersistentVolumeClaimsEach PVC contains a spec and status, which is the specification and status of the claim. The name of a PersistentVolumeClaim object must be a valid DNS subdomain name.apiVersion: v1 kind: PersistentVolumeClaim metadata: name: myclaim spec: accessModes: \- ReadWriteOnce volumeMode: Filesystem resources: requests: storage: 8Gi storageClassName: slow selector: matchLabels: release: "stable" matchExpressions: \- {key: environment, operator: In, values: [dev]} Access ModesClaims use the same conventions as volumes when requesting storage with specific access modes.Volume ModesClaims use the same convention as volumes to indicate the consumption of the volume as either a filesystem or block device.Volume NameClaims can use the volumeName field to explicitly bind to a specific PersistentVolume. You can also leave volumeName unset, indicating that you'd like Kubernetes to set up a new PersistentVolume that matches the claim. If the specified PV is already bound to another PVC, the binding will be stuck in a pending state.ResourcesClaims, like Pods, can request specific quantities of a resource. In this case, the request is for storage. The same resource model applies to both volumes and claims.Note:For Filesystem volumes, the storage request refers to the "outer" volume size (i.e. the allocated size from the storage backend). This means that the writeable size may be slightly lower for providers that build a filesystem on top of a block device, due to filesystem overhead. This is especially visible with XFS, where many metadata features are enabled by default.SelectorClaims can specify a label selector to further filter the set of volumes. Only the volumes whose labels match the selector can be bound to the claim. The selector can consist of two fields:matchLabels - the volume must have a label with this valuematchExpressions - a list of requirements made by specifying key, list of values, and operator that relates the key and values. Valid operators include In, NotIn, Exists, and DoesNotExist.All of the requirements, from both matchLabels and matchExpressions, are ANDed together – they must all be satisfied in order to match.ClassA claim can request a particular class by specifying the name of a StorageClass using the attribute storageClassName. Only PVs of the requested class, ones with the same storageClassName as the PVC, can be bound to the PVC.PVCs don't necessarily have to request a class. A PVC with its storageClassName set equal to "" is always interpreted to be requesting a PV with no class, so it can only be bound to PVs with no class (no annotation or one set equal to ""). A PVC with no storageClassName is not quite the same and is treated differently by the cluster, depending on whether the DefaultStorageClass admission plugin is turned on.If the admission plugin is turned on, the administrator may specify a default StorageClass. All PVCs that have no storageClassName can be bound only to PVs of that default. Specifying a default StorageClass is done by setting the annotation storageclass.kubernetes.io/is-default-class equal to true in a StorageClass object. If the administrator does not specify a default, the cluster responds to PVC creation as if the admission plugin were turned off. If more than one default StorageClass is specified, the newest default is used when the PVC is dynamically provisioned.If the admission plugin is turned off, there is no notion of a default StorageClass. All PVCs that have storageClassName set to "" can be bound only to PVs that have storageClassName also set to "". However, PVCs with missing storageClassName can be updated later once default StorageClass becomes available. If the PVC gets updated it will no longer bind to PVs that have storageClassName also set to "".See retroactive default StorageClass assignment for more details.Depending on installation method, a default StorageClass may be deployed to a Kubernetes cluster by addon manager during installation.When a PVC specifies a selector in addition to requesting a StorageClass, the requirements are ANDed together: only a PV of the requested class and with the requested labels may be bound to the PVC.Note:Currently, a PVC with a non-empty selector can't have a PV dynamically provisioned for it.In the past, the annotation volume.beta.kubernetes.io/storage-class was used instead of storageClassName attribute. This annotation is still working; however, it won't be supported in a future Kubernetes release.Retroactive default StorageClass assignmentFEATURE STATE: Kubernetes v1.28 [stable]You can create a PersistentVolumeClaim without specifying a storageClassName for the new PVC, and you can do so even when no default StorageClass exists in your cluster. In this case, the new PVC creates as you defined it, and the storageClassName of that PVC remains unset until default becomes available.When a default StorageClass becomes available, the control plane identifies any existing PVCs without storageClassName. For the PVCs that either have an empty value for storageClassName or do not have this key, the control plane then updates those PVCs to set storageClassName to match the new default StorageClass. If you have an existing PVC where the storageClassName is "", and you configure a default StorageClass, then this PVC will not get updated.In order to keep binding to PVs with storageClassName set to "" (while a default StorageClass is present), you need to set the storageClassName of the associated PVC to "".This behavior helps administrators change default StorageClass by removing the old one first and then creating or setting another one. This brief window while there is no default causes PVCs without storageClassName created at that time to not have any default, but due to the retroactive default StorageClass assignment this way of changing defaults is safe.Claims As VolumesPods access storage by using the claim as a volume. Claims must exist in the same namespace as the Pod using the claim. The cluster finds the claim in the Pod's namespace and uses it to get the PersistentVolume backing the claim. The volume is then mounted to the host and into the Pod.apiVersion: v1 kind: Pod metadata: name: mypod spec: containers: \- name: myfrontend image: nginx volumeMounts: \- mountPath: "/var/www/html" name: mypd volumes: \- name: mypd persistentVolumeClaim: claimName: myclaim A Note on NamespacesPersistentVolumes binds are exclusive, and since PersistentVolumeClaims are namespaced objects, mounting claims with "Many" modes (ROX, RWX) is only possible within one namespace.PersistentVolumes typed hostPathA hostPath PersistentVolume uses a file or directory on the Node to emulate network-attached storage. See an example of hostPath typed volume.Raw Block Volume SupportFEATURE STATE: Kubernetes v1.18 [stable]The following volume plugins support raw block volumes, including dynamic provisioning where applicable:CSI (including some CSI migrated volume types)FC (Fibre Channel)iSCSILocal volumePersistentVolume using a Raw Block VolumeapiVersion: v1 kind: PersistentVolume metadata: name: block-pv spec: capacity: storage: 10Gi accessModes: \- ReadWriteOnce volumeMode: Block persistentVolumeReclaimPolicy: Retain fc: targetWWNs: ["50060e801049cfd1"] lun: 0 readOnly: false PersistentVolumeClaim requesting a Raw Block VolumeapiVersion: v1 kind: PersistentVolumeClaim metadata: name: block-pvc spec: accessModes: \- ReadWriteOnce volumeMode: Block resources: requests: storage: 10Gi Pod specification adding Raw Block Device path in containerapiVersion: v1 kind: Pod metadata: name: pod-with-block-volume spec: containers: \- name: fc-container image: fedora:26 command: ["/bin/sh", "-c"] args: [ "tail -f /dev/null" ] volumeDevices: \- name: data devicePath: /dev/xvda volumes: \- name: data persistentVolumeClaim: claimName: block-pvc Note:When adding a raw block device for a Pod, you specify the device path in the container instead of a mount path.Binding Block VolumesIf a user requests a raw block volume by indicating this using the volumeMode field in the PersistentVolumeClaim spec, the binding rules differ slightly from previous releases that didn't consider this mode as part of the spec. Listed is a table of possible combinations the user and admin might specify for requesting a raw block device. The table indicates if the volume will be bound or not given the combinations: Volume binding matrix for statically provisioned volumes:PV volumeModePVC volumeModeResultunspecifiedunspecifiedBINDunspecifiedBlockNO BINDunspecifiedFilesystemBINDBlockunspecifiedNO BINDBlockBlockBINDBlockFilesystemNO BINDFilesystemFilesystemBINDFilesystemBlockNO BINDFilesystemunspecifiedBINDNote:Only statically provisioned volumes are supported for alpha release. Administrators should take care to consider these values when working with raw block devices.Volume Snapshot and Restore Volume from Snapshot SupportFEATURE STATE: Kubernetes v1.20 [stable]Volume snapshots only support the out-of-tree CSI volume plugins. For details, see Volume Snapshots. In-tree volume plugins are deprecated. You can read about the deprecated volume plugins in the Volume Plugin FAQ.Create a PersistentVolumeClaim from a Volume SnapshotapiVersion: v1 kind: PersistentVolumeClaim metadata: name: restore-pvc spec: storageClassName: csi-hostpath-sc dataSource: name: new-snapshot-test kind: VolumeSnapshot apiGroup: snapshot.storage.k8s.io accessModes: \- ReadWriteOnce resources: requests: storage: 10Gi Volume CloningVolume Cloning only available for CSI volume plugins.Create PersistentVolumeClaim from an existing PVCapiVersion: v1 kind: PersistentVolumeClaim metadata: name: cloned-pvc spec: storageClassName: my-csi-plugin dataSource: name: existing-src-pvc-name kind: PersistentVolumeClaim accessModes: \- ReadWriteOnce resources: requests: storage: 10Gi Volume populators and data sourcesFEATURE STATE: Kubernetes v1.24 [beta]Kubernetes supports custom volume populators. To use custom volume populators, you must enable the AnyVolumeDataSource feature gate for the kube-apiserver and kube-controller-manager.Volume populators take advantage of a PVC spec field called dataSourceRef. Unlike the dataSource field, which can only contain either a reference to another PersistentVolumeClaim or to a VolumeSnapshot, the dataSourceRef field can contain a reference to any object in the same namespace, except for core objects other than PVCs. For clusters that have the feature gate enabled, use of the dataSourceRef is preferred over dataSource.Cross namespace data sourcesFEATURE STATE: Kubernetes v1.26 [alpha]Kubernetes supports cross namespace volume data sources. To use cross namespace volume data sources, you must enable the AnyVolumeDataSource and CrossNamespaceVolumeDataSource feature gates for the kube-apiserver and kube-controller-manager. Also, you must enable the CrossNamespaceVolumeDataSource feature gate for the csi-provisioner.Enabling the CrossNamespaceVolumeDataSource feature gate allows you to specify a namespace in the dataSourceRef field.Note:When you specify a namespace for a volume data source, Kubernetes checks for a ReferenceGrant in the other namespace before accepting the reference. ReferenceGrant is part of the gateway.networking.k8s.io extension APIs. See ReferenceGrant in the Gateway API documentation for details. This means that you must extend your Kubernetes cluster with at least ReferenceGrant from the Gateway API before you can use this mechanism.Data source referencesThe dataSourceRef field behaves almost the same as the dataSource field. If one is specified while the other is not, the API server will give both fields the same value. Neither field can be changed after creation, and attempting to specify different values for the two fields will result in a validation error. Therefore the two fields will always have the same contents.There are two differences between the dataSourceRef field and the dataSource field that users should be aware of:The dataSource field ignores invalid values (as if the field was blank) while the dataSourceRef field never ignores values and will cause an error if an invalid value is used. Invalid values are any core object (objects with no apiGroup) except for PVCs.The dataSourceRef field may contain different types of objects, while the dataSource field only allows PVCs and VolumeSnapshots.When the CrossNamespaceVolumeDataSource feature is enabled, there are additional differences:The dataSource field only allows local objects, while the dataSourceRef field allows objects in any namespaces.When namespace is specified, dataSource and dataSourceRef are not synced.Users should always use dataSourceRef on clusters that have the feature gate enabled, and fall back to dataSource on clusters that do not. It is not necessary to look at both fields under any circumstance. The duplicated values with slightly different semantics exist only for backwards compatibility. In particular, a mixture of older and newer controllers are able to interoperate because the fields are the same.Using volume populatorsVolume populators are controllers that can create non-empty volumes, where the contents of the volume are determined by a Custom Resource. Users create a populated volume by referring to a Custom Resource using the dataSourceRef field:apiVersion: v1 kind: PersistentVolumeClaim metadata: name: populated-pvc spec: dataSourceRef: name: example-name kind: ExampleDataSource apiGroup: example.storage.k8s.io accessModes: \- ReadWriteOnce resources: requests: storage: 10Gi Because volume populators are external components, attempts to create a PVC that uses one can fail if not all the correct components are installed. External controllers should generate events on the PVC to provide feedback on the status of the creation, including warnings if the PVC cannot be created due to some missing component.You can install the alpha volume data source validator controller into your cluster. That controller generates warning Events on a PVC in the case that no populator is registered to handle that kind of data source. When a suitable populator is installed for a PVC, it's the responsibility of that populator controller to report Events that relate to volume creation and issues during the process.Using a cross-namespace volume data sourceFEATURE STATE: Kubernetes v1.26 [alpha]Create a ReferenceGrant to allow the namespace owner to accept the reference. You define a populated volume by specifying a cross namespace volume data source using the dataSourceRef field. You must already have a valid ReferenceGrant in the source namespace:apiVersion: gateway.networking.k8s.io/v1beta1 kind: ReferenceGrant metadata: name: allow-ns1-pvc namespace: default spec: from: \- group: "" kind: PersistentVolumeClaim namespace: ns1 to: \- group: snapshot.storage.k8s.io kind: VolumeSnapshot name: new-snapshot-demo apiVersion: v1 kind: PersistentVolumeClaim metadata: name: foo-pvc namespace: ns1 spec: storageClassName: example accessModes: \- ReadWriteOnce resources: requests: storage: 1Gi dataSourceRef: apiGroup: snapshot.storage.k8s.io kind: VolumeSnapshot name: new-snapshot-demo namespace: default volumeMode: Filesystem Writing Portable ConfigurationIf you're writing configuration templates or examples that run on a wide range of clusters and need persistent storage, it is recommended that you use the following pattern:Include PersistentVolumeClaim objects in your bundle of config (alongside Deployments, ConfigMaps, etc).Do not include PersistentVolume objects in the config, since the user instantiating the config may not have permission to create PersistentVolumes.Give the user the option of providing a storage class name when instantiating the template.If the user provides a storage class name, put that value into the persistentVolumeClaim.storageClassName field. This will cause the PVC to match the right storage class if the cluster has StorageClasses enabled by the admin.If the user does not provide a storage class name, leave the persistentVolumeClaim.storageClassName field as nil. This will cause a PV to be automatically provisioned for the user with the default StorageClass in the cluster. Many cluster environments have a default StorageClass installed, or administrators can create their own default StorageClass.In your tooling, watch for PVCs that are not getting bound after some time and surface this to the user, as this may indicate that the cluster has no dynamic storage support (in which case the user should create a matching PV) or the cluster has no storage system (in which case the user cannot deploy config requiring PVCs).What's nextLearn more about Creating a PersistentVolume.Learn more about Creating a PersistentVolumeClaim.Read the Persistent Storage design document.API referencesRead about the APIs described in this page:PersistentVolumePersistentVolumeClaim3.6.3 - Projected VolumesThis document describes projected volumes in Kubernetes. Familiarity with volumes is suggested.IntroductionA projected volume maps several existing volume sources into the same directory.Currently, the following types of volume sources can be projected:secretdownwardAPIconfigMapserviceAccountTokenclusterTrustBundleAll sources are required to be in the same namespace as the Pod. For more details, see the all-in-one volume design document.Example configuration with a secret, a downwardAPI, and a configMappods/storage/projected-secret-downwardapi-configmap.yaml apiVersion: v1 kind: Pod metadata: name: volume-test spec: containers: \- name: container-test image: busybox:1.28 command: ["sleep", "3600"] volumeMounts: \- name: all-in-one mountPath: "/projected-volume" readOnly: true volumes: \- name: all-in-one projected: sources: \- secret: name: mysecret items: \- key: username path: my-group/my-username \- downwardAPI: items: \- path: "labels" fieldRef: fieldPath: metadata.labels \- path: "cpu_limit" resourceFieldRef: containerName: container-test resource: limits.cpu \- configMap: name: myconfigmap items: \- key: config path: my-group/my-config Example configuration: secrets with a non-default permission mode setpods/storage/projected-secrets-nondefault-permission-mode.yaml apiVersion: v1 kind: Pod metadata: name: volume-test spec: containers: \- name: container-test image: busybox:1.28 command: ["sleep", "3600"] volumeMounts: \- name: all-in-one mountPath: "/projected-volume" readOnly: true volumes: \- name: all-in-one projected: sources: \- secret: name: mysecret items: \- key: username path: my-group/my-username \- secret: name: mysecret2 items: \- key: password path: my-group/my-password mode: 511 Each projected volume source is listed in the spec under sources. The parameters are nearly the same with two exceptions:For secrets, the secretName field has been changed to name to be consistent with ConfigMap naming.The defaultMode can only be specified at the projected level and not for each volume source. However, as illustrated above, you can explicitly set the mode for each individual projection.serviceAccountToken projected volumesYou can inject the token for the current service account into a Pod at a specified path. For example:pods/storage/projected-service-account-token.yaml apiVersion: v1 kind: Pod metadata: name: sa-token-test spec: containers: \- name: container-test image: busybox:1.28 command: ["sleep", "3600"] volumeMounts: \- name: token-vol mountPath: "/service-account" readOnly: true serviceAccountName: default volumes: \- name: token-vol projected: sources: \- serviceAccountToken: audience: api expirationSeconds: 3600 path: token The example Pod has a projected volume containing the injected service account token. Containers in this Pod can use that token to access the Kubernetes API server, authenticating with the identity of the pod's ServiceAccount. The audience field contains the intended audience of the token. A recipient of the token must identify itself with an identifier specified in the audience of the token, and otherwise should reject the token. This field is optional and it defaults to the identifier of the API server.The expirationSeconds is the expected duration of validity of the service account token. It defaults to 1 hour and must be at least 10 minutes (600 seconds). An administrator can also limit its maximum value by specifying the --service-account-max-token-expiration option for the API server. The path field specifies a relative path to the mount point of the projected volume.Note:A container using a projected volume source as a subPath volume mount will not receive updates for those volume sources.clusterTrustBundle projected volumesFEATURE STATE: Kubernetes v1.33 [beta] (enabled by default: false)Note:To use this feature in Kubernetes 1.33, you must enable support for ClusterTrustBundle objects with the ClusterTrustBundle feature gate and --runtime-config=certificates.k8s.io/v1beta1/clustertrustbundles=true kube-apiserver flag, then enable the ClusterTrustBundleProjection feature gate.The clusterTrustBundle projected volume source injects the contents of one or more ClusterTrustBundle objects as an automatically-updating file in the container filesystem.ClusterTrustBundles can be selected either by name or by signer name.To select by name, use the name field to designate a single ClusterTrustBundle object.To select by signer name, use the signerName field (and optionally the labelSelector field) to designate a set of ClusterTrustBundle objects that use the given signer name. If labelSelector is not present, then all ClusterTrustBundles for that signer are selected.The kubelet deduplicates the certificates in the selected ClusterTrustBundle objects, normalizes the PEM representations (discarding comments and headers), reorders the certificates, and writes them into the file named by path. As the set of selected ClusterTrustBundles or their content changes, kubelet keeps the file up-to-date.By default, the kubelet will prevent the pod from starting if the named ClusterTrustBundle is not found, or if signerName / labelSelector do not match any ClusterTrustBundles. If this behavior is not what you want, then set the optional field to true, and the pod will start up with an empty file at path.pods/storage/projected-clustertrustbundle.yaml apiVersion: v1 kind: Pod metadata: name: sa-ctb-name-test spec: containers: \- name: container-test image: busybox command: ["sleep", "3600"] volumeMounts: \- name: token-vol mountPath: "/root-certificates" readOnly: true serviceAccountName: default volumes: \- name: token-vol projected: sources: \- clusterTrustBundle: name: example path: example-roots.pem \- clusterTrustBundle: signerName: "example.com/mysigner" labelSelector: matchLabels: version: live path: mysigner-roots.pem optional: true SecurityContext interactionsThe proposal for file permission handling in projected service account volume enhancement introduced the projected files having the correct owner permissions set.LinuxIn Linux pods that have a projected volume and RunAsUser set in the Pod SecurityContext, the projected files have the correct ownership set including container user ownership.When all containers in a pod have the same runAsUser set in their PodSecurityContext or container SecurityContext, then the kubelet ensures that the contents of the serviceAccountToken volume are owned by that user, and the token file has its permission mode set to 0600.Note:Ephemeral containers added to a Pod after it is created do not change volume permissions that were set when the pod was created.If a Pod's serviceAccountToken volume permissions were set to 0600 because all other containers in the Pod have the same runAsUser, ephemeral containers must use the same runAsUser to be able to read the token.WindowsIn Windows pods that have a projected volume and RunAsUsername set in the Pod SecurityContext, the ownership is not enforced due to the way user accounts are managed in Windows. Windows stores and manages local user and group accounts in a database file called Security Account Manager (SAM). Each container maintains its own instance of the SAM database, to which the host has no visibility into while the container is running. Windows containers are designed to run the user mode portion of the OS in isolation from the host, hence the maintenance of a virtual SAM database. As a result, the kubelet running on the host does not have the ability to dynamically configure host file ownership for virtualized container accounts. It is recommended that if files on the host machine are to be shared with the container then they should be placed into their own volume mount outside of C:\\.By default, the projected files will have the following ownership as shown for an example projected volume file:PS C:\> Get-Acl C:\var\run\secrets\kubernetes.io\serviceaccount\\..2021_08_31_22_22_18.318230061\ca.crt | Format-List Path : Microsoft.PowerShell.Core\FileSystem::C:\var\run\secrets\kubernetes.io\serviceaccount\\..2021_08_31_22_22_18.318230061\ca.crt Owner : BUILTIN\Administrators Group : NT AUTHORITY\SYSTEM Access : NT AUTHORITY\SYSTEM Allow FullControl BUILTIN\Administrators Allow FullControl BUILTIN\Users Allow ReadAndExecute, Synchronize Audit : Sddl : O:BAG:SYD:AI(A;ID;FA;;;SY)(A;ID;FA;;;BA)(A;ID;0x1200a9;;;BU) This implies all administrator users like ContainerAdministrator will have read, write and execute access while, non-administrator users will have read and execute access.Note:In general, granting the container access to the host is discouraged as it can open the door for potential security exploits.Creating a Windows Pod with RunAsUser in it's SecurityContext will result in the Pod being stuck at ContainerCreating forever. So it is advised to not use the Linux only RunAsUser option with Windows Pods.3.6.4 - Ephemeral VolumesThis document describes ephemeral volumes in Kubernetes. Familiarity with volumes is suggested, in particular PersistentVolumeClaim and PersistentVolume.Some applications need additional storage but don't care whether that data is stored persistently across restarts. For example, caching services are often limited by memory size and can move infrequently used data into storage that is slower than memory with little impact on overall performance.Other applications expect some read-only input data to be present in files, like configuration data or secret keys.Ephemeral volumes are designed for these use cases. Because volumes follow the Pod's lifetime and get created and deleted along with the Pod, Pods can be stopped and restarted without being limited to where some persistent volume is available.Ephemeral volumes are specified inline in the Pod spec, which simplifies application deployment and management.Types of ephemeral volumesKubernetes supports several different kinds of ephemeral volumes for different purposes:emptyDir: empty at Pod startup, with storage coming locally from the kubelet base directory (usually the root disk) or RAMconfigMap, downwardAPI, secret: inject different kinds of Kubernetes data into a Podimage: allows mounting container image files or artifacts, directly to a Pod.CSI ephemeral volumes: similar to the previous volume kinds, but provided by special CSI drivers which specifically support this featuregeneric ephemeral volumes, which can be provided by all storage drivers that also support persistent volumesemptyDir, configMap, downwardAPI, secret are provided as local ephemeral storage. They are managed by kubelet on each node.CSI ephemeral volumes must be provided by third-party CSI storage drivers.Generic ephemeral volumes can be provided by third-party CSI storage drivers, but also by any other storage driver that supports dynamic provisioning. Some CSI drivers are written specifically for CSI ephemeral volumes and do not support dynamic provisioning: those then cannot be used for generic ephemeral volumes.The advantage of using third-party drivers is that they can offer functionality that Kubernetes itself does not support, for example storage with different performance characteristics than the disk that is managed by kubelet, or injecting different data.CSI ephemeral volumesFEATURE STATE: Kubernetes v1.25 [stable]Note:CSI ephemeral volumes are only supported by a subset of CSI drivers. The Kubernetes CSI Drivers list shows which drivers support ephemeral volumes.Conceptually, CSI ephemeral volumes are similar to configMap, downwardAPI and secret volume types: the storage is managed locally on each node and is created together with other local resources after a Pod has been scheduled onto a node. Kubernetes has no concept of rescheduling Pods anymore at this stage. Volume creation has to be unlikely to fail, otherwise Pod startup gets stuck. In particular, storage capacity aware Pod scheduling is not supported for these volumes. They are currently also not covered by the storage resource usage limits of a Pod, because that is something that kubelet can only enforce for storage that it manages itself.Here's an example manifest for a Pod that uses CSI ephemeral storage:kind: Pod apiVersion: v1 metadata: name: my-csi-app spec: containers: \- name: my-frontend image: busybox:1.28 volumeMounts: \- mountPath: "/data" name: my-csi-inline-vol command: [ "sleep", "1000000" ] volumes: \- name: my-csi-inline-vol csi: driver: inline.storage.kubernetes.io volumeAttributes: foo: bar The volumeAttributes determine what volume is prepared by the driver. These attributes are specific to each driver and not standardized. See the documentation of each CSI driver for further instructions.CSI driver restrictionsCSI ephemeral volumes allow users to provide volumeAttributes directly to the CSI driver as part of the Pod spec. A CSI driver allowing volumeAttributes that are typically restricted to administrators is NOT suitable for use in an inline ephemeral volume. For example, parameters that are normally defined in the StorageClass should not be exposed to users through the use of inline ephemeral volumes.Cluster administrators who need to restrict the CSI drivers that are allowed to be used as inline volumes within a Pod spec may do so by:Removing Ephemeral from volumeLifecycleModes in the CSIDriver spec, which prevents the driver from being used as an inline ephemeral volume.Using an admission webhook to restrict how this driver is used.Generic ephemeral volumesFEATURE STATE: Kubernetes v1.23 [stable]Generic ephemeral volumes are similar to emptyDir volumes in the sense that they provide a per-pod directory for scratch data that is usually empty after provisioning. But they may also have additional features:Storage can be local or network-attached.Volumes can have a fixed size that Pods are not able to exceed.Volumes may have some initial data, depending on the driver and parameters.Typical operations on volumes are supported assuming that the driver supports them, including snapshotting, cloning, resizing, and storage capacity tracking.Example:kind: Pod apiVersion: v1 metadata: name: my-app spec: containers: \- name: my-frontend image: busybox:1.28 volumeMounts: \- mountPath: "/scratch" name: scratch-volume command: [ "sleep", "1000000" ] volumes: \- name: scratch-volume ephemeral: volumeClaimTemplate: metadata: labels: type: my-frontend-volume spec: accessModes: [ "ReadWriteOnce" ] storageClassName: "scratch-storage-class" resources: requests: storage: 1Gi Lifecycle and PersistentVolumeClaimThe key design idea is that the parameters for a volume claim are allowed inside a volume source of the Pod. Labels, annotations and the whole set of fields for a PersistentVolumeClaim are supported. When such a Pod gets created, the ephemeral volume controller then creates an actual PersistentVolumeClaim object in the same namespace as the Pod and ensures that the PersistentVolumeClaim gets deleted when the Pod gets deleted.That triggers volume binding and/or provisioning, either immediately if the StorageClass uses immediate volume binding or when the Pod is tentatively scheduled onto a node (WaitForFirstConsumer volume binding mode). The latter is recommended for generic ephemeral volumes because then the scheduler is free to choose a suitable node for the Pod. With immediate binding, the scheduler is forced to select a node that has access to the volume once it is available.In terms of resource ownership, a Pod that has generic ephemeral storage is the owner of the PersistentVolumeClaim(s) that provide that ephemeral storage. When the Pod is deleted, the Kubernetes garbage collector deletes the PVC, which then usually triggers deletion of the volume because the default reclaim policy of storage classes is to delete volumes. You can create quasi-ephemeral local storage using a StorageClass with a reclaim policy of retain: the storage outlives the Pod, and in this case you need to ensure that volume clean up happens separately.While these PVCs exist, they can be used like any other PVC. In particular, they can be referenced as data source in volume cloning or snapshotting. The PVC object also holds the current status of the volume.PersistentVolumeClaim namingNaming of the automatically created PVCs is deterministic: the name is a combination of the Pod name and volume name, with a hyphen (-) in the middle. In the example above, the PVC name will be my-app-scratch-volume. This deterministic naming makes it easier to interact with the PVC because one does not have to search for it once the Pod name and volume name are known.The deterministic naming also introduces a potential conflict between different Pods (a Pod "pod-a" with volume "scratch" and another Pod with name "pod" and volume "a-scratch" both end up with the same PVC name "pod-a-scratch") and between Pods and manually created PVCs.Such conflicts are detected: a PVC is only used for an ephemeral volume if it was created for the Pod. This check is based on the ownership relationship. An existing PVC is not overwritten or modified. But this does not resolve the conflict because without the right PVC, the Pod cannot start.Caution:Take care when naming Pods and volumes inside the same namespace, so that these conflicts can't occur.SecurityUsing generic ephemeral volumes allows users to create PVCs indirectly if they can create Pods, even if they do not have permission to create PVCs directly. Cluster administrators must be aware of this. If this does not fit their security model, they should use an admission webhook that rejects objects like Pods that have a generic ephemeral volume.The normal namespace quota for PVCs still applies, so even if users are allowed to use this new mechanism, they cannot use it to circumvent other policies.What's nextEphemeral volumes managed by kubeletSee local ephemeral storage.CSI ephemeral volumesFor more information on the design, see the Ephemeral Inline CSI volumes KEP.For more information on further development of this feature, see the enhancement tracking issue #596.Generic ephemeral volumesFor more information on the design, see the Generic ephemeral inline volumes KEP.3.6.5 - Storage ClassesThis document describes the concept of a StorageClass in Kubernetes. Familiarity with volumes and persistent volumes is suggested.A StorageClass provides a way for administrators to describe the classes of storage they offer. Different classes might map to quality-of-service levels, or to backup policies, or to arbitrary policies determined by the cluster administrators. Kubernetes itself is unopinionated about what classes represent.The Kubernetes concept of a storage class is similar to “profiles” in some other storage system designs.StorageClass objectsEach StorageClass contains the fields provisioner, parameters, and reclaimPolicy, which are used when a PersistentVolume belonging to the class needs to be dynamically provisioned to satisfy a PersistentVolumeClaim (PVC).The name of a StorageClass object is significant, and is how users can request a particular class. Administrators set the name and other parameters of a class when first creating StorageClass objects.As an administrator, you can specify a default StorageClass that applies to any PVCs that don't request a specific class. For more details, see the PersistentVolumeClaim concept.Here's an example of a StorageClass:storage/storageclass-low-latency.yaml apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: low-latency annotations: storageclass.kubernetes.io/is-default-class: "false" provisioner: csi-driver.example-vendor.example reclaimPolicy: Retain # default value is Delete allowVolumeExpansion: true mountOptions: \- discard # this might enable UNMAP / TRIM at the block storage layer volumeBindingMode: WaitForFirstConsumer parameters: guaranteedReadWriteLatency: "true" # provider-specific Default StorageClassYou can mark a StorageClass as the default for your cluster. For instructions on setting the default StorageClass, see Change the default StorageClass.When a PVC does not specify a storageClassName, the default StorageClass is used.If you set the storageclass.kubernetes.io/is-default-class annotation to true on more than one StorageClass in your cluster, and you then create a PersistentVolumeClaim with no storageClassName set, Kubernetes uses the most recently created default StorageClass.Note:You should try to only have one StorageClass in your cluster that is marked as the default. The reason that Kubernetes allows you to have multiple default StorageClasses is to allow for seamless migration.You can create a PersistentVolumeClaim without specifying a storageClassName for the new PVC, and you can do so even when no default StorageClass exists in your cluster. In this case, the new PVC creates as you defined it, and the storageClassName of that PVC remains unset until a default becomes available.You can have a cluster without any default StorageClass. If you don't mark any StorageClass as default (and one hasn't been set for you by, for example, a cloud provider), then Kubernetes cannot apply that defaulting for PersistentVolumeClaims that need it.If or when a default StorageClass becomes available, the control plane identifies any existing PVCs without storageClassName. For the PVCs that either have an empty value for storageClassName or do not have this key, the control plane then updates those PVCs to set storageClassName to match the new default StorageClass. If you have an existing PVC where the storageClassName is "", and you configure a default StorageClass, then this PVC will not get updated.In order to keep binding to PVs with storageClassName set to "" (while a default StorageClass is present), you need to set the storageClassName of the associated PVC to "".ProvisionerEach StorageClass has a provisioner that determines what volume plugin is used for provisioning PVs. This field must be specified.Volume PluginInternal ProvisionerConfig ExampleAzureFile✓Azure FileCephFS--FC--FlexVolume--iSCSI--Local-LocalNFS-NFSPortworxVolume✓Portworx VolumeRBD-Ceph RBDVsphereVolume✓vSphereYou are not restricted to specifying the "internal" provisioners listed here (whose names are prefixed with "kubernetes.io" and shipped alongside Kubernetes). You can also run and specify external provisioners, which are independent programs that follow a specification defined by Kubernetes. Authors of external provisioners have full discretion over where their code lives, how the provisioner is shipped, how it needs to be run, what volume plugin it uses (including Flex), etc. The repository kubernetes-sigs/sig-storage-lib-external-provisioner houses a library for writing external provisioners that implements the bulk of the specification. Some external provisioners are listed under the repository kubernetes-sigs/sig-storage-lib-external-provisioner.For example, NFS doesn't provide an internal provisioner, but an external provisioner can be used. There are also cases when 3rd party storage vendors provide their own external provisioner.Reclaim policyPersistentVolumes that are dynamically created by a StorageClass will have the reclaim policy specified in the reclaimPolicy field of the class, which can be either Delete or Retain. If no reclaimPolicy is specified when a StorageClass object is created, it will default to Delete.PersistentVolumes that are created manually and managed via a StorageClass will have whatever reclaim policy they were assigned at creation.Volume expansionPersistentVolumes can be configured to be expandable. This allows you to resize the volume by editing the corresponding PVC object, requesting a new larger amount of storage.The following types of volumes support volume expansion, when the underlying StorageClass has the field allowVolumeExpansion set to true.Table of Volume types and the version of Kubernetes they requireVolume typeRequired Kubernetes version for volume expansionAzure File1.11CSI1.24FlexVolume1.13Portworx1.11rbd1.11Note:You can only use the volume expansion feature to grow a Volume, not to shrink it.Mount optionsPersistentVolumes that are dynamically created by a StorageClass will have the mount options specified in the mountOptions field of the class.If the volume plugin does not support mount options but mount options are specified, provisioning will fail. Mount options are not validated on either the class or PV. If a mount option is invalid, the PV mount fails.Volume binding modeThe volumeBindingMode field controls when volume binding and dynamic provisioning should occur. When unset, Immediate mode is used by default.The Immediate mode indicates that volume binding and dynamic provisioning occurs once the PersistentVolumeClaim is created. For storage backends that are topology-constrained and not globally accessible from all Nodes in the cluster, PersistentVolumes will be bound or provisioned without knowledge of the Pod's scheduling requirements. This may result in unschedulable Pods.A cluster administrator can address this issue by specifying the WaitForFirstConsumer mode which will delay the binding and provisioning of a PersistentVolume until a Pod using the PersistentVolumeClaim is created. PersistentVolumes will be selected or provisioned conforming to the topology that is specified by the Pod's scheduling constraints. These include, but are not limited to, resource requirements, node selectors, pod affinity and anti-affinity, and taints and tolerations.The following plugins support WaitForFirstConsumer with dynamic provisioning:CSI volumes, provided that the specific CSI driver supports thisThe following plugins support WaitForFirstConsumer with pre-created PersistentVolume binding:CSI volumes, provided that the specific CSI driver supports thislocalNote:If you choose to use WaitForFirstConsumer, do not use nodeName in the Pod spec to specify node affinity. If nodeName is used in this case, the scheduler will be bypassed and PVC will remain in pending state.Instead, you can use node selector for kubernetes.io/hostname:storage/storageclass/pod-volume-binding.yaml apiVersion: v1 kind: Pod metadata: name: task-pv-pod spec: nodeSelector: kubernetes.io/hostname: kube-01 volumes: \- name: task-pv-storage persistentVolumeClaim: claimName: task-pv-claim containers: \- name: task-pv-container image: nginx ports: \- containerPort: 80 name: "http-server" volumeMounts: \- mountPath: "/usr/share/nginx/html" name: task-pv-storage Allowed topologiesWhen a cluster operator specifies the WaitForFirstConsumer volume binding mode, it is no longer necessary to restrict provisioning to specific topologies in most situations. However, if still required, allowedTopologies can be specified.This example demonstrates how to restrict the topology of provisioned volumes to specific zones and should be used as a replacement for the zone and zones parameters for the supported plugins.storage/storageclass/storageclass-topology.yaml apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: standard provisioner: example.com/example parameters: type: pd-standard volumeBindingMode: WaitForFirstConsumer allowedTopologies: \- matchLabelExpressions: \- key: topology.kubernetes.io/zone values: \- us-central-1a \- us-central-1b ParametersStorageClasses have parameters that describe volumes belonging to the storage class. Different parameters may be accepted depending on the provisioner. When a parameter is omitted, some default is used.There can be at most 512 parameters defined for a StorageClass. The total length of the parameters object including its keys and values cannot exceed 256 KiB.AWS EBSKubernetes 1.33 does not include a awsElasticBlockStore volume type.The AWSElasticBlockStore in-tree storage driver was deprecated in the Kubernetes v1.19 release and then removed entirely in the v1.27 release.The Kubernetes project suggests that you use the AWS EBS out-of-tree storage driver instead.Here is an example StorageClass for the AWS EBS CSI driver:storage/storageclass/storageclass-aws-ebs.yaml apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: ebs-sc provisioner: ebs.csi.aws.com volumeBindingMode: WaitForFirstConsumer parameters: csi.storage.k8s.io/fstype: xfs type: io1 iopsPerGB: "50" encrypted: "true" tagSpecification_1: "key1=value1" tagSpecification_2: "key2=value2" allowedTopologies: \- matchLabelExpressions: \- key: topology.ebs.csi.aws.com/zone values: \- us-east-2c tagSpecification: Tags with this prefix are applied to dynamically provisioned EBS volumes.AWS EFSTo configure AWS EFS storage, you can use the out-of-tree AWS_EFS_CSI_DRIVER.storage/storageclass/storageclass-aws-efs.yaml kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: efs-sc provisioner: efs.csi.aws.com parameters: provisioningMode: efs-ap fileSystemId: fs-92107410 directoryPerms: "700" provisioningMode: The type of volume to be provisioned by Amazon EFS. Currently, only access point based provisioning is supported (efs-ap).fileSystemId: The file system under which the access point is created.directoryPerms: The directory permissions of the root directory created by the access point.For more details, refer to the AWS_EFS_CSI_Driver Dynamic Provisioning documentation.NFSTo configure NFS storage, you can use the in-tree driver or the NFS CSI driver for Kubernetes (recommended).storage/storageclass/storageclass-nfs.yaml apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: example-nfs provisioner: example.com/external-nfs parameters: server: nfs-server.example.com path: /share readOnly: "false" server: Server is the hostname or IP address of the NFS server.path: Path that is exported by the NFS server.readOnly: A flag indicating whether the storage will be mounted as read only (default false).Kubernetes doesn't include an internal NFS provisioner. You need to use an external provisioner to create a StorageClass for NFS. Here are some examples:NFS Ganesha server and external provisionerNFS subdir external provisionervSphereThere are two types of provisioners for vSphere storage classes:CSI provisioner: csi.vsphere.vmware.comvCP provisioner: kubernetes.io/vsphere-volumeIn-tree provisioners are deprecated. For more information on the CSI provisioner, see Kubernetes vSphere CSI Driver and vSphereVolume CSI migration.CSI ProvisionerThe vSphere CSI StorageClass provisioner works with Tanzu Kubernetes clusters. For an example, refer to the vSphere CSI repository.vCP ProvisionerThe following examples use the VMware Cloud Provider (vCP) StorageClass provisioner.Create a StorageClass with a user specified disk format.apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: fast provisioner: kubernetes.io/vsphere-volume parameters: diskformat: zeroedthick diskformat: thin, zeroedthick and eagerzeroedthick. Default: "thin".Create a StorageClass with a disk format on a user specified datastore.apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: fast provisioner: kubernetes.io/vsphere-volume parameters: diskformat: zeroedthick datastore: VSANDatastore datastore: The user can also specify the datastore in the StorageClass. The volume will be created on the datastore specified in the StorageClass, which in this case is VSANDatastore. This field is optional. If the datastore is not specified, then the volume will be created on the datastore specified in the vSphere config file used to initialize the vSphere Cloud Provider.Storage Policy Management inside kubernetesUsing existing vCenter SPBM policyOne of the most important features of vSphere for Storage Management is policy based Management. Storage Policy Based Management (SPBM) is a storage policy framework that provides a single unified control plane across a broad range of data services and storage solutions. SPBM enables vSphere administrators to overcome upfront storage provisioning challenges, such as capacity planning, differentiated service levels and managing capacity headroom.The SPBM policies can be specified in the StorageClass using the storagePolicyName parameter.Virtual SAN policy support inside KubernetesVsphere Infrastructure (VI) Admins will have the ability to specify custom Virtual SAN Storage Capabilities during dynamic volume provisioning. You can now define storage requirements, such as performance and availability, in the form of storage capabilities during dynamic volume provisioning. The storage capability requirements are converted into a Virtual SAN policy which are then pushed down to the Virtual SAN layer when a persistent volume (virtual disk) is being created. The virtual disk is distributed across the Virtual SAN datastore to meet the requirements.You can see Storage Policy Based Management for dynamic provisioning of volumes for more details on how to use storage policies for persistent volumes management.There are few vSphere examples which you try out for persistent volume management inside Kubernetes for vSphere.Ceph RBD (deprecated)Note:FEATURE STATE: Kubernetes v1.28 [deprecated]This internal provisioner of Ceph RBD is deprecated. Please use CephFS RBD CSI driver.storage/storageclass/storageclass-ceph-rbd.yaml apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: fast provisioner: kubernetes.io/rbd # This provisioner is deprecated parameters: monitors: 198.19.254.105:6789 adminId: kube adminSecretName: ceph-secret adminSecretNamespace: kube-system pool: kube userId: kube userSecretName: ceph-secret-user userSecretNamespace: default fsType: ext4 imageFormat: "2" imageFeatures: "layering" monitors: Ceph monitors, comma delimited. This parameter is required.adminId: Ceph client ID that is capable of creating images in the pool. Default is "admin".adminSecretName: Secret Name for adminId. This parameter is required. The provided secret must have type "kubernetes.io/rbd".adminSecretNamespace: The namespace for adminSecretName. Default is "default".pool: Ceph RBD pool. Default is "rbd".userId: Ceph client ID that is used to map the RBD image. Default is the same as adminId.userSecretName: The name of Ceph Secret for userId to map RBD image. It must exist in the same namespace as PVCs. This parameter is required. The provided secret must have type "kubernetes.io/rbd", for example created in this way:kubectl create secret generic ceph-secret --type="kubernetes.io/rbd" \ \--from-literal=key='QVFEQ1pMdFhPUnQrSmhBQUFYaERWNHJsZ3BsMmNjcDR6RFZST0E9PQ==' \ \--namespace=kube-system userSecretNamespace: The namespace for userSecretName.fsType: fsType that is supported by kubernetes. Default: "ext4".imageFormat: Ceph RBD image format, "1" or "2". Default is "2".imageFeatures: This parameter is optional and should only be used if you set imageFormat to "2". Currently supported features are layering only. Default is "", and no features are turned on.Azure DiskKubernetes 1.33 does not include a azureDisk volume type.The azureDisk in-tree storage driver was deprecated in the Kubernetes v1.19 release and then removed entirely in the v1.27 release.The Kubernetes project suggests that you use the Azure Disk third party storage driver instead.Azure File (deprecated)storage/storageclass/storageclass-azure-file.yaml apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: azurefile provisioner: kubernetes.io/azure-file parameters: skuName: Standard_LRS location: eastus storageAccount: azure_storage_account_name # example value skuName: Azure storage account SKU tier. Default is empty.location: Azure storage account location. Default is empty.storageAccount: Azure storage account name. Default is empty. If a storage account is not provided, all storage accounts associated with the resource group are searched to find one that matches skuName and location. If a storage account is provided, it must reside in the same resource group as the cluster, and skuName and location are ignored.secretNamespace: the namespace of the secret that contains the Azure Storage Account Name and Key. Default is the same as the Pod.secretName: the name of the secret that contains the Azure Storage Account Name and Key. Default is azure-storage-account--secretreadOnly: a flag indicating whether the storage will be mounted as read only. Defaults to false which means a read/write mount. This setting will impact the ReadOnly setting in VolumeMounts as well.During storage provisioning, a secret named by secretName is created for the mounting credentials. If the cluster has enabled both RBAC and Controller Roles, add the create permission of resource secret for clusterrole system:controller:persistent-volume-binder.In a multi-tenancy context, it is strongly recommended to set the value for secretNamespace explicitly, otherwise the storage account credentials may be read by other users.Portworx volume (deprecated)storage/storageclass/storageclass-portworx-volume.yaml apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: portworx-io-priority-high provisioner: kubernetes.io/portworx-volume # This provisioner is deprecated parameters: repl: "1" snap_interval: "70" priority_io: "high" fs: filesystem to be laid out: none/xfs/ext4 (default: ext4).block_size: block size in Kbytes (default: 32).repl: number of synchronous replicas to be provided in the form of replication factor 1..3 (default: 1) A string is expected here i.e. "1" and not 1.priority_io: determines whether the volume will be created from higher performance or a lower priority storage high/medium/low (default: low).snap_interval: clock/time interval in minutes for when to trigger snapshots. Snapshots are incremental based on difference with the prior snapshot, 0 disables snaps (default: 0). A string is expected here i.e. "70" and not 70.aggregation_level: specifies the number of chunks the volume would be distributed into, 0 indicates a non-aggregated volume (default: 0). A string is expected here i.e. "0" and not 0ephemeral: specifies whether the volume should be cleaned-up after unmount or should be persistent. emptyDir use case can set this value to true and persistent volumes use case such as for databases like Cassandra should set to false, true/false (default false). A string is expected here i.e. "true" and not true.Localstorage/storageclass/storageclass-local.yaml apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: local-storage provisioner: kubernetes.io/no-provisioner # indicates that this StorageClass does not support automatic provisioning volumeBindingMode: WaitForFirstConsumer Local volumes do not support dynamic provisioning in Kubernetes 1.33; however a StorageClass should still be created to delay volume binding until a Pod is actually scheduled to the appropriate node. This is specified by the WaitForFirstConsumer volume binding mode.Delaying volume binding allows the scheduler to consider all of a Pod's scheduling constraints when choosing an appropriate PersistentVolume for a PersistentVolumeClaim.3.6.6 - Volume Attributes ClassesFEATURE STATE: Kubernetes v1.31 [beta] (enabled by default: false)This page assumes that you are familiar with StorageClasses, volumes and PersistentVolumes in Kubernetes.A VolumeAttributesClass provides a way for administrators to describe the mutable "classes" of storage they offer. Different classes might map to different quality-of-service levels. Kubernetes itself is un-opinionated about what these classes represent.This is a beta feature and disabled by default.If you want to test the feature whilst it's beta, you need to enable the VolumeAttributesClass feature gate for the kube-controller-manager, kube-scheduler, and the kube-apiserver. You use the --feature-gates command line argument:--feature-gates="...,VolumeAttributesClass=true" You will also have to enable the storage.k8s.io/v1beta1 API group through the kube-apiserver runtime-config. You use the following command line argument:--runtime-config=storage.k8s.io/v1beta1=true You can also only use VolumeAttributesClasses with storage backed by Container Storage Interface, and only where the relevant CSI driver implements the ModifyVolume API.The VolumeAttributesClass APIEach VolumeAttributesClass contains the driverName and parameters, which are used when a PersistentVolume (PV) belonging to the class needs to be dynamically provisioned or modified.The name of a VolumeAttributesClass object is significant and is how users can request a particular class. Administrators set the name and other parameters of a class when first creating VolumeAttributesClass objects. While the name of a VolumeAttributesClass object in a PersistentVolumeClaim is mutable, the parameters in an existing class are immutable.apiVersion: storage.k8s.io/v1beta1 kind: VolumeAttributesClass metadata: name: silver driverName: pd.csi.storage.gke.io parameters: provisioned-iops: "3000" provisioned-throughput: "50" ProvisionerEach VolumeAttributesClass has a provisioner that determines what volume plugin is used for provisioning PVs. The field driverName must be specified.The feature support for VolumeAttributesClass is implemented in kubernetes-csi/external-provisioner.You are not restricted to specifying the kubernetes-csi/external-provisioner. You can also run and specify external provisioners, which are independent programs that follow a specification defined by Kubernetes. Authors of external provisioners have full discretion over where their code lives, how the provisioner is shipped, how it needs to be run, what volume plugin it uses, etc.ResizerEach VolumeAttributesClass has a resizer that determines what volume plugin is used for modifying PVs. The field driverName must be specified.The modifying volume feature support for VolumeAttributesClass is implemented in kubernetes-csi/external-resizer.For example, an existing PersistentVolumeClaim is using a VolumeAttributesClass named silver:apiVersion: v1 kind: PersistentVolumeClaim metadata: name: test-pv-claim spec: … volumeAttributesClassName: silver … A new VolumeAttributesClass gold is available in the cluster:apiVersion: storage.k8s.io/v1beta1 kind: VolumeAttributesClass metadata: name: gold driverName: pd.csi.storage.gke.io parameters: iops: "4000" throughput: "60" The end user can update the PVC with the new VolumeAttributesClass gold and apply:apiVersion: v1 kind: PersistentVolumeClaim metadata: name: test-pv-claim spec: … volumeAttributesClassName: gold … ParametersVolumeAttributeClasses have parameters that describe volumes belonging to them. Different parameters may be accepted depending on the provisioner or the resizer. For example, the value 4000, for the parameter iops, and the parameter throughput are specific to GCE PD. When a parameter is omitted, the default is used at volume provisioning. If a user applies the PVC with a different VolumeAttributesClass with omitted parameters, the default value of the parameters may be used depending on the CSI driver implementation. Please refer to the related CSI driver documentation for more details.There can be at most 512 parameters defined for a VolumeAttributesClass. The total length of the parameters object including its keys and values cannot exceed 256 KiB.3.6.7 - Dynamic Volume ProvisioningDynamic volume provisioning allows storage volumes to be created on-demand. Without dynamic provisioning, cluster administrators have to manually make calls to their cloud or storage provider to create new storage volumes, and then create PersistentVolume objects to represent them in Kubernetes. The dynamic provisioning feature eliminates the need for cluster administrators to pre-provision storage. Instead, it automatically provisions storage when users create PersistentVolumeClaim objects.BackgroundThe implementation of dynamic volume provisioning is based on the API object StorageClass from the API group storage.k8s.io. A cluster administrator can define as many StorageClass objects as needed, each specifying a volume plugin (aka provisioner) that provisions a volume and the set of parameters to pass to that provisioner when provisioning. A cluster administrator can define and expose multiple flavors of storage (from the same or different storage systems) within a cluster, each with a custom set of parameters. This design also ensures that end users don't have to worry about the complexity and nuances of how storage is provisioned, but still have the ability to select from multiple storage options.More information on storage classes can be found here.Enabling Dynamic ProvisioningTo enable dynamic provisioning, a cluster administrator needs to pre-create one or more StorageClass objects for users. StorageClass objects define which provisioner should be used and what parameters should be passed to that provisioner when dynamic provisioning is invoked. The name of a StorageClass object must be a valid DNS subdomain name.The following manifest creates a storage class "slow" which provisions standard disk-like persistent disks.apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: slow provisioner: kubernetes.io/gce-pd parameters: type: pd-standard The following manifest creates a storage class "fast" which provisions SSD-like persistent disks.apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: fast provisioner: kubernetes.io/gce-pd parameters: type: pd-ssd Using Dynamic ProvisioningUsers request dynamically provisioned storage by including a storage class in their PersistentVolumeClaim. Before Kubernetes v1.6, this was done via the volume.beta.kubernetes.io/storage-class annotation. However, this annotation is deprecated since v1.9. Users now can and should instead use the storageClassName field of the PersistentVolumeClaim object. The value of this field must match the name of a StorageClass configured by the administrator (see below).To select the "fast" storage class, for example, a user would create the following PersistentVolumeClaim:apiVersion: v1 kind: PersistentVolumeClaim metadata: name: claim1 spec: accessModes: \- ReadWriteOnce storageClassName: fast resources: requests: storage: 30Gi This claim results in an SSD-like Persistent Disk being automatically provisioned. When the claim is deleted, the volume is destroyed.Defaulting BehaviorDynamic provisioning can be enabled on a cluster such that all claims are dynamically provisioned if no storage class is specified. A cluster administrator can enable this behavior by:Marking one StorageClass object as default;Making sure that the DefaultStorageClass admission controller is enabled on the API server.An administrator can mark a specific StorageClass as default by adding the storageclass.kubernetes.io/is-default-class annotation to it. When a default StorageClass exists in a cluster and a user creates a PersistentVolumeClaim with storageClassName unspecified, the DefaultStorageClass admission controller automatically adds the storageClassName field pointing to the default storage class.Note that if you set the storageclass.kubernetes.io/is-default-class annotation to true on more than one StorageClass in your cluster, and you then create a PersistentVolumeClaim with no storageClassName set, Kubernetes uses the most recently created default StorageClass.Topology AwarenessIn Multi-Zone clusters, Pods can be spread across Zones in a Region. Single-Zone storage backends should be provisioned in the Zones where Pods are scheduled. This can be accomplished by setting the Volume Binding Mode.3.6.8 - Volume SnapshotsIn Kubernetes, a VolumeSnapshot represents a snapshot of a volume on a storage system. This document assumes that you are already familiar with Kubernetes persistent volumes.IntroductionSimilar to how API resources PersistentVolume and PersistentVolumeClaim are used to provision volumes for users and administrators, VolumeSnapshotContent and VolumeSnapshot API resources are provided to create volume snapshots for users and administrators.A VolumeSnapshotContent is a snapshot taken from a volume in the cluster that has been provisioned by an administrator. It is a resource in the cluster just like a PersistentVolume is a cluster resource.A VolumeSnapshot is a request for snapshot of a volume by a user. It is similar to a PersistentVolumeClaim.VolumeSnapshotClass allows you to specify different attributes belonging to a VolumeSnapshot. These attributes may differ among snapshots taken from the same volume on the storage system and therefore cannot be expressed by using the same StorageClass of a PersistentVolumeClaim.Volume snapshots provide Kubernetes users with a standardized way to copy a volume's contents at a particular point in time without creating an entirely new volume. This functionality enables, for example, database administrators to backup databases before performing edit or delete modifications.Users need to be aware of the following when using this feature:API Objects VolumeSnapshot, VolumeSnapshotContent, and VolumeSnapshotClass are CRDs, not part of the core API.VolumeSnapshot support is only available for CSI drivers.As part of the deployment process of VolumeSnapshot, the Kubernetes team provides a snapshot controller to be deployed into the control plane, and a sidecar helper container called csi-snapshotter to be deployed together with the CSI driver. The snapshot controller watches VolumeSnapshot and VolumeSnapshotContent objects and is responsible for the creation and deletion of VolumeSnapshotContent object. The sidecar csi-snapshotter watches VolumeSnapshotContent objects and triggers CreateSnapshot and DeleteSnapshot operations against a CSI endpoint.There is also a validating webhook server which provides tightened validation on snapshot objects. This should be installed by the Kubernetes distros along with the snapshot controller and CRDs, not CSI drivers. It should be installed in all Kubernetes clusters that has the snapshot feature enabled.CSI drivers may or may not have implemented the volume snapshot functionality. The CSI drivers that have provided support for volume snapshot will likely use the csi-snapshotter. See CSI Driver documentation for details.The CRDs and snapshot controller installations are the responsibility of the Kubernetes distribution.For advanced use cases, such as creating group snapshots of multiple volumes, see the external CSI Volume Group Snapshot documentation.Lifecycle of a volume snapshot and volume snapshot contentVolumeSnapshotContents are resources in the cluster. VolumeSnapshots are requests for those resources. The interaction between VolumeSnapshotContents and VolumeSnapshots follow this lifecycle:Provisioning Volume SnapshotThere are two ways snapshots may be provisioned: pre-provisioned or dynamically provisioned.Pre-provisionedA cluster administrator creates a number of VolumeSnapshotContents. They carry the details of the real volume snapshot on the storage system which is available for use by cluster users. They exist in the Kubernetes API and are available for consumption.DynamicInstead of using a pre-existing snapshot, you can request that a snapshot to be dynamically taken from a PersistentVolumeClaim. The VolumeSnapshotClass specifies storage provider-specific parameters to use when taking a snapshot.BindingThe snapshot controller handles the binding of a VolumeSnapshot object with an appropriate VolumeSnapshotContent object, in both pre-provisioned and dynamically provisioned scenarios. The binding is a one-to-one mapping.In the case of pre-provisioned binding, the VolumeSnapshot will remain unbound until the requested VolumeSnapshotContent object is created.Persistent Volume Claim as Snapshot Source ProtectionThe purpose of this protection is to ensure that in-use PersistentVolumeClaim API objects are not removed from the system while a snapshot is being taken from it (as this may result in data loss).While a snapshot is being taken of a PersistentVolumeClaim, that PersistentVolumeClaim is in-use. If you delete a PersistentVolumeClaim API object in active use as a snapshot source, the PersistentVolumeClaim object is not removed immediately. Instead, removal of the PersistentVolumeClaim object is postponed until the snapshot is readyToUse or aborted.DeleteDeletion is triggered by deleting the VolumeSnapshot object, and the DeletionPolicy will be followed. If the DeletionPolicy is Delete, then the underlying storage snapshot will be deleted along with the VolumeSnapshotContent object. If the DeletionPolicy is Retain, then both the underlying snapshot and VolumeSnapshotContent remain.VolumeSnapshotsEach VolumeSnapshot contains a spec and a status.apiVersion: snapshot.storage.k8s.io/v1 kind: VolumeSnapshot metadata: name: new-snapshot-test spec: volumeSnapshotClassName: csi-hostpath-snapclass source: persistentVolumeClaimName: pvc-test persistentVolumeClaimName is the name of the PersistentVolumeClaim data source for the snapshot. This field is required for dynamically provisioning a snapshot.A volume snapshot can request a particular class by specifying the name of a VolumeSnapshotClass using the attribute volumeSnapshotClassName. If nothing is set, then the default class is used if available.For pre-provisioned snapshots, you need to specify a volumeSnapshotContentName as the source for the snapshot as shown in the following example. The volumeSnapshotContentName source field is required for pre-provisioned snapshots.apiVersion: snapshot.storage.k8s.io/v1 kind: VolumeSnapshot metadata: name: test-snapshot spec: source: volumeSnapshotContentName: test-content Volume Snapshot ContentsEach VolumeSnapshotContent contains a spec and status. In dynamic provisioning, the snapshot common controller creates VolumeSnapshotContent objects. Here is an example:apiVersion: snapshot.storage.k8s.io/v1 kind: VolumeSnapshotContent metadata: name: snapcontent-72d9a349-aacd-42d2-a240-d775650d2455 spec: deletionPolicy: Delete driver: hostpath.csi.k8s.io source: volumeHandle: ee0cfb94-f8d4-11e9-b2d8-0242ac110002 sourceVolumeMode: Filesystem volumeSnapshotClassName: csi-hostpath-snapclass volumeSnapshotRef: name: new-snapshot-test namespace: default uid: 72d9a349-aacd-42d2-a240-d775650d2455 volumeHandle is the unique identifier of the volume created on the storage backend and returned by the CSI driver during the volume creation. This field is required for dynamically provisioning a snapshot. It specifies the volume source of the snapshot.For pre-provisioned snapshots, you (as cluster administrator) are responsible for creating the VolumeSnapshotContent object as follows.apiVersion: snapshot.storage.k8s.io/v1 kind: VolumeSnapshotContent metadata: name: new-snapshot-content-test spec: deletionPolicy: Delete driver: hostpath.csi.k8s.io source: snapshotHandle: 7bdd0de3-aaeb-11e8-9aae-0242ac110002 sourceVolumeMode: Filesystem volumeSnapshotRef: name: new-snapshot-test namespace: default snapshotHandle is the unique identifier of the volume snapshot created on the storage backend. This field is required for the pre-provisioned snapshots. It specifies the CSI snapshot id on the storage system that this VolumeSnapshotContent represents.sourceVolumeMode is the mode of the volume whose snapshot is taken. The value of the sourceVolumeMode field can be either Filesystem or Block. If the source volume mode is not specified, Kubernetes treats the snapshot as if the source volume's mode is unknown.volumeSnapshotRef is the reference of the corresponding VolumeSnapshot. Note that when the VolumeSnapshotContent is being created as a pre-provisioned snapshot, the VolumeSnapshot referenced in volumeSnapshotRef might not exist yet.Converting the volume mode of a SnapshotIf the VolumeSnapshots API installed on your cluster supports the sourceVolumeMode field, then the API has the capability to prevent unauthorized users from converting the mode of a volume.To check if your cluster has capability for this feature, run the following command:$ kubectl get crd volumesnapshotcontent -o yaml If you want to allow users to create a PersistentVolumeClaim from an existing VolumeSnapshot, but with a different volume mode than the source, the annotation snapshot.storage.kubernetes.io/allow-volume-mode-change: "true"needs to be added to the VolumeSnapshotContent that corresponds to the VolumeSnapshot.For pre-provisioned snapshots, spec.sourceVolumeMode needs to be populated by the cluster administrator.An example VolumeSnapshotContent resource with this feature enabled would look like:apiVersion: snapshot.storage.k8s.io/v1 kind: VolumeSnapshotContent metadata: name: new-snapshot-content-test annotations: \- snapshot.storage.kubernetes.io/allow-volume-mode-change: "true" spec: deletionPolicy: Delete driver: hostpath.csi.k8s.io source: snapshotHandle: 7bdd0de3-aaeb-11e8-9aae-0242ac110002 sourceVolumeMode: Filesystem volumeSnapshotRef: name: new-snapshot-test namespace: default Provisioning Volumes from SnapshotsYou can provision a new volume, pre-populated with data from a snapshot, by using the dataSource field in the PersistentVolumeClaim object.For more details, see Volume Snapshot and Restore Volume from Snapshot.3.6.9 - Volume Snapshot ClassesThis document describes the concept of VolumeSnapshotClass in Kubernetes. Familiarity with volume snapshots and storage classes is suggested.IntroductionJust like StorageClass provides a way for administrators to describe the "classes" of storage they offer when provisioning a volume, VolumeSnapshotClass provides a way to describe the "classes" of storage when provisioning a volume snapshot.The VolumeSnapshotClass ResourceEach VolumeSnapshotClass contains the fields driver, deletionPolicy, and parameters, which are used when a VolumeSnapshot belonging to the class needs to be dynamically provisioned.The name of a VolumeSnapshotClass object is significant, and is how users can request a particular class. Administrators set the name and other parameters of a class when first creating VolumeSnapshotClass objects, and the objects cannot be updated once they are created.Note:Installation of the CRDs is the responsibility of the Kubernetes distribution. Without the required CRDs present, the creation of a VolumeSnapshotClass fails.apiVersion: snapshot.storage.k8s.io/v1 kind: VolumeSnapshotClass metadata: name: csi-hostpath-snapclass driver: hostpath.csi.k8s.io deletionPolicy: Delete parameters: Administrators can specify a default VolumeSnapshotClass for VolumeSnapshots that don't request any particular class to bind to by adding the snapshot.storage.kubernetes.io/is-default-class: "true" annotation:apiVersion: snapshot.storage.k8s.io/v1 kind: VolumeSnapshotClass metadata: name: csi-hostpath-snapclass annotations: snapshot.storage.kubernetes.io/is-default-class: "true" driver: hostpath.csi.k8s.io deletionPolicy: Delete parameters: If multiple CSI drivers exist, a default VolumeSnapshotClass can be specified for each of them.VolumeSnapshotClass dependenciesWhen you create a VolumeSnapshot without specifying a VolumeSnapshotClass, Kubernetes automatically selects a default VolumeSnapshotClass that has a CSI driver matching the CSI driver of the PVC’s StorageClass.This behavior allows multiple default VolumeSnapshotClass objects to coexist in a cluster, as long as each one is associated with a unique CSI driver.Always ensure that there is only one default VolumeSnapshotClass for each CSI driver. If multiple default VolumeSnapshotClass objects are created using the same CSI driver, a VolumeSnapshot creation will fail because Kubernetes cannot determine which one to use.DriverVolume snapshot classes have a driver that determines what CSI volume plugin is used for provisioning VolumeSnapshots. This field must be specified.DeletionPolicyVolume snapshot classes have a deletionPolicy. It enables you to configure what happens to a VolumeSnapshotContent when the VolumeSnapshot object it is bound to is to be deleted. The deletionPolicy of a volume snapshot class can either be Retain or Delete. This field must be specified.If the deletionPolicy is Delete, then the underlying storage snapshot will be deleted along with the VolumeSnapshotContent object. If the deletionPolicy is Retain, then both the underlying snapshot and VolumeSnapshotContent remain.ParametersVolume snapshot classes have parameters that describe volume snapshots belonging to the volume snapshot class. Different parameters may be accepted depending on the driver.3.6.10 - CSI Volume CloningThis document describes the concept of cloning existing CSI Volumes in Kubernetes. Familiarity with Volumes is suggested.IntroductionThe CSI Volume Cloning feature adds support for specifying existing PVCs in the dataSource field to indicate a user would like to clone a Volume.A Clone is defined as a duplicate of an existing Kubernetes Volume that can be consumed as any standard Volume would be. The only difference is that upon provisioning, rather than creating a "new" empty Volume, the back end device creates an exact duplicate of the specified Volume.The implementation of cloning, from the perspective of the Kubernetes API, adds the ability to specify an existing PVC as a dataSource during new PVC creation. The source PVC must be bound and available (not in use).Users need to be aware of the following when using this feature:Cloning support (VolumePVCDataSource) is only available for CSI drivers.Cloning support is only available for dynamic provisioners.CSI drivers may or may not have implemented the volume cloning functionality.You can only clone a PVC when it exists in the same namespace as the destination PVC (source and destination must be in the same namespace).Cloning is supported with a different Storage Class.Destination volume can be the same or a different storage class as the source.Default storage class can be used and storageClassName omitted in the spec.Cloning can only be performed between two volumes that use the same VolumeMode setting (if you request a block mode volume, the source MUST also be block mode)ProvisioningClones are provisioned like any other PVC with the exception of adding a dataSource that references an existing PVC in the same namespace.apiVersion: v1 kind: PersistentVolumeClaim metadata: name: clone-of-pvc-1 namespace: myns spec: accessModes: \- ReadWriteOnce storageClassName: cloning resources: requests: storage: 5Gi dataSource: kind: PersistentVolumeClaim name: pvc-1 Note:You must specify a capacity value for spec.resources.requests.storage, and the value you specify must be the same or larger than the capacity of the source volume.The result is a new PVC with the name clone-of-pvc-1 that has the exact same content as the specified source pvc-1.UsageUpon availability of the new PVC, the cloned PVC is consumed the same as other PVC. It's also expected at this point that the newly created PVC is an independent object. It can be consumed, cloned, snapshotted, or deleted independently and without consideration for it's original dataSource PVC. This also implies that the source is not linked in any way to the newly created clone, it may also be modified or deleted without affecting the newly created clone.3.6.11 - Storage CapacityStorage capacity is limited and may vary depending on the node on which a pod runs: network-attached storage might not be accessible by all nodes, or storage is local to a node to begin with.FEATURE STATE: Kubernetes v1.24 [stable]This page describes how Kubernetes keeps track of storage capacity and how the scheduler uses that information to schedule Pods onto nodes that have access to enough storage capacity for the remaining missing volumes. Without storage capacity tracking, the scheduler may choose a node that doesn't have enough capacity to provision a volume and multiple scheduling retries will be needed.Before you beginKubernetes v1.33 includes cluster-level API support for storage capacity tracking. To use this you must also be using a CSI driver that supports capacity tracking. Consult the documentation for the CSI drivers that you use to find out whether this support is available and, if so, how to use it. If you are not running Kubernetes v1.33, check the documentation for that version of Kubernetes.APIThere are two API extensions for this feature:CSIStorageCapacity objects: these get produced by a CSI driver in the namespace where the driver is installed. Each object contains capacity information for one storage class and defines which nodes have access to that storage.The CSIDriverSpec.StorageCapacity field: when set to true, the Kubernetes scheduler will consider storage capacity for volumes that use the CSI driver.SchedulingStorage capacity information is used by the Kubernetes scheduler if:a Pod uses a volume that has not been created yet,that volume uses a StorageClass which references a CSI driver and uses WaitForFirstConsumer volume binding mode, andthe CSIDriver object for the driver has StorageCapacity set to true.In that case, the scheduler only considers nodes for the Pod which have enough storage available to them. This check is very simplistic and only compares the size of the volume against the capacity listed in CSIStorageCapacity objects with a topology that includes the node.For volumes with Immediate volume binding mode, the storage driver decides where to create the volume, independently of Pods that will use the volume. The scheduler then schedules Pods onto nodes where the volume is available after the volume has been created.For CSI ephemeral volumes, scheduling always happens without considering storage capacity. This is based on the assumption that this volume type is only used by special CSI drivers which are local to a node and do not need significant resources there.ReschedulingWhen a node has been selected for a Pod with WaitForFirstConsumer volumes, that decision is still tentative. The next step is that the CSI storage driver gets asked to create the volume with a hint that the volume is supposed to be available on the selected node.Because Kubernetes might have chosen a node based on out-dated capacity information, it is possible that the volume cannot really be created. The node selection is then reset and the Kubernetes scheduler tries again to find a node for the Pod.LimitationsStorage capacity tracking increases the chance that scheduling works on the first try, but cannot guarantee this because the scheduler has to decide based on potentially out-dated information. Usually, the same retry mechanism as for scheduling without any storage capacity information handles scheduling failures.One situation where scheduling can fail permanently is when a Pod uses multiple volumes: one volume might have been created already in a topology segment which then does not have enough capacity left for another volume. Manual intervention is necessary to recover from this, for example by increasing capacity or deleting the volume that was already created.What's nextFor more information on the design, see the Storage Capacity Constraints for Pod Scheduling KEP.3.6.12 - Node-specific Volume LimitsThis page describes the maximum number of volumes that can be attached to a Node for various cloud providers.Cloud providers like Google, Amazon, and Microsoft typically have a limit on how many volumes can be attached to a Node. It is important for Kubernetes to respect those limits. Otherwise, Pods scheduled on a Node could get stuck waiting for volumes to attach.Kubernetes default limitsThe Kubernetes scheduler has default limits on the number of volumes that can be attached to a Node:Cloud serviceMaximum volumes per NodeAmazon Elastic Block Store (EBS)39Google Persistent Disk16Microsoft Azure Disk Storage16Custom limitsYou can change these limits by setting the value of the KUBE_MAX_PD_VOLS environment variable, and then starting the scheduler. CSI drivers might have a different procedure, see their documentation on how to customize their limits.Use caution if you set a limit that is higher than the default limit. Consult the cloud provider's documentation to make sure that Nodes can actually support the limit you set.The limit applies to the entire cluster, so it affects all Nodes.Dynamic volume limitsFEATURE STATE: Kubernetes v1.17 [stable]Dynamic volume limits are supported for following volume types.Amazon EBSGoogle Persistent DiskAzure DiskCSIFor volumes managed by in-tree volume plugins, Kubernetes automatically determines the Node type and enforces the appropriate maximum number of volumes for the node. For example:On Google Compute Engine, up to 127 volumes can be attached to a node, depending on the node type.For Amazon EBS disks on M5,C5,R5,T3 and Z1D instance types, Kubernetes allows only 25 volumes to be attached to a Node. For other instance types on Amazon Elastic Compute Cloud (EC2), Kubernetes allows 39 volumes to be attached to a Node.On Azure, up to 64 disks can be attached to a node, depending on the node type. For more details, refer to Sizes for virtual machines in Azure.If a CSI storage driver advertises a maximum number of volumes for a Node (using NodeGetInfo), the kube-scheduler honors that limit. Refer to the CSI specifications for details.For volumes managed by in-tree plugins that have been migrated to a CSI driver, the maximum number of volumes will be the one reported by the CSI driver.Mutable CSI Node Allocatable CountFEATURE STATE: Kubernetes v1.33 [alpha]CSI drivers can dynamically adjust the maximum number of volumes that can be attached to a Node at runtime. This enhances scheduling accuracy and reduces pod scheduling failures due to changes in resource availability.This is an alpha feature and is disabled by default.To use this feature, you must enable the MutableCSINodeAllocatableCount feature gate on the following components:kube-apiserverkubeletPeriodic UpdatesWhen enabled, CSI drivers can request periodic updates to their volume limits by setting the nodeAllocatableUpdatePeriodSeconds field in the CSIDriver specification. For example:apiVersion: storage.k8s.io/v1 kind: CSIDriver metadata: name: hostpath.csi.k8s.io spec: nodeAllocatableUpdatePeriodSeconds: 60 Kubelet will periodically call the corresponding CSI driver’s NodeGetInfo endpoint to refresh the maximum number of attachable volumes, using the interval specified in nodeAllocatableUpdatePeriodSeconds. The minimum allowed value for this field is 10 seconds.Additionally, if a volume attachment operation fails with a ResourceExhausted error (gRPC code 8), Kubernetes triggers an immediate update to the allocatable volume count for that Node.3.6.13 - Volume Health MonitoringFEATURE STATE: Kubernetes v1.21 [alpha]CSI volume health monitoring allows CSI Drivers to detect abnormal volume conditions from the underlying storage systems and report them as events on PVCs or Pods.Volume health monitoringKubernetes volume health monitoring is part of how Kubernetes implements the Container Storage Interface (CSI). Volume health monitoring feature is implemented in two components: an External Health Monitor controller, and the kubelet.If a CSI Driver supports Volume Health Monitoring feature from the controller side, an event will be reported on the related PersistentVolumeClaim (PVC) when an abnormal volume condition is detected on a CSI volume.The External Health Monitor controller also watches for node failure events. You can enable node failure monitoring by setting the enable-node-watcher flag to true. When the external health monitor detects a node failure event, the controller reports an Event will be reported on the PVC to indicate that pods using this PVC are on a failed node.If a CSI Driver supports Volume Health Monitoring feature from the node side, an Event will be reported on every Pod using the PVC when an abnormal volume condition is detected on a CSI volume. In addition, Volume Health information is exposed as Kubelet VolumeStats metrics. A new metric kubelet_volume_stats_health_status_abnormal is added. This metric includes two labels: namespace and persistentvolumeclaim. The count is either 1 or 0. 1 indicates the volume is unhealthy, 0 indicates volume is healthy. For more information, please check KEP.Note:You need to enable the CSIVolumeHealth feature gate to use this feature from the node side.What's nextSee the CSI driver documentation to find out which CSI drivers have implemented this feature.3.6.14 - Windows StorageThis page provides an storage overview specific to the Windows operating system.Persistent storageWindows has a layered filesystem driver to mount container layers and create a copy filesystem based on NTFS. All file paths in the container are resolved only within the context of that container.With Docker, volume mounts can only target a directory in the container, and not an individual file. This limitation does not apply to containerd.Volume mounts cannot project files or directories back to the host filesystem.Read-only filesystems are not supported because write access is always required for the Windows registry and SAM database. However, read-only volumes are supported.Volume user-masks and permissions are not available. Because the SAM is not shared between the host & container, there's no mapping between them. All permissions are resolved within the context of the container.As a result, the following storage functionality is not supported on Windows nodes:Volume subpath mounts: only the entire volume can be mounted in a Windows containerSubpath volume mounting for SecretsHost mount projectionRead-only root filesystem (mapped volumes still support readOnly)Block device mappingMemory as the storage medium (for example, emptyDir.medium set to Memory)File system features like uid/gid; per-user Linux filesystem permissionsSetting secret permissions with DefaultMode (due to UID/GID dependency)NFS based storage/volume supportExpanding the mounted volume (resizefs)Kubernetes volumes enable complex applications, with data persistence and Pod volume sharing requirements, to be deployed on Kubernetes. Management of persistent volumes associated with a specific storage back-end or protocol includes actions such as provisioning/de-provisioning/resizing of volumes, attaching/detaching a volume to/from a Kubernetes node and mounting/dismounting a volume to/from individual containers in a pod that needs to persist data.Volume management components are shipped as Kubernetes volume plugin. The following broad classes of Kubernetes volume plugins are supported on Windows:FlexVolume pluginsPlease note that FlexVolumes have been deprecated as of 1.23CSI PluginsIn-tree volume pluginsThe following in-tree plugins support persistent storage on Windows nodes:azureFilevsphereVolume3.7 - ConfigurationResources that Kubernetes provides for configuring Pods.3.7.1 - Configuration Best PracticesThis document highlights and consolidates configuration best practices that are introduced throughout the user guide, Getting Started documentation, and examples.This is a living document. If you think of something that is not on this list but might be useful to others, please don't hesitate to file an issue or submit a PR.General Configuration TipsWhen defining configurations, specify the latest stable API version.Configuration files should be stored in version control before being pushed to the cluster. This allows you to quickly roll back a configuration change if necessary. It also aids cluster re-creation and restoration.Write your configuration files using YAML rather than JSON. Though these formats can be used interchangeably in almost all scenarios, YAML tends to be more user-friendly.Group related objects into a single file whenever it makes sense. One file is often easier to manage than several. See the guestbook-all-in-one.yaml file as an example of this syntax.Note also that many kubectl commands can be called on a directory. For example, you can call kubectl apply on a directory of config files.Don't specify default values unnecessarily: simple, minimal configuration will make errors less likely.Put object descriptions in annotations, to allow better introspection.Note:There is a breaking change introduced in the YAML 1.2 boolean values specification with respect to YAML 1.1. This is a known issue in Kubernetes. YAML 1.2 only recognizes true and false as valid booleans, while YAML 1.1 also accepts yes, no, on, and off as booleans. However, Kubernetes uses YAML parsers that are mostly compatible with YAML 1.1, which means that using yes or no instead of true or false in a YAML manifest may cause unexpected errors or behaviors. To avoid this issue, it is recommended to always use true or false for boolean values in YAML manifests, and to quote any strings that may be confused with booleans, such as "yes" or "no".Besides booleans, there are additional specifications changes between YAML versions. Please refer to the YAML Specification Changes documentation for a comprehensive list."Naked" Pods versus ReplicaSets, Deployments, and JobsDon't use naked Pods (that is, Pods not bound to a ReplicaSet or Deployment) if you can avoid it. Naked Pods will not be rescheduled in the event of a node failure.A Deployment, which both creates a ReplicaSet to ensure that the desired number of Pods is always available, and specifies a strategy to replace Pods (such as RollingUpdate), is almost always preferable to creating Pods directly, except for some explicit restartPolicy: Never scenarios. A Job may also be appropriate.ServicesCreate a Service before its corresponding backend workloads (Deployments or ReplicaSets), and before any workloads that need to access it. When Kubernetes starts a container, it provides environment variables pointing to all the Services which were running when the container was started. For example, if a Service named foo exists, all containers will get the following variables in their initial environment:FOO_SERVICE_HOST= FOO_SERVICE_PORT= This does imply an ordering requirement - any Service that a Pod wants to access must be created before the Pod itself, or else the environment variables will not be populated. DNS does not have this restriction.An optional (though strongly recommended) cluster add-on is a DNS server. The DNS server watches the Kubernetes API for new Services and creates a set of DNS records for each. If DNS has been enabled throughout the cluster then all Pods should be able to do name resolution of Services automatically.Don't specify a hostPort for a Pod unless it is absolutely necessary. When you bind a Pod to a hostPort, it limits the number of places the Pod can be scheduled, because each  combination must be unique. If you don't specify the hostIP and protocol explicitly, Kubernetes will use 0.0.0.0 as the default hostIP and TCP as the default protocol.If you only need access to the port for debugging purposes, you can use the apiserver proxy or kubectl port-forward.If you explicitly need to expose a Pod's port on the node, consider using a NodePort Service before resorting to hostPort.Avoid using hostNetwork, for the same reasons as hostPort.Use headless Services (which have a ClusterIP of None) for service discovery when you don't need kube-proxy load balancing.Using LabelsDefine and use labels that identify semantic attributes of your application or Deployment, such as { app.kubernetes.io/name: MyApp, tier: frontend, phase: test, deployment: v3 }. You can use these labels to select the appropriate Pods for other resources; for example, a Service that selects all tier: frontend Pods, or all phase: test components of app.kubernetes.io/name: MyApp. See the guestbook app for examples of this approach.A Service can be made to span multiple Deployments by omitting release-specific labels from its selector. When you need to update a running service without downtime, use a Deployment.A desired state of an object is described by a Deployment, and if changes to that spec are applied, the deployment controller changes the actual state to the desired state at a controlled rate.Use the Kubernetes common labels for common use cases. These standardized labels enrich the metadata in a way that allows tools, including kubectl and dashboard, to work in an interoperable way.You can manipulate labels for debugging. Because Kubernetes controllers (such as ReplicaSet) and Services match to Pods using selector labels, removing the relevant labels from a Pod will stop it from being considered by a controller or from being served traffic by a Service. If you remove the labels of an existing Pod, its controller will create a new Pod to take its place. This is a useful way to debug a previously "live" Pod in a "quarantine" environment. To interactively remove or add labels, use kubectl label.Using kubectlUse kubectl apply -f . This looks for Kubernetes configuration in all .yaml, .yml, and .json files in  and passes it to apply.Use label selectors for get and delete operations instead of specific object names. See the sections on label selectors and using labels effectively.Use kubectl create deployment and kubectl expose to quickly create single-container Deployments and Services. See Use a Service to Access an Application in a Cluster for an example.3.7.2 - ConfigMapsA ConfigMap is an API object used to store non-confidential data in key-value pairs. Pods can consume ConfigMaps as environment variables, command-line arguments, or as configuration files in a volume.A ConfigMap allows you to decouple environment-specific configuration from your container images, so that your applications are easily portable.Caution:ConfigMap does not provide secrecy or encryption. If the data you want to store are confidential, use a Secret rather than a ConfigMap, or use additional (third party) tools to keep your data private.MotivationUse a ConfigMap for setting configuration data separately from application code.For example, imagine that you are developing an application that you can run on your own computer (for development) and in the cloud (to handle real traffic). You write the code to look in an environment variable named DATABASE_HOST. Locally, you set that variable to localhost. In the cloud, you set it to refer to a Kubernetes Service that exposes the database component to your cluster. This lets you fetch a container image running in the cloud and debug the exact same code locally if needed.Note:A ConfigMap is not designed to hold large chunks of data. The data stored in a ConfigMap cannot exceed 1 MiB. If you need to store settings that are larger than this limit, you may want to consider mounting a volume or use a separate database or file service.ConfigMap objectA ConfigMap is an API object that lets you store configuration for other objects to use. Unlike most Kubernetes objects that have a spec, a ConfigMap has data and binaryData fields. These fields accept key-value pairs as their values. Both the data field and the binaryData are optional. The data field is designed to contain UTF-8 strings while the binaryData field is designed to contain binary data as base64-encoded strings.The name of a ConfigMap must be a valid DNS subdomain name.Each key under the data or the binaryData field must consist of alphanumeric characters, -, _ or .. The keys stored in data must not overlap with the keys in the binaryData field.Starting from v1.19, you can add an immutable field to a ConfigMap definition to create an immutable ConfigMap.ConfigMaps and PodsYou can write a Pod spec that refers to a ConfigMap and configures the container(s) in that Pod based on the data in the ConfigMap. The Pod and the ConfigMap must be in the same namespace.Note:The spec of a static Pod cannot refer to a ConfigMap or any other API objects.Here's an example ConfigMap that has some keys with single values, and other keys where the value looks like a fragment of a configuration format.apiVersion: v1 kind: ConfigMap metadata: name: game-demo data: # property-like keys; each key maps to a simple value player_initial_lives: "3" ui_properties_file_name: "user-interface.properties" # file-like keys game.properties: | enemy.types=aliens,monsters player.maximum-lives=5 user-interface.properties: | color.good=purple color.bad=yellow allow.textmode=true There are four different ways that you can use a ConfigMap to configure a container inside a Pod:Inside a container command and argsEnvironment variables for a containerAdd a file in read-only volume, for the application to readWrite code to run inside the Pod that uses the Kubernetes API to read a ConfigMapThese different methods lend themselves to different ways of modeling the data being consumed. For the first three methods, the kubelet uses the data from the ConfigMap when it launches container(s) for a Pod.The fourth method means you have to write code to read the ConfigMap and its data. However, because you're using the Kubernetes API directly, your application can subscribe to get updates whenever the ConfigMap changes, and react when that happens. By accessing the Kubernetes API directly, this technique also lets you access a ConfigMap in a different namespace.Here's an example Pod that uses values from game-demo to configure a Pod:configmap/configure-pod.yaml apiVersion: v1 kind: Pod metadata: name: configmap-demo-pod spec: containers: \- name: demo image: alpine command: ["sleep", "3600"] env: # Define the environment variable \- name: PLAYER_INITIAL_LIVES # Notice that the case is different here # from the key name in the ConfigMap. valueFrom: configMapKeyRef: name: game-demo # The ConfigMap this value comes from. key: player_initial_lives # The key to fetch. \- name: UI_PROPERTIES_FILE_NAME valueFrom: configMapKeyRef: name: game-demo key: ui_properties_file_name volumeMounts: \- name: config mountPath: "/config" readOnly: true volumes: # You set volumes at the Pod level, then mount them into containers inside that Pod \- name: config configMap: # Provide the name of the ConfigMap you want to mount. name: game-demo # An array of keys from the ConfigMap to create as files items: \- key: "game.properties" path: "game.properties" \- key: "user-interface.properties" path: "user-interface.properties" A ConfigMap doesn't differentiate between single line property values and multi-line file-like values. What matters is how Pods and other objects consume those values.For this example, defining a volume and mounting it inside the demo container as /config creates two files, /config/game.properties and /config/user-interface.properties, even though there are four keys in the ConfigMap. This is because the Pod definition specifies an items array in the volumes section. If you omit the items array entirely, every key in the ConfigMap becomes a file with the same name as the key, and you get 4 files.Using ConfigMapsConfigMaps can be mounted as data volumes. ConfigMaps can also be used by other parts of the system, without being directly exposed to the Pod. For example, ConfigMaps can hold data that other parts of the system should use for configuration.The most common way to use ConfigMaps is to configure settings for containers running in a Pod in the same namespace. You can also use a ConfigMap separately.For example, you might encounter addons or operators that adjust their behavior based on a ConfigMap.Using ConfigMaps as files from a PodTo consume a ConfigMap in a volume in a Pod:Create a ConfigMap or use an existing one. Multiple Pods can reference the same ConfigMap.Modify your Pod definition to add a volume under .spec.volumes[]. Name the volume anything, and have a .spec.volumes[].configMap.name field set to reference your ConfigMap object.Add a .spec.containers[].volumeMounts[] to each container that needs the ConfigMap. Specify .spec.containers[].volumeMounts[].readOnly = true and .spec.containers[].volumeMounts[].mountPath to an unused directory name where you would like the ConfigMap to appear.Modify your image or command line so that the program looks for files in that directory. Each key in the ConfigMap data map becomes the filename under mountPath.This is an example of a Pod that mounts a ConfigMap in a volume:apiVersion: v1 kind: Pod metadata: name: mypod spec: containers: \- name: mypod image: redis volumeMounts: \- name: foo mountPath: "/etc/foo" readOnly: true volumes: \- name: foo configMap: name: myconfigmap Each ConfigMap you want to use needs to be referred to in .spec.volumes.If there are multiple containers in the Pod, then each container needs its own volumeMounts block, but only one .spec.volumes is needed per ConfigMap.Mounted ConfigMaps are updated automaticallyWhen a ConfigMap currently consumed in a volume is updated, projected keys are eventually updated as well. The kubelet checks whether the mounted ConfigMap is fresh on every periodic sync. However, the kubelet uses its local cache for getting the current value of the ConfigMap. The type of the cache is configurable using the configMapAndSecretChangeDetectionStrategy field in the KubeletConfiguration struct. A ConfigMap can be either propagated by watch (default), ttl-based, or by redirecting all requests directly to the API server. As a result, the total delay from the moment when the ConfigMap is updated to the moment when new keys are projected to the Pod can be as long as the kubelet sync period + cache propagation delay, where the cache propagation delay depends on the chosen cache type (it equals to watch propagation delay, ttl of cache, or zero correspondingly).ConfigMaps consumed as environment variables are not updated automatically and require a pod restart.Note:A container using a ConfigMap as a subPath volume mount will not receive ConfigMap updates.Using Configmaps as environment variablesTo use a Configmap in an environment variable in a Pod:For each container in your Pod specification, add an environment variable for each Configmap key that you want to use to the env[].valueFrom.configMapKeyRef field.Modify your image and/or command line so that the program looks for values in the specified environment variables.This is an example of defining a ConfigMap as a pod environment variable:The following ConfigMap (myconfigmap.yaml) stores two properties: username and access_level:apiVersion: v1 kind: ConfigMap metadata: name: myconfigmap data: username: k8s-admin access_level: "1" The following command will create the ConfigMap object:kubectl apply -f myconfigmap.yaml The following Pod consumes the content of the ConfigMap as environment variables:configmap/env-configmap.yaml apiVersion: v1 kind: Pod metadata: name: env-configmap spec: containers: \- name: app command: ["/bin/sh", "-c", "printenv"] image: busybox:latest envFrom: \- configMapRef: name: myconfigmap The envFrom field instructs Kubernetes to create environment variables from the sources nested within it. The inner configMapRef refers to a ConfigMap by its name and selects all its key-value pairs. Add the Pod to your cluster, then retrieve its logs to see the output from the printenv command. This should confirm that the two key-value pairs from the ConfigMap have been set as environment variables:kubectl apply -f env-configmap.yaml kubectl logs pod/ env-configmap The output is similar to this:... username: "k8s-admin" access_level: "1" ... Sometimes a Pod won't require access to all the values in a ConfigMap. For example, you could have another Pod which only uses the username value from the ConfigMap. For this use case, you can use the env.valueFrom syntax instead, which lets you select individual keys in a ConfigMap. The name of the environment variable can also be different from the key within the ConfigMap. For example:apiVersion: v1 kind: Pod metadata: name: env-configmap spec: containers: \- name: envars-test-container image: nginx env: \- name: CONFIGMAP_USERNAME valueFrom: configMapKeyRef: name: myconfigmap key: username In the Pod created from this manifest, you will see that the environment variable CONFIGMAP_USERNAME is set to the value of the username value from the ConfigMap. Other keys from the ConfigMap data are not copied into the environment.It's important to note that the range of characters allowed for environment variable names in pods is restricted. If any keys do not meet the rules, those keys are not made available to your container, though the Pod is allowed to start.Immutable ConfigMapsFEATURE STATE: Kubernetes v1.21 [stable]The Kubernetes feature Immutable Secrets and ConfigMaps provides an option to set individual Secrets and ConfigMaps as immutable. For clusters that extensively use ConfigMaps (at least tens of thousands of unique ConfigMap to Pod mounts), preventing changes to their data has the following advantages:protects you from accidental (or unwanted) updates that could cause applications outagesimproves performance of your cluster by significantly reducing load on kube-apiserver, by closing watches for ConfigMaps marked as immutable.You can create an immutable ConfigMap by setting the immutable field to true. For example:apiVersion: v1 kind: ConfigMap metadata: ... data: ... immutable: true Once a ConfigMap is marked as immutable, it is not possible to revert this change nor to mutate the contents of the data or the binaryData field. You can only delete and recreate the ConfigMap. Because existing Pods maintain a mount point to the deleted ConfigMap, it is recommended to recreate these pods.What's nextRead about Secrets.Read Configure a Pod to Use a ConfigMap.Read about changing a ConfigMap (or any other Kubernetes object)Read The Twelve-Factor App to understand the motivation for separating code from configuration.3.7.3 - SecretsA Secret is an object that contains a small amount of sensitive data such as a password, a token, or a key. Such information might otherwise be put in a Pod specification or in a container image. Using a Secret means that you don't need to include confidential data in your application code.Because Secrets can be created independently of the Pods that use them, there is less risk of the Secret (and its data) being exposed during the workflow of creating, viewing, and editing Pods. Kubernetes, and applications that run in your cluster, can also take additional precautions with Secrets, such as avoiding writing sensitive data to nonvolatile storage.Secrets are similar to ConfigMaps but are specifically intended to hold confidential data.Caution:Kubernetes Secrets are, by default, stored unencrypted in the API server's underlying data store (etcd). Anyone with API access can retrieve or modify a Secret, and so can anyone with access to etcd. Additionally, anyone who is authorized to create a Pod in a namespace can use that access to read any Secret in that namespace; this includes indirect access such as the ability to create a Deployment.In order to safely use Secrets, take at least the following steps:Enable Encryption at Rest for Secrets.Enable or configure RBAC rules with least-privilege access to Secrets.Restrict Secret access to specific containers.Consider using external Secret store providers.For more guidelines to manage and improve the security of your Secrets, refer to Good practices for Kubernetes Secrets.See Information security for Secrets for more details.Uses for SecretsYou can use Secrets for purposes such as the following:Set environment variables for a container.Provide credentials such as SSH keys or passwords to Pods.Allow the kubelet to pull container images from private registries.The Kubernetes control plane also uses Secrets; for example, bootstrap token Secrets are a mechanism to help automate node registration.Use case: dotfiles in a secret volumeYou can make your data "hidden" by defining a key that begins with a dot. This key represents a dotfile or "hidden" file. For example, when the following Secret is mounted into a volume, secret-volume, the volume will contain a single file, called .secret-file, and the dotfile-test-container will have this file present at the path /etc/secret-volume/.secret-file.Note:Files beginning with dot characters are hidden from the output of ls -l; you must use ls -la to see them when listing directory contents.secret/dotfile-secret.yaml apiVersion: v1 kind: Secret metadata: name: dotfile-secret data: .secret-file: dmFsdWUtMg0KDQo= \--- apiVersion: v1 kind: Pod metadata: name: secret-dotfiles-pod spec: volumes: \- name: secret-volume secret: secretName: dotfile-secret containers: \- name: dotfile-test-container image: registry.k8s.io/busybox command: \- ls \- "-l" \- "/etc/secret-volume" volumeMounts: \- name: secret-volume readOnly: true mountPath: "/etc/secret-volume"Use case: Secret visible to one container in a PodConsider a program that needs to handle HTTP requests, do some complex business logic, and then sign some messages with an HMAC. Because it has complex application logic, there might be an unnoticed remote file reading exploit in the server, which could expose the private key to an attacker.This could be divided into two processes in two containers: a frontend container which handles user interaction and business logic, but which cannot see the private key; and a signer container that can see the private key, and responds to simple signing requests from the frontend (for example, over localhost networking).With this partitioned approach, an attacker now has to trick the application server into doing something rather arbitrary, which may be harder than getting it to read a file.Alternatives to SecretsRather than using a Secret to protect confidential data, you can pick from alternatives.Here are some of your options:If your cloud-native component needs to authenticate to another application that you know is running within the same Kubernetes cluster, you can use a ServiceAccount and its tokens to identify your client.There are third-party tools that you can run, either within or outside your cluster, that manage sensitive data. For example, a service that Pods access over HTTPS, that reveals a Secret if the client correctly authenticates (for example, with a ServiceAccount token).For authentication, you can implement a custom signer for X.509 certificates, and use CertificateSigningRequests to let that custom signer issue certificates to Pods that need them.You can use a device plugin to expose node-local encryption hardware to a specific Pod. For example, you can schedule trusted Pods onto nodes that provide a Trusted Platform Module, configured out-of-band.You can also combine two or more of those options, including the option to use Secret objects themselves.For example: implement (or deploy) an operator that fetches short-lived session tokens from an external service, and then creates Secrets based on those short-lived session tokens. Pods running in your cluster can make use of the session tokens, and operator ensures they are valid. This separation means that you can run Pods that are unaware of the exact mechanisms for issuing and refreshing those session tokens.Types of SecretWhen creating a Secret, you can specify its type using the type field of the Secret resource, or certain equivalent kubectl command line flags (if available). The Secret type is used to facilitate programmatic handling of the Secret data.Kubernetes provides several built-in types for some common usage scenarios. These types vary in terms of the validations performed and the constraints Kubernetes imposes on them.Built-in TypeUsageOpaquearbitrary user-defined datakubernetes.io/service-account-tokenServiceAccount tokenkubernetes.io/dockercfgserialized ~/.dockercfg filekubernetes.io/dockerconfigjsonserialized ~/.docker/config.json filekubernetes.io/basic-authcredentials for basic authenticationkubernetes.io/ssh-authcredentials for SSH authenticationkubernetes.io/tlsdata for a TLS client or serverbootstrap.kubernetes.io/tokenbootstrap token dataYou can define and use your own Secret type by assigning a non-empty string as the type value for a Secret object (an empty string is treated as an Opaque type).Kubernetes doesn't impose any constraints on the type name. However, if you are using one of the built-in types, you must meet all the requirements defined for that type.If you are defining a type of Secret that's for public use, follow the convention and structure the Secret type to have your domain name before the name, separated by a /. For example: cloud-hosting.example.net/cloud-api-credentials.Opaque SecretsOpaque is the default Secret type if you don't explicitly specify a type in a Secret manifest. When you create a Secret using kubectl, you must use the generic subcommand to indicate an Opaque Secret type. For example, the following command creates an empty Secret of type Opaque:kubectl create secret generic empty-secret kubectl get secret empty-secret The output looks like:NAME TYPE DATA AGE empty-secret Opaque 0 2m6s The DATA column shows the number of data items stored in the Secret. In this case, 0 means you have created an empty Secret.ServiceAccount token SecretsA kubernetes.io/service-account-token type of Secret is used to store a token credential that identifies a ServiceAccount. This is a legacy mechanism that provides long-lived ServiceAccount credentials to Pods.In Kubernetes v1.22 and later, the recommended approach is to obtain a short-lived, automatically rotating ServiceAccount token by using the TokenRequest API instead. You can get these short-lived tokens using the following methods:Call the TokenRequest API either directly or by using an API client like kubectl. For example, you can use the kubectl create token command.Request a mounted token in a projected volume in your Pod manifest. Kubernetes creates the token and mounts it in the Pod. The token is automatically invalidated when the Pod that it's mounted in is deleted. For details, see Launch a Pod using service account token projection.Note:You should only create a ServiceAccount token Secret if you can't use the TokenRequest API to obtain a token, and the security exposure of persisting a non-expiring token credential in a readable API object is acceptable to you. For instructions, see Manually create a long-lived API token for a ServiceAccount.When using this Secret type, you need to ensure that the kubernetes.io/service-account.name annotation is set to an existing ServiceAccount name. If you are creating both the ServiceAccount and the Secret objects, you should create the ServiceAccount object first.After the Secret is created, a Kubernetes controller fills in some other fields such as the kubernetes.io/service-account.uid annotation, and the token key in the data field, which is populated with an authentication token.The following example configuration declares a ServiceAccount token Secret:secret/serviceaccount-token-secret.yaml apiVersion: v1 kind: Secret metadata: name: secret-sa-sample annotations: kubernetes.io/service-account.name: "sa-name" type: kubernetes.io/service-account-token data: extra: YmFyCg==After creating the Secret, wait for Kubernetes to populate the token key in the data field.See the ServiceAccount documentation for more information on how ServiceAccounts work. You can also check the automountServiceAccountToken field and the serviceAccountName field of the Pod for information on referencing ServiceAccount credentials from within Pods.Docker config SecretsIf you are creating a Secret to store credentials for accessing a container image registry, you must use one of the following type values for that Secret:kubernetes.io/dockercfg: store a serialized ~/.dockercfg which is the legacy format for configuring Docker command line. The Secret data field contains a .dockercfg key whose value is the content of a base64 encoded ~/.dockercfg file.kubernetes.io/dockerconfigjson: store a serialized JSON that follows the same format rules as the ~/.docker/config.json file, which is a new format for ~/.dockercfg. The Secret data field must contain a .dockerconfigjson key for which the value is the content of a base64 encoded ~/.docker/config.json file.Below is an example for a kubernetes.io/dockercfg type of Secret:secret/dockercfg-secret.yaml apiVersion: v1 kind: Secret metadata: name: secret-dockercfg type: kubernetes.io/dockercfg data: .dockercfg: | eyJhdXRocyI6eyJodHRwczovL2V4YW1wbGUvdjEvIjp7ImF1dGgiOiJvcGVuc2VzYW1lIn19fQo= Note:If you do not want to perform the base64 encoding, you can choose to use the stringData field instead.When you create Docker config Secrets using a manifest, the API server checks whether the expected key exists in the data field, and it verifies if the value provided can be parsed as a valid JSON. The API server doesn't validate if the JSON actually is a Docker config file.You can also use kubectl to create a Secret for accessing a container registry, such as when you don't have a Docker configuration file:kubectl create secret docker-registry secret-tiger-docker \ \--docker-email=tiger@acme.example \ \--docker-username=tiger \ \--docker-password=pass1234 \ \--docker-server=my-registry.example:5000 This command creates a Secret of type kubernetes.io/dockerconfigjson.Retrieve the .data.dockerconfigjson field from that new Secret and decode the data:kubectl get secret secret-tiger-docker -o jsonpath='{.data.*}' | base64 -d The output is equivalent to the following JSON document (which is also a valid Docker configuration file):{ "auths": { "my-registry.example:5000": { "username": "tiger", "password": "pass1234", "email": "tiger@acme.example", "auth": "dGlnZXI6cGFzczEyMzQ=" } } } Caution:The auth value there is base64 encoded; it is obscured but not secret. Anyone who can read that Secret can learn the registry access bearer token.It is suggested to use credential providers to dynamically and securely provide pull secrets on-demand.Basic authentication SecretThe kubernetes.io/basic-auth type is provided for storing credentials needed for basic authentication. When using this Secret type, the data field of the Secret must contain one of the following two keys:username: the user name for authenticationpassword: the password or token for authenticationBoth values for the above two keys are base64 encoded strings. You can alternatively provide the clear text content using the stringData field in the Secret manifest.The following manifest is an example of a basic authentication Secret:secret/basicauth-secret.yaml apiVersion: v1 kind: Secret metadata: name: secret-basic-auth type: kubernetes.io/basic-auth stringData: username: admin # required field for kubernetes.io/basic-auth password: t0p-Secret # required field for kubernetes.io/basic-authNote:The stringData field for a Secret does not work well with server-side apply.The basic authentication Secret type is provided only for convenience. You can create an Opaque type for credentials used for basic authentication. However, using the defined and public Secret type (kubernetes.io/basic-auth) helps other people to understand the purpose of your Secret, and sets a convention for what key names to expect.SSH authentication SecretsThe builtin type kubernetes.io/ssh-auth is provided for storing data used in SSH authentication. When using this Secret type, you will have to specify a ssh-privatekey key-value pair in the data (or stringData) field as the SSH credential to use.The following manifest is an example of a Secret used for SSH public/private key authentication:secret/ssh-auth-secret.yaml apiVersion: v1 kind: Secret metadata: name: secret-ssh-auth type: kubernetes.io/ssh-auth data: # the data is abbreviated in this example ssh-privatekey: | UG91cmluZzYlRW1vdGljb24lU2N1YmE= The SSH authentication Secret type is provided only for convenience. You can create an Opaque type for credentials used for SSH authentication. However, using the defined and public Secret type (kubernetes.io/ssh-auth) helps other people to understand the purpose of your Secret, and sets a convention for what key names to expect. The Kubernetes API verifies that the required keys are set for a Secret of this type.Caution:SSH private keys do not establish trusted communication between an SSH client and host server on their own. A secondary means of establishing trust is needed to mitigate "man in the middle" attacks, such as a known_hosts file added to a ConfigMap.TLS SecretsThe kubernetes.io/tls Secret type is for storing a certificate and its associated key that are typically used for TLS.One common use for TLS Secrets is to configure encryption in transit for an Ingress, but you can also use it with other resources or directly in your workload. When using this type of Secret, the tls.key and the tls.crt key must be provided in the data (or stringData) field of the Secret configuration, although the API server doesn't actually validate the values for each key.As an alternative to using stringData, you can use the data field to provide the base64 encoded certificate and private key. For details, see Constraints on Secret names and data.The following YAML contains an example config for a TLS Secret:secret/tls-auth-secret.yaml apiVersion: v1 kind: Secret metadata: name: secret-tls type: kubernetes.io/tls data: # values are base64 encoded, which obscures them but does NOT provide # any useful level of confidentiality tls.crt: | LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUNVakNDQWJzQ0FnMytNQTBHQ1NxR1NJYjNE UUVCQlFVQU1JR2JNUXN3Q1FZRFZRUUdFd0pLVURFT01Bd0cKQTFVRUNCTUZWRzlyZVc4eEVEQU9C Z05WQkFjVEIwTm9kVzh0YTNVeEVUQVBCZ05WQkFvVENFWnlZVzVyTkVSRQpNUmd3RmdZRFZRUUxF dzlYWldKRFpYSjBJRk4xY0hCdmNuUXhHREFXQmdOVkJBTVREMFp5WVc1ck5FUkVJRmRsCllpQkRR VEVqTUNFR0NTcUdTSWIzRFFFSkFSWVVjM1Z3Y0c5eWRFQm1jbUZ1YXpSa1pDNWpiMjB3SGhjTk1U TXcKTVRFeE1EUTFNVE01V2hjTk1UZ3dNVEV3TURRMU1UTTVXakJMTVFzd0NRWURWUVFHREFKS1VE RVBNQTBHQTFVRQpDQXdHWEZSdmEzbHZNUkV3RHdZRFZRUUtEQWhHY21GdWF6UkVSREVZTUJZR0Ex VUVBd3dQZDNkM0xtVjRZVzF3CmJHVXVZMjl0TUlHYU1BMEdDU3FHU0liM0RRRUJBUVVBQTRHSUFE Q0JoQUo5WThFaUhmeHhNL25PbjJTbkkxWHgKRHdPdEJEVDFKRjBReTliMVlKanV2YjdjaTEwZjVN Vm1UQllqMUZTVWZNOU1vejJDVVFZdW4yRFljV29IcFA4ZQpqSG1BUFVrNVd5cDJRN1ArMjh1bklI QkphVGZlQ09PekZSUFY2MEdTWWUzNmFScG04L3dVVm16eGFLOGtCOWVaCmhPN3F1TjdtSWQxL2pW cTNKODhDQXdFQUFUQU5CZ2txaGtpRzl3MEJBUVVGQUFPQmdRQU1meTQzeE15OHh3QTUKVjF2T2NS OEtyNWNaSXdtbFhCUU8xeFEzazlxSGtyNFlUY1JxTVQ5WjVKTm1rWHYxK2VSaGcwTi9WMW5NUTRZ RgpnWXcxbnlESnBnOTduZUV4VzQyeXVlMFlHSDYyV1hYUUhyOVNVREgrRlowVnQvRGZsdklVTWRj UUFEZjM4aU9zCjlQbG1kb3YrcE0vNCs5a1h5aDhSUEkzZXZ6OS9NQT09Ci0tLS0tRU5EIENFUlRJ RklDQVRFLS0tLS0K # In this example, the key data is not a real PEM-encoded private key tls.key: | RXhhbXBsZSBkYXRhIGZvciB0aGUgVExTIGNydCBmaWVsZA== The TLS Secret type is provided only for convenience. You can create an Opaque type for credentials used for TLS authentication. However, using the defined and public Secret type (kubernetes.io/tls) helps ensure the consistency of Secret format in your project. The API server verifies if the required keys are set for a Secret of this type.To create a TLS Secret using kubectl, use the tls subcommand:kubectl create secret tls my-tls-secret \ \--cert=path/to/cert/file \ \--key=path/to/key/file The public/private key pair must exist before hand. The public key certificate for --cert must be .PEM encoded and must match the given private key for --key.Bootstrap token SecretsThe bootstrap.kubernetes.io/token Secret type is for tokens used during the node bootstrap process. It stores tokens used to sign well-known ConfigMaps.A bootstrap token Secret is usually created in the kube-system namespace and named in the form bootstrap-token- where  is a 6 character string of the token ID.As a Kubernetes manifest, a bootstrap token Secret might look like the following:secret/bootstrap-token-secret-base64.yaml apiVersion: v1 kind: Secret metadata: name: bootstrap-token-5emitj namespace: kube-system type: bootstrap.kubernetes.io/token data: auth-extra-groups: c3lzdGVtOmJvb3RzdHJhcHBlcnM6a3ViZWFkbTpkZWZhdWx0LW5vZGUtdG9rZW4= expiration: MjAyMC0wOS0xM1QwNDozOToxMFo= token-id: NWVtaXRq token-secret: a3E0Z2lodnN6emduMXAwcg== usage-bootstrap-authentication: dHJ1ZQ== usage-bootstrap-signing: dHJ1ZQ==A bootstrap token Secret has the following keys specified under data:token-id: A random 6 character string as the token identifier. Required.token-secret: A random 16 character string as the actual token Secret. Required.description: A human-readable string that describes what the token is used for. Optional.expiration: An absolute UTC time using RFC3339 specifying when the token should be expired. Optional.usage-bootstrap-: A boolean flag indicating additional usage for the bootstrap token.auth-extra-groups: A comma-separated list of group names that will be authenticated as in addition to the system:bootstrappers group.You can alternatively provide the values in the stringData field of the Secret without base64 encoding them:secret/bootstrap-token-secret-literal.yaml apiVersion: v1 kind: Secret metadata: # Note how the Secret is named name: bootstrap-token-5emitj # A bootstrap token Secret usually resides in the kube-system namespace namespace: kube-system type: bootstrap.kubernetes.io/token stringData: auth-extra-groups: "system:bootstrappers:kubeadm:default-node-token" expiration: "2020-09-13T04:39:10Z" # This token ID is used in the name token-id: "5emitj" token-secret: "kq4gihvszzgn1p0r" # This token can be used for authentication usage-bootstrap-authentication: "true" # and it can be used for signing usage-bootstrap-signing: "true"Note:The stringData field for a Secret does not work well with server-side apply.Working with SecretsCreating a SecretThere are several options to create a Secret:Use kubectlUse a configuration fileUse the Kustomize toolConstraints on Secret names and dataThe name of a Secret object must be a valid DNS subdomain name.You can specify the data and/or the stringData field when creating a configuration file for a Secret. The data and the stringData fields are optional. The values for all keys in the data field have to be base64-encoded strings. If the conversion to base64 string is not desirable, you can choose to specify the stringData field instead, which accepts arbitrary strings as values.The keys of data and stringData must consist of alphanumeric characters, -, _ or .. All key-value pairs in the stringData field are internally merged into the data field. If a key appears in both the data and the stringData field, the value specified in the stringData field takes precedence.Size limitIndividual Secrets are limited to 1MiB in size. This is to discourage creation of very large Secrets that could exhaust the API server and kubelet memory. However, creation of many smaller Secrets could also exhaust memory. You can use a resource quota to limit the number of Secrets (or other resources) in a namespace.Editing a SecretYou can edit an existing Secret unless it is immutable. To edit a Secret, use one of the following methods:Use kubectlUse a configuration fileYou can also edit the data in a Secret using the Kustomize tool. However, this method creates a new Secret object with the edited data.Depending on how you created the Secret, as well as how the Secret is used in your Pods, updates to existing Secret objects are propagated automatically to Pods that use the data. For more information, refer to Using Secrets as files from a Pod section.Using a SecretSecrets can be mounted as data volumes or exposed as environment variables to be used by a container in a Pod. Secrets can also be used by other parts of the system, without being directly exposed to the Pod. For example, Secrets can hold credentials that other parts of the system should use to interact with external systems on your behalf.Secret volume sources are validated to ensure that the specified object reference actually points to an object of type Secret. Therefore, a Secret needs to be created before any Pods that depend on it.If the Secret cannot be fetched (perhaps because it does not exist, or due to a temporary lack of connection to the API server) the kubelet periodically retries running that Pod. The kubelet also reports an Event for that Pod, including details of the problem fetching the Secret.Optional SecretsWhen you reference a Secret in a Pod, you can mark the Secret as optional, such as in the following example. If an optional Secret doesn't exist, Kubernetes ignores it.secret/optional-secret.yaml apiVersion: v1 kind: Pod metadata: name: mypod spec: containers: \- name: mypod image: redis volumeMounts: \- name: foo mountPath: "/etc/foo" readOnly: true volumes: \- name: foo secret: secretName: mysecret optional: trueBy default, Secrets are required. None of a Pod's containers will start until all non-optional Secrets are available.If a Pod references a specific key in a non-optional Secret and that Secret does exist, but is missing the named key, the Pod fails during startup.Using Secrets as files from a PodIf you want to access data from a Secret in a Pod, one way to do that is to have Kubernetes make the value of that Secret be available as a file inside the filesystem of one or more of the Pod's containers.For instructions, refer to Create a Pod that has access to the secret data through a Volume.When a volume contains data from a Secret, and that Secret is updated, Kubernetes tracks this and updates the data in the volume, using an eventually-consistent approach.Note:A container using a Secret as a subPath volume mount does not receive automated Secret updates.The kubelet keeps a cache of the current keys and values for the Secrets that are used in volumes for pods on that node. You can configure the way that the kubelet detects changes from the cached values. The configMapAndSecretChangeDetectionStrategy field in the kubelet configuration controls which strategy the kubelet uses. The default strategy is Watch.Updates to Secrets can be either propagated by an API watch mechanism (the default), based on a cache with a defined time-to-live, or polled from the cluster API server on each kubelet synchronisation loop.As a result, the total delay from the moment when the Secret is updated to the moment when new keys are projected to the Pod can be as long as the kubelet sync period + cache propagation delay, where the cache propagation delay depends on the chosen cache type (following the same order listed in the previous paragraph, these are: watch propagation delay, the configured cache TTL, or zero for direct polling).Using Secrets as environment variablesTo use a Secret in an environment variable in a Pod:For each container in your Pod specification, add an environment variable for each Secret key that you want to use to the env[].valueFrom.secretKeyRef field.Modify your image and/or command line so that the program looks for values in the specified environment variables.For instructions, refer to Define container environment variables using Secret data.It's important to note that the range of characters allowed for environment variable names in pods is restricted. If any keys do not meet the rules, those keys are not made available to your container, though the Pod is allowed to start.Container image pull SecretsIf you want to fetch container images from a private repository, you need a way for the kubelet on each node to authenticate to that repository. You can configure image pull Secrets to make this possible. These Secrets are configured at the Pod level.Using imagePullSecretsThe imagePullSecrets field is a list of references to Secrets in the same namespace. You can use an imagePullSecrets to pass a Secret that contains a Docker (or other) image registry password to the kubelet. The kubelet uses this information to pull a private image on behalf of your Pod. See the PodSpec API for more information about the imagePullSecrets field.Manually specifying an imagePullSecretYou can learn how to specify imagePullSecrets from the container images documentation.Arranging for imagePullSecrets to be automatically attachedYou can manually create imagePullSecrets, and reference these from a ServiceAccount. Any Pods created with that ServiceAccount or created with that ServiceAccount by default, will get their imagePullSecrets field set to that of the service account. See Add ImagePullSecrets to a service account for a detailed explanation of that process.Using Secrets with static PodsYou cannot use ConfigMaps or Secrets with static Pods.Immutable SecretsFEATURE STATE: Kubernetes v1.21 [stable]Kubernetes lets you mark specific Secrets (and ConfigMaps) as immutable. Preventing changes to the data of an existing Secret has the following benefits:protects you from accidental (or unwanted) updates that could cause applications outages(for clusters that extensively use Secrets - at least tens of thousands of unique Secret to Pod mounts), switching to immutable Secrets improves the performance of your cluster by significantly reducing load on kube-apiserver. The kubelet does not need to maintain a [watch] on any Secrets that are marked as immutable.Marking a Secret as immutableYou can create an immutable Secret by setting the immutable field to true. For example,apiVersion: v1 kind: Secret metadata: ... data: ... immutable: true You can also update any existing mutable Secret to make it immutable.Note:Once a Secret or ConfigMap is marked as immutable, it is not possible to revert this change nor to mutate the contents of the data field. You can only delete and recreate the Secret. Existing Pods maintain a mount point to the deleted Secret - it is recommended to recreate these pods.Information security for SecretsAlthough ConfigMap and Secret work similarly, Kubernetes applies some additional protection for Secret objects.Secrets often hold values that span a spectrum of importance, many of which can cause escalations within Kubernetes (e.g. service account tokens) and to external systems. Even if an individual app can reason about the power of the Secrets it expects to interact with, other apps within the same namespace can render those assumptions invalid.A Secret is only sent to a node if a Pod on that node requires it. For mounting Secrets into Pods, the kubelet stores a copy of the data into a tmpfs so that the confidential data is not written to durable storage. Once the Pod that depends on the Secret is deleted, the kubelet deletes its local copy of the confidential data from the Secret.There may be several containers in a Pod. By default, containers you define only have access to the default ServiceAccount and its related Secret. You must explicitly define environment variables or map a volume into a container in order to provide access to any other Secret.There may be Secrets for several Pods on the same node. However, only the Secrets that a Pod requests are potentially visible within its containers. Therefore, one Pod does not have access to the Secrets of another Pod.Configure least-privilege access to SecretsTo enhance the security measures around Secrets, use separate namespaces to isolate access to mounted secrets.Warning:Any containers that run with privileged: true on a node can access all Secrets used on that node.What's nextFor guidelines to manage and improve the security of your Secrets, refer to Good practices for Kubernetes Secrets.Learn how to manage Secrets using kubectlLearn how to manage Secrets using config fileLearn how to manage Secrets using kustomizeRead the API reference for Secret3.7.4 - Liveness, Readiness, and Startup ProbesKubernetes has various types of probes:Liveness probeReadiness probeStartup probeLiveness probeLiveness probes determine when to restart a container. For example, liveness probes could catch a deadlock when an application is running but unable to make progress.If a container fails its liveness probe repeatedly, the kubelet restarts the container.Liveness probes do not wait for readiness probes to succeed. If you want to wait before executing a liveness probe, you can either define initialDelaySeconds or use a startup probe.Readiness probeReadiness probes determine when a container is ready to start accepting traffic. This is useful when waiting for an application to perform time-consuming initial tasks, such as establishing network connections, loading files, and warming caches.If the readiness probe returns a failed state, Kubernetes removes the pod from all matching service endpoints.Readiness probes run on the container during its whole lifecycle.Startup probeA startup probe verifies whether the application within a container is started. This can be used to adopt liveness checks on slow starting containers, avoiding them getting killed by the kubelet before they are up and running.If such a probe is configured, it disables liveness and readiness checks until it succeeds.This type of probe is only executed at startup, unlike liveness and readiness probes, which are run periodically.Read more about the Configure Liveness, Readiness and Startup Probes.3.7.5 - Resource Management for Pods and ContainersWhen you specify a Pod, you can optionally specify how much of each resource a container needs. The most common resources to specify are CPU and memory (RAM); there are others.When you specify the resource request for containers in a Pod, the kube-scheduler uses this information to decide which node to place the Pod on. When you specify a resource limit for a container, the kubelet enforces those limits so that the running container is not allowed to use more of that resource than the limit you set. The kubelet also reserves at least the request amount of that system resource specifically for that container to use.Requests and limitsIf the node where a Pod is running has enough of a resource available, it's possible (and allowed) for a container to use more resource than its request for that resource specifies.For example, if you set a memory request of 256 MiB for a container, and that container is in a Pod scheduled to a Node with 8GiB of memory and no other Pods, then the container can try to use more RAM.Limits are a different story. Both cpu and memory limits are applied by the kubelet (and container runtime), and are ultimately enforced by the kernel. On Linux nodes, the Linux kernel enforces limits with cgroups. The behavior of cpu and memory limit enforcement is slightly different.cpu limits are enforced by CPU throttling. When a container approaches its cpu limit, the kernel will restrict access to the CPU corresponding to the container's limit. Thus, a cpu limit is a hard limit the kernel enforces. Containers may not use more CPU than is specified in their cpu limit.memory limits are enforced by the kernel with out of memory (OOM) kills. When a container uses more than its memory limit, the kernel may terminate it. However, terminations only happen when the kernel detects memory pressure. Thus, a container that over allocates memory may not be immediately killed. This means memory limits are enforced reactively. A container may use more memory than its memory limit, but if it does, it may get killed.Note:There is an alpha feature MemoryQoS which attempts to add more preemptive limit enforcement for memory (as opposed to reactive enforcement by the OOM killer). However, this effort is stalled due to a potential livelock situation a memory hungry can cause.Note:If you specify a limit for a resource, but do not specify any request, and no admission-time mechanism has applied a default request for that resource, then Kubernetes copies the limit you specified and uses it as the requested value for the resource.Resource typesCPU and memory are each a resource type. A resource type has a base unit. CPU represents compute processing and is specified in units of Kubernetes CPUs. Memory is specified in units of bytes. For Linux workloads, you can specify huge page resources. Huge pages are a Linux-specific feature where the node kernel allocates blocks of memory that are much larger than the default page size.For example, on a system where the default page size is 4KiB, you could specify a limit, hugepages-2Mi: 80Mi. If the container tries allocating over 40 2MiB huge pages (a total of 80 MiB), that allocation fails.Note:You cannot overcommit hugepages-* resources. This is different from the memory and cpu resources.CPU and memory are collectively referred to as compute resources, or resources. Compute resources are measurable quantities that can be requested, allocated, and consumed. They are distinct from API resources. API resources, such as Pods and Services are objects that can be read and modified through the Kubernetes API server.Resource requests and limits of Pod and containerFor each container, you can specify resource limits and requests, including the following:spec.containers[].resources.limits.cpuspec.containers[].resources.limits.memoryspec.containers[].resources.limits.hugepages-spec.containers[].resources.requests.cpuspec.containers[].resources.requests.memoryspec.containers[].resources.requests.hugepages-Although you can only specify requests and limits for individual containers, it is also useful to think about the overall resource requests and limits for a Pod. For a particular resource, a Pod resource request/limit is the sum of the resource requests/limits of that type for each container in the Pod.Pod-level resource specificationFEATURE STATE: Kubernetes v1.32 [alpha] (enabled by default: false)Starting in Kubernetes 1.32, you can also specify resource requests and limits at the Pod level. At the Pod level, Kubernetes 1.33 only supports resource requests or limits for specific resource types: cpu and / or memory. This feature is currently in alpha and with the feature enabled, Kubernetes allows you to declare an overall resource budget for the Pod, which is especially helpful when dealing with a large number of containers where it can be difficult to accurately gauge individual resource needs. Additionally, it enables containers within a Pod to share idle resources with each other, improving resource utilization.For a Pod, you can specify resource limits and requests for CPU and memory by including the following:spec.resources.limits.cpuspec.resources.limits.memoryspec.resources.requests.cpuspec.resources.requests.memoryResource units in KubernetesCPU resource unitsLimits and requests for CPU resources are measured in cpu units. In Kubernetes, 1 CPU unit is equivalent to 1 physical CPU core, or 1 virtual core, depending on whether the node is a physical host or a virtual machine running inside a physical machine.Fractional requests are allowed. When you define a container with spec.containers[].resources.requests.cpu set to 0.5, you are requesting half as much CPU time compared to if you asked for 1.0 CPU. For CPU resource units, the quantity expression 0.1 is equivalent to the expression 100m, which can be read as "one hundred millicpu". Some people say "one hundred millicores", and this is understood to mean the same thing.CPU resource is always specified as an absolute amount of resource, never as a relative amount. For example, 500m CPU represents the roughly same amount of computing power whether that container runs on a single-core, dual-core, or 48-core machine.Note:Kubernetes doesn't allow you to specify CPU resources with a precision finer than 1m or 0.001 CPU. To avoid accidentally using an invalid CPU quantity, it's useful to specify CPU units using the milliCPU form instead of the decimal form when using less than 1 CPU unit.For example, you have a Pod that uses 5m or 0.005 CPU and would like to decrease its CPU resources. By using the decimal form, it's harder to spot that 0.0005 CPU is an invalid value, while by using the milliCPU form, it's easier to spot that 0.5m is an invalid value.Memory resource unitsLimits and requests for memory are measured in bytes. You can express memory as a plain integer or as a fixed-point number using one of these quantity suffixes: E, P, T, G, M, k. You can also use the power-of-two equivalents: Ei, Pi, Ti, Gi, Mi, Ki. For example, the following represent roughly the same value:128974848, 129e6, 129M, 128974848000m, 123Mi Pay attention to the case of the suffixes. If you request 400m of memory, this is a request for 0.4 bytes. Someone who types that probably meant to ask for 400 mebibytes (400Mi) or 400 megabytes (400M).Container resources exampleThe following Pod has two containers. Both containers are defined with a request for 0.25 CPU and 64MiB (226 bytes) of memory. Each container has a limit of 0.5 CPU and 128MiB of memory. You can say the Pod has a request of 0.5 CPU and 128 MiB of memory, and a limit of 1 CPU and 256MiB of memory.--- apiVersion: v1 kind: Pod metadata: name: frontend spec: containers: \- name: app image: images.my-company.example/app:v4 resources: requests: memory: "64Mi" cpu: "250m" limits: memory: "128Mi" cpu: "500m" \- name: log-aggregator image: images.my-company.example/log-aggregator:v6 resources: requests: memory: "64Mi" cpu: "250m" limits: memory: "128Mi" cpu: "500m" Pod resources exampleFEATURE STATE: Kubernetes v1.32 [alpha] (enabled by default: false)This feature can be enabled by setting the PodLevelResources feature gate. The following Pod has an explicit request of 1 CPU and 100 MiB of memory, and an explicit limit of 1 CPU and 200 MiB of memory. The pod-resources-demo-ctr-1 container has explicit requests and limits set. However, the pod-resources-demo-ctr-2 container will simply share the resources available within the Pod resource boundaries, as it does not have explicit requests and limits set.pods/resource/pod-level-resources.yaml apiVersion: v1 kind: Pod metadata: name: pod-resources-demo namespace: pod-resources-example spec: resources: limits: cpu: "1" memory: "200Mi" requests: cpu: "1" memory: "100Mi" containers: \- name: pod-resources-demo-ctr-1 image: nginx resources: limits: cpu: "0.5" memory: "100Mi" requests: cpu: "0.5" memory: "50Mi" \- name: pod-resources-demo-ctr-2 image: fedora command: \- sleep \- inf How Pods with resource requests are scheduledWhen you create a Pod, the Kubernetes scheduler selects a node for the Pod to run on. Each node has a maximum capacity for each of the resource types: the amount of CPU and memory it can provide for Pods. The scheduler ensures that, for each resource type, the sum of the resource requests of the scheduled containers is less than the capacity of the node. Note that although actual memory or CPU resource usage on nodes is very low, the scheduler still refuses to place a Pod on a node if the capacity check fails. This protects against a resource shortage on a node when resource usage later increases, for example, during a daily peak in request rate.How Kubernetes applies resource requests and limitsWhen the kubelet starts a container as part of a Pod, the kubelet passes that container's requests and limits for memory and CPU to the container runtime.On Linux, the container runtime typically configures kernel cgroups that apply and enforce the limits you defined.The CPU limit defines a hard ceiling on how much CPU time the container can use. During each scheduling interval (time slice), the Linux kernel checks to see if this limit is exceeded; if so, the kernel waits before allowing that cgroup to resume execution.The CPU request typically defines a weighting. If several different containers (cgroups) want to run on a contended system, workloads with larger CPU requests are allocated more CPU time than workloads with small requests.The memory request is mainly used during (Kubernetes) Pod scheduling. On a node that uses cgroups v2, the container runtime might use the memory request as a hint to set memory.min and memory.low.The memory limit defines a memory limit for that cgroup. If the container tries to allocate more memory than this limit, the Linux kernel out-of-memory subsystem activates and, typically, intervenes by stopping one of the processes in the container that tried to allocate memory. If that process is the container's PID 1, and the container is marked as restartable, Kubernetes restarts the container.The memory limit for the Pod or container can also apply to pages in memory backed volumes, such as an emptyDir. The kubelet tracks tmpfs emptyDir volumes as container memory use, rather than as local ephemeral storage. When using memory backed emptyDir, be sure to check the notes below.If a container exceeds its memory request and the node that it runs on becomes short of memory overall, it is likely that the Pod the container belongs to will be evicted.A container might or might not be allowed to exceed its CPU limit for extended periods of time. However, container runtimes don't terminate Pods or containers for excessive CPU usage.To determine whether a container cannot be scheduled or is being killed due to resource limits, see the Troubleshooting section.Monitoring compute & memory resource usageThe kubelet reports the resource usage of a Pod as part of the Pod status.If optional tools for monitoring are available in your cluster, then Pod resource usage can be retrieved either from the Metrics API directly or from your monitoring tools.Considerations for memory backed emptyDir volumesCaution:If you do not specify a sizeLimit for an emptyDir volume, that volume may consume up to that pod's memory limit (Pod.spec.containers[].resources.limits.memory). If you do not set a memory limit, the pod has no upper bound on memory consumption, and can consume all available memory on the node. Kubernetes schedules pods based on resource requests (Pod.spec.containers[].resources.requests) and will not consider memory usage above the request when deciding if another pod can fit on a given node. This can result in a denial of service and cause the OS to do out-of-memory (OOM) handling. It is possible to create any number of emptyDirs that could potentially consume all available memory on the node, making OOM more likely.From the perspective of memory management, there are some similarities between when a process uses memory as a work area and when using memory-backed emptyDir. But when using memory as a volume, like memory-backed emptyDir, there are additional points below that you should be careful of:Files stored on a memory-backed volume are almost entirely managed by the user application. Unlike when used as a work area for a process, you can not rely on things like language-level garbage collection.The purpose of writing files to a volume is to save data or pass it between applications. Neither Kubernetes nor the OS may automatically delete files from a volume, so memory used by those files can not be reclaimed when the system or the pod are under memory pressure.A memory-backed emptyDir is useful because of its performance, but memory is generally much smaller in size and much higher in cost than other storage media, such as disks or SSDs. Using large amounts of memory for emptyDir volumes may affect the normal operation of your pod or of the whole node, so should be used carefully.If you are administering a cluster or namespace, you can also set ResourceQuota that limits memory use; you may also want to define a LimitRange for additional enforcement. If you specify a spec.containers[].resources.limits.memory for each Pod, then the maximum size of an emptyDir volume will be the pod's memory limit.As an alternative, a cluster administrator can enforce size limits for emptyDir volumes in new Pods using a policy mechanism such as ValidationAdmissionPolicy.Local ephemeral storageFEATURE STATE: Kubernetes v1.25 [stable]Nodes have local ephemeral storage, backed by locally-attached writeable devices or, sometimes, by RAM. "Ephemeral" means that there is no long-term guarantee about durability.Pods use ephemeral local storage for scratch space, caching, and for logs. The kubelet can provide scratch space to Pods using local ephemeral storage to mount emptyDir volumes into containers.The kubelet also uses this kind of storage to hold node-level container logs, container images, and the writable layers of running containers.Caution:If a node fails, the data in its ephemeral storage can be lost. Your applications cannot expect any performance SLAs (disk IOPS for example) from local ephemeral storage.Note:To make the resource quota work on ephemeral-storage, two things need to be done:An admin sets the resource quota for ephemeral-storage in a namespace.A user needs to specify limits for the ephemeral-storage resource in the Pod spec.If the user doesn't specify the ephemeral-storage resource limit in the Pod spec, the resource quota is not enforced on ephemeral-storage.Kubernetes lets you track, reserve and limit the amount of ephemeral local storage a Pod can consume.Configurations for local ephemeral storageKubernetes supports two ways to configure local ephemeral storage on a node:Single filesystemTwo filesystemsIn this configuration, you place all different kinds of ephemeral local data (emptyDir volumes, writeable layers, container images, logs) into one filesystem. The most effective way to configure the kubelet means dedicating this filesystem to Kubernetes (kubelet) data.The kubelet also writes node-level container logs and treats these similarly to ephemeral local storage.The kubelet writes logs to files inside its configured log directory (/var/log by default); and has a base directory for other locally stored data (/var/lib/kubelet by default).Typically, both /var/lib/kubelet and /var/log are on the system root filesystem, and the kubelet is designed with that layout in mind.Your node can have as many other filesystems, not used for Kubernetes, as you like.You have a filesystem on the node that you're using for ephemeral data that comes from running Pods: logs, and emptyDir volumes. You can use this filesystem for other data (for example: system logs not related to Kubernetes); it can even be the root filesystem.The kubelet also writes node-level container logs into the first filesystem, and treats these similarly to ephemeral local storage.You also use a separate filesystem, backed by a different logical storage device. In this configuration, the directory where you tell the kubelet to place container image layers and writeable layers is on this second filesystem.The first filesystem does not hold any image layers or writeable layers.Your node can have as many other filesystems, not used for Kubernetes, as you like.The kubelet can measure how much local storage it is using. It does this provided that you have set up the node using one of the supported configurations for local ephemeral storage.If you have a different configuration, then the kubelet does not apply resource limits for ephemeral local storage.Note:The kubelet tracks tmpfs emptyDir volumes as container memory use, rather than as local ephemeral storage.Note:The kubelet will only track the root filesystem for ephemeral storage. OS layouts that mount a separate disk to /var/lib/kubelet or /var/lib/containers will not report ephemeral storage correctly.Setting requests and limits for local ephemeral storageYou can specify ephemeral-storage for managing local ephemeral storage. Each container of a Pod can specify either or both of the following:spec.containers[].resources.limits.ephemeral-storagespec.containers[].resources.requests.ephemeral-storageLimits and requests for ephemeral-storage are measured in byte quantities. You can express storage as a plain integer or as a fixed-point number using one of these suffixes: E, P, T, G, M, k. You can also use the power-of-two equivalents: Ei, Pi, Ti, Gi, Mi, Ki. For example, the following quantities all represent roughly the same value:128974848129e6129M123MiPay attention to the case of the suffixes. If you request 400m of ephemeral-storage, this is a request for 0.4 bytes. Someone who types that probably meant to ask for 400 mebibytes (400Mi) or 400 megabytes (400M).In the following example, the Pod has two containers. Each container has a request of 2GiB of local ephemeral storage. Each container has a limit of 4GiB of local ephemeral storage. Therefore, the Pod has a request of 4GiB of local ephemeral storage, and a limit of 8GiB of local ephemeral storage. 500Mi of that limit could be consumed by the emptyDir volume.apiVersion: v1 kind: Pod metadata: name: frontend spec: containers: \- name: app image: images.my-company.example/app:v4 resources: requests: ephemeral-storage: "2Gi" limits: ephemeral-storage: "4Gi" volumeMounts: \- name: ephemeral mountPath: "/tmp" \- name: log-aggregator image: images.my-company.example/log-aggregator:v6 resources: requests: ephemeral-storage: "2Gi" limits: ephemeral-storage: "4Gi" volumeMounts: \- name: ephemeral mountPath: "/tmp" volumes: \- name: ephemeral emptyDir: sizeLimit: 500Mi How Pods with ephemeral-storage requests are scheduledWhen you create a Pod, the Kubernetes scheduler selects a node for the Pod to run on. Each node has a maximum amount of local ephemeral storage it can provide for Pods. For more information, see Node Allocatable.The scheduler ensures that the sum of the resource requests of the scheduled containers is less than the capacity of the node.Ephemeral storage consumption managementIf the kubelet is managing local ephemeral storage as a resource, then the kubelet measures storage use in:emptyDir volumes, except tmpfs emptyDir volumesdirectories holding node-level logswriteable container layersIf a Pod is using more ephemeral storage than you allow it to, the kubelet sets an eviction signal that triggers Pod eviction.For container-level isolation, if a container's writable layer and log usage exceeds its storage limit, the kubelet marks the Pod for eviction.For pod-level isolation the kubelet works out an overall Pod storage limit by summing the limits for the containers in that Pod. In this case, if the sum of the local ephemeral storage usage from all containers and also the Pod's emptyDir volumes exceeds the overall Pod storage limit, then the kubelet also marks the Pod for eviction.Caution:If the kubelet is not measuring local ephemeral storage, then a Pod that exceeds its local storage limit will not be evicted for breaching local storage resource limits.However, if the filesystem space for writeable container layers, node-level logs, or emptyDir volumes falls low, the node taints itself as short on local storage and this taint triggers eviction for any Pods that don't specifically tolerate the taint.See the supported configurations for ephemeral local storage.The kubelet supports different ways to measure Pod storage use:Periodic scanningFilesystem project quotaThe kubelet performs regular, scheduled checks that scan each emptyDir volume, container log directory, and writeable container layer.The scan measures how much space is used.Note:In this mode, the kubelet does not track open file descriptors for deleted files.If you (or a container) create a file inside an emptyDir volume, something then opens that file, and you delete the file while it is still open, then the inode for the deleted file stays until you close that file but the kubelet does not categorize the space as in use.FEATURE STATE: Kubernetes v1.31 [beta] (enabled by default: false)Project quotas are an operating-system level feature for managing storage use on filesystems. With Kubernetes, you can enable project quotas for monitoring storage use. Make sure that the filesystem backing the emptyDir volumes, on the node, provides project quota support. For example, XFS and ext4fs offer project quotas.Note:Project quotas let you monitor storage use; they do not enforce limits.Kubernetes uses project IDs starting from 1048576. The IDs in use are registered in /etc/projects and /etc/projid. If project IDs in this range are used for other purposes on the system, those project IDs must be registered in /etc/projects and /etc/projid so that Kubernetes does not use them.Quotas are faster and more accurate than directory scanning. When a directory is assigned to a project, all files created under a directory are created in that project, and the kernel merely has to keep track of how many blocks are in use by files in that project. If a file is created and deleted, but has an open file descriptor, it continues to consume space. Quota tracking records that space accurately whereas directory scans overlook the storage used by deleted files.To use quotas to track a pod's resource usage, the pod must be in a user namespace. Within user namespaces, the kernel restricts changes to projectIDs on the filesystem, ensuring the reliability of storage metrics calculated by quotas.If you want to use project quotas, you should:Enable the LocalStorageCapacityIsolationFSQuotaMonitoring=true feature gate using the featureGates field in the kubelet configuration.Ensure the UserNamespacesSupport feature gate is enabled, and that the kernel, CRI implementation and OCI runtime support user namespaces.Ensure that the root filesystem (or optional runtime filesystem) has project quotas enabled. All XFS filesystems support project quotas. For ext4 filesystems, you need to enable the project quota tracking feature while the filesystem is not mounted.# For ext4, with /dev/block-device not mounted sudo tune2fs -O project -Q prjquota /dev/block-device Ensure that the root filesystem (or optional runtime filesystem) is mounted with project quotas enabled. For both XFS and ext4fs, the mount option is named prjquota.If you don't want to use project quotas, you should:Disable the LocalStorageCapacityIsolationFSQuotaMonitoring feature gate using the featureGates field in the kubelet configuration.Extended resourcesExtended resources are fully-qualified resource names outside the kubernetes.io domain. They allow cluster operators to advertise and users to consume the non-Kubernetes-built-in resources.There are two steps required to use Extended Resources. First, the cluster operator must advertise an Extended Resource. Second, users must request the Extended Resource in Pods.Managing extended resourcesNode-level extended resourcesNode-level extended resources are tied to nodes.Device plugin managed resourcesSee Device Plugin for how to advertise device plugin managed resources on each node.Other resourcesTo advertise a new node-level extended resource, the cluster operator can submit a PATCH HTTP request to the API server to specify the available quantity in the status.capacity for a node in the cluster. After this operation, the node's status.capacity will include a new resource. The status.allocatable field is updated automatically with the new resource asynchronously by the kubelet.Because the scheduler uses the node's status.allocatable value when evaluating Pod fitness, the scheduler only takes account of the new value after that asynchronous update. There may be a short delay between patching the node capacity with a new resource and the time when the first Pod that requests the resource can be scheduled on that node.Example:Here is an example showing how to use curl to form an HTTP request that advertises five "example.com/foo" resources on node k8s-node-1 whose master is k8s-master.curl --header "Content-Type: application/json-patch+json" \ \--request PATCH \ \--data '[{"op": "add", "path": "/status/capacity/example.com~1foo", "value": "5"}]' \ http://k8s-master:8080/api/v1/nodes/k8s-node-1/status Note:In the preceding request, ~1 is the encoding for the character / in the patch path. The operation path value in JSON-Patch is interpreted as a JSON-Pointer. For more details, see IETF RFC 6901, section 3.Cluster-level extended resourcesCluster-level extended resources are not tied to nodes. They are usually managed by scheduler extenders, which handle the resource consumption and resource quota.You can specify the extended resources that are handled by scheduler extenders in scheduler configurationExample:The following configuration for a scheduler policy indicates that the cluster-level extended resource "example.com/foo" is handled by the scheduler extender.The scheduler sends a Pod to the scheduler extender only if the Pod requests "example.com/foo".The ignoredByScheduler field specifies that the scheduler does not check the "example.com/foo" resource in its PodFitsResources predicate.{ "kind": "Policy", "apiVersion": "v1", "extenders": [ { "urlPrefix":"", "bindVerb": "bind", "managedResources": [ { "name": "example.com/foo", "ignoredByScheduler": true } ] } ] } Consuming extended resourcesUsers can consume extended resources in Pod specs like CPU and memory. The scheduler takes care of the resource accounting so that no more than the available amount is simultaneously allocated to Pods.The API server restricts quantities of extended resources to whole numbers. Examples of valid quantities are 3, 3000m and 3Ki. Examples of invalid quantities are 0.5 and 1500m (because 1500m would result in 1.5).Note:Extended resources replace Opaque Integer Resources. Users can use any domain name prefix other than kubernetes.io which is reserved.To consume an extended resource in a Pod, include the resource name as a key in the spec.containers[].resources.limits map in the container spec.Note:Extended resources cannot be overcommitted, so request and limit must be equal if both are present in a container spec.A Pod is scheduled only if all of the resource requests are satisfied, including CPU, memory and any extended resources. The Pod remains in the PENDING state as long as the resource request cannot be satisfied.Example:The Pod below requests 2 CPUs and 1 "example.com/foo" (an extended resource).apiVersion: v1 kind: Pod metadata: name: my-pod spec: containers: \- name: my-container image: myimage resources: requests: cpu: 2 example.com/foo: 1 limits: example.com/foo: 1 PID limitingProcess ID (PID) limits allow for the configuration of a kubelet to limit the number of PIDs that a given Pod can consume. See PID Limiting for information.TroubleshootingMy Pods are pending with event message FailedSchedulingIf the scheduler cannot find any node where a Pod can fit, the Pod remains unscheduled until a place can be found. An Event is produced each time the scheduler fails to find a place for the Pod. You can use kubectl to view the events for a Pod; for example:kubectl describe pod frontend | grep -A 9999999999 Events Events: Type Reason Age From Message \---- ------ ---- ---- ------- Warning FailedScheduling 23s default-scheduler 0/42 nodes available: insufficient cpu In the preceding example, the Pod named "frontend" fails to be scheduled due to insufficient CPU resource on any node. Similar error messages can also suggest failure due to insufficient memory (PodExceedsFreeMemory). In general, if a Pod is pending with a message of this type, there are several things to try:Add more nodes to the cluster.Terminate unneeded Pods to make room for pending Pods.Check that the Pod is not larger than all the nodes. For example, if all the nodes have a capacity of cpu: 1, then a Pod with a request of cpu: 1.1 will never be scheduled.Check for node taints. If most of your nodes are tainted, and the new Pod does not tolerate that taint, the scheduler only considers placements onto the remaining nodes that don't have that taint.You can check node capacities and amounts allocated with the kubectl describe nodes command. For example:kubectl describe nodes e2e-test-node-pool-4lw4 Name: e2e-test-node-pool-4lw4 [ ... lines removed for clarity ...] Capacity: cpu: 2 memory: 7679792Ki pods: 110 Allocatable: cpu: 1800m memory: 7474992Ki pods: 110 [ ... lines removed for clarity ...] Non-terminated Pods: (5 in total) Namespace Name CPU Requests CPU Limits Memory Requests Memory Limits \--------- ---- ------------ ---------- --------------- ------------- kube-system fluentd-gcp-v1.38-28bv1 100m (5%) 0 (0%) 200Mi (2%) 200Mi (2%) kube-system kube-dns-3297075139-61lj3 260m (13%) 0 (0%) 100Mi (1%) 170Mi (2%) kube-system kube-proxy-e2e-test-... 100m (5%) 0 (0%) 0 (0%) 0 (0%) kube-system monitoring-influxdb-grafana-v4-z1m12 200m (10%) 200m (10%) 600Mi (8%) 600Mi (8%) kube-system node-problem-detector-v0.1-fj7m3 20m (1%) 200m (10%) 20Mi (0%) 100Mi (1%) Allocated resources: (Total limits may be over 100 percent, i.e., overcommitted.) CPU Requests CPU Limits Memory Requests Memory Limits \------------ ---------- --------------- ------------- 680m (34%) 400m (20%) 920Mi (11%) 1070Mi (13%) In the preceding output, you can see that if a Pod requests more than 1.120 CPUs or more than 6.23Gi of memory, that Pod will not fit on the node.By looking at the “Pods” section, you can see which Pods are taking up space on the node.The amount of resources available to Pods is less than the node capacity because system daemons use a portion of the available resources. Within the Kubernetes API, each Node has a .status.allocatable field (see NodeStatus for details).The .status.allocatable field describes the amount of resources that are available to Pods on that node (for example: 15 virtual CPUs and 7538 MiB of memory). For more information on node allocatable resources in Kubernetes, see Reserve Compute Resources for System Daemons.You can configure resource quotas to limit the total amount of resources that a namespace can consume. Kubernetes enforces quotas for objects in particular namespace when there is a ResourceQuota in that namespace. For example, if you assign specific namespaces to different teams, you can add ResourceQuotas into those namespaces. Setting resource quotas helps to prevent one team from using so much of any resource that this over-use affects other teams.You should also consider what access you grant to that namespace: full write access to a namespace allows someone with that access to remove any resource, including a configured ResourceQuota.My container is terminatedYour container might get terminated because it is resource-starved. To check whether a container is being killed because it is hitting a resource limit, call kubectl describe pod on the Pod of interest:kubectl describe pod simmemleak-hra99 The output is similar to:Name: simmemleak-hra99 Namespace: default Image(s): saadali/simmemleak Node: kubernetes-node-tf0f/10.240.216.66 Labels: name=simmemleak Status: Running Reason: Message: IP: 10.244.2.75 Containers: simmemleak: Image: saadali/simmemleak:latest Limits: cpu: 100m memory: 50Mi State: Running Started: Tue, 07 Jul 2019 12:54:41 -0700 Last State: Terminated Reason: OOMKilled Exit Code: 137 Started: Fri, 07 Jul 2019 12:54:30 -0700 Finished: Fri, 07 Jul 2019 12:54:33 -0700 Ready: False Restart Count: 5 Conditions: Type Status Ready False Events: Type Reason Age From Message \---- ------ ---- ---- ------- Normal Scheduled 42s default-scheduler Successfully assigned simmemleak-hra99 to kubernetes-node-tf0f Normal Pulled 41s kubelet Container image "saadali/simmemleak:latest" already present on machine Normal Created 41s kubelet Created container simmemleak Normal Started 40s kubelet Started container simmemleak Normal Killing 32s kubelet Killing container with id ead3fb35-5cf5-44ed-9ae1-488115be66c6: Need to kill Pod In the preceding example, the Restart Count: 5 indicates that the simmemleak container in the Pod was terminated and restarted five times (so far). The OOMKilled reason shows that the container tried to use more memory than its limit.Your next step might be to check the application code for a memory leak. If you find that the application is behaving how you expect, consider setting a higher memory limit (and possibly request) for that container.What's nextGet hands-on experience assigning Memory resources to containers and Pods.Get hands-on experience assigning CPU resources to containers and Pods.Read how the API reference defines a container and its resource requirementsRead about project quotas in XFSRead more about the kube-scheduler configuration reference (v1)Read more about Quality of Service classes for Pods3.7.6 - Organizing Cluster Access Using kubeconfig FilesUse kubeconfig files to organize information about clusters, users, namespaces, and authentication mechanisms. The kubectl command-line tool uses kubeconfig files to find the information it needs to choose a cluster and communicate with the API server of a cluster.Note:A file that is used to configure access to clusters is called a kubeconfig file. This is a generic way of referring to configuration files. It does not mean that there is a file named kubeconfig.Warning:Only use kubeconfig files from trusted sources. Using a specially-crafted kubeconfig file could result in malicious code execution or file exposure. If you must use an untrusted kubeconfig file, inspect it carefully first, much as you would a shell script.By default, kubectl looks for a file named config in the $HOME/.kube directory. You can specify other kubeconfig files by setting the KUBECONFIG environment variable or by setting the \--kubeconfig flag.For step-by-step instructions on creating and specifying kubeconfig files, see Configure Access to Multiple Clusters.Supporting multiple clusters, users, and authentication mechanismsSuppose you have several clusters, and your users and components authenticate in a variety of ways. For example:A running kubelet might authenticate using certificates.A user might authenticate using tokens.Administrators might have sets of certificates that they provide to individual users.With kubeconfig files, you can organize your clusters, users, and namespaces. You can also define contexts to quickly and easily switch between clusters and namespaces.ContextA context element in a kubeconfig file is used to group access parameters under a convenient name. Each context has three parameters: cluster, namespace, and user. By default, the kubectl command-line tool uses parameters from the current context to communicate with the cluster.To choose the current context:kubectl config use-context The KUBECONFIG environment variableThe KUBECONFIG environment variable holds a list of kubeconfig files. For Linux and Mac, the list is colon-delimited. For Windows, the list is semicolon-delimited. The KUBECONFIG environment variable is not required. If the KUBECONFIG environment variable doesn't exist, kubectl uses the default kubeconfig file, $HOME/.kube/config.If the KUBECONFIG environment variable does exist, kubectl uses an effective configuration that is the result of merging the files listed in the KUBECONFIG environment variable.Merging kubeconfig filesTo see your configuration, enter this command:kubectl config view As described previously, the output might be from a single kubeconfig file, or it might be the result of merging several kubeconfig files.Here are the rules that kubectl uses when it merges kubeconfig files:If the --kubeconfig flag is set, use only the specified file. Do not merge. Only one instance of this flag is allowed.Otherwise, if the KUBECONFIG environment variable is set, use it as a list of files that should be merged. Merge the files listed in the KUBECONFIG environment variable according to these rules:Ignore empty filenames.Produce errors for files with content that cannot be deserialized.The first file to set a particular value or map key wins.Never change the value or map key. Example: Preserve the context of the first file to set current-context. Example: If two files specify a red-user, use only values from the first file's red-user. Even if the second file has non-conflicting entries under red-user, discard them.For an example of setting the KUBECONFIG environment variable, see Setting the KUBECONFIG environment variable.Otherwise, use the default kubeconfig file, $HOME/.kube/config, with no merging.Determine the context to use based on the first hit in this chain:Use the --context command-line flag if it exists.Use the current-context from the merged kubeconfig files.An empty context is allowed at this point.Determine the cluster and user. At this point, there might or might not be a context. Determine the cluster and user based on the first hit in this chain, which is run twice: once for user and once for cluster:Use a command-line flag if it exists: --user or --cluster.If the context is non-empty, take the user or cluster from the context.The user and cluster can be empty at this point.Determine the actual cluster information to use. At this point, there might or might not be cluster information. Build each piece of the cluster information based on this chain; the first hit wins:Use command line flags if they exist: --server, --certificate-authority, --insecure-skip-tls-verify.If any cluster information attributes exist from the merged kubeconfig files, use them.If there is no server location, fail.Determine the actual user information to use. Build user information using the same rules as cluster information, except allow only one authentication technique per user:Use command line flags if they exist: --client-certificate, --client-key, --username, --password, --token.Use the user fields from the merged kubeconfig files.If there are two conflicting techniques, fail.For any information still missing, use default values and potentially prompt for authentication information.File referencesFile and path references in a kubeconfig file are relative to the location of the kubeconfig file. File references on the command line are relative to the current working directory. In $HOME/.kube/config, relative paths are stored relatively, and absolute paths are stored absolutely.ProxyYou can configure kubectl to use a proxy per cluster using proxy-url in your kubeconfig file, like this:apiVersion: v1 kind: Config clusters: \- cluster: proxy-url: http://proxy.example.org:3128 server: https://k8s.example.org/k8s/clusters/c-xxyyzz name: development users: \- name: developer contexts: \- context: name: development What's nextConfigure Access to Multiple Clusterskubectl config3.7.7 - Resource Management for Windows nodesThis page outlines the differences in how resources are managed between Linux and Windows.On Linux nodes, cgroups are used as a pod boundary for resource control. Containers are created within that boundary for network, process and file system isolation. The Linux cgroup APIs can be used to gather CPU, I/O, and memory use statistics.In contrast, Windows uses a job object per container with a system namespace filter to contain all processes in a container and provide logical isolation from the host. (Job objects are a Windows process isolation mechanism and are different from what Kubernetes refers to as a Job).There is no way to run a Windows container without the namespace filtering in place. This means that system privileges cannot be asserted in the context of the host, and thus privileged containers are not available on Windows. Containers cannot assume an identity from the host because the Security Account Manager (SAM) is separate.Memory managementWindows does not have an out-of-memory process killer as Linux does. Windows always treats all user-mode memory allocations as virtual, and pagefiles are mandatory.Windows nodes do not overcommit memory for processes. The net effect is that Windows won't reach out of memory conditions the same way Linux does, and processes page to disk instead of being subject to out of memory (OOM) termination. If memory is over-provisioned and all physical memory is exhausted, then paging can slow down performance.CPU managementWindows can limit the amount of CPU time allocated for different processes but cannot guarantee a minimum amount of CPU time.On Windows, the kubelet supports a command-line flag to set the scheduling priority of the kubelet process: --windows-priorityclass. This flag allows the kubelet process to get more CPU time slices when compared to other processes running on the Windows host. More information on the allowable values and their meaning is available at Windows Priority Classes. To ensure that running Pods do not starve the kubelet of CPU cycles, set this flag to ABOVE_NORMAL_PRIORITY_CLASS or above.Resource reservationTo account for memory and CPU used by the operating system, the container runtime, and by Kubernetes host processes such as the kubelet, you can (and should) reserve memory and CPU resources with the --kube-reserved and/or --system-reserved kubelet flags. On Windows these values are only used to calculate the node's allocatable resources.Caution:As you deploy workloads, set resource memory and CPU limits on containers. This also subtracts from NodeAllocatable and helps the cluster-wide scheduler in determining which pods to place on which nodes.Scheduling pods without limits may over-provision the Windows nodes and in extreme cases can cause the nodes to become unhealthy.On Windows, a good practice is to reserve at least 2GiB of memory.To determine how much CPU to reserve, identify the maximum pod density for each node and monitor the CPU usage of the system services running there, then choose a value that meets your workload needs.3.8 - SecurityConcepts for keeping your cloud-native workload secure.This section of the Kubernetes documentation aims to help you learn to run workloads more securely, and about the essential aspects of keeping a Kubernetes cluster secure.Kubernetes is based on a cloud-native architecture, and draws on advice from the CNCF about good practice for cloud native information security.Read Cloud Native Security and Kubernetes for the broader context about how to secure your cluster and the applications that you're running on it.Kubernetes security mechanismsKubernetes includes several APIs and security controls, as well as ways to define policies that can form part of how you manage information security.Control plane protectionA key security mechanism for any Kubernetes cluster is to control access to the Kubernetes API.Kubernetes expects you to configure and use TLS to provide data encryption in transit within the control plane, and between the control plane and its clients. You can also enable encryption at rest for the data stored within Kubernetes control plane; this is separate from using encryption at rest for your own workloads' data, which might also be a good idea.SecretsThe Secret API provides basic protection for configuration values that require confidentiality.Workload protectionEnforce Pod security standards to ensure that Pods and their containers are isolated appropriately. You can also use RuntimeClasses to define custom isolation if you need it.Network policies let you control network traffic between Pods, or between Pods and the network outside your cluster.You can deploy security controls from the wider ecosystem to implement preventative or detective controls around Pods, their containers, and the images that run in them.Admission controlAdmission controllers are plugins that intercept Kubernetes API requests and can validate or mutate the requests based on specific fields in the request. Thoughtfully designing these controllers helps to avoid unintended disruptions as Kubernetes APIs change across version updates. For design considerations, see Admission Webhook Good Practices.AuditingKubernetes audit logging provides a security-relevant, chronological set of records documenting the sequence of actions in a cluster. The cluster audits the activities generated by users, by applications that use the Kubernetes API, and by the control plane itself.Cloud provider securityNote: Items on this page refer to vendors external to Kubernetes. The Kubernetes project authors aren't responsible for those third-party products or projects. To add a vendor, product or project to this list, read the content guide before submitting a change. More information.If you are running a Kubernetes cluster on your own hardware or a different cloud provider, consult your documentation for security best practices. Here are links to some of the popular cloud providers' security documentation:Cloud provider securityIaaS ProviderLinkAlibaba Cloudhttps://www.alibabacloud.com/trust-centerAmazon Web Serviceshttps://aws.amazon.com/securityGoogle Cloud Platformhttps://cloud.google.com/securityHuawei Cloudhttps://www.huaweicloud.com/intl/en-us/securecenter/overallsafetyIBM Cloudhttps://www.ibm.com/cloud/securityMicrosoft Azurehttps://docs.microsoft.com/en-us/azure/security/azure-securityOracle Cloud Infrastructurehttps://www.oracle.com/securityTencent Cloudhttps://www.tencentcloud.com/solutions/data-security-and-information-protectionVMware vSpherehttps://www.vmware.com/solutions/security/hardening-guidesPoliciesYou can define security policies using Kubernetes-native mechanisms, such as NetworkPolicy (declarative control over network packet filtering) or ValidatingAdmissionPolicy (declarative restrictions on what changes someone can make using the Kubernetes API).However, you can also rely on policy implementations from the wider ecosystem around Kubernetes. Kubernetes provides extension mechanisms to let those ecosystem projects implement their own policy controls on source code review, container image approval, API access controls, networking, and more.For more information about policy mechanisms and Kubernetes, read Policies.What's nextLearn about related Kubernetes security topics:Securing your clusterKnown vulnerabilities in Kubernetes (and links to further information)Data encryption in transit for the control planeData encryption at restControlling Access to the Kubernetes APINetwork policies for PodsSecrets in KubernetesPod security standardsRuntimeClassesLearn the context:Cloud Native Security and KubernetesGet certified:Certified Kubernetes Security Specialist certification and official training course.Read more in this section:3.8.1 - Cloud Native Security and KubernetesConcepts for keeping your cloud-native workload secure.Kubernetes is based on a cloud-native architecture, and draws on advice from the CNCF about good practice for cloud native information security.Read on through this page for an overview of how Kubernetes is designed to help you deploy a secure cloud native platform.Cloud native information securityThe CNCF white paper on cloud native security defines security controls and practices that are appropriate to different lifecycle phases.Develop lifecycle phaseEnsure the integrity of development environments.Design applications following good practice for information security, appropriate for your context.Consider end user security as part of solution design.To achieve this, you can:Adopt an architecture, such as zero trust, that minimizes attack surfaces, even for internal threats.Define a code review process that considers security concerns.Build a threat model of your system or application that identifies trust boundaries. Use that to model to identify risks and to help find ways to treat those risks.Incorporate advanced security automation, such as fuzzing and security chaos engineering, where it's justified.Distribute lifecycle phaseEnsure the security of the supply chain for container images you execute.Ensure the security of the supply chain for the cluster and other components that execute your application. An example of another component might be an external database that your cloud-native application uses for persistence.To achieve this, you can:Scan container images and other artifacts for known vulnerabilities.Ensure that software distribution uses encryption in transit, with a chain of trust for the software source.Adopt and follow processes to update dependencies when updates are available, especially in response to security announcements.Use validation mechanisms such as digital certificates for supply chain assurance.Subscribe to feeds and other mechanisms to alert you to security risks.Restrict access to artifacts. Place container images in a private registry that only allows authorized clients to pull images.Deploy lifecycle phaseEnsure appropriate restrictions on what can be deployed, who can deploy it, and where it can be deployed to. You can enforce measures from the distribute phase, such as verifying the cryptographic identity of container image artifacts.When you deploy Kubernetes, you also set the foundation for your applications' runtime environment: a Kubernetes cluster (or multiple clusters). That IT infrastructure must provide the security guarantees that higher layers expect.Runtime lifecycle phaseThe Runtime phase comprises three critical areas: access, compute, and storage.Runtime protection: accessThe Kubernetes API is what makes your cluster work. Protecting this API is key to providing effective cluster security.Other pages in the Kubernetes documentation have more detail about how to set up specific aspects of access control. The security checklist has a set of suggested basic checks for your cluster.Beyond that, securing your cluster means implementing effective authentication and authorization for API access. Use ServiceAccounts to provide and manage security identities for workloads and cluster components.Kubernetes uses TLS to protect API traffic; make sure to deploy the cluster using TLS (including for traffic between nodes and the control plane), and protect the encryption keys. If you use Kubernetes' own API for CertificateSigningRequests, pay special attention to restricting misuse there.Runtime protection: computeContainers provide two things: isolation between different applications, and a mechanism to combine those isolated applications to run on the same host computer. Those two aspects, isolation and aggregation, mean that runtime security involves identifying trade-offs and finding an appropriate balance.Kubernetes relies on a container runtime to actually set up and run containers. The Kubernetes project does not recommend a specific container runtime and you should make sure that the runtime(s) that you choose meet your information security needs.To protect your compute at runtime, you can:Enforce Pod security standards for applications, to help ensure they run with only the necessary privileges.Run a specialized operating system on your nodes that is designed specifically for running containerized workloads. This is typically based on a read-only operating system (immutable image) that provides only the services essential for running containers.Container-specific operating systems help to isolate system components and present a reduced attack surface in case of a container escape.Define ResourceQuotas to fairly allocate shared resources, and use mechanisms such as LimitRanges to ensure that Pods specify their resource requirements.Partition workloads across different nodes. Use node isolation mechanisms, either from Kubernetes itself or from the ecosystem, to ensure that Pods with different trust contexts are run on separate sets of nodes.Use a container runtime that provides security restrictions.On Linux nodes, use a Linux security module such as AppArmor or seccomp.Runtime protection: storageTo protect storage for your cluster and the applications that run there, you can:Integrate your cluster with an external storage plugin that provides encryption at rest for volumes.Enable encryption at rest for API objects.Protect data durability using backups. Verify that you can restore these, whenever you need to.Authenticate connections between cluster nodes and any network storage they rely upon.Implement data encryption within your own application.For encryption keys, generating these within specialized hardware provides the best protection against disclosure risks. A hardware security module can let you perform cryptographic operations without allowing the security key to be copied elsewhere.Networking and securityYou should also consider network security measures, such as NetworkPolicy or a service mesh. Some network plugins for Kubernetes provide encryption for your cluster network, using technologies such as a virtual private network (VPN) overlay. By design, Kubernetes lets you use your own networking plugin for your cluster (if you use managed Kubernetes, the person or organization managing your cluster may have chosen a network plugin for you).The network plugin you choose and the way you integrate it can have a strong impact on the security of information in transit.Observability and runtime securityKubernetes lets you extend your cluster with extra tooling. You can set up third party solutions to help you monitor or troubleshoot your applications and the clusters they are running. You also get some basic observability features built in to Kubernetes itself. Your code running in containers can generate logs, publish metrics or provide other observability data; at deploy time, you need to make sure your cluster provides an appropriate level of protection there.If you set up a metrics dashboard or something similar, review the chain of components that populate data into that dashboard, as well as the dashboard itself. Make sure that the whole chain is designed with enough resilience and enough integrity protection that you can rely on it even during an incident where your cluster might be degraded.Where appropriate, deploy security measures below the level of Kubernetes itself, such as cryptographically measured boot, or authenticated distribution of time (which helps ensure the fidelity of logs and audit records).For a high assurance environment, deploy cryptographic protections to ensure that logs are both tamper-proof and confidential.What's nextCloud native securityCNCF white paper on cloud native security.CNCF white paper on good practices for securing a software supply chain.Fixing the Kubernetes clusterf**k: Understanding security from the kernel up (FOSDEM 2020)Kubernetes Security Best Practices (Kubernetes Forum Seoul 2019)Towards Measured Boot Out of the Box (Linux Security Summit 2016)Kubernetes and information securityKubernetes securitySecuring your clusterData encryption in transit for the control planeData encryption at restSecrets in KubernetesControlling Access to the Kubernetes APINetwork policies for PodsPod security standardsRuntimeClasses3.8.2 - Pod Security StandardsA detailed look at the different policy levels defined in the Pod Security Standards.The Pod Security Standards define three different policies to broadly cover the security spectrum. These policies are cumulative and range from highly-permissive to highly-restrictive. This guide outlines the requirements of each policy.ProfileDescriptionPrivilegedUnrestricted policy, providing the widest possible level of permissions. This policy allows for known privilege escalations.BaselineMinimally restrictive policy which prevents known privilege escalations. Allows the default (minimally specified) Pod configuration.RestrictedHeavily restricted policy, following current Pod hardening best practices.Profile DetailsPrivilegedThe Privileged policy is purposely-open, and entirely unrestricted. This type of policy is typically aimed at system- and infrastructure-level workloads managed by privileged, trusted users.The Privileged policy is defined by an absence of restrictions. If you define a Pod where the Privileged security policy applies, the Pod you define is able to bypass typical container isolation mechanisms. For example, you can define a Pod that has access to the node's host network.BaselineThe Baseline policy is aimed at ease of adoption for common containerized workloads while preventing known privilege escalations. This policy is targeted at application operators and developers of non-critical applications. The following listed controls should be enforced/disallowed:Note:In this table, wildcards (*) indicate all elements in a list. For example, spec.containers[*].securityContext refers to the Security Context object for all defined containers. If any of the listed containers fails to meet the requirements, the entire pod will fail validation.Baseline policy specificationControlPolicyHostProcessWindows Pods offer the ability to run HostProcess containers which enables privileged access to the Windows host machine. Privileged access to the host is disallowed in the Baseline policy.FEATURE STATE: Kubernetes v1.26 [stable]Restricted Fieldsspec.securityContext.windowsOptions.hostProcessspec.containers[*].securityContext.windowsOptions.hostProcessspec.initContainers[*].securityContext.windowsOptions.hostProcessspec.ephemeralContainers[*].securityContext.windowsOptions.hostProcessAllowed ValuesUndefined/nilfalseHost NamespacesSharing the host namespaces must be disallowed.Restricted Fieldsspec.hostNetworkspec.hostPIDspec.hostIPCAllowed ValuesUndefined/nilfalsePrivileged ContainersPrivileged Pods disable most security mechanisms and must be disallowed.Restricted Fieldsspec.containers[*].securityContext.privilegedspec.initContainers[*].securityContext.privilegedspec.ephemeralContainers[*].securityContext.privilegedAllowed ValuesUndefined/nilfalseCapabilitiesAdding additional capabilities beyond those listed below must be disallowed.Restricted Fieldsspec.containers[*].securityContext.capabilities.addspec.initContainers[*].securityContext.capabilities.addspec.ephemeralContainers[*].securityContext.capabilities.addAllowed ValuesUndefined/nilAUDIT_WRITECHOWNDAC_OVERRIDEFOWNERFSETIDKILLMKNODNET_BIND_SERVICESETFCAPSETGIDSETPCAPSETUIDSYS_CHROOTHostPath VolumesHostPath volumes must be forbidden.Restricted Fieldsspec.volumes[*].hostPathAllowed ValuesUndefined/nilHost PortsHostPorts should be disallowed entirely (recommended) or restricted to a known listRestricted Fieldsspec.containers[*].ports[*].hostPortspec.initContainers[*].ports[*].hostPortspec.ephemeralContainers[*].ports[*].hostPortAllowed ValuesUndefined/nilKnown list (not supported by the built-in Pod Security Admission controller)0AppArmorOn supported hosts, the RuntimeDefault AppArmor profile is applied by default. The baseline policy should prevent overriding or disabling the default AppArmor profile, or restrict overrides to an allowed set of profiles.Restricted Fieldsspec.securityContext.appArmorProfile.typespec.containers[*].securityContext.appArmorProfile.typespec.initContainers[*].securityContext.appArmorProfile.typespec.ephemeralContainers[*].securityContext.appArmorProfile.typeAllowed ValuesUndefined/nilRuntimeDefaultLocalhostmetadata.annotations["container.apparmor.security.beta.kubernetes.io/*"]Allowed ValuesUndefined/nilruntime/defaultlocalhost/*SELinuxSetting the SELinux type is restricted, and setting a custom SELinux user or role option is forbidden.Restricted Fieldsspec.securityContext.seLinuxOptions.typespec.containers[*].securityContext.seLinuxOptions.typespec.initContainers[*].securityContext.seLinuxOptions.typespec.ephemeralContainers[*].securityContext.seLinuxOptions.typeAllowed ValuesUndefined/""container_tcontainer_init_tcontainer_kvm_tcontainer_engine_t (since Kubernetes 1.31)Restricted Fieldsspec.securityContext.seLinuxOptions.userspec.containers[*].securityContext.seLinuxOptions.userspec.initContainers[*].securityContext.seLinuxOptions.userspec.ephemeralContainers[*].securityContext.seLinuxOptions.userspec.securityContext.seLinuxOptions.rolespec.containers[*].securityContext.seLinuxOptions.rolespec.initContainers[*].securityContext.seLinuxOptions.rolespec.ephemeralContainers[*].securityContext.seLinuxOptions.roleAllowed ValuesUndefined/""/proc Mount TypeThe default /proc masks are set up to reduce attack surface, and should be required.Restricted Fieldsspec.containers[*].securityContext.procMountspec.initContainers[*].securityContext.procMountspec.ephemeralContainers[*].securityContext.procMountAllowed ValuesUndefined/nilDefaultSeccompSeccomp profile must not be explicitly set to Unconfined.Restricted Fieldsspec.securityContext.seccompProfile.typespec.containers[*].securityContext.seccompProfile.typespec.initContainers[*].securityContext.seccompProfile.typespec.ephemeralContainers[*].securityContext.seccompProfile.typeAllowed ValuesUndefined/nilRuntimeDefaultLocalhostSysctlsSysctls can disable security mechanisms or affect all containers on a host, and should be disallowed except for an allowed "safe" subset. A sysctl is considered safe if it is namespaced in the container or the Pod, and it is isolated from other Pods or processes on the same Node.Restricted Fieldsspec.securityContext.sysctls[*].nameAllowed ValuesUndefined/nilkernel.shm_rmid_forcednet.ipv4.ip_local_port_rangenet.ipv4.ip_unprivileged_port_startnet.ipv4.tcp_syncookiesnet.ipv4.ping_group_rangenet.ipv4.ip_local_reserved_ports (since Kubernetes 1.27)net.ipv4.tcp_keepalive_time (since Kubernetes 1.29)net.ipv4.tcp_fin_timeout (since Kubernetes 1.29)net.ipv4.tcp_keepalive_intvl (since Kubernetes 1.29)net.ipv4.tcp_keepalive_probes (since Kubernetes 1.29)RestrictedThe Restricted policy is aimed at enforcing current Pod hardening best practices, at the expense of some compatibility. It is targeted at operators and developers of security-critical applications, as well as lower-trust users. The following listed controls should be enforced/disallowed:Note:In this table, wildcards (*) indicate all elements in a list. For example, spec.containers[*].securityContext refers to the Security Context object for all defined containers. If any of the listed containers fails to meet the requirements, the entire pod will fail validation.Restricted policy specificationControlPolicyEverything from the Baseline policyVolume TypesThe Restricted policy only permits the following volume types.Restricted Fieldsspec.volumes[*]Allowed ValuesEvery item in the spec.volumes[*] list must set one of the following fields to a non-null value:spec.volumes[*].configMapspec.volumes[*].csispec.volumes[*].downwardAPIspec.volumes[*].emptyDirspec.volumes[*].ephemeralspec.volumes[*].persistentVolumeClaimspec.volumes[*].projectedspec.volumes[*].secretPrivilege Escalation (v1.8+)Privilege escalation (such as via set-user-ID or set-group-ID file mode) should not be allowed. This is Linux only policy in v1.25+ (spec.os.name != windows)Restricted Fieldsspec.containers[*].securityContext.allowPrivilegeEscalationspec.initContainers[*].securityContext.allowPrivilegeEscalationspec.ephemeralContainers[*].securityContext.allowPrivilegeEscalationAllowed ValuesfalseRunning as Non-rootContainers must be required to run as non-root users.Restricted Fieldsspec.securityContext.runAsNonRootspec.containers[*].securityContext.runAsNonRootspec.initContainers[*].securityContext.runAsNonRootspec.ephemeralContainers[*].securityContext.runAsNonRootAllowed ValuestrueThe container fields may be undefined/nil if the pod-level spec.securityContext.runAsNonRoot is set to true.Running as Non-root user (v1.23+)Containers must not set runAsUser to 0Restricted Fieldsspec.securityContext.runAsUserspec.containers[*].securityContext.runAsUserspec.initContainers[*].securityContext.runAsUserspec.ephemeralContainers[*].securityContext.runAsUserAllowed Valuesany non-zero valueundefined/nullSeccomp (v1.19+)Seccomp profile must be explicitly set to one of the allowed values. Both the Unconfined profile and the absence of a profile are prohibited. This is Linux only policy in v1.25+ (spec.os.name != windows)Restricted Fieldsspec.securityContext.seccompProfile.typespec.containers[*].securityContext.seccompProfile.typespec.initContainers[*].securityContext.seccompProfile.typespec.ephemeralContainers[*].securityContext.seccompProfile.typeAllowed ValuesRuntimeDefaultLocalhostThe container fields may be undefined/nil if the pod-level spec.securityContext.seccompProfile.type field is set appropriately. Conversely, the pod-level field may be undefined/nil if _all_ container- level fields are set.Capabilities (v1.22+)Containers must drop ALL capabilities, and are only permitted to add back the NET_BIND_SERVICE capability. This is Linux only policy in v1.25+ (.spec.os.name != "windows")Restricted Fieldsspec.containers[*].securityContext.capabilities.dropspec.initContainers[*].securityContext.capabilities.dropspec.ephemeralContainers[*].securityContext.capabilities.dropAllowed ValuesAny list of capabilities that includes ALLRestricted Fieldsspec.containers[*].securityContext.capabilities.addspec.initContainers[*].securityContext.capabilities.addspec.ephemeralContainers[*].securityContext.capabilities.addAllowed ValuesUndefined/nilNET_BIND_SERVICEPolicy InstantiationDecoupling policy definition from policy instantiation allows for a common understanding and consistent language of policies across clusters, independent of the underlying enforcement mechanism.As mechanisms mature, they will be defined below on a per-policy basis. The methods of enforcement of individual policies are not defined here.Pod Security Admission ControllerPrivileged namespaceBaseline namespaceRestricted namespaceAlternativesNote: This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the content guide before submitting a change. More information.Other alternatives for enforcing policies are being developed in the Kubernetes ecosystem, such as:KubewardenKyvernoOPA GatekeeperPod OS fieldKubernetes lets you use nodes that run either Linux or Windows. You can mix both kinds of node in one cluster. Windows in Kubernetes has some limitations and differentiators from Linux-based workloads. Specifically, many of the Pod securityContext fields have no effect on Windows.Note:Kubelets prior to v1.24 don't enforce the pod OS field, and if a cluster has nodes on versions earlier than v1.24 the Restricted policies should be pinned to a version prior to v1.25.Restricted Pod Security Standard changesAnother important change, made in Kubernetes v1.25 is that the Restricted policy has been updated to use the pod.spec.os.name field. Based on the OS name, certain policies that are specific to a particular OS can be relaxed for the other OS.OS-specific policy controlsRestrictions on the following controls are only required if .spec.os.name is not windows:Privilege EscalationSeccompLinux CapabilitiesUser namespacesUser Namespaces are a Linux-only feature to run workloads with increased isolation. How they work together with Pod Security Standards is described in the documentation for Pods that use user namespaces.FAQWhy isn't there a profile between Privileged and Baseline?The three profiles defined here have a clear linear progression from most secure (Restricted) to least secure (Privileged), and cover a broad set of workloads. Privileges required above the Baseline policy are typically very application specific, so we do not offer a standard profile in this niche. This is not to say that the privileged profile should always be used in this case, but that policies in this space need to be defined on a case-by-case basis.SIG Auth may reconsider this position in the future, should a clear need for other profiles arise.What's the difference between a security profile and a security context?Security Contexts configure Pods and Containers at runtime. Security contexts are defined as part of the Pod and container specifications in the Pod manifest, and represent parameters to the container runtime.Security profiles are control plane mechanisms to enforce specific settings in the Security Context, as well as other related parameters outside the Security Context. As of July 2021, Pod Security Policies are deprecated in favor of the built-in Pod Security Admission Controller.What about sandboxed Pods?There is currently no API standard that controls whether a Pod is considered sandboxed or not. Sandbox Pods may be identified by the use of a sandboxed runtime (such as gVisor or Kata Containers), but there is no standard definition of what a sandboxed runtime is.The protections necessary for sandboxed workloads can differ from others. For example, the need to restrict privileged permissions is lessened when the workload is isolated from the underlying kernel. This allows for workloads requiring heightened permissions to still be isolated.Additionally, the protection of sandboxed workloads is highly dependent on the method of sandboxing. As such, no single recommended profile is recommended for all sandboxed workloads.3.8.3 - Pod Security AdmissionAn overview of the Pod Security Admission Controller, which can enforce the Pod Security Standards.FEATURE STATE: Kubernetes v1.25 [stable]The Kubernetes Pod Security Standards define different isolation levels for Pods. These standards let you define how you want to restrict the behavior of pods in a clear, consistent fashion.Kubernetes offers a built-in Pod Security admission controller to enforce the Pod Security Standards. Pod security restrictions are applied at the namespace level when pods are created.Built-in Pod Security admission enforcementThis page is part of the documentation for Kubernetes v1.33. If you are running a different version of Kubernetes, consult the documentation for that release.Pod Security levelsPod Security admission places requirements on a Pod's Security Context and other related fields according to the three levels defined by the Pod Security Standards: privileged, baseline, and restricted. Refer to the Pod Security Standards page for an in-depth look at those requirements.Pod Security Admission labels for namespacesOnce the feature is enabled or the webhook is installed, you can configure namespaces to define the admission control mode you want to use for pod security in each namespace. Kubernetes defines a set of labels that you can set to define which of the predefined Pod Security Standard levels you want to use for a namespace. The label you select defines what action the control plane takes if a potential violation is detected:Pod Security Admission modesModeDescriptionenforcePolicy violations will cause the pod to be rejected.auditPolicy violations will trigger the addition of an audit annotation to the event recorded in the audit log, but are otherwise allowed.warnPolicy violations will trigger a user-facing warning, but are otherwise allowed.A namespace can configure any or all modes, or even set a different level for different modes.For each mode, there are two labels that determine the policy used:# The per-mode level label indicates which policy level to apply for the mode. # # MODE must be one of `enforce`, `audit`, or `warn`. # LEVEL must be one of `privileged`, `baseline`, or `restricted`. pod-security.kubernetes.io/:  # Optional: per-mode version label that can be used to pin the policy to the # version that shipped with a given Kubernetes minor version (for example v1.33). # # MODE must be one of `enforce`, `audit`, or `warn`. # VERSION must be a valid Kubernetes minor version, or `latest`. pod-security.kubernetes.io/-version:  Check out Enforce Pod Security Standards with Namespace Labels to see example usage.Workload resources and Pod templatesPods are often created indirectly, by creating a workload object such as a Deployment or Job. The workload object defines a Pod template and a controller for the workload resource creates Pods based on that template. To help catch violations early, both the audit and warning modes are applied to the workload resources. However, enforce mode is not applied to workload resources, only to the resulting pod objects.ExemptionsYou can define exemptions from pod security enforcement in order to allow the creation of pods that would have otherwise been prohibited due to the policy associated with a given namespace. Exemptions can be statically configured in the Admission Controller configuration.Exemptions must be explicitly enumerated. Requests meeting exemption criteria are ignored by the Admission Controller (all enforce, audit and warn behaviors are skipped). Exemption dimensions include:Usernames: requests from users with an exempt authenticated (or impersonated) username are ignored.RuntimeClassNames: pods and workload resources specifying an exempt runtime class name are ignored.Namespaces: pods and workload resources in an exempt namespace are ignored.Caution:Most pods are created by a controller in response to a workload resource, meaning that exempting an end user will only exempt them from enforcement when creating pods directly, but not when creating a workload resource. Controller service accounts (such as system:serviceaccount:kube-system:replicaset-controller) should generally not be exempted, as doing so would implicitly exempt any user that can create the corresponding workload resource.Updates to the following pod fields are exempt from policy checks, meaning that if a pod update request only changes these fields, it will not be denied even if the pod is in violation of the current policy level:Any metadata updates except changes to the seccomp or AppArmor annotations:seccomp.security.alpha.kubernetes.io/pod (deprecated)container.seccomp.security.alpha.kubernetes.io/* (deprecated)container.apparmor.security.beta.kubernetes.io/* (deprecated)Valid updates to .spec.activeDeadlineSecondsValid updates to .spec.tolerationsMetricsHere are the Prometheus metrics exposed by kube-apiserver:pod_security_errors_total: This metric indicates the number of errors preventing normal evaluation. Non-fatal errors may result in the latest restricted profile being used for enforcement.pod_security_evaluations_total: This metric indicates the number of policy evaluations that have occurred, not counting ignored or exempt requests during exporting.pod_security_exemptions_total: This metric indicates the number of exempt requests, not counting ignored or out of scope requests.What's nextPod Security StandardsEnforcing Pod Security StandardsEnforce Pod Security Standards by Configuring the Built-in Admission ControllerEnforce Pod Security Standards with Namespace LabelsIf you are running an older version of Kubernetes and want to upgrade to a version of Kubernetes that does not include PodSecurityPolicies, read migrate from PodSecurityPolicy to the Built-In PodSecurity Admission Controller.3.8.4 - Service AccountsLearn about ServiceAccount objects in Kubernetes.This page introduces the ServiceAccount object in Kubernetes, providing information about how service accounts work, use cases, limitations, alternatives, and links to resources for additional guidance.What are service accounts?A service account is a type of non-human account that, in Kubernetes, provides a distinct identity in a Kubernetes cluster. Application Pods, system components, and entities inside and outside the cluster can use a specific ServiceAccount's credentials to identify as that ServiceAccount. This identity is useful in various situations, including authenticating to the API server or implementing identity-based security policies.Service accounts exist as ServiceAccount objects in the API server. Service accounts have the following properties:Namespaced: Each service account is bound to a Kubernetes namespace. Every namespace gets a default ServiceAccount upon creation.Lightweight: Service accounts exist in the cluster and are defined in the Kubernetes API. You can quickly create service accounts to enable specific tasks.Portable: A configuration bundle for a complex containerized workload might include service account definitions for the system's components. The lightweight nature of service accounts and the namespaced identities make the configurations portable.Service accounts are different from user accounts, which are authenticated human users in the cluster. By default, user accounts don't exist in the Kubernetes API server; instead, the API server treats user identities as opaque data. You can authenticate as a user account using multiple methods. Some Kubernetes distributions might add custom extension APIs to represent user accounts in the API server.Comparison between service accounts and usersDescriptionServiceAccountUser or groupLocationKubernetes API (ServiceAccount object)ExternalAccess controlKubernetes RBAC or other authorization mechanismsKubernetes RBAC or other identity and access management mechanismsIntended useWorkloads, automationPeopleDefault service accountsWhen you create a cluster, Kubernetes automatically creates a ServiceAccount object named default for every namespace in your cluster. The default service accounts in each namespace get no permissions by default other than the default API discovery permissions that Kubernetes grants to all authenticated principals if role-based access control (RBAC) is enabled. If you delete the default ServiceAccount object in a namespace, the control plane replaces it with a new one.If you deploy a Pod in a namespace, and you don't manually assign a ServiceAccount to the Pod, Kubernetes assigns the default ServiceAccount for that namespace to the Pod.Use cases for Kubernetes service accountsAs a general guideline, you can use service accounts to provide identities in the following scenarios:Your Pods need to communicate with the Kubernetes API server, for example in situations such as the following:Providing read-only access to sensitive information stored in Secrets.Granting cross-namespace access, such as allowing a Pod in namespace example to read, list, and watch for Lease objects in the kube-node-lease namespace.Your Pods need to communicate with an external service. For example, a workload Pod requires an identity for a commercially available cloud API, and the commercial provider allows configuring a suitable trust relationship.Authenticating to a private image registry using an imagePullSecret.An external service needs to communicate with the Kubernetes API server. For example, authenticating to the cluster as part of a CI/CD pipeline.You use third-party security software in your cluster that relies on the ServiceAccount identity of different Pods to group those Pods into different contexts.How to use service accountsTo use a Kubernetes service account, you do the following:Create a ServiceAccount object using a Kubernetes client like kubectl or a manifest that defines the object.Grant permissions to the ServiceAccount object using an authorization mechanism such as RBAC.Assign the ServiceAccount object to Pods during Pod creation.If you're using the identity from an external service, retrieve the ServiceAccount token and use it from that service instead.For instructions, refer to Configure Service Accounts for Pods.Grant permissions to a ServiceAccountYou can use the built-in Kubernetes role-based access control (RBAC) mechanism to grant the minimum permissions required by each service account. You create a role, which grants access, and then bind the role to your ServiceAccount. RBAC lets you define a minimum set of permissions so that the service account permissions follow the principle of least privilege. Pods that use that service account don't get more permissions than are required to function correctly.For instructions, refer to ServiceAccount permissions.Cross-namespace access using a ServiceAccountYou can use RBAC to allow service accounts in one namespace to perform actions on resources in a different namespace in the cluster. For example, consider a scenario where you have a service account and Pod in the dev namespace and you want your Pod to see Jobs running in the maintenance namespace. You could create a Role object that grants permissions to list Job objects. Then, you'd create a RoleBinding object in the maintenance namespace to bind the Role to the ServiceAccount object. Now, Pods in the dev namespace can list Job objects in the maintenance namespace using that service account.Assign a ServiceAccount to a PodTo assign a ServiceAccount to a Pod, you set the spec.serviceAccountName field in the Pod specification. Kubernetes then automatically provides the credentials for that ServiceAccount to the Pod. In v1.22 and later, Kubernetes gets a short-lived, automatically rotating token using the TokenRequest API and mounts the token as a projected volume.By default, Kubernetes provides the Pod with the credentials for an assigned ServiceAccount, whether that is the default ServiceAccount or a custom ServiceAccount that you specify.To prevent Kubernetes from automatically injecting credentials for a specified ServiceAccount or the default ServiceAccount, set the automountServiceAccountToken field in your Pod specification to false.In versions earlier than 1.22, Kubernetes provides a long-lived, static token to the Pod as a Secret.Manually retrieve ServiceAccount credentialsIf you need the credentials for a ServiceAccount to mount in a non-standard location, or for an audience that isn't the API server, use one of the following methods:TokenRequest API (recommended): Request a short-lived service account token from within your own application code. The token expires automatically and can rotate upon expiration. If you have a legacy application that is not aware of Kubernetes, you could use a sidecar container within the same pod to fetch these tokens and make them available to the application workload.Token Volume Projection (also recommended): In Kubernetes v1.20 and later, use the Pod specification to tell the kubelet to add the service account token to the Pod as a projected volume. Projected tokens expire automatically, and the kubelet rotates the token before it expires.Service Account Token Secrets (not recommended): You can mount service account tokens as Kubernetes Secrets in Pods. These tokens don't expire and don't rotate. In versions prior to v1.24, a permanent token was automatically created for each service account. This method is not recommended anymore, especially at scale, because of the risks associated with static, long-lived credentials. The LegacyServiceAccountTokenNoAutoGeneration feature gate (which was enabled by default from Kubernetes v1.24 to v1.26), prevented Kubernetes from automatically creating these tokens for ServiceAccounts. The feature gate is removed in v1.27, because it was elevated to GA status; you can still create indefinite service account tokens manually, but should take into account the security implications.Note:For applications running outside your Kubernetes cluster, you might be considering creating a long-lived ServiceAccount token that is stored in a Secret. This allows authentication, but the Kubernetes project recommends you avoid this approach. Long-lived bearer tokens represent a security risk as, once disclosed, the token can be misused. Instead, consider using an alternative. For example, your external application can authenticate using a well-protected private key and a certificate, or using a custom mechanism such as an authentication webhook that you implement yourself.You can also use TokenRequest to obtain short-lived tokens for your external application.Restricting access to Secrets (deprecated)FEATURE STATE: Kubernetes v1.32 [deprecated]Note:kubernetes.io/enforce-mountable-secrets is deprecated since Kubernetes v1.32. Use separate namespaces to isolate access to mounted secrets.Kubernetes provides an annotation called kubernetes.io/enforce-mountable-secrets that you can add to your ServiceAccounts. When this annotation is applied, the ServiceAccount's secrets can only be mounted on specified types of resources, enhancing the security posture of your cluster.You can add the annotation to a ServiceAccount using a manifest:apiVersion: v1 kind: ServiceAccount metadata: annotations: kubernetes.io/enforce-mountable-secrets: "true" name: my-serviceaccount namespace: my-namespace When this annotation is set to "true", the Kubernetes control plane ensures that the Secrets from this ServiceAccount are subject to certain mounting restrictions.The name of each Secret that is mounted as a volume in a Pod must appear in the secrets field of the Pod's ServiceAccount.The name of each Secret referenced using envFrom in a Pod must also appear in the secrets field of the Pod's ServiceAccount.The name of each Secret referenced using imagePullSecrets in a Pod must also appear in the secrets field of the Pod's ServiceAccount.By understanding and enforcing these restrictions, cluster administrators can maintain a tighter security profile and ensure that secrets are accessed only by the appropriate resources.Authenticating service account credentialsServiceAccounts use signed JSON Web Tokens (JWTs) to authenticate to the Kubernetes API server, and to any other system where a trust relationship exists. Depending on how the token was issued (either time-limited using a TokenRequest or using a legacy mechanism with a Secret), a ServiceAccount token might also have an expiry time, an audience, and a time after which the token starts being valid. When a client that is acting as a ServiceAccount tries to communicate with the Kubernetes API server, the client includes an Authorization: Bearer  header with the HTTP request. The API server checks the validity of that bearer token as follows:Checks the token signature.Checks whether the token has expired.Checks whether object references in the token claims are currently valid.Checks whether the token is currently valid.Checks the audience claims.The TokenRequest API produces bound tokens for a ServiceAccount. This binding is linked to the lifetime of the client, such as a Pod, that is acting as that ServiceAccount. See Token Volume Projection for an example of a bound pod service account token's JWT schema and payload.For tokens issued using the TokenRequest API, the API server also checks that the specific object reference that is using the ServiceAccount still exists, matching by the unique ID of that object. For legacy tokens that are mounted as Secrets in Pods, the API server checks the token against the Secret.For more information about the authentication process, refer to Authentication.Authenticating service account credentials in your own codeIf you have services of your own that need to validate Kubernetes service account credentials, you can use the following methods:TokenReview API (recommended)OIDC discoveryThe Kubernetes project recommends that you use the TokenReview API, because this method invalidates tokens that are bound to API objects such as Secrets, ServiceAccounts, Pods or Nodes when those objects are deleted. For example, if you delete the Pod that contains a projected ServiceAccount token, the cluster invalidates that token immediately and a TokenReview immediately fails. If you use OIDC validation instead, your clients continue to treat the token as valid until the token reaches its expiration timestamp.Your application should always define the audience that it accepts, and should check that the token's audiences match the audiences that the application expects. This helps to minimize the scope of the token so that it can only be used in your application and nowhere else.AlternativesIssue your own tokens using another mechanism, and then use Webhook Token Authentication to validate bearer tokens using your own validation service.Provide your own identities to Pods.Use the SPIFFE CSI driver plugin to provide SPIFFE SVIDs as X.509 certificate pairs to Pods.🛇 This item links to a third party project or product that is not part of Kubernetes itself. More informationUse a service mesh such as Istio to provide certificates to Pods.Authenticate from outside the cluster to the API server without using service account tokens:Configure the API server to accept OpenID Connect (OIDC) tokens from your identity provider.Use service accounts or user accounts created using an external Identity and Access Management (IAM) service, such as from a cloud provider, to authenticate to your cluster.Use the CertificateSigningRequest API with client certificates.Configure the kubelet to retrieve credentials from an image registry.Use a Device Plugin to access a virtual Trusted Platform Module (TPM), which then allows authentication using a private key.What's nextLearn how to manage your ServiceAccounts as a cluster administrator.Learn how to assign a ServiceAccount to a Pod.Read the ServiceAccount API reference.3.8.5 - Pod Security PoliciesRemoved featurePodSecurityPolicy was deprecated in Kubernetes v1.21, and removed from Kubernetes in v1.25.Instead of using PodSecurityPolicy, you can enforce similar restrictions on Pods using either or both:Pod Security Admissiona 3rd party admission plugin, that you deploy and configure yourselfFor a migration guide, see Migrate from PodSecurityPolicy to the Built-In PodSecurity Admission Controller. For more information on the removal of this API, see PodSecurityPolicy Deprecation: Past, Present, and Future.If you are not running Kubernetes v1.33, check the documentation for your version of Kubernetes.3.8.6 - Security For Windows NodesThis page describes security considerations and best practices specific to the Windows operating system.Protection for Secret data on nodesOn Windows, data from Secrets are written out in clear text onto the node's local storage (as compared to using tmpfs / in-memory filesystems on Linux). As a cluster operator, you should take both of the following additional measures:Use file ACLs to secure the Secrets' file location.Apply volume-level encryption using BitLocker.Container usersRunAsUsername can be specified for Windows Pods or containers to execute the container processes as specific user. This is roughly equivalent to RunAsUser.Windows containers offer two default user accounts, ContainerUser and ContainerAdministrator. The differences between these two user accounts are covered in When to use ContainerAdmin and ContainerUser user accounts within Microsoft's Secure Windows containers documentation.Local users can be added to container images during the container build process.Note:Nano Server based images run as ContainerUser by defaultServer Core based images run as ContainerAdministrator by defaultWindows containers can also run as Active Directory identities by utilizing Group Managed Service AccountsPod-level security isolationLinux-specific pod security context mechanisms (such as SELinux, AppArmor, Seccomp, or custom POSIX capabilities) are not supported on Windows nodes.Privileged containers are not supported on Windows. Instead HostProcess containers can be used on Windows to perform many of the tasks performed by privileged containers on Linux.3.8.7 - Controlling Access to the Kubernetes APIThis page provides an overview of controlling access to the Kubernetes API.Users access the Kubernetes API using kubectl, client libraries, or by making REST requests. Both human users and Kubernetes service accounts can be authorized for API access. When a request reaches the API, it goes through several stages, illustrated in the following diagram:Transport securityBy default, the Kubernetes API server listens on port 6443 on the first non-localhost network interface, protected by TLS. In a typical production Kubernetes cluster, the API serves on port 443. The port can be changed with the --secure-port, and the listening IP address with the --bind-address flag.The API server presents a certificate. This certificate may be signed using a private certificate authority (CA), or based on a public key infrastructure linked to a generally recognized CA. The certificate and corresponding private key can be set by using the --tls-cert-file and --tls-private-key-file flags.If your cluster uses a private certificate authority, you need a copy of that CA certificate configured into your ~/.kube/config on the client, so that you can trust the connection and be confident it was not intercepted.Your client can present a TLS client certificate at this stage.AuthenticationOnce TLS is established, the HTTP request moves to the Authentication step. This is shown as step 1 in the diagram. The cluster creation script or cluster admin configures the API server to run one or more Authenticator modules. Authenticators are described in more detail in Authentication.The input to the authentication step is the entire HTTP request; however, it typically examines the headers and/or client certificate.Authentication modules include client certificates, password, and plain tokens, bootstrap tokens, and JSON Web Tokens (used for service accounts).Multiple authentication modules can be specified, in which case each one is tried in sequence, until one of them succeeds.If the request cannot be authenticated, it is rejected with HTTP status code 401. Otherwise, the user is authenticated as a specific username, and the user name is available to subsequent steps to use in their decisions. Some authenticators also provide the group memberships of the user, while other authenticators do not.While Kubernetes uses usernames for access control decisions and in request logging, it does not have a User object nor does it store usernames or other information about users in its API.AuthorizationAfter the request is authenticated as coming from a specific user, the request must be authorized. This is shown as step 2 in the diagram.A request must include the username of the requester, the requested action, and the object affected by the action. The request is authorized if an existing policy declares that the user has permissions to complete the requested action.For example, if Bob has the policy below, then he can read pods only in the namespace projectCaribou:{ "apiVersion": "abac.authorization.kubernetes.io/v1beta1", "kind": "Policy", "spec": { "user": "bob", "namespace": "projectCaribou", "resource": "pods", "readonly": true } } If Bob makes the following request, the request is authorized because he is allowed to read objects in the projectCaribou namespace:{ "apiVersion": "authorization.k8s.io/v1beta1", "kind": "SubjectAccessReview", "spec": { "resourceAttributes": { "namespace": "projectCaribou", "verb": "get", "group": "unicorn.example.org", "resource": "pods" } } } If Bob makes a request to write (create or update) to the objects in the projectCaribou namespace, his authorization is denied. If Bob makes a request to read (get) objects in a different namespace such as projectFish, then his authorization is denied.Kubernetes authorization requires that you use common REST attributes to interact with existing organization-wide or cloud-provider-wide access control systems. It is important to use REST formatting because these control systems might interact with other APIs besides the Kubernetes API.Kubernetes supports multiple authorization modules, such as ABAC mode, RBAC Mode, and Webhook mode. When an administrator creates a cluster, they configure the authorization modules that should be used in the API server. If more than one authorization modules are configured, Kubernetes checks each module, and if any module authorizes the request, then the request can proceed. If all of the modules deny the request, then the request is denied (HTTP status code 403).To learn more about Kubernetes authorization, including details about creating policies using the supported authorization modules, see Authorization.Admission controlAdmission Control modules are software modules that can modify or reject requests. In addition to the attributes available to Authorization modules, Admission Control modules can access the contents of the object that is being created or modified.Admission controllers act on requests that create, modify, delete, or connect to (proxy) an object. Admission controllers do not act on requests that merely read objects. When multiple admission controllers are configured, they are called in order.This is shown as step 3 in the diagram.Unlike Authentication and Authorization modules, if any admission controller module rejects, then the request is immediately rejected.In addition to rejecting objects, admission controllers can also set complex defaults for fields.The available Admission Control modules are described in Admission Controllers.Once a request passes all admission controllers, it is validated using the validation routines for the corresponding API object, and then written to the object store (shown as step 4).AuditingKubernetes auditing provides a security-relevant, chronological set of records documenting the sequence of actions in a cluster. The cluster audits the activities generated by users, by applications that use the Kubernetes API, and by the control plane itself.For more information, see Auditing.What's nextRead more documentation on authentication, authorization and API access control:AuthenticatingAuthenticating with Bootstrap TokensAdmission ControllersDynamic Admission ControlAuthorizationRole Based Access ControlAttribute Based Access ControlNode AuthorizationWebhook AuthorizationCertificate Signing Requestsincluding CSR approval and certificate signingService accountsDeveloper guideAdministrationYou can learn about:how Pods can use Secrets to obtain API credentials.3.8.8 - Role Based Access Control Good PracticesPrinciples and practices for good RBAC design for cluster operators.Kubernetes RBAC is a key security control to ensure that cluster users and workloads have only the access to resources required to execute their roles. It is important to ensure that, when designing permissions for cluster users, the cluster administrator understands the areas where privilege escalation could occur, to reduce the risk of excessive access leading to security incidents.The good practices laid out here should be read in conjunction with the general RBAC documentation.General good practiceLeast privilegeIdeally, minimal RBAC rights should be assigned to users and service accounts. Only permissions explicitly required for their operation should be used. While each cluster will be different, some general rules that can be applied are :Assign permissions at the namespace level where possible. Use RoleBindings as opposed to ClusterRoleBindings to give users rights only within a specific namespace.Avoid providing wildcard permissions when possible, especially to all resources. As Kubernetes is an extensible system, providing wildcard access gives rights not just to all object types that currently exist in the cluster, but also to all object types which are created in the future.Administrators should not use cluster-admin accounts except where specifically needed. Providing a low privileged account with impersonation rights can avoid accidental modification of cluster resources.Avoid adding users to the system:masters group. Any user who is a member of this group bypasses all RBAC rights checks and will always have unrestricted superuser access, which cannot be revoked by removing RoleBindings or ClusterRoleBindings. As an aside, if a cluster is using an authorization webhook, membership of this group also bypasses that webhook (requests from users who are members of that group are never sent to the webhook)Minimize distribution of privileged tokensIdeally, pods shouldn't be assigned service accounts that have been granted powerful permissions (for example, any of the rights listed under privilege escalation risks). In cases where a workload requires powerful permissions, consider the following practices:Limit the number of nodes running powerful pods. Ensure that any DaemonSets you run are necessary and are run with least privilege to limit the blast radius of container escapes.Avoid running powerful pods alongside untrusted or publicly-exposed ones. Consider using Taints and Toleration, NodeAffinity, or PodAntiAffinity to ensure pods don't run alongside untrusted or less-trusted Pods. Pay special attention to situations where less-trustworthy Pods are not meeting the Restricted Pod Security Standard.HardeningKubernetes defaults to providing access which may not be required in every cluster. Reviewing the RBAC rights provided by default can provide opportunities for security hardening. In general, changes should not be made to rights provided to system: accounts some options to harden cluster rights exist:Review bindings for the system:unauthenticated group and remove them where possible, as this gives access to anyone who can contact the API server at a network level.Avoid the default auto-mounting of service account tokens by setting automountServiceAccountToken: false. For more details, see using default service account token. Setting this value for a Pod will overwrite the service account setting, workloads which require service account tokens can still mount them.Periodic reviewIt is vital to periodically review the Kubernetes RBAC settings for redundant entries and possible privilege escalations. If an attacker is able to create a user account with the same name as a deleted user, they can automatically inherit all the rights of the deleted user, especially the rights assigned to that user.Kubernetes RBAC - privilege escalation risksWithin Kubernetes RBAC there are a number of privileges which, if granted, can allow a user or a service account to escalate their privileges in the cluster or affect systems outside the cluster.This section is intended to provide visibility of the areas where cluster operators should take care, to ensure that they do not inadvertently allow for more access to clusters than intended.Listing secretsIt is generally clear that allowing get access on Secrets will allow a user to read their contents. It is also important to note that list and watch access also effectively allow for users to reveal the Secret contents. For example, when a List response is returned (for example, via kubectl get secrets -A -o yaml), the response includes the contents of all Secrets.Workload creationPermission to create workloads (either Pods, or workload resources that manage Pods) in a namespace implicitly grants access to many other resources in that namespace, such as Secrets, ConfigMaps, and PersistentVolumes that can be mounted in Pods. Additionally, since Pods can run as any ServiceAccount, granting permission to create workloads also implicitly grants the API access levels of any service account in that namespace.Users who can run privileged Pods can use that access to gain node access and potentially to further elevate their privileges. Where you do not fully trust a user or other principal with the ability to create suitably secure and isolated Pods, you should enforce either the Baseline or Restricted Pod Security Standard. You can use Pod Security admission or other (third party) mechanisms to implement that enforcement.For these reasons, namespaces should be used to separate resources requiring different levels of trust or tenancy. It is still considered best practice to follow least privilege principles and assign the minimum set of permissions, but boundaries within a namespace should be considered weak.Persistent volume creationIf someone - or some application - is allowed to create arbitrary PersistentVolumes, that access includes the creation of hostPath volumes, which then means that a Pod would get access to the underlying host filesystem(s) on the associated node. Granting that ability is a security risk.There are many ways a container with unrestricted access to the host filesystem can escalate privileges, including reading data from other containers, and abusing the credentials of system services, such as Kubelet.You should only allow access to create PersistentVolume objects for:Users (cluster operators) that need this access for their work, and who you trust.The Kubernetes control plane components which creates PersistentVolumes based on PersistentVolumeClaims that are configured for automatic provisioning. This is usually setup by the Kubernetes provider or by the operator when installing a CSI driver.Where access to persistent storage is required trusted administrators should create PersistentVolumes, and constrained users should use PersistentVolumeClaims to access that storage.Access to proxy subresource of NodesUsers with access to the proxy sub-resource of node objects have rights to the Kubelet API, which allows for command execution on every pod on the node(s) to which they have rights. This access bypasses audit logging and admission control, so care should be taken before granting rights to this resource.Escalate verbGenerally, the RBAC system prevents users from creating clusterroles with more rights than the user possesses. The exception to this is the escalate verb. As noted in the RBAC documentation, users with this right can effectively escalate their privileges.Bind verbSimilar to the escalate verb, granting users this right allows for the bypass of Kubernetes in-built protections against privilege escalation, allowing users to create bindings to roles with rights they do not already have.Impersonate verbThis verb allows users to impersonate and gain the rights of other users in the cluster. Care should be taken when granting it, to ensure that excessive permissions cannot be gained via one of the impersonated accounts.CSRs and certificate issuingThe CSR API allows for users with create rights to CSRs and update rights on certificatesigningrequests/approval where the signer is kubernetes.io/kube-apiserver-client to create new client certificates which allow users to authenticate to the cluster. Those client certificates can have arbitrary names including duplicates of Kubernetes system components. This will effectively allow for privilege escalation.Token requestUsers with create rights on serviceaccounts/token can create TokenRequests to issue tokens for existing service accounts.Control admission webhooksUsers with control over validatingwebhookconfigurations or mutatingwebhookconfigurations can control webhooks that can read any object admitted to the cluster, and in the case of mutating webhooks, also mutate admitted objects.Namespace modificationUsers who can perform patch operations on Namespace objects (through a namespaced RoleBinding to a Role with that access) can modify labels on that namespace. In clusters where Pod Security Admission is used, this may allow a user to configure the namespace for a more permissive policy than intended by the administrators. For clusters where NetworkPolicy is used, users may be set labels that indirectly allow access to services that an administrator did not intend to allow.Kubernetes RBAC - denial of service risksObject creation denial-of-serviceUsers who have rights to create objects in a cluster may be able to create sufficient large objects to create a denial of service condition either based on the size or number of objects, as discussed in etcd used by Kubernetes is vulnerable to OOM attack. This may be specifically relevant in multi-tenant clusters if semi-trusted or untrusted users are allowed limited access to a system.One option for mitigation of this issue would be to use resource quotas to limit the quantity of objects which can be created.What's nextTo learn more about RBAC, see the RBAC documentation.3.8.9 - Good practices for Kubernetes SecretsPrinciples and practices for good Secret management for cluster administrators and application developers.In Kubernetes, a Secret is an object that stores sensitive information, such as passwords, OAuth tokens, and SSH keys.Secrets give you more control over how sensitive information is used and reduces the risk of accidental exposure. Secret values are encoded as base64 strings and are stored unencrypted by default, but can be configured to be encrypted at rest.A Pod can reference the Secret in a variety of ways, such as in a volume mount or as an environment variable. Secrets are designed for confidential data and ConfigMaps are designed for non-confidential data.The following good practices are intended for both cluster administrators and application developers. Use these guidelines to improve the security of your sensitive information in Secret objects, as well as to more effectively manage your Secrets.Cluster administratorsThis section provides good practices that cluster administrators can use to improve the security of confidential information in the cluster.Configure encryption at restBy default, Secret objects are stored unencrypted in etcd. You should configure encryption of your Secret data in etcd. For instructions, refer to Encrypt Secret Data at Rest.Configure least-privilege access to SecretsWhen planning your access control mechanism, such as Kubernetes Role-based Access Control (RBAC), consider the following guidelines for access to Secret objects. You should also follow the other guidelines in RBAC good practices.Components: Restrict watch or list access to only the most privileged, system-level components. Only grant get access for Secrets if the component's normal behavior requires it.Humans: Restrict get, watch, or list access to Secrets. Only allow cluster administrators to access etcd. This includes read-only access. For more complex access control, such as restricting access to Secrets with specific annotations, consider using third-party authorization mechanisms.Caution:Granting list access to Secrets implicitly lets the subject fetch the contents of the Secrets.A user who can create a Pod that uses a Secret can also see the value of that Secret. Even if cluster policies do not allow a user to read the Secret directly, the same user could have access to run a Pod that then exposes the Secret. You can detect or limit the impact caused by Secret data being exposed, either intentionally or unintentionally, by a user with this access. Some recommendations include:Use short-lived SecretsImplement audit rules that alert on specific events, such as concurrent reading of multiple Secrets by a single userRestrict Access for SecretsUse separate namespaces to isolate access to mounted secrets.Improve etcd management policiesConsider wiping or shredding the durable storage used by etcd once it is no longer in use.If there are multiple etcd instances, configure encrypted SSL/TLS communication between the instances to protect the Secret data in transit.Configure access to external SecretsNote: This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the content guide before submitting a change. More information.You can use third-party Secrets store providers to keep your confidential data outside your cluster and then configure Pods to access that information. The Kubernetes Secrets Store CSI Driver is a DaemonSet that lets the kubelet retrieve Secrets from external stores, and mount the Secrets as a volume into specific Pods that you authorize to access the data.For a list of supported providers, refer to Providers for the Secret Store CSI Driver.DevelopersThis section provides good practices for developers to use to improve the security of confidential data when building and deploying Kubernetes resources.Restrict Secret access to specific containersIf you are defining multiple containers in a Pod, and only one of those containers needs access to a Secret, define the volume mount or environment variable configuration so that the other containers do not have access to that Secret.Protect Secret data after readingApplications still need to protect the value of confidential information after reading it from an environment variable or volume. For example, your application must avoid logging the secret data in the clear or transmitting it to an untrusted party.Avoid sharing Secret manifestsIf you configure a Secret through a manifest, with the secret data encoded as base64, sharing this file or checking it in to a source repository means the secret is available to everyone who can read the manifest.Caution:Base64 encoding is not an encryption method, it provides no additional confidentiality over plain text.3.8.10 - Multi-tenancyThis page provides an overview of available configuration options and best practices for cluster multi-tenancy.Sharing clusters saves costs and simplifies administration. However, sharing clusters also presents challenges such as security, fairness, and managing noisy neighbors.Clusters can be shared in many ways. In some cases, different applications may run in the same cluster. In other cases, multiple instances of the same application may run in the same cluster, one for each end user. All these types of sharing are frequently described using the umbrella term multi-tenancy.While Kubernetes does not have first-class concepts of end users or tenants, it provides several features to help manage different tenancy requirements. These are discussed below.Use casesThe first step to determining how to share your cluster is understanding your use case, so you can evaluate the patterns and tools available. In general, multi-tenancy in Kubernetes clusters falls into two broad categories, though many variations and hybrids are also possible.Multiple teamsA common form of multi-tenancy is to share a cluster between multiple teams within an organization, each of whom may operate one or more workloads. These workloads frequently need to communicate with each other, and with other workloads located on the same or different clusters.In this scenario, members of the teams often have direct access to Kubernetes resources via tools such as kubectl, or indirect access through GitOps controllers or other types of release automation tools. There is often some level of trust between members of different teams, but Kubernetes policies such as RBAC, quotas, and network policies are essential to safely and fairly share clusters.Multiple customersThe other major form of multi-tenancy frequently involves a Software-as-a-Service (SaaS) vendor running multiple instances of a workload for customers. This business model is so strongly associated with this deployment style that many people call it "SaaS tenancy." However, a better term might be "multi-customer tenancy," since SaaS vendors may also use other deployment models, and this deployment model can also be used outside of SaaS.In this scenario, the customers do not have access to the cluster; Kubernetes is invisible from their perspective and is only used by the vendor to manage the workloads. Cost optimization is frequently a critical concern, and Kubernetes policies are used to ensure that the workloads are strongly isolated from each other.TerminologyTenantsWhen discussing multi-tenancy in Kubernetes, there is no single definition for a "tenant". Rather, the definition of a tenant will vary depending on whether multi-team or multi-customer tenancy is being discussed.In multi-team usage, a tenant is typically a team, where each team typically deploys a small number of workloads that scales with the complexity of the service. However, the definition of "team" may itself be fuzzy, as teams may be organized into higher-level divisions or subdivided into smaller teams.By contrast, if each team deploys dedicated workloads for each new client, they are using a multi-customer model of tenancy. In this case, a "tenant" is simply a group of users who share a single workload. This may be as large as an entire company, or as small as a single team at that company.In many cases, the same organization may use both definitions of "tenants" in different contexts. For example, a platform team may offer shared services such as security tools and databases to multiple internal “customers” and a SaaS vendor may also have multiple teams sharing a development cluster. Finally, hybrid architectures are also possible, such as a SaaS provider using a combination of per-customer workloads for sensitive data, combined with multi-tenant shared services.A cluster showing coexisting tenancy modelsIsolationThere are several ways to design and build multi-tenant solutions with Kubernetes. Each of these methods comes with its own set of tradeoffs that impact the isolation level, implementation effort, operational complexity, and cost of service.A Kubernetes cluster consists of a control plane which runs Kubernetes software, and a data plane consisting of worker nodes where tenant workloads are executed as pods. Tenant isolation can be applied in both the control plane and the data plane based on organizational requirements.The level of isolation offered is sometimes described using terms like “hard” multi-tenancy, which implies strong isolation, and “soft” multi-tenancy, which implies weaker isolation. In particular, "hard" multi-tenancy is often used to describe cases where the tenants do not trust each other, often from security and resource sharing perspectives (e.g. guarding against attacks such as data exfiltration or DoS). Since data planes typically have much larger attack surfaces, "hard" multi-tenancy often requires extra attention to isolating the data-plane, though control plane isolation also remains critical.However, the terms "hard" and "soft" can often be confusing, as there is no single definition that will apply to all users. Rather, "hardness" or "softness" is better understood as a broad spectrum, with many different techniques that can be used to maintain different types of isolation in your clusters, based on your requirements.In more extreme cases, it may be easier or necessary to forgo any cluster-level sharing at all and assign each tenant their dedicated cluster, possibly even running on dedicated hardware if VMs are not considered an adequate security boundary. This may be easier with managed Kubernetes clusters, where the overhead of creating and operating clusters is at least somewhat taken on by a cloud provider. The benefit of stronger tenant isolation must be evaluated against the cost and complexity of managing multiple clusters. The Multi-cluster SIG is responsible for addressing these types of use cases.The remainder of this page focuses on isolation techniques used for shared Kubernetes clusters. However, even if you are considering dedicated clusters, it may be valuable to review these recommendations, as it will give you the flexibility to shift to shared clusters in the future if your needs or capabilities change.Control plane isolationControl plane isolation ensures that different tenants cannot access or affect each others' Kubernetes API resources.NamespacesIn Kubernetes, a Namespace provides a mechanism for isolating groups of API resources within a single cluster. This isolation has two key dimensions:Object names within a namespace can overlap with names in other namespaces, similar to files in folders. This allows tenants to name their resources without having to consider what other tenants are doing.Many Kubernetes security policies are scoped to namespaces. For example, RBAC Roles and Network Policies are namespace-scoped resources. Using RBAC, Users and Service Accounts can be restricted to a namespace.In a multi-tenant environment, a Namespace helps segment a tenant's workload into a logical and distinct management unit. In fact, a common practice is to isolate every workload in its own namespace, even if multiple workloads are operated by the same tenant. This ensures that each workload has its own identity and can be configured with an appropriate security policy.The namespace isolation model requires configuration of several other Kubernetes resources, networking plugins, and adherence to security best practices to properly isolate tenant workloads. These considerations are discussed below.Access controlsThe most important type of isolation for the control plane is authorization. If teams or their workloads can access or modify each others' API resources, they can change or disable all other types of policies thereby negating any protection those policies may offer. As a result, it is critical to ensure that each tenant has the appropriate access to only the namespaces they need, and no more. This is known as the "Principle of Least Privilege."Role-based access control (RBAC) is commonly used to enforce authorization in the Kubernetes control plane, for both users and workloads (service accounts). Roles and RoleBindings are Kubernetes objects that are used at a namespace level to enforce access control in your application; similar objects exist for authorizing access to cluster-level objects, though these are less useful for multi-tenant clusters.In a multi-team environment, RBAC must be used to restrict tenants' access to the appropriate namespaces, and ensure that cluster-wide resources can only be accessed or modified by privileged users such as cluster administrators.If a policy ends up granting a user more permissions than they need, this is likely a signal that the namespace containing the affected resources should be refactored into finer-grained namespaces. Namespace management tools may simplify the management of these finer-grained namespaces by applying common RBAC policies to different namespaces, while still allowing fine-grained policies where necessary.QuotasKubernetes workloads consume node resources, like CPU and memory. In a multi-tenant environment, you can use Resource Quotas to manage resource usage of tenant workloads. For the multiple teams use case, where tenants have access to the Kubernetes API, you can use resource quotas to limit the number of API resources (for example: the number of Pods, or the number of ConfigMaps) that a tenant can create. Limits on object count ensure fairness and aim to avoid noisy neighbor issues from affecting other tenants that share a control plane.Resource quotas are namespaced objects. By mapping tenants to namespaces, cluster admins can use quotas to ensure that a tenant cannot monopolize a cluster's resources or overwhelm its control plane. Namespace management tools simplify the administration of quotas. In addition, while Kubernetes quotas only apply within a single namespace, some namespace management tools allow groups of namespaces to share quotas, giving administrators far more flexibility with less effort than built-in quotas.Quotas prevent a single tenant from consuming greater than their allocated share of resources hence minimizing the “noisy neighbor” issue, where one tenant negatively impacts the performance of other tenants' workloads.When you apply a quota to namespace, Kubernetes requires you to also specify resource requests and limits for each container. Limits are the upper bound for the amount of resources that a container can consume. Containers that attempt to consume resources that exceed the configured limits will either be throttled or killed, based on the resource type. When resource requests are set lower than limits, each container is guaranteed the requested amount but there may still be some potential for impact across workloads.Quotas cannot protect against all kinds of resource sharing, such as network traffic. Node isolation (described below) may be a better solution for this problem.Data Plane IsolationData plane isolation ensures that pods and workloads for different tenants are sufficiently isolated.Network isolationBy default, all pods in a Kubernetes cluster are allowed to communicate with each other, and all network traffic is unencrypted. This can lead to security vulnerabilities where traffic is accidentally or maliciously sent to an unintended destination, or is intercepted by a workload on a compromised node.Pod-to-pod communication can be controlled using Network Policies, which restrict communication between pods using namespace labels or IP address ranges. In a multi-tenant environment where strict network isolation between tenants is required, starting with a default policy that denies communication between pods is recommended with another rule that allows all pods to query the DNS server for name resolution. With such a default policy in place, you can begin adding more permissive rules that allow for communication within a namespace. It is also recommended not to use empty label selector '{}' for namespaceSelector field in network policy definition, in case traffic need to be allowed between namespaces. This scheme can be further refined as required. Note that this only applies to pods within a single control plane; pods that belong to different virtual control planes cannot talk to each other via Kubernetes networking.Namespace management tools may simplify the creation of default or common network policies. In addition, some of these tools allow you to enforce a consistent set of namespace labels across your cluster, ensuring that they are a trusted basis for your policies.Warning:Network policies require a CNI plugin that supports the implementation of network policies. Otherwise, NetworkPolicy resources will be ignored.More advanced network isolation may be provided by service meshes, which provide OSI Layer 7 policies based on workload identity, in addition to namespaces. These higher-level policies can make it easier to manage namespace-based multi-tenancy, especially when multiple namespaces are dedicated to a single tenant. They frequently also offer encryption using mutual TLS, protecting your data even in the presence of a compromised node, and work across dedicated or virtual clusters. However, they can be significantly more complex to manage and may not be appropriate for all users.Storage isolationKubernetes offers several types of volumes that can be used as persistent storage for workloads. For security and data-isolation, dynamic volume provisioning is recommended and volume types that use node resources should be avoided.StorageClasses allow you to describe custom "classes" of storage offered by your cluster, based on quality-of-service levels, backup policies, or custom policies determined by the cluster administrators.Pods can request storage using a PersistentVolumeClaim. A PersistentVolumeClaim is a namespaced resource, which enables isolating portions of the storage system and dedicating it to tenants within the shared Kubernetes cluster. However, it is important to note that a PersistentVolume is a cluster-wide resource and has a lifecycle independent of workloads and namespaces.For example, you can configure a separate StorageClass for each tenant and use this to strengthen isolation. If a StorageClass is shared, you should set a reclaim policy of Delete to ensure that a PersistentVolume cannot be reused across different namespaces.Sandboxing containersNote: This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the content guide before submitting a change. More information.Kubernetes pods are composed of one or more containers that execute on worker nodes. Containers utilize OS-level virtualization and hence offer a weaker isolation boundary than virtual machines that utilize hardware-based virtualization.In a shared environment, unpatched vulnerabilities in the application and system layers can be exploited by attackers for container breakouts and remote code execution that allow access to host resources. In some applications, like a Content Management System (CMS), customers may be allowed the ability to upload and execute untrusted scripts or code. In either case, mechanisms to further isolate and protect workloads using strong isolation are desirable.Sandboxing provides a way to isolate workloads running in a shared cluster. It typically involves running each pod in a separate execution environment such as a virtual machine or a userspace kernel. Sandboxing is often recommended when you are running untrusted code, where workloads are assumed to be malicious. Part of the reason this type of isolation is necessary is because containers are processes running on a shared kernel; they mount file systems like /sys and /proc from the underlying host, making them less secure than an application that runs on a virtual machine which has its own kernel. While controls such as seccomp, AppArmor, and SELinux can be used to strengthen the security of containers, it is hard to apply a universal set of rules to all workloads running in a shared cluster. Running workloads in a sandbox environment helps to insulate the host from container escapes, where an attacker exploits a vulnerability to gain access to the host system and all the processes/files running on that host.Virtual machines and userspace kernels are two popular approaches to sandboxing. The following sandboxing implementations are available:gVisor intercepts syscalls from containers and runs them through a userspace kernel, written in Go, with limited access to the underlying host.Kata Containers provide a secure container runtime that allows you to run containers in a VM. The hardware virtualization available in Kata offers an added layer of security for containers running untrusted code.Node IsolationNode isolation is another technique that you can use to isolate tenant workloads from each other. With node isolation, a set of nodes is dedicated to running pods from a particular tenant and co-mingling of tenant pods is prohibited. This configuration reduces the noisy tenant issue, as all pods running on a node will belong to a single tenant. The risk of information disclosure is slightly lower with node isolation because an attacker that manages to escape from a container will only have access to the containers and volumes mounted to that node.Although workloads from different tenants are running on different nodes, it is important to be aware that the kubelet and (unless using virtual control planes) the API service are still shared services. A skilled attacker could use the permissions assigned to the kubelet or other pods running on the node to move laterally within the cluster and gain access to tenant workloads running on other nodes. If this is a major concern, consider implementing compensating controls such as seccomp, AppArmor or SELinux or explore using sandboxed containers or creating separate clusters for each tenant.Node isolation is a little easier to reason about from a billing standpoint than sandboxing containers since you can charge back per node rather than per pod. It also has fewer compatibility and performance issues and may be easier to implement than sandboxing containers. For example, nodes for each tenant can be configured with taints so that only pods with the corresponding toleration can run on them. A mutating webhook could then be used to automatically add tolerations and node affinities to pods deployed into tenant namespaces so that they run on a specific set of nodes designated for that tenant.Node isolation can be implemented using an pod node selectors or a Virtual Kubelet.Additional ConsiderationsThis section discusses other Kubernetes constructs and patterns that are relevant for multi-tenancy.API Priority and FairnessAPI priority and fairness is a Kubernetes feature that allows you to assign a priority to certain pods running within the cluster. When an application calls the Kubernetes API, the API server evaluates the priority assigned to pod. Calls from pods with higher priority are fulfilled before those with a lower priority. When contention is high, lower priority calls can be queued until the server is less busy or you can reject the requests.Using API priority and fairness will not be very common in SaaS environments unless you are allowing customers to run applications that interface with the Kubernetes API, for example, a controller.Quality-of-Service (QoS)When you’re running a SaaS application, you may want the ability to offer different Quality-of-Service (QoS) tiers of service to different tenants. For example, you may have freemium service that comes with fewer performance guarantees and features and a for-fee service tier with specific performance guarantees. Fortunately, there are several Kubernetes constructs that can help you accomplish this within a shared cluster, including network QoS, storage classes, and pod priority and preemption. The idea with each of these is to provide tenants with the quality of service that they paid for. Let’s start by looking at networking QoS.Typically, all pods on a node share a network interface. Without network QoS, some pods may consume an unfair share of the available bandwidth at the expense of other pods. The Kubernetes bandwidth plugin creates an extended resource for networking that allows you to use Kubernetes resources constructs, i.e. requests/limits, to apply rate limits to pods by using Linux tc queues. Be aware that the plugin is considered experimental as per the Network Plugins documentation and should be thoroughly tested before use in production environments.For storage QoS, you will likely want to create different storage classes or profiles with different performance characteristics. Each storage profile can be associated with a different tier of service that is optimized for different workloads such IO, redundancy, or throughput. Additional logic might be necessary to allow the tenant to associate the appropriate storage profile with their workload.Finally, there’s pod priority and preemption where you can assign priority values to pods. When scheduling pods, the scheduler will try evicting pods with lower priority when there are insufficient resources to schedule pods that are assigned a higher priority. If you have a use case where tenants have different service tiers in a shared cluster e.g. free and paid, you may want to give higher priority to certain tiers using this feature.DNSKubernetes clusters include a Domain Name System (DNS) service to provide translations from names to IP addresses, for all Services and Pods. By default, the Kubernetes DNS service allows lookups across all namespaces in the cluster.In multi-tenant environments where tenants can access pods and other Kubernetes resources, or where stronger isolation is required, it may be necessary to prevent pods from looking up services in other Namespaces. You can restrict cross-namespace DNS lookups by configuring security rules for the DNS service. For example, CoreDNS (the default DNS service for Kubernetes) can leverage Kubernetes metadata to restrict queries to Pods and Services within a namespace. For more information, read an example of configuring this within the CoreDNS documentation.When a Virtual Control Plane per tenant model is used, a DNS service must be configured per tenant or a multi-tenant DNS service must be used. Here is an example of a customized version of CoreDNS that supports multiple tenants.OperatorsOperators are Kubernetes controllers that manage applications. Operators can simplify the management of multiple instances of an application, like a database service, which makes them a common building block in the multi-consumer (SaaS) multi-tenancy use case.Operators used in a multi-tenant environment should follow a stricter set of guidelines. Specifically, the Operator should:Support creating resources within different tenant namespaces, rather than just in the namespace in which the Operator is deployed.Ensure that the Pods are configured with resource requests and limits, to ensure scheduling and fairness.Support configuration of Pods for data-plane isolation techniques such as node isolation and sandboxed containers.ImplementationsNote: This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the content guide before submitting a change. More information.There are two primary ways to share a Kubernetes cluster for multi-tenancy: using Namespaces (that is, a Namespace per tenant) or by virtualizing the control plane (that is, virtual control plane per tenant).In both cases, data plane isolation, and management of additional considerations such as API Priority and Fairness, is also recommended.Namespace isolation is well-supported by Kubernetes, has a negligible resource cost, and provides mechanisms to allow tenants to interact appropriately, such as by allowing service-to-service communication. However, it can be difficult to configure, and doesn't apply to Kubernetes resources that can't be namespaced, such as Custom Resource Definitions, Storage Classes, and Webhooks.Control plane virtualization allows for isolation of non-namespaced resources at the cost of somewhat higher resource usage and more difficult cross-tenant sharing. It is a good option when namespace isolation is insufficient but dedicated clusters are undesirable, due to the high cost of maintaining them (especially on-prem) or due to their higher overhead and lack of resource sharing. However, even within a virtualized control plane, you will likely see benefits by using namespaces as well.The two options are discussed in more detail in the following sections.Namespace per tenantAs previously mentioned, you should consider isolating each workload in its own namespace, even if you are using dedicated clusters or virtualized control planes. This ensures that each workload only has access to its own resources, such as ConfigMaps and Secrets, and allows you to tailor dedicated security policies for each workload. In addition, it is a best practice to give each namespace names that are unique across your entire fleet (that is, even if they are in separate clusters), as this gives you the flexibility to switch between dedicated and shared clusters in the future, or to use multi-cluster tooling such as service meshes.Conversely, there are also advantages to assigning namespaces at the tenant level, not just the workload level, since there are often policies that apply to all workloads owned by a single tenant. However, this raises its own problems. Firstly, this makes it difficult or impossible to customize policies to individual workloads, and secondly, it may be challenging to come up with a single level of "tenancy" that should be given a namespace. For example, an organization may have divisions, teams, and subteams - which should be assigned a namespace?One possible approach is to organize your namespaces into hierarchies, and share certain policies and resources between them. This could include managing namespace labels, namespace lifecycles, delegated access, and shared resource quotas across related namespaces. These capabilities can be useful in both multi-team and multi-customer scenarios.Some third-party projects that provide capabilities like this and aid in managing namespaced resources are listed below.Note: This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the content guide before submitting a change. More information.Multi-team tenancyCapsuleMulti Tenant OperatorMulti-customer tenancyKubeplusPolicy enginesPolicy engines provide features to validate and generate tenant configurations:KyvernoOPA/GatekeeperVirtual control plane per tenantAnother form of control-plane isolation is to use Kubernetes extensions to provide each tenant a virtual control-plane that enables segmentation of cluster-wide API resources. Data plane isolation techniques can be used with this model to securely manage worker nodes across tenants.The virtual control plane based multi-tenancy model extends namespace-based multi-tenancy by providing each tenant with dedicated control plane components, and hence complete control over cluster-wide resources and add-on services. Worker nodes are shared across all tenants, and are managed by a Kubernetes cluster that is normally inaccessible to tenants. This cluster is often referred to as a super-cluster (or sometimes as a host-cluster). Since a tenant’s control-plane is not directly associated with underlying compute resources it is referred to as a virtual control plane.A virtual control plane typically consists of the Kubernetes API server, the controller manager, and the etcd data store. It interacts with the super cluster via a metadata synchronization controller which coordinates changes across tenant control planes and the control plane of the super-cluster.By using per-tenant dedicated control planes, most of the isolation problems due to sharing one API server among all tenants are solved. Examples include noisy neighbors in the control plane, uncontrollable blast radius of policy misconfigurations, and conflicts between cluster scope objects such as webhooks and CRDs. Hence, the virtual control plane model is particularly suitable for cases where each tenant requires access to a Kubernetes API server and expects the full cluster manageability.The improved isolation comes at the cost of running and maintaining an individual virtual control plane per tenant. In addition, per-tenant control planes do not solve isolation problems in the data plane, such as node-level noisy neighbors or security threats. These must still be addressed separately.The Kubernetes Cluster API - Nested (CAPN) project provides an implementation of virtual control planes.Other implementationsKamajivcluster3.8.11 - Hardening Guide - Authentication MechanismsInformation on authentication options in Kubernetes and their security properties.Selecting the appropriate authentication mechanism(s) is a crucial aspect of securing your cluster. Kubernetes provides several built-in mechanisms, each with its own strengths and weaknesses that should be carefully considered when choosing the best authentication mechanism for your cluster.In general, it is recommended to enable as few authentication mechanisms as possible to simplify user management and prevent cases where users retain access to a cluster that is no longer required.It is important to note that Kubernetes does not have an in-built user database within the cluster. Instead, it takes user information from the configured authentication system and uses that to make authorization decisions. Therefore, to audit user access, you need to review credentials from every configured authentication source.For production clusters with multiple users directly accessing the Kubernetes API, it is recommended to use external authentication sources such as OIDC. The internal authentication mechanisms, such as client certificates and service account tokens, described below, are not suitable for this use case.X.509 client certificate authenticationKubernetes leverages X.509 client certificate authentication for system components, such as when the kubelet authenticates to the API Server. While this mechanism can also be used for user authentication, it might not be suitable for production use due to several restrictions:Client certificates cannot be individually revoked. Once compromised, a certificate can be used by an attacker until it expires. To mitigate this risk, it is recommended to configure short lifetimes for user authentication credentials created using client certificates.If a certificate needs to be invalidated, the certificate authority must be re-keyed, which can introduce availability risks to the cluster.There is no permanent record of client certificates created in the cluster. Therefore, all issued certificates must be recorded if you need to keep track of them.Private keys used for client certificate authentication cannot be password-protected. Anyone who can read the file containing the key will be able to make use of it.Using client certificate authentication requires a direct connection from the client to the API server without any intervening TLS termination points, which can complicate network architectures.Group data is embedded in the O value of the client certificate, which means the user's group memberships cannot be changed for the lifetime of the certificate.Static token fileAlthough Kubernetes allows you to load credentials from a static token file located on the control plane node disks, this approach is not recommended for production servers due to several reasons:Credentials are stored in clear text on control plane node disks, which can be a security risk.Changing any credential requires a restart of the API server process to take effect, which can impact availability.There is no mechanism available to allow users to rotate their credentials. To rotate a credential, a cluster administrator must modify the token on disk and distribute it to the users.There is no lockout mechanism available to prevent brute-force attacks.Bootstrap tokensBootstrap tokens are used for joining nodes to clusters and are not recommended for user authentication due to several reasons:They have hard-coded group memberships that are not suitable for general use, making them unsuitable for authentication purposes.Manually generating bootstrap tokens can lead to weak tokens that can be guessed by an attacker, which can be a security risk.There is no lockout mechanism available to prevent brute-force attacks, making it easier for attackers to guess or crack the token.ServiceAccount secret tokensService account secrets are available as an option to allow workloads running in the cluster to authenticate to the API server. In Kubernetes < 1.23, these were the default option, however, they are being replaced with TokenRequest API tokens. While these secrets could be used for user authentication, they are generally unsuitable for a number of reasons:They cannot be set with an expiry and will remain valid until the associated service account is deleted.The authentication tokens are visible to any cluster user who can read secrets in the namespace that they are defined in.Service accounts cannot be added to arbitrary groups complicating RBAC management where they are used.TokenRequest API tokensThe TokenRequest API is a useful tool for generating short-lived credentials for service authentication to the API server or third-party systems. However, it is not generally recommended for user authentication as there is no revocation method available, and distributing credentials to users in a secure manner can be challenging.When using TokenRequest tokens for service authentication, it is recommended to implement a short lifespan to reduce the impact of compromised tokens.OpenID Connect token authenticationKubernetes supports integrating external authentication services with the Kubernetes API using OpenID Connect (OIDC). There is a wide variety of software that can be used to integrate Kubernetes with an identity provider. However, when using OIDC authentication in Kubernetes, it is important to consider the following hardening measures:The software installed in the cluster to support OIDC authentication should be isolated from general workloads as it will run with high privileges.Some Kubernetes managed services are limited in the OIDC providers that can be used.As with TokenRequest tokens, OIDC tokens should have a short lifespan to reduce the impact of compromised tokens.Webhook token authenticationWebhook token authentication is another option for integrating external authentication providers into Kubernetes. This mechanism allows for an authentication service, either running inside the cluster or externally, to be contacted for an authentication decision over a webhook. It is important to note that the suitability of this mechanism will likely depend on the software used for the authentication service, and there are some Kubernetes-specific considerations to take into account.To configure Webhook authentication, access to control plane server filesystems is required. This means that it will not be possible with Managed Kubernetes unless the provider specifically makes it available. Additionally, any software installed in the cluster to support this access should be isolated from general workloads, as it will run with high privileges.Authenticating proxyAnother option for integrating external authentication systems into Kubernetes is to use an authenticating proxy. With this mechanism, Kubernetes expects to receive requests from the proxy with specific header values set, indicating the username and group memberships to assign for authorization purposes. It is important to note that there are specific considerations to take into account when using this mechanism.Firstly, securely configured TLS must be used between the proxy and Kubernetes API server to mitigate the risk of traffic interception or sniffing attacks. This ensures that the communication between the proxy and Kubernetes API server is secure.Secondly, it is important to be aware that an attacker who is able to modify the headers of the request may be able to gain unauthorized access to Kubernetes resources. As such, it is important to ensure that the headers are properly secured and cannot be tampered with.What's nextUser AuthenticationAuthenticating with Bootstrap Tokenskubelet AuthenticationAuthenticating with Service Account Tokens3.8.12 - Kubernetes API Server Bypass RisksSecurity architecture information relating to the API server and other componentsThe Kubernetes API server is the main point of entry to a cluster for external parties (users and services) interacting with it.As part of this role, the API server has several key built-in security controls, such as audit logging and admission controllers. However, there are ways to modify the configuration or content of the cluster that bypass these controls.This page describes the ways in which the security controls built into the Kubernetes API server can be bypassed, so that cluster operators and security architects can ensure that these bypasses are appropriately restricted.Static PodsThe kubelet on each node loads and directly manages any manifests that are stored in a named directory or fetched from a specific URL as static Pods in your cluster. The API server doesn't manage these static Pods. An attacker with write access to this location could modify the configuration of static pods loaded from that source, or could introduce new static Pods.Static Pods are restricted from accessing other objects in the Kubernetes API. For example, you can't configure a static Pod to mount a Secret from the cluster. However, these Pods can take other security sensitive actions, such as using hostPath mounts from the underlying node.By default, the kubelet creates a mirror pod so that the static Pods are visible in the Kubernetes API. However, if the attacker uses an invalid namespace name when creating the Pod, it will not be visible in the Kubernetes API and can only be discovered by tooling that has access to the affected host(s).If a static Pod fails admission control, the kubelet won't register the Pod with the API server. However, the Pod still runs on the node. For more information, refer to kubeadm issue #1541.MitigationsOnly enable the kubelet static Pod manifest functionality if required by the node.If a node uses the static Pod functionality, restrict filesystem access to the static Pod manifest directory or URL to users who need the access.Restrict access to kubelet configuration parameters and files to prevent an attacker setting a static Pod path or URL.Regularly audit and centrally report all access to directories or web storage locations that host static Pod manifests and kubelet configuration files.The kubelet APIThe kubelet provides an HTTP API that is typically exposed on TCP port 10250 on cluster worker nodes. The API might also be exposed on control plane nodes depending on the Kubernetes distribution in use. Direct access to the API allows for disclosure of information about the pods running on a node, the logs from those pods, and execution of commands in every container running on the node.When Kubernetes cluster users have RBAC access to Node object sub-resources, that access serves as authorization to interact with the kubelet API. The exact access depends on which sub-resource access has been granted, as detailed in kubelet authorization.Direct access to the kubelet API is not subject to admission control and is not logged by Kubernetes audit logging. An attacker with direct access to this API may be able to bypass controls that detect or prevent certain actions.The kubelet API can be configured to authenticate requests in a number of ways. By default, the kubelet configuration allows anonymous access. Most Kubernetes providers change the default to use webhook and certificate authentication. This lets the control plane ensure that the caller is authorized to access the nodes API resource or sub-resources. The default anonymous access doesn't make this assertion with the control plane.MitigationsRestrict access to sub-resources of the nodes API object using mechanisms such as RBAC. Only grant this access when required, such as by monitoring services.Restrict access to the kubelet port. Only allow specified and trusted IP address ranges to access the port.Ensure that kubelet authentication. is set to webhook or certificate mode.Ensure that the unauthenticated "read-only" Kubelet port is not enabled on the cluster.The etcd APIKubernetes clusters use etcd as a datastore. The etcd service listens on TCP port 2379. The only clients that need access are the Kubernetes API server and any backup tooling that you use. Direct access to this API allows for disclosure or modification of any data held in the cluster.Access to the etcd API is typically managed by client certificate authentication. Any certificate issued by a certificate authority that etcd trusts allows full access to the data stored inside etcd.Direct access to etcd is not subject to Kubernetes admission control and is not logged by Kubernetes audit logging. An attacker who has read access to the API server's etcd client certificate private key (or can create a new trusted client certificate) can gain cluster admin rights by accessing cluster secrets or modifying access rules. Even without elevating their Kubernetes RBAC privileges, an attacker who can modify etcd can retrieve any API object or create new workloads inside the cluster.Many Kubernetes providers configure etcd to use mutual TLS (both client and server verify each other's certificate for authentication). There is no widely accepted implementation of authorization for the etcd API, although the feature exists. Since there is no authorization model, any certificate with client access to etcd can be used to gain full access to etcd. Typically, etcd client certificates that are only used for health checking can also grant full read and write access.MitigationsEnsure that the certificate authority trusted by etcd is used only for the purposes of authentication to that service.Control access to the private key for the etcd server certificate, and to the API server's client certificate and key.Consider restricting access to the etcd port at a network level, to only allow access from specified and trusted IP address ranges.Container runtime socketOn each node in a Kubernetes cluster, access to interact with containers is controlled by the container runtime (or runtimes, if you have configured more than one). Typically, the container runtime exposes a Unix socket that the kubelet can access. An attacker with access to this socket can launch new containers or interact with running containers.At the cluster level, the impact of this access depends on whether the containers that run on the compromised node have access to Secrets or other confidential data that an attacker could use to escalate privileges to other worker nodes or to control plane components.MitigationsEnsure that you tightly control filesystem access to container runtime sockets. When possible, restrict this access to the root user.Isolate the kubelet from other components running on the node, using mechanisms such as Linux kernel namespaces.Ensure that you restrict or forbid the use of hostPath mounts that include the container runtime socket, either directly or by mounting a parent directory. Also hostPath mounts must be set as read-only to mitigate risks of attackers bypassing directory restrictions.Restrict user access to nodes, and especially restrict superuser access to nodes.3.8.13 - Linux kernel security constraints for Pods and containersOverview of Linux kernel security modules and constraints that you can use to harden your Pods and containers.This page describes some of the security features that are built into the Linux kernel that you can use in your Kubernetes workloads. To learn how to apply these features to your Pods and containers, refer to Configure a SecurityContext for a Pod or Container. You should already be familiar with Linux and with the basics of Kubernetes workloads.Run workloads without root privilegesWhen you deploy a workload in Kubernetes, use the Pod specification to restrict that workload from running as the root user on the node. You can use the Pod securityContext to define the specific Linux user and group for the processes in the Pod, and explicitly restrict containers from running as root users. Setting these values in the Pod manifest takes precedence over similar values in the container image, which is especially useful if you're running images that you don't own.Caution:Ensure that the user or group that you assign to the workload has the permissions required for the application to function correctly. Changing the user or group to one that doesn't have the correct permissions could lead to file access issues or failed operations.Configuring the kernel security features on this page provides fine-grained control over the actions that processes in your cluster can take, but managing these configurations can be challenging at scale. Running containers as non-root, or in user namespaces if you need root privileges, helps to reduce the chance that you'll need to enforce your configured kernel security capabilities.Security features in the Linux kernelKubernetes lets you configure and use Linux kernel features to improve isolation and harden your containerized workloads. Common features include the following:Secure computing mode (seccomp): Filter which system calls a process can makeAppArmor: Restrict the access privileges of individual programsSecurity Enhanced Linux (SELinux): Assign security labels to objects for more manageable security policy enforcementTo configure settings for one of these features, the operating system that you choose for your nodes must enable the feature in the kernel. For example, Ubuntu 7.10 and later enable AppArmor by default. To learn whether your OS enables a specific feature, consult the OS documentation.You use the securityContext field in your Pod specification to define the constraints that apply to those processes. The securityContext field also supports other security settings, such as specific Linux capabilities or file access permissions using UIDs and GIDs. To learn more, refer to Configure a SecurityContext for a Pod or Container.seccompSome of your workloads might need privileges to perform specific actions as the root user on your node's host machine. Linux uses capabilities to divide the available privileges into categories, so that processes can get the privileges required to perform specific actions without being granted all privileges. Each capability has a set of system calls (syscalls) that a process can make. seccomp lets you restrict these individual syscalls. It can be used to sandbox the privileges of a process, restricting the calls it is able to make from userspace into the kernel.In Kubernetes, you use a container runtime on each node to run your containers. Example runtimes include CRI-O, Docker, or containerd. Each runtime allows only a subset of Linux capabilities by default. You can further limit the allowed syscalls individually by using a seccomp profile. Container runtimes usually include a default seccomp profile. Kubernetes lets you automatically apply seccomp profiles loaded onto a node to your Pods and containers.Note:Kubernetes also has the allowPrivilegeEscalation setting for Pods and containers. When set to false, this prevents processes from gaining new capabilities and restricts unprivileged users from changing the applied seccomp profile to a more permissive profile.To learn how to implement seccomp in Kubernetes, refer to Restrict a Container's Syscalls with seccomp or the Seccomp node referenceTo learn more about seccomp, see Seccomp BPF in the Linux kernel documentation.Considerations for seccompseccomp is a low-level security configuration that you should only configure yourself if you require fine-grained control over Linux syscalls. Using seccomp, especially at scale, has the following risks:Configurations might break during application updatesAttackers can still use allowed syscalls to exploit vulnerabilitiesProfile management for individual applications becomes challenging at scaleRecommendation: Use the default seccomp profile that's bundled with your container runtime. If you need a more isolated environment, consider using a sandbox, such as gVisor. Sandboxes solve the preceding risks with custom seccomp profiles, but require more compute resources on your nodes and might have compatibility issues with GPUs and other specialized hardware.AppArmor and SELinux: policy-based mandatory access controlYou can use Linux policy-based mandatory access control (MAC) mechanisms, such as AppArmor and SELinux, to harden your Kubernetes workloads.AppArmorAppArmor is a Linux kernel security module that supplements the standard Linux user and group based permissions to confine programs to a limited set of resources. AppArmor can be configured for any application to reduce its potential attack surface and provide greater in-depth defense. It is configured through profiles tuned to allow the access needed by a specific program or container, such as Linux capabilities, network access, and file permissions. Each profile can be run in either enforcing mode, which blocks access to disallowed resources, or complain mode, which only reports violations.AppArmor can help you to run a more secure deployment by restricting what containers are allowed to do, and/or provide better auditing through system logs. The container runtime that you use might ship with a default AppArmor profile, or you can use a custom profile.To learn how to use AppArmor in Kubernetes, refer to Restrict a Container's Access to Resources with AppArmor.SELinuxSELinux is a Linux kernel security module that lets you restrict the access that a specific subject, such as a process, has to the files on your system. You define security policies that apply to subjects that have specific SELinux labels. When a process that has an SELinux label attempts to access a file, the SELinux server checks whether that process' security policy allows the access and makes an authorization decision.In Kubernetes, you can set an SELinux label in the securityContext field of your manifest. The specified labels are assigned to those processes. If you have configured security policies that affect those labels, the host OS kernel enforces these policies.To learn how to use SELinux in Kubernetes, refer to Assign SELinux labels to a container.Differences between AppArmor and SELinuxThe operating system on your Linux nodes usually includes one of either AppArmor or SELinux. Both mechanisms provide similar types of protection, but have differences such as the following:Configuration: AppArmor uses profiles to define access to resources. SELinux uses policies that apply to specific labels.Policy application: In AppArmor, you define resources using file paths. SELinux uses the index node (inode) of a resource to identify the resource.Summary of featuresThe following table describes the use cases and scope of each security control. You can use all of these controls together to build a more hardened system.Summary of Linux kernel security featuresSecurity featureDescriptionHow to useExampleseccompRestrict individual kernel calls in the userspace. Reduces the likelihood that a vulnerability that uses a restricted syscall would compromise the system.Specify a loaded seccomp profile in the Pod or container specification to apply its constraints to the processes in the Pod.Reject the unshare syscall, which was used in CVE-2022-0185.AppArmorRestrict program access to specific resources. Reduces the attack surface of the program. Improves audit logging.Specify a loaded AppArmor profile in the container specification.Restrict a read-only program from writing to any file path in the system.SELinuxRestrict access to resources such as files, applications, ports, and processes using labels and security policies.Specify access restrictions for specific labels. Tag processes with those labels to enforce the access restrictions related to the label.Restrict a container from accessing files outside its own filesystem.Note:Mechanisms like AppArmor and SELinux can provide protection that extends beyond the container. For example, you can use SELinux to help mitigate CVE-2019-5736.Considerations for managing custom configurationsseccomp, AppArmor, and SELinux usually have a default configuration that offers basic protections. You can also create custom profiles and policies that meet the requirements of your workloads. Managing and distributing these custom configurations at scale might be challenging, especially if you use all three features together. To help you to manage these configurations at scale, use a tool like the Kubernetes Security Profiles Operator.Kernel-level security features and privileged containersKubernetes lets you specify that some trusted containers can run in privileged mode. Any container in a Pod can run in privileged mode to use operating system administrative capabilities that would otherwise be inaccessible. This is available for both Windows and Linux.Privileged containers explicitly override some of the Linux kernel constraints that you might use in your workloads, as follows:seccomp: Privileged containers run as the Unconfined seccomp profile, overriding any seccomp profile that you specified in your manifest.AppArmor: Privileged containers ignore any applied AppArmor profiles.SELinux: Privileged containers run as the unconfined_t domain.Privileged containersAny container in a Pod can enable Privileged mode if you set the privileged: true field in the securityContext field for the container. Privileged containers override or undo many other hardening settings such as the applied seccomp profile, AppArmor profile, or SELinux constraints. Privileged containers are given all Linux capabilities, including capabilities that they don't require. For example, a root user in a privileged container might be able to use the CAP_SYS_ADMIN and CAP_NET_ADMIN capabilities on the node, bypassing the runtime seccomp configuration and other restrictions.In most cases, you should avoid using privileged containers, and instead grant the specific capabilities required by your container using the capabilities field in the securityContext field. Only use privileged mode if you have a capability that you can't grant with the securityContext. This is useful for containers that want to use operating system administrative capabilities such as manipulating the network stack or accessing hardware devices.In Kubernetes version 1.26 and later, you can also run Windows containers in a similarly privileged mode by setting the windowsOptions.hostProcess flag on the security context of the Pod spec. For details and instructions, see Create a Windows HostProcess Pod.Recommendations and best practicesBefore configuring kernel-level security capabilities, you should consider implementing network-level isolation. For more information, read the Security Checklist.Unless necessary, run Linux workloads as non-root by setting specific user and group IDs in your Pod manifest and by specifying runAsNonRoot: true.Additionally, you can run workloads in user namespaces by setting hostUsers: false in your Pod manifest. This lets you run containers as root users in the user namespace, but as non-root users in the host namespace on the node. This is still in early stages of development and might not have the level of support that you need. For instructions, refer to Use a User Namespace With a Pod.What's nextLearn how to use AppArmorLearn how to use seccompLearn how to use SELinuxSeccomp Node Reference3.8.14 - Security ChecklistBaseline checklist for ensuring security in Kubernetes clusters.This checklist aims at providing a basic list of guidance with links to more comprehensive documentation on each topic. It does not claim to be exhaustive and is meant to evolve.On how to read and use this document:The order of topics does not reflect an order of priority.Some checklist items are detailed in the paragraph below the list of each section.Caution:Checklists are not sufficient for attaining a good security posture on their own. A good security posture requires constant attention and improvement, but a checklist can be the first step on the never-ending journey towards security preparedness. Some of the recommendations in this checklist may be too restrictive or too lax for your specific security needs. Since Kubernetes security is not "one size fits all", each category of checklist items should be evaluated on its merits.Authentication & Authorization system:masters group is not used for user or component authentication after bootstrapping. The kube-controller-manager is running with --use-service-account-credentials enabled. The root certificate is protected (either an offline CA, or a managed online CA with effective access controls). Intermediate and leaf certificates have an expiry date no more than 3 years in the future. A process exists for periodic access review, and reviews occur no more than 24 months apart. The Role Based Access Control Good Practices are followed for guidance related to authentication and authorization.After bootstrapping, neither users nor components should authenticate to the Kubernetes API as system:masters. Similarly, running all of kube-controller-manager as system:masters should be avoided. In fact, system:masters should only be used as a break-glass mechanism, as opposed to an admin user.Network security CNI plugins in use support network policies. Ingress and egress network policies are applied to all workloads in the cluster. Default network policies within each namespace, selecting all pods, denying everything, are in place. If appropriate, a service mesh is used to encrypt all communications inside of the cluster. The Kubernetes API, kubelet API and etcd are not exposed publicly on Internet. Access from the workloads to the cloud metadata API is filtered. Use of LoadBalancer and ExternalIPs is restricted.A number of Container Network Interface (CNI) plugins plugins provide the functionality to restrict network resources that pods may communicate with. This is most commonly done through Network Policies which provide a namespaced resource to define rules. Default network policies that block all egress and ingress, in each namespace, selecting all pods, can be useful to adopt an allow list approach to ensure that no workloads are missed.Not all CNI plugins provide encryption in transit. If the chosen plugin lacks this feature, an alternative solution could be to use a service mesh to provide that functionality.The etcd datastore of the control plane should have controls to limit access and not be publicly exposed on the Internet. Furthermore, mutual TLS (mTLS) should be used to communicate securely with it. The certificate authority for this should be unique to etcd.External Internet access to the Kubernetes API server should be restricted to not expose the API publicly. Be careful, as many managed Kubernetes distributions are publicly exposing the API server by default. You can then use a bastion host to access the server.The kubelet API access should be restricted and not exposed publicly, the default authentication and authorization settings, when no configuration file specified with the --config flag, are overly permissive.If a cloud provider is used for hosting Kubernetes, the access from pods to the cloud metadata API 169.254.169.254 should also be restricted or blocked if not needed because it may leak information.For restricted LoadBalancer and ExternalIPs use, see CVE-2020-8554: Man in the middle using LoadBalancer or ExternalIPs and the DenyServiceExternalIPs admission controller for further information.Pod security RBAC rights to create, update, patch, delete workloads is only granted if necessary. Appropriate Pod Security Standards policy is applied for all namespaces and enforced. Memory limit is set for the workloads with a limit equal or inferior to the request. CPU limit might be set on sensitive workloads. For nodes that support it, Seccomp is enabled with appropriate syscalls profile for programs. For nodes that support it, AppArmor or SELinux is enabled with appropriate profile for programs.RBAC authorization is crucial but cannot be granular enough to have authorization on the Pods' resources (or on any resource that manages Pods). The only granularity is the API verbs on the resource itself, for example, create on Pods. Without additional admission, the authorization to create these resources allows direct unrestricted access to the schedulable nodes of a cluster.The Pod Security Standards define three different policies, privileged, baseline and restricted that limit how fields can be set in the PodSpec regarding security. These standards can be enforced at the namespace level with the new Pod Security admission, enabled by default, or by third-party admission webhook. Please note that, contrary to the removed PodSecurityPolicy admission it replaces, Pod Security admission can be easily combined with admission webhooks and external services.Pod Security admission restricted policy, the most restrictive policy of the Pod Security Standards set, can operate in several modes, warn, audit or enforce to gradually apply the most appropriate security context according to security best practices. Nevertheless, pods' security context should be separately investigated to limit the privileges and access pods may have on top of the predefined security standards, for specific use cases.For a hands-on tutorial on Pod Security, see the blog post Kubernetes 1.23: Pod Security Graduates to Beta.Memory and CPU limits should be set in order to restrict the memory and CPU resources a pod can consume on a node, and therefore prevent potential DoS attacks from malicious or breached workloads. Such policy can be enforced by an admission controller. Please note that CPU limits will throttle usage and thus can have unintended effects on auto-scaling features or efficiency i.e. running the process in best effort with the CPU resource available.Caution:Memory limit superior to request can expose the whole node to OOM issues.Enabling SeccompSeccomp stands for secure computing mode and has been a feature of the Linux kernel since version 2.6.12. It can be used to sandbox the privileges of a process, restricting the calls it is able to make from userspace into the kernel. Kubernetes lets you automatically apply seccomp profiles loaded onto a node to your Pods and containers.Seccomp can improve the security of your workloads by reducing the Linux kernel syscall attack surface available inside containers. The seccomp filter mode leverages BPF to create an allow or deny list of specific syscalls, named profiles.Since Kubernetes 1.27, you can enable the use of RuntimeDefault as the default seccomp profile for all workloads. A security tutorial is available on this topic. In addition, the Kubernetes Security Profiles Operator is a project that facilitates the management and use of seccomp in clusters.Note:Seccomp is only available on Linux nodes.Enabling AppArmor or SELinuxAppArmorAppArmor is a Linux kernel security module that can provide an easy way to implement Mandatory Access Control (MAC) and better auditing through system logs. A default AppArmor profile is enforced on nodes that support it, or a custom profile can be configured. Like seccomp, AppArmor is also configured through profiles, where each profile is either running in enforcing mode, which blocks access to disallowed resources or complain mode, which only reports violations. AppArmor profiles are enforced on a per-container basis, with an annotation, allowing for processes to gain just the right privileges.Note:AppArmor is only available on Linux nodes, and enabled in some Linux distributions.SELinuxSELinux is also a Linux kernel security module that can provide a mechanism for supporting access control security policies, including Mandatory Access Controls (MAC). SELinux labels can be assigned to containers or pods via their securityContext section.Note:SELinux is only available on Linux nodes, and enabled in some Linux distributions.Logs and auditing Audit logs, if enabled, are protected from general access.Pod placement Pod placement is done in accordance with the tiers of sensitivity of the application. Sensitive applications are running isolated on nodes or with specific sandboxed runtimes.Pods that are on different tiers of sensitivity, for example, an application pod and the Kubernetes API server, should be deployed onto separate nodes. The purpose of node isolation is to prevent an application container breakout to directly providing access to applications with higher level of sensitivity to easily pivot within the cluster. This separation should be enforced to prevent pods accidentally being deployed onto the same node. This could be enforced with the following features:Node SelectorsKey-value pairs, as part of the pod specification, that specify which nodes to deploy onto. These can be enforced at the namespace and cluster level with the PodNodeSelector admission controller.PodTolerationRestrictionAn admission controller that allows administrators to restrict permitted tolerations within a namespace. Pods within a namespace may only utilize the tolerations specified on the namespace object annotation keys that provide a set of default and allowed tolerations.RuntimeClassRuntimeClass is a feature for selecting the container runtime configuration. The container runtime configuration is used to run a Pod's containers and can provide more or less isolation from the host at the cost of performance overhead.Secrets ConfigMaps are not used to hold confidential data. Encryption at rest is configured for the Secret API. If appropriate, a mechanism to inject secrets stored in third-party storage is deployed and available. Service account tokens are not mounted in pods that don't require them. Bound service account token volume is in-use instead of non-expiring tokens.Secrets required for pods should be stored within Kubernetes Secrets as opposed to alternatives such as ConfigMap. Secret resources stored within etcd should be encrypted at rest.Pods needing secrets should have these automatically mounted through volumes, preferably stored in memory like with the emptyDir.medium option. Mechanism can be used to also inject secrets from third-party storages as volume, like the Secrets Store CSI Driver. This should be done preferentially as compared to providing the pods service account RBAC access to secrets. This would allow adding secrets into the pod as environment variables or files. Please note that the environment variable method might be more prone to leakage due to crash dumps in logs and the non-confidential nature of environment variable in Linux, as opposed to the permission mechanism on files.Service account tokens should not be mounted into pods that do not require them. This can be configured by setting automountServiceAccountToken to false either within the service account to apply throughout the namespace or specifically for a pod. For Kubernetes v1.22 and above, use Bound Service Accounts for time-bound service account credentials.Images Minimize unnecessary content in container images. Container images are configured to be run as unprivileged user. References to container images are made by sha256 digests (rather than tags) or the provenance of the image is validated by verifying the image's digital signature at deploy time via admission control. Container images are regularly scanned during creation and in deployment, and known vulnerable software is patched.Container image should contain the bare minimum to run the program they package. Preferably, only the program and its dependencies, building the image from the minimal possible base. In particular, image used in production should not contain shells or debugging utilities, as an ephemeral debug container can be used for troubleshooting.Build images to directly start with an unprivileged user by using the USER instruction in Dockerfile. The Security Context allows a container image to be started with a specific user and group with runAsUser and runAsGroup, even if not specified in the image manifest. However, the file permissions in the image layers might make it impossible to just start the process with a new unprivileged user without image modification.Avoid using image tags to reference an image, especially the latest tag, the image behind a tag can be easily modified in a registry. Prefer using the complete sha256 digest which is unique to the image manifest. This policy can be enforced via an ImagePolicyWebhook. Image signatures can also be automatically verified with an admission controller at deploy time to validate their authenticity and integrity.Scanning a container image can prevent critical vulnerabilities from being deployed to the cluster alongside the container image. Image scanning should be completed before deploying a container image to a cluster and is usually done as part of the deployment process in a CI/CD pipeline. The purpose of an image scan is to obtain information about possible vulnerabilities and their prevention in the container image, such as a Common Vulnerability Scoring System (CVSS) score. If the result of the image scans is combined with the pipeline compliance rules, only properly patched container images will end up in Production.Admission controllers An appropriate selection of admission controllers is enabled. A pod security policy is enforced by the Pod Security Admission or/and a webhook admission controller. The admission chain plugins and webhooks are securely configured.Admission controllers can help improve the security of the cluster. However, they can present risks themselves as they extend the API server and should be properly secured.The following lists present a number of admission controllers that could be considered to enhance the security posture of your cluster and application. It includes controllers that may be referenced in other parts of this document.This first group of admission controllers includes plugins enabled by default, consider to leave them enabled unless you know what you are doing:CertificateApprovalPerforms additional authorization checks to ensure the approving user has permission to approve certificate request.CertificateSigningPerforms additional authorization checks to ensure the signing user has permission to sign certificate requests.CertificateSubjectRestrictionRejects any certificate request that specifies a 'group' (or 'organization attribute') of system:masters.LimitRangerEnforces the LimitRange API constraints.MutatingAdmissionWebhookAllows the use of custom controllers through webhooks, these controllers may mutate requests that they review.PodSecurityReplacement for Pod Security Policy, restricts security contexts of deployed Pods.ResourceQuotaEnforces resource quotas to prevent over-usage of resources.ValidatingAdmissionWebhookAllows the use of custom controllers through webhooks, these controllers do not mutate requests that it reviews.The second group includes plugins that are not enabled by default but are in general availability state and are recommended to improve your security posture:DenyServiceExternalIPsRejects all net-new usage of the Service.spec.externalIPs field. This is a mitigation for CVE-2020-8554: Man in the middle using LoadBalancer or ExternalIPs.NodeRestrictionRestricts kubelet's permissions to only modify the pods API resources they own or the node API resource that represent themselves. It also prevents kubelet from using the node-restriction.kubernetes.io/ annotation, which can be used by an attacker with access to the kubelet's credentials to influence pod placement to the controlled node.The third group includes plugins that are not enabled by default but could be considered for certain use cases:AlwaysPullImagesEnforces the usage of the latest version of a tagged image and ensures that the deployer has permissions to use the image.ImagePolicyWebhookAllows enforcing additional controls for images through webhooks.What's nextPrivilege escalation via Pod creation warns you about a specific access control risk; check how you're managing that threat.If you use Kubernetes RBAC, read RBAC Good Practices for further information on authorization.Securing a Cluster for information on protecting a cluster from accidental or malicious access.Cluster Multi-tenancy guide for configuration options recommendations and best practices on multi-tenancy.Blog post "A Closer Look at NSA/CISA Kubernetes Hardening Guidance" for complementary resource on hardening Kubernetes clusters.3.8.15 - Application Security ChecklistBaseline guidelines around ensuring application security on Kubernetes, aimed at application developersThis checklist aims to provide basic guidelines on securing applications running in Kubernetes from a developer's perspective. This list is not meant to be exhaustive and is intended to evolve over time.On how to read and use this document:The order of topics does not reflect an order of priority.Some checklist items are detailed in the paragraph below the list of each section.This checklist assumes that a developer is a Kubernetes cluster user who interacts with namespaced scope objects.Caution:Checklists are not sufficient for attaining a good security posture on their own. A good security posture requires constant attention and improvement, but a checklist can be the first step on the never-ending journey towards security preparedness. Some recommendations in this checklist may be too restrictive or too lax for your specific security needs. Since Kubernetes security is not "one size fits all", each category of checklist items should be evaluated on its merits.Base security hardeningThe following checklist provides base security hardening recommendations that would apply to most applications deploying to Kubernetes.Application design Follow the right security principles when designing applications. Application configured with appropriate QoS class through resource request and limits. Memory limit is set for the workloads with a limit equal to or greater than the request. CPU limit might be set on sensitive workloads.Service account Avoid using the default ServiceAccount. Instead, create ServiceAccounts for each workload or microservice. automountServiceAccountToken should be set to false unless the pod specifically requires access to the Kubernetes API to operate.Pod-level securityContext recommendations Set runAsNonRoot: true. Configure the container to execute as a less privileged user (for example, using runAsUser and runAsGroup), and configure appropriate permissions on files or directories inside the container image. Optionally add a supplementary group with fsGroup to access persistent volumes. The application deploys into a namespace that enforces an appropriate Pod security standard. If you cannot control this enforcement for the cluster(s) where the application is deployed, take this into account either through documentation or additional defense in depth.Container-level securityContext recommendations Disable privilege escalations using allowPrivilegeEscalation: false. Configure the root filesystem to be read-only with readOnlyRootFilesystem: true. Avoid running privileged containers (set privileged: false). Drop all capabilities from the containers and add back only specific ones that are needed for operation of the container.Role Based Access Control (RBAC) Permissions such as create, patch, update and delete should be only granted if necessary. Avoid creating RBAC permissions to create or update roles which can lead to privilege escalation. Review bindings for the system:unauthenticated group and remove them where possible, as this gives access to anyone who can contact the API server at a network level.The create, update and delete verbs should be permitted judiciously. The patch verb if allowed on a Namespace can allow users to update labels on the namespace or deployments which can increase the attack surface.For sensitive workloads, consider providing a recommended ValidatingAdmissionPolicy that further restricts the permitted write actions.Image security Using an image scanning tool to scan an image before deploying containers in the Kubernetes cluster. Use container signing to validate the container image signature before deploying to the Kubernetes cluster.Network policies Configure NetworkPolicies to only allow expected ingress and egress traffic from the pods.Make sure that your cluster provides and enforces NetworkPolicy. If you are writing an application that users will deploy to different clusters, consider whether you can assume that NetworkPolicy is available and enforced.Advanced security hardeningThis section of this guide covers some advanced security hardening points which might be valuable based on different Kubernetes environment setup.Linux container securityConfigure Security Context for the pod-container. Set the Seccomp Profile for a Container. Restrict a Container's Access to Resources with AppArmor. Assign SELinux Labels to a Container.Runtime classes Configure appropriate runtime classes for containers.Note: This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the content guide before submitting a change. More information.Some containers may require a different isolation level from what is provided by the default runtime of the cluster. runtimeClassName can be used in a podspec to define a different runtime class.For sensitive workloads consider using kernel emulation tools like gVisor, or virtualized isolation using a mechanism such as kata-containers.In high trust environments, consider using confidential virtual machines to improve cluster security even further.3.9 - PoliciesManage security and best-practices with policies.Kubernetes policies are configurations that manage other configurations or runtime behaviors. Kubernetes offers various forms of policies, described below:Apply policies using API objectsSome API objects act as policies. Here are some examples:NetworkPolicies can be used to restrict ingress and egress traffic for a workload.LimitRanges manage resource allocation constraints across different object kinds.ResourceQuotas limit resource consumption for a namespace.Apply policies using admission controllersAn admission controller runs in the API server and can validate or mutate API requests. Some admission controllers act to apply policies. For example, the AlwaysPullImages admission controller modifies a new Pod to set the image pull policy to Always.Kubernetes has several built-in admission controllers that are configurable via the API server --enable-admission-plugins flag.Details on admission controllers, with the complete list of available admission controllers, are documented in a dedicated section:Admission ControllersApply policies using ValidatingAdmissionPolicyValidating admission policies allow configurable validation checks to be executed in the API server using the Common Expression Language (CEL). For example, a ValidatingAdmissionPolicy can be used to disallow use of the latest image tag.A ValidatingAdmissionPolicy operates on an API request and can be used to block, audit, and warn users about non-compliant configurations.Details on the ValidatingAdmissionPolicy API, with examples, are documented in a dedicated section:Validating Admission PolicyApply policies using dynamic admission controlDynamic admission controllers (or admission webhooks) run outside the API server as separate applications that register to receive webhooks requests to perform validation or mutation of API requests.Dynamic admission controllers can be used to apply policies on API requests and trigger other policy-based workflows. A dynamic admission controller can perform complex checks including those that require retrieval of other cluster resources and external data. For example, an image verification check can lookup data from OCI registries to validate the container image signatures and attestations.Details on dynamic admission control are documented in a dedicated section:Dynamic Admission ControlImplementationsNote: This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the content guide before submitting a change. More information.Dynamic Admission Controllers that act as flexible policy engines are being developed in the Kubernetes ecosystem, such as:KubewardenKyvernoOPA GatekeeperPolarisApply policies using Kubelet configurationsKubernetes allows configuring the Kubelet on each worker node. Some Kubelet configurations act as policies:Process ID limits and reservations are used to limit and reserve allocatable PIDs.Node Resource Managers can manage compute, memory, and device resources for latency-critical and high-throughput workloads.3.9.1 - Limit RangesBy default, containers run with unbounded compute resources on a Kubernetes cluster. Using Kubernetes resource quotas, administrators (also termed cluster operators) can restrict consumption and creation of cluster resources (such as CPU time, memory, and persistent storage) within a specified namespace. Within a namespace, a Pod can consume as much CPU and memory as is allowed by the ResourceQuotas that apply to that namespace. As a cluster operator, or as a namespace-level administrator, you might also be concerned about making sure that a single object cannot monopolize all available resources within a namespace.A LimitRange is a policy to constrain the resource allocations (limits and requests) that you can specify for each applicable object kind (such as Pod or PersistentVolumeClaim) in a namespace.A LimitRange provides constraints that can:Enforce minimum and maximum compute resources usage per Pod or Container in a namespace.Enforce minimum and maximum storage request per PersistentVolumeClaim in a namespace.Enforce a ratio between request and limit for a resource in a namespace.Set default request/limit for compute resources in a namespace and automatically inject them to Containers at runtime.Kubernetes constrains resource allocations to Pods in a particular namespace whenever there is at least one LimitRange object in that namespace.The name of a LimitRange object must be a valid DNS subdomain name.Constraints on resource limits and requestsThe administrator creates a LimitRange in a namespace.Users create (or try to create) objects in that namespace, such as Pods or PersistentVolumeClaims.First, the LimitRange admission controller applies default request and limit values for all Pods (and their containers) that do not set compute resource requirements.Second, the LimitRange tracks usage to ensure it does not exceed resource minimum, maximum and ratio defined in any LimitRange present in the namespace.If you attempt to create or update an object (Pod or PersistentVolumeClaim) that violates a LimitRange constraint, your request to the API server will fail with anHTTP status code 403 Forbidden and a message explaining the constraint that has been violated.If you add a LimitRange in a namespace that applies to compute-related resources such as cpu and memory, you must specify requests or limits for those values. Otherwise, the system may reject Pod creation.LimitRange validations occur only at Pod admission stage, not on running Pods. If you add or modify a LimitRange, the Pods that already exist in that namespace continue unchanged.If two or more LimitRange objects exist in the namespace, it is not deterministic which default value will be applied.LimitRange and admission checks for PodsA LimitRange does not check the consistency of the default values it applies. This means that a default value for the limit that is set by LimitRange may be less than the request value specified for the container in the spec that a client submits to the API server. If that happens, the final Pod will not be schedulable.For example, you define a LimitRange with below manifest:Note:The following examples operate within the default namespace of your cluster, as the namespace parameter is undefined and the LimitRange scope is limited to the namespace level. This implies that any references or operations within these examples will interact with elements within the default namespace of your cluster. You can override the operating namespace by configuring namespace in the metadata.namespace field.concepts/policy/limit-range/problematic-limit-range.yaml apiVersion: v1 kind: LimitRange metadata: name: cpu-resource-constraint spec: limits: \- default: # this section defines default limits cpu: 500m defaultRequest: # this section defines default requests cpu: 500m max: # max and min define the limit range cpu: "1" min: cpu: 100m type: Container along with a Pod that declares a CPU resource request of 700m, but not a limit:concepts/policy/limit-range/example-conflict-with-limitrange-cpu.yaml apiVersion: v1 kind: Pod metadata: name: example-conflict-with-limitrange-cpu spec: containers: \- name: demo image: registry.k8s.io/pause:3.8 resources: requests: cpu: 700m then that Pod will not be scheduled, failing with an error similar to:Pod "example-conflict-with-limitrange-cpu" is invalid: spec.containers[0].resources.requests: Invalid value: "700m": must be less than or equal to cpu limit If you set both request and limit, then that new Pod will be scheduled successfully even with the same LimitRange in place:concepts/policy/limit-range/example-no-conflict-with-limitrange-cpu.yaml apiVersion: v1 kind: Pod metadata: name: example-no-conflict-with-limitrange-cpu spec: containers: \- name: demo image: registry.k8s.io/pause:3.8 resources: requests: cpu: 700m limits: cpu: 700m Example resource constraintsExamples of policies that could be created using LimitRange are:In a 2 node cluster with a capacity of 8 GiB RAM and 16 cores, constrain Pods in a namespace to request 100m of CPU with a max limit of 500m for CPU and request 200Mi for Memory with a max limit of 600Mi for Memory.Define default CPU limit and request to 150m and memory default request to 300Mi for Containers started with no cpu and memory requests in their specs.In the case where the total limits of the namespace is less than the sum of the limits of the Pods/Containers, there may be contention for resources. In this case, the Containers or Pods will not be created.Neither contention nor changes to a LimitRange will affect already created resources.What's nextFor examples on using limits, see:how to configure minimum and maximum CPU constraints per namespace.how to configure minimum and maximum Memory constraints per namespace.how to configure default CPU Requests and Limits per namespace.how to configure default Memory Requests and Limits per namespace.how to configure minimum and maximum Storage consumption per namespace.a detailed example on configuring quota per namespace.Refer to the LimitRanger design document for context and historical information.3.9.2 - Resource QuotasWhen several users or teams share a cluster with a fixed number of nodes, there is a concern that one team could use more than its fair share of resources.Resource quotas are a tool for administrators to address this concern.A resource quota, defined by a ResourceQuota object, provides constraints that limit aggregate resource consumption per namespace. It can limit the quantity of objects that can be created in a namespace by type, as well as the total amount of compute resources that may be consumed by resources in that namespace.Resource quotas work like this:Different teams work in different namespaces. This can be enforced with RBAC.The administrator creates one ResourceQuota for each namespace.Users create resources (pods, services, etc.) in the namespace, and the quota system tracks usage to ensure it does not exceed hard resource limits defined in a ResourceQuota.If creating or updating a resource violates a quota constraint, the request will fail with HTTP status code 403 FORBIDDEN with a message explaining the constraint that would have been violated.If quotas are enabled in a namespace for compute resources like cpu and memory, users must specify requests or limits for those values; otherwise, the quota system may reject pod creation. Hint: Use the LimitRanger admission controller to force defaults for pods that make no compute resource requirements.See the walkthrough for an example of how to avoid this problem.Note:For cpu and memory resources, ResourceQuotas enforce that every (new) pod in that namespace sets a limit for that resource. If you enforce a resource quota in a namespace for either cpu or memory, you and other clients, must specify either requests or limits for that resource, for every new Pod you submit. If you don't, the control plane may reject admission for that Pod.For other resources: ResourceQuota works and will ignore pods in the namespace without setting a limit or request for that resource. It means that you can create a new pod without limit/request for ephemeral storage if the resource quota limits the ephemeral storage of this namespace.You can use a LimitRange to automatically set a default request for these resources.The name of a ResourceQuota object must be a valid DNS subdomain name.Examples of policies that could be created using namespaces and quotas are:In a cluster with a capacity of 32 GiB RAM, and 16 cores, let team A use 20 GiB and 10 cores, let B use 10GiB and 4 cores, and hold 2GiB and 2 cores in reserve for future allocation.Limit the "testing" namespace to using 1 core and 1GiB RAM. Let the "production" namespace use any amount.In the case where the total capacity of the cluster is less than the sum of the quotas of the namespaces, there may be contention for resources. This is handled on a first-come-first-served basis.Neither contention nor changes to quota will affect already created resources.Enabling Resource QuotaResourceQuota support is enabled by default for many Kubernetes distributions. It is enabled when the API server \--enable-admission-plugins= flag has ResourceQuota as one of its arguments.A resource quota is enforced in a particular namespace when there is a ResourceQuota in that namespace.Compute Resource QuotaYou can limit the total sum of compute resources that can be requested in a given namespace.The following resource types are supported:Resource NameDescriptionlimits.cpuAcross all pods in a non-terminal state, the sum of CPU limits cannot exceed this value.limits.memoryAcross all pods in a non-terminal state, the sum of memory limits cannot exceed this value.requests.cpuAcross all pods in a non-terminal state, the sum of CPU requests cannot exceed this value.requests.memoryAcross all pods in a non-terminal state, the sum of memory requests cannot exceed this value.hugepages-Across all pods in a non-terminal state, the number of huge page requests of the specified size cannot exceed this value.cpuSame as requests.cpumemorySame as requests.memoryResource Quota For Extended ResourcesIn addition to the resources mentioned above, in release 1.10, quota support for extended resources is added.As overcommit is not allowed for extended resources, it makes no sense to specify both requests and limits for the same extended resource in a quota. So for extended resources, only quota items with prefix requests. are allowed.Take the GPU resource as an example, if the resource name is nvidia.com/gpu, and you want to limit the total number of GPUs requested in a namespace to 4, you can define a quota as follows:requests.nvidia.com/gpu: 4See Viewing and Setting Quotas for more details.Storage Resource QuotaYou can limit the total sum of storage resources that can be requested in a given namespace.In addition, you can limit consumption of storage resources based on associated storage-class.Resource NameDescriptionrequests.storageAcross all persistent volume claims, the sum of storage requests cannot exceed this value.persistentvolumeclaimsThe total number of PersistentVolumeClaims that can exist in the namespace..storageclass.storage.k8s.io/requests.storageAcross all persistent volume claims associated with the , the sum of storage requests cannot exceed this value..storageclass.storage.k8s.io/persistentvolumeclaimsAcross all persistent volume claims associated with the , the total number of persistent volume claims that can exist in the namespace.For example, if you want to quota storage with gold StorageClass separate from a bronze StorageClass, you can define a quota as follows:gold.storageclass.storage.k8s.io/requests.storage: 500Gibronze.storageclass.storage.k8s.io/requests.storage: 100GiIn release 1.8, quota support for local ephemeral storage is added as an alpha feature:Resource NameDescriptionrequests.ephemeral-storageAcross all pods in the namespace, the sum of local ephemeral storage requests cannot exceed this value.limits.ephemeral-storageAcross all pods in the namespace, the sum of local ephemeral storage limits cannot exceed this value.ephemeral-storageSame as requests.ephemeral-storage.Note:When using a CRI container runtime, container logs will count against the ephemeral storage quota. This can result in the unexpected eviction of pods that have exhausted their storage quotas. Refer to Logging Architecture for details.Object Count QuotaYou can set quota for the total number of one particular resource kind in the Kubernetes API, using the following syntax:count/. for resources from non-core groupscount/ for resources from the core groupHere is an example set of resources users may want to put under object count quota:count/persistentvolumeclaimscount/servicescount/secretscount/configmapscount/replicationcontrollerscount/deployments.appscount/replicasets.appscount/statefulsets.appscount/jobs.batchcount/cronjobs.batchIf you define a quota this way, it applies to Kubernetes' APIs that are part of the API server, and to any custom resources backed by a CustomResourceDefinition. If you use API aggregation to add additional, custom APIs that are not defined as CustomResourceDefinitions, the core Kubernetes control plane does not enforce quota for the aggregated API. The extension API server is expected to provide quota enforcement if that's appropriate for the custom API. For example, to create a quota on a widgets custom resource in the example.com API group, use count/widgets.example.com.When using such a resource quota (nearly for all object kinds), an object is charged against the quota if the object kind exists (is defined) in the control plane. These types of quotas are useful to protect against exhaustion of storage resources. For example, you may want to limit the number of Secrets in a server given their large size. Too many Secrets in a cluster can actually prevent servers and controllers from starting. You can set a quota for Jobs to protect against a poorly configured CronJob. CronJobs that create too many Jobs in a namespace can lead to a denial of service.There is another syntax only to set the same type of quota for certain resources. The following types are supported:Resource NameDescriptionconfigmapsThe total number of ConfigMaps that can exist in the namespace.persistentvolumeclaimsThe total number of PersistentVolumeClaims that can exist in the namespace.podsThe total number of Pods in a non-terminal state that can exist in the namespace. A pod is in a terminal state if .status.phase in (Failed, Succeeded) is true.replicationcontrollersThe total number of ReplicationControllers that can exist in the namespace.resourcequotasThe total number of ResourceQuotas that can exist in the namespace.servicesThe total number of Services that can exist in the namespace.services.loadbalancersThe total number of Services of type LoadBalancer that can exist in the namespace.services.nodeportsThe total number of NodePorts allocated to Services of type NodePort or LoadBalancer that can exist in the namespace.secretsThe total number of Secrets that can exist in the namespace.For example, pods quota counts and enforces a maximum on the number of pods created in a single namespace that are not terminal. You might want to set a pods quota on a namespace to avoid the case where a user creates many small pods and exhausts the cluster's supply of Pod IPs.You can find more examples on Viewing and Setting Quotas.Quota ScopesEach quota can have an associated set of scopes. A quota will only measure usage for a resource if it matches the intersection of enumerated scopes.When a scope is added to the quota, it limits the number of resources it supports to those that pertain to the scope. Resources specified on the quota outside of the allowed set results in a validation error.ScopeDescriptionTerminatingMatch pods where .spec.activeDeadlineSeconds >= 0NotTerminatingMatch pods where .spec.activeDeadlineSeconds is nilBestEffortMatch pods that have best effort quality of service.NotBestEffortMatch pods that do not have best effort quality of service.PriorityClassMatch pods that references the specified priority class.CrossNamespacePodAffinityMatch pods that have cross-namespace pod (anti)affinity terms.VolumeAttributesClassMatch persistentvolumeclaims that references the specified volume attributes class.The BestEffort scope restricts a quota to tracking the following resource:podsThe Terminating, NotTerminating, NotBestEffort and PriorityClass scopes restrict a quota to tracking the following resources:podscpumemoryrequests.cpurequests.memorylimits.cpulimits.memoryNote that you cannot specify both the Terminating and the NotTerminating scopes in the same quota, and you cannot specify both the BestEffort and NotBestEffort scopes in the same quota either.The scopeSelector supports the following values in the operator field:InNotInExistsDoesNotExistWhen using one of the following values as the scopeName when defining the scopeSelector, the operator must be Exists.TerminatingNotTerminatingBestEffortNotBestEffortIf the operator is In or NotIn, the values field must have at least one value. For example: scopeSelector: matchExpressions: \- scopeName: PriorityClass operator: In values: \- middle If the operator is Exists or DoesNotExist, the values field must NOT be specified.Resource Quota Per PriorityClassFEATURE STATE: Kubernetes v1.17 [stable]Pods can be created at a specific priority. You can control a pod's consumption of system resources based on a pod's priority, by using the scopeSelector field in the quota spec.A quota is matched and consumed only if scopeSelector in the quota spec selects the pod.When quota is scoped for priority class using scopeSelector field, quota object is restricted to track only following resources:podscpumemoryephemeral-storagelimits.cpulimits.memorylimits.ephemeral-storagerequests.cpurequests.memoryrequests.ephemeral-storageThis example creates a quota object and matches it with pods at specific priorities. The example works as follows:Pods in the cluster have one of the three priority classes, "low", "medium", "high".One quota object is created for each priority.Save the following YAML to a file quota.yaml.policy/quota.yaml apiVersion: v1 kind: List items: \- apiVersion: v1 kind: ResourceQuota metadata: name: pods-high spec: hard: cpu: "1000" memory: "200Gi" pods: "10" scopeSelector: matchExpressions: \- operator: In scopeName: PriorityClass values: ["high"] \- apiVersion: v1 kind: ResourceQuota metadata: name: pods-medium spec: hard: cpu: "10" memory: "20Gi" pods: "10" scopeSelector: matchExpressions: \- operator: In scopeName: PriorityClass values: ["medium"] \- apiVersion: v1 kind: ResourceQuota metadata: name: pods-low spec: hard: cpu: "5" memory: "10Gi" pods: "10" scopeSelector: matchExpressions: \- operator: In scopeName: PriorityClass values: ["low"] Apply the YAML using kubectl create.kubectl create -f ./quota.yaml resourcequota/pods-high created resourcequota/pods-medium created resourcequota/pods-low created Verify that Used quota is 0 using kubectl describe quota.kubectl describe quota Name: pods-high Namespace: default Resource Used Hard \-------- ---- ---- cpu 0 1k memory 0 200Gi pods 0 10 Name: pods-low Namespace: default Resource Used Hard \-------- ---- ---- cpu 0 5 memory 0 10Gi pods 0 10 Name: pods-medium Namespace: default Resource Used Hard \-------- ---- ---- cpu 0 10 memory 0 20Gi pods 0 10 Create a pod with priority "high". Save the following YAML to a file high-priority-pod.yaml.policy/high-priority-pod.yaml apiVersion: v1 kind: Pod metadata: name: high-priority spec: containers: \- name: high-priority image: ubuntu command: ["/bin/sh"] args: ["-c", "while true; do echo hello; sleep 10;done"] resources: requests: memory: "10Gi" cpu: "500m" limits: memory: "10Gi" cpu: "500m" priorityClassName: high Apply it with kubectl create.kubectl create -f ./high-priority-pod.yaml Verify that "Used" stats for "high" priority quota, pods-high, has changed and that the other two quotas are unchanged.kubectl describe quota Name: pods-high Namespace: default Resource Used Hard \-------- ---- ---- cpu 500m 1k memory 10Gi 200Gi pods 1 10 Name: pods-low Namespace: default Resource Used Hard \-------- ---- ---- cpu 0 5 memory 0 10Gi pods 0 10 Name: pods-medium Namespace: default Resource Used Hard \-------- ---- ---- cpu 0 10 memory 0 20Gi pods 0 10 Cross-namespace Pod Affinity QuotaFEATURE STATE: Kubernetes v1.24 [stable]Operators can use CrossNamespacePodAffinity quota scope to limit which namespaces are allowed to have pods with affinity terms that cross namespaces. Specifically, it controls which pods are allowed to set namespaces or namespaceSelector fields in pod affinity terms.Preventing users from using cross-namespace affinity terms might be desired since a pod with anti-affinity constraints can block pods from all other namespaces from getting scheduled in a failure domain.Using this scope operators can prevent certain namespaces (foo-ns in the example below) from having pods that use cross-namespace pod affinity by creating a resource quota object in that namespace with CrossNamespacePodAffinity scope and hard limit of 0:apiVersion: v1 kind: ResourceQuota metadata: name: disable-cross-namespace-affinity namespace: foo-ns spec: hard: pods: "0" scopeSelector: matchExpressions: \- scopeName: CrossNamespacePodAffinity operator: Exists If operators want to disallow using namespaces and namespaceSelector by default, and only allow it for specific namespaces, they could configure CrossNamespacePodAffinity as a limited resource by setting the kube-apiserver flag --admission-control-config-file to the path of the following configuration file:apiVersion: apiserver.config.k8s.io/v1 kind: AdmissionConfiguration plugins: \- name: "ResourceQuota" configuration: apiVersion: apiserver.config.k8s.io/v1 kind: ResourceQuotaConfiguration limitedResources: \- resource: pods matchScopes: \- scopeName: CrossNamespacePodAffinity operator: Exists With the above configuration, pods can use namespaces and namespaceSelector in pod affinity only if the namespace where they are created have a resource quota object with CrossNamespacePodAffinity scope and a hard limit greater than or equal to the number of pods using those fields.Resource Quota Per VolumeAttributesClassFEATURE STATE: Kubernetes v1.31 [beta] (enabled by default: false)PersistentVolumeClaims can be created with a specific volume attributes class, and might be modified after creation. You can control a PVC's consumption of storage resources based on the associated volume attributes classes, by using the scopeSelector field in the quota spec.The PVC references the associated volume attributes class by the following fields:spec.volumeAttributesClassNamestatus.currentVolumeAttributesClassNamestatus.modifyVolumeStatus.targetVolumeAttributesClassNameA quota is matched and consumed only if scopeSelector in the quota spec selects the PVC.When the quota is scoped for the volume attributes class using the scopeSelector field, the quota object is restricted to track only the following resources:persistentvolumeclaimsrequests.storageThis example creates a quota object and matches it with PVC at specific volume attributes classes. The example works as follows:PVCs in the cluster have at least one of the three volume attributes classes, "gold", "silver", "copper".One quota object is created for each volume attributes class.Save the following YAML to a file quota-vac.yaml.policy/quota-vac.yaml apiVersion: v1 kind: List items: \- apiVersion: v1 kind: ResourceQuota metadata: name: pvcs-gold spec: hard: requests.storage: "10Gi" persistentvolumeclaims: "10" scopeSelector: matchExpressions: \- operator: In scopeName: VolumeAttributesClass values: ["gold"] \- apiVersion: v1 kind: ResourceQuota metadata: name: pvcs-silver spec: hard: requests.storage: "20Gi" persistentvolumeclaims: "10" scopeSelector: matchExpressions: \- operator: In scopeName: VolumeAttributesClass values: ["silver"] \- apiVersion: v1 kind: ResourceQuota metadata: name: pvcs-copper spec: hard: requests.storage: "30Gi" persistentvolumeclaims: "10" scopeSelector: matchExpressions: \- operator: In scopeName: VolumeAttributesClass values: ["copper"] Apply the YAML using kubectl create.kubectl create -f ./quota-vac.yaml resourcequota/pvcs-gold created resourcequota/pvcs-silver created resourcequota/pvcs-copper created Verify that Used quota is 0 using kubectl describe quota.kubectl describe quota Name: pvcs-gold Namespace: default Resource Used Hard \-------- ---- ---- persistentvolumeclaims 0 10 requests.storage 0 10Gi Name: pvcs-silver Namespace: default Resource Used Hard \-------- ---- ---- persistentvolumeclaims 0 10 requests.storage 0 20Gi Name: pvcs-copper Namespace: default Resource Used Hard \-------- ---- ---- persistentvolumeclaims 0 10 requests.storage 0 30Gi Create a pvc with volume attributes class "gold". Save the following YAML to a file gold-vac-pvc.yaml.policy/gold-vac-pvc.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: gold-vac-pvc spec: accessModes: \- ReadWriteOnce resources: requests: storage: 2Gi storageClassName: # change this to the name of the storage class you want to use volumeAttributesClassName: gold Apply it with kubectl create.kubectl create -f ./gold-vac-pvc.yaml Verify that "Used" stats for "gold" volume attributes class quota, pvcs-gold has changed and that the other two quotas are unchanged.kubectl describe quota Name: pvcs-gold Namespace: default Resource Used Hard \-------- ---- ---- persistentvolumeclaims 1 10 requests.storage 2Gi 10Gi Name: pvcs-silver Namespace: default Resource Used Hard \-------- ---- ---- persistentvolumeclaims 0 10 requests.storage 0 20Gi Name: pvcs-copper Namespace: default Resource Used Hard \-------- ---- ---- persistentvolumeclaims 0 10 requests.storage 0 30Gi Once the PVC is bound, it is allowed to modify the desired volume attributes class. Let's change it to "silver" with kubectl patch.kubectl patch pvc gold-vac-pvc --type='merge' -p '{"spec":{"volumeAttributesClassName":"silver"}}' Verify that "Used" stats for "silver" volume attributes class quota, pvcs-silver has changed, pvcs-copper is unchanged, and pvcs-gold might be unchanged or released, which depends on the PVC's status.kubectl describe quota Name: pvcs-gold Namespace: default Resource Used Hard \-------- ---- ---- persistentvolumeclaims 1 10 requests.storage 2Gi 10Gi Name: pvcs-silver Namespace: default Resource Used Hard \-------- ---- ---- persistentvolumeclaims 1 10 requests.storage 2Gi 20Gi Name: pvcs-copper Namespace: default Resource Used Hard \-------- ---- ---- persistentvolumeclaims 0 10 requests.storage 0 30Gi Let's change it to "copper" with kubectl patch.kubectl patch pvc gold-vac-pvc --type='merge' -p '{"spec":{"volumeAttributesClassName":"copper"}}' Verify that "Used" stats for "copper" volume attributes class quota, pvcs-copper has changed, pvcs-silver and pvcs-gold might be unchanged or released, which depends on the PVC's status.kubectl describe quota Name: pvcs-gold Namespace: default Resource Used Hard \-------- ---- ---- persistentvolumeclaims 1 10 requests.storage 2Gi 10Gi Name: pvcs-silver Namespace: default Resource Used Hard \-------- ---- ---- persistentvolumeclaims 1 10 requests.storage 2Gi 20Gi Name: pvcs-copper Namespace: default Resource Used Hard \-------- ---- ---- persistentvolumeclaims 1 10 requests.storage 2Gi 30Gi Print the manifest of the PVC using the following command:kubectl get pvc gold-vac-pvc -o yaml It might show the following output:apiVersion: v1 kind: PersistentVolumeClaim metadata: name: gold-vac-pvc spec: accessModes: \- ReadWriteOnce resources: requests: storage: 2Gi storageClassName: default volumeAttributesClassName: copper status: accessModes: \- ReadWriteOnce capacity: storage: 2Gi currentVolumeAttributesClassName: gold phase: Bound modifyVolumeStatus: status: InProgress targetVolumeAttributesClassName: silver storageClassName: default Wait a moment for the volume modification to complete, then verify the quota again.kubectl describe quota Name: pvcs-gold Namespace: default Resource Used Hard \-------- ---- ---- persistentvolumeclaims 0 10 requests.storage 0 10Gi Name: pvcs-silver Namespace: default Resource Used Hard \-------- ---- ---- persistentvolumeclaims 0 10 requests.storage 0 20Gi Name: pvcs-copper Namespace: default Resource Used Hard \-------- ---- ---- persistentvolumeclaims 1 10 requests.storage 2Gi 30Gi Requests compared to LimitsWhen allocating compute resources, each container may specify a request and a limit value for either CPU or memory. The quota can be configured to quota either value.If the quota has a value specified for requests.cpu or requests.memory, then it requires that every incoming container makes an explicit request for those resources. If the quota has a value specified for limits.cpu or limits.memory, then it requires that every incoming container specifies an explicit limit for those resources.Viewing and Setting Quotaskubectl supports creating, updating, and viewing quotas:kubectl create namespace myspace cat < compute-resources.yaml apiVersion: v1 kind: ResourceQuota metadata: name: compute-resources spec: hard: requests.cpu: "1" requests.memory: "1Gi" limits.cpu: "2" limits.memory: "2Gi" requests.nvidia.com/gpu: 4 EOF kubectl create -f ./compute-resources.yaml --namespace=myspace cat < object-counts.yaml apiVersion: v1 kind: ResourceQuota metadata: name: object-counts spec: hard: configmaps: "10" persistentvolumeclaims: "4" pods: "4" replicationcontrollers: "20" secrets: "10" services: "10" services.loadbalancers: "2" EOF kubectl create -f ./object-counts.yaml --namespace=myspace kubectl get quota --namespace=myspace NAME AGE compute-resources 30s object-counts 32s kubectl describe quota compute-resources --namespace=myspace Name: compute-resources Namespace: myspace Resource Used Hard \-------- ---- ---- limits.cpu 0 2 limits.memory 0 2Gi requests.cpu 0 1 requests.memory 0 1Gi requests.nvidia.com/gpu 0 4 kubectl describe quota object-counts --namespace=myspace Name: object-counts Namespace: myspace Resource Used Hard \-------- ---- ---- configmaps 0 10 persistentvolumeclaims 0 4 pods 0 4 replicationcontrollers 0 20 secrets 1 10 services 0 10 services.loadbalancers 0 2 kubectl also supports object count quota for all standard namespaced resources using the syntax count/.:kubectl create namespace myspace kubectl create quota test --hard=count/deployments.apps=2,count/replicasets.apps=4,count/pods=3,count/secrets=4 --namespace=myspace kubectl create deployment nginx --image=nginx --namespace=myspace --replicas=2 kubectl describe quota --namespace=myspace Name: test Namespace: myspace Resource Used Hard \-------- ---- ---- count/deployments.apps 1 2 count/pods 2 3 count/replicasets.apps 1 4 count/secrets 1 4 Quota and Cluster CapacityResourceQuotas are independent of the cluster capacity. They are expressed in absolute units. So, if you add nodes to your cluster, this does not automatically give each namespace the ability to consume more resources.Sometimes more complex policies may be desired, such as:Proportionally divide total cluster resources among several teams.Allow each tenant to grow resource usage as needed, but have a generous limit to prevent accidental resource exhaustion.Detect demand from one namespace, add nodes, and increase quota.Such policies could be implemented using ResourceQuotas as building blocks, by writing a "controller" that watches the quota usage and adjusts the quota hard limits of each namespace according to other signals.Note that resource quota divides up aggregate cluster resources, but it creates no restrictions around nodes: pods from several namespaces may run on the same node.Limit Priority Class consumption by defaultIt may be desired that pods at a particular priority, such as "cluster-services", should be allowed in a namespace, if and only if, a matching quota object exists.With this mechanism, operators are able to restrict usage of certain high priority classes to a limited number of namespaces and not every namespace will be able to consume these priority classes by default.To enforce this, kube-apiserver flag --admission-control-config-file should be used to pass path to the following configuration file:apiVersion: apiserver.config.k8s.io/v1 kind: AdmissionConfiguration plugins: \- name: "ResourceQuota" configuration: apiVersion: apiserver.config.k8s.io/v1 kind: ResourceQuotaConfiguration limitedResources: \- resource: pods matchScopes: \- scopeName: PriorityClass operator: In values: ["cluster-services"] Then, create a resource quota object in the kube-system namespace:policy/priority-class-resourcequota.yaml apiVersion: v1 kind: ResourceQuota metadata: name: pods-cluster-services spec: scopeSelector: matchExpressions: \- operator : In scopeName: PriorityClass values: ["cluster-services"]kubectl apply -f https://k8s.io/examples/policy/priority-class-resourcequota.yaml -n kube-system resourcequota/pods-cluster-services created In this case, a pod creation will be allowed if:the Pod's priorityClassName is not specified.the Pod's priorityClassName is specified to a value other than cluster-services.the Pod's priorityClassName is set to cluster-services, it is to be created in the kube-system namespace, and it has passed the resource quota check.A Pod creation request is rejected if its priorityClassName is set to cluster-services and it is to be created in a namespace other than kube-system.What's nextSee ResourceQuota design document for more information.See a detailed example for how to use resource quota.Read Quota support for priority class design document.See LimitedResources.3.9.3 - Process ID Limits And ReservationsFEATURE STATE: Kubernetes v1.20 [stable]Kubernetes allow you to limit the number of process IDs (PIDs) that a Pod can use. You can also reserve a number of allocatable PIDs for each node for use by the operating system and daemons (rather than by Pods).Process IDs (PIDs) are a fundamental resource on nodes. It is trivial to hit the task limit without hitting any other resource limits, which can then cause instability to a host machine.Cluster administrators require mechanisms to ensure that Pods running in the cluster cannot induce PID exhaustion that prevents host daemons (such as the kubelet or kube-proxy, and potentially also the container runtime) from running. In addition, it is important to ensure that PIDs are limited among Pods in order to ensure they have limited impact on other workloads on the same node.Note:On certain Linux installations, the operating system sets the PIDs limit to a low default, such as 32768. Consider raising the value of /proc/sys/kernel/pid_max.You can configure a kubelet to limit the number of PIDs a given Pod can consume. For example, if your node's host OS is set to use a maximum of 262144 PIDs and expect to host less than 250 Pods, one can give each Pod a budget of 1000 PIDs to prevent using up that node's overall number of available PIDs. If the admin wants to overcommit PIDs similar to CPU or memory, they may do so as well with some additional risks. Either way, a single Pod will not be able to bring the whole machine down. This kind of resource limiting helps to prevent simple fork bombs from affecting operation of an entire cluster.Per-Pod PID limiting allows administrators to protect one Pod from another, but does not ensure that all Pods scheduled onto that host are unable to impact the node overall. Per-Pod limiting also does not protect the node agents themselves from PID exhaustion.You can also reserve an amount of PIDs for node overhead, separate from the allocation to Pods. This is similar to how you can reserve CPU, memory, or other resources for use by the operating system and other facilities outside of Pods and their containers.PID limiting is an important sibling to compute resource requests and limits. However, you specify it in a different way: rather than defining a Pod's resource limit in the .spec for a Pod, you configure the limit as a setting on the kubelet. Pod-defined PID limits are not currently supported.Caution:This means that the limit that applies to a Pod may be different depending on where the Pod is scheduled. To make things simple, it's easiest if all Nodes use the same PID resource limits and reservations.Node PID limitsKubernetes allows you to reserve a number of process IDs for the system use. To configure the reservation, use the parameter pid= in the \--system-reserved and --kube-reserved command line options to the kubelet. The value you specified declares that the specified number of process IDs will be reserved for the system as a whole and for Kubernetes system daemons respectively.Pod PID limitsKubernetes allows you to limit the number of processes running in a Pod. You specify this limit at the node level, rather than configuring it as a resource limit for a particular Pod. Each Node can have a different PID limit.To configure the limit, you can specify the command line parameter --pod-max-pids to the kubelet, or set PodPidsLimit in the kubelet configuration file.PID based evictionYou can configure kubelet to start terminating a Pod when it is misbehaving and consuming abnormal amount of resources. This feature is called eviction. You can Configure Out of Resource Handling for various eviction signals. Use pid.available eviction signal to configure the threshold for number of PIDs used by Pod. You can set soft and hard eviction policies. However, even with the hard eviction policy, if the number of PIDs growing very fast, node can still get into unstable state by hitting the node PIDs limit. Eviction signal value is calculated periodically and does NOT enforce the limit.PID limiting - per Pod and per Node sets the hard limit. Once the limit is hit, workload will start experiencing failures when trying to get a new PID. It may or may not lead to rescheduling of a Pod, depending on how workload reacts on these failures and how liveness and readiness probes are configured for the Pod. However, if limits were set correctly, you can guarantee that other Pods workload and system processes will not run out of PIDs when one Pod is misbehaving.What's nextRefer to the PID Limiting enhancement document for more information.For historical context, read Process ID Limiting for Stability Improvements in Kubernetes 1.14.Read Managing Resources for Containers.Learn how to Configure Out of Resource Handling.3.9.4 - Node Resource ManagersIn order to support latency-critical and high-throughput workloads, Kubernetes offers a suite of Resource Managers. The managers aim to co-ordinate and optimise the alignment of node's resources for pods configured with a specific requirement for CPUs, devices, and memory (hugepages) resources.Hardware topology alignment policiesTopology Manager is a kubelet component that aims to coordinate the set of components that are responsible for these optimizations. The overall resource management process is governed using the policy you specify. To learn more, read Control Topology Management Policies on a Node.Policies for assigning CPUs to PodsFEATURE STATE: Kubernetes v1.26 [stable] (enabled by default: true)Once a Pod is bound to a Node, the kubelet on that node may need to either multiplex the existing hardware (for example, sharing CPUs across multiple Pods) or allocate hardware by dedicating some resource (for example, assigning one of more CPUs for a Pod's exclusive use).By default, the kubelet uses CFS quota to enforce pod CPU limits. When the node runs many CPU-bound pods, the workload can move to different CPU cores depending on whether the pod is throttled and which CPU cores are available at scheduling time. Many workloads are not sensitive to this migration and thus work fine without any intervention.However, in workloads where CPU cache affinity and scheduling latency significantly affect workload performance, the kubelet allows alternative CPU management policies to determine some placement preferences on the node. This is implemented using the CPU Manager and its policy. There are two available policies:none: the none policy explicitly enables the existing default CPU affinity scheme, providing no affinity beyond what the OS scheduler does automatically. Limits on CPU usage for Guaranteed pods and Burstable pods are enforced using CFS quota.static: the static policy allows containers in Guaranteed pods with integer CPU requests access to exclusive CPUs on the node. This exclusivity is enforced using the cpuset cgroup controller.Note:System services such as the container runtime and the kubelet itself can continue to run on these exclusive CPUs. The exclusivity only extends to other pods.CPU Manager doesn't support offlining and onlining of CPUs at runtime.Static policyThe static policy enables finer-grained CPU management and exclusive CPU assignment. This policy manages a shared pool of CPUs that initially contains all CPUs in the node. The amount of exclusively allocatable CPUs is equal to the total number of CPUs in the node minus any CPU reservations set by the kubelet configuration. CPUs reserved by these options are taken, in integer quantity, from the initial shared pool in ascending order by physical core ID. This shared pool is the set of CPUs on which any containers in BestEffort and Burstable pods run. Containers in Guaranteed pods with fractional CPU requests also run on CPUs in the shared pool. Only containers that are part of a Guaranteed pod and have integer CPU requests are assigned exclusive CPUs.Note:The kubelet requires a CPU reservation greater than zero when the static policy is enabled. This is because a zero CPU reservation would allow the shared pool to become empty.As Guaranteed pods whose containers fit the requirements for being statically assigned are scheduled to the node, CPUs are removed from the shared pool and placed in the cpuset for the container. CFS quota is not used to bound the CPU usage of these containers as their usage is bound by the scheduling domain itself. In others words, the number of CPUs in the container cpuset is equal to the integer CPU limit specified in the pod spec. This static assignment increases CPU affinity and decreases context switches due to throttling for the CPU-bound workload.Consider the containers in the following pod specs:spec: containers: \- name: nginx image: nginx The pod above runs in the BestEffort QoS class because no resource requests or limits are specified. It runs in the shared pool.spec: containers: \- name: nginx image: nginx resources: limits: memory: "200Mi" requests: memory: "100Mi" The pod above runs in the Burstable QoS class because resource requests do not equal limits and the cpu quantity is not specified. It runs in the shared pool.spec: containers: \- name: nginx image: nginx resources: limits: memory: "200Mi" cpu: "2" requests: memory: "100Mi" cpu: "1" The pod above runs in the Burstable QoS class because resource requests do not equal limits. It runs in the shared pool.spec: containers: \- name: nginx image: nginx resources: limits: memory: "200Mi" cpu: "2" requests: memory: "200Mi" cpu: "2" The pod above runs in the Guaranteed QoS class because requests are equal to limits. And the container's resource limit for the CPU resource is an integer greater than or equal to one. The nginx container is granted 2 exclusive CPUs.spec: containers: \- name: nginx image: nginx resources: limits: memory: "200Mi" cpu: "1.5" requests: memory: "200Mi" cpu: "1.5" The pod above runs in the Guaranteed QoS class because requests are equal to limits. But the container's resource limit for the CPU resource is a fraction. It runs in the shared pool.spec: containers: \- name: nginx image: nginx resources: limits: memory: "200Mi" cpu: "2" The pod above runs in the Guaranteed QoS class because only limits are specified and requests are set equal to limits when not explicitly specified. And the container's resource limit for the CPU resource is an integer greater than or equal to one. The nginx container is granted 2 exclusive CPUs.Static policy optionsHere are the available policy options for the static CPU management policy, listed in alphabetical order:align-by-socket (alpha, hidden by default)Align CPUs by physical package / socket boundary, rather than logical NUMA boundaries (available since Kubernetes v1.25)distribute-cpus-across-cores (alpha, hidden by default)Allocate virtual cores, sometimes called hardware threads, across different physical cores (available since Kubernetes v1.31)distribute-cpus-across-numa (beta, visible by default)Spread CPUs across different NUMA domains, aiming for an even balance between the selected domains (available since Kubernetes v1.23)full-pcpus-only (GA, visible by default)Always allocate full physical cores (available since Kubernetes v1.22, GA since Kubernetes v1.33)strict-cpu-reservation (beta, visible by default)Prevent all the pods regardless of their Quality of Service class to run on reserved CPUs (available since Kubernetes v1.32)prefer-align-cpus-by-uncorecache (alpha, hidden by default)Align CPUs by uncore (Last-Level) cache boundary on a best-effort way (available since Kubernetes v1.32)You can toggle groups of options on and off based upon their maturity level using the following feature gates:CPUManagerPolicyBetaOptions (default enabled). Disable to hide beta-level options.CPUManagerPolicyAlphaOptions (default disabled). Enable to show alpha-level options.You will still have to enable each option using the cpuManagerPolicyOptions field in the kubelet configuration file.For more detail about the individual options you can configure, read on.full-pcpus-onlyIf the full-pcpus-only policy option is specified, the static policy will always allocate full physical cores. By default, without this option, the static policy allocates CPUs using a topology-aware best-fit allocation. On SMT enabled systems, the policy can allocate individual virtual cores, which correspond to hardware threads. This can lead to different containers sharing the same physical cores; this behaviour in turn contributes to the noisy neighbours problem. With the option enabled, the pod will be admitted by the kubelet only if the CPU request of all its containers can be fulfilled by allocating full physical cores. If the pod does not pass the admission, it will be put in Failed state with the message SMTAlignmentError.distribute-cpus-across-numaIf the distribute-cpus-across-numapolicy option is specified, the static policy will evenly distribute CPUs across NUMA nodes in cases where more than one NUMA node is required to satisfy the allocation. By default, the CPUManager will pack CPUs onto one NUMA node until it is filled, with any remaining CPUs simply spilling over to the next NUMA node. This can cause undesired bottlenecks in parallel code relying on barriers (and similar synchronization primitives), as this type of code tends to run only as fast as its slowest worker (which is slowed down by the fact that fewer CPUs are available on at least one NUMA node). By distributing CPUs evenly across NUMA nodes, application developers can more easily ensure that no single worker suffers from NUMA effects more than any other, improving the overall performance of these types of applications.align-by-socketIf the align-by-socket policy option is specified, CPUs will be considered aligned at the socket boundary when deciding how to allocate CPUs to a container. By default, the CPUManager aligns CPU allocations at the NUMA boundary, which could result in performance degradation if CPUs need to be pulled from more than one NUMA node to satisfy the allocation. Although it tries to ensure that all CPUs are allocated from the minimum number of NUMA nodes, there is no guarantee that those NUMA nodes will be on the same socket. By directing the CPUManager to explicitly align CPUs at the socket boundary rather than the NUMA boundary, we are able to avoid such issues. Note, this policy option is not compatible with TopologyManager single-numa-node policy and does not apply to hardware where the number of sockets is greater than number of NUMA nodes.distribute-cpus-across-coresIf the distribute-cpus-across-cores policy option is specified, the static policy will attempt to allocate virtual cores (hardware threads) across different physical cores. By default, the CPUManager tends to pack CPUs onto as few physical cores as possible, which can lead to contention among CPUs on the same physical core and result in performance bottlenecks. By enabling the distribute-cpus-across-cores policy, the static policy ensures that CPUs are distributed across as many physical cores as possible, reducing the contention on the same physical core and thereby improving overall performance. However, it is important to note that this strategy might be less effective when the system is heavily loaded. Under such conditions, the benefit of reducing contention diminishes. Conversely, default behavior can help in reducing inter-core communication overhead, potentially providing better performance under high load conditions.strict-cpu-reservationThe reservedSystemCPUs parameter in KubeletConfiguration, or the deprecated kubelet command line option --reserved-cpus, defines an explicit CPU set for OS system daemons and kubernetes system daemons. More details of this parameter can be found on the Explicitly Reserved CPU List page. By default, this isolation is implemented only for guaranteed pods with integer CPU requests not for burstable and best-effort pods (and guaranteed pods with fractional CPU requests). Admission is only comparing the CPU requests against the allocatable CPUs. Since the CPU limit is higher than the request, the default behaviour allows burstable and best-effort pods to use up the capacity of reservedSystemCPUs and cause host OS services to starve in real life deployments. If the strict-cpu-reservation policy option is enabled, the static policy will not allow any workload to use the CPU cores specified in reservedSystemCPUs.prefer-align-cpus-by-uncorecacheIf the prefer-align-cpus-by-uncorecache policy is specified, the static policy will allocate CPU resources for individual containers such that all CPUs assigned to a container share the same uncore cache block (also known as the Last-Level Cache or LLC). By default, the CPUManager will tightly pack CPU assignments which can result in containers being assigned CPUs from multiple uncore caches. This option enables the CPUManager to allocate CPUs in a way that maximizes the efficient use of the uncore cache. Allocation is performed on a best-effort basis, aiming to affine as many CPUs as possible within the same uncore cache. If the container's CPU requirement exceeds the CPU capacity of a single uncore cache, the CPUManager minimizes the number of uncore caches used in order to maintain optimal uncore cache alignment. Specific workloads can benefit in performance from the reduction of inter-cache latency and noisy neighbors at the cache level. If the CPUManager cannot align optimally while the node has sufficient resources, the container will still be admitted using the default packed behavior.Memory Management PoliciesFEATURE STATE: Kubernetes v1.32 [stable] (enabled by default: true)The Kubernetes Memory Manager enables the feature of guaranteed memory (and hugepages) allocation for pods in the Guaranteed QoS class.The Memory Manager employs hint generation protocol to yield the most suitable NUMA affinity for a pod. The Memory Manager feeds the central manager (Topology Manager) with these affinity hints. Based on both the hints and Topology Manager policy, the pod is rejected or admitted to the node.Moreover, the Memory Manager ensures that the memory which a pod requests is allocated from a minimum number of NUMA nodes.Other resource managersThe configuration of individual managers is elaborated in dedicated documents:Device Manager3.10 - Scheduling, Preemption and EvictionIn Kubernetes, scheduling refers to making sure that Pods are matched to Nodes so that the kubelet can run them. Preemption is the process of terminating Pods with lower Priority so that Pods with higher Priority can schedule on Nodes. Eviction is the process of terminating one or more Pods on Nodes.SchedulingKubernetes SchedulerAssigning Pods to NodesPod OverheadPod Topology Spread ConstraintsTaints and TolerationsScheduling FrameworkDynamic Resource AllocationScheduler Performance TuningResource Bin Packing for Extended ResourcesPod Scheduling ReadinessDeschedulerPod DisruptionPod disruption is the process by which Pods on Nodes are terminated either voluntarily or involuntarily.Voluntary disruptions are started intentionally by application owners or cluster administrators. Involuntary disruptions are unintentional and can be triggered by unavoidable issues like Nodes running out of resources, or by accidental deletions.Pod Priority and PreemptionNode-pressure EvictionAPI-initiated Eviction3.10.1 - Kubernetes SchedulerIn Kubernetes, scheduling refers to making sure that Pods are matched to Nodes so that Kubelet can run them.Scheduling overviewA scheduler watches for newly created Pods that have no Node assigned. For every Pod that the scheduler discovers, the scheduler becomes responsible for finding the best Node for that Pod to run on. The scheduler reaches this placement decision taking into account the scheduling principles described below.If you want to understand why Pods are placed onto a particular Node, or if you're planning to implement a custom scheduler yourself, this page will help you learn about scheduling.kube-schedulerkube-scheduler is the default scheduler for Kubernetes and runs as part of the control plane. kube-scheduler is designed so that, if you want and need to, you can write your own scheduling component and use that instead.Kube-scheduler selects an optimal node to run newly created or not yet scheduled (unscheduled) pods. Since containers in pods - and pods themselves - can have different requirements, the scheduler filters out any nodes that don't meet a Pod's specific scheduling needs. Alternatively, the API lets you specify a node for a Pod when you create it, but this is unusual and is only done in special cases.In a cluster, Nodes that meet the scheduling requirements for a Pod are called feasible nodes. If none of the nodes are suitable, the pod remains unscheduled until the scheduler is able to place it.The scheduler finds feasible Nodes for a Pod and then runs a set of functions to score the feasible Nodes and picks a Node with the highest score among the feasible ones to run the Pod. The scheduler then notifies the API server about this decision in a process called binding.Factors that need to be taken into account for scheduling decisions include individual and collective resource requirements, hardware / software / policy constraints, affinity and anti-affinity specifications, data locality, inter-workload interference, and so on.Node selection in kube-schedulerkube-scheduler selects a node for the pod in a 2-step operation:FilteringScoringThe filtering step finds the set of Nodes where it's feasible to schedule the Pod. For example, the PodFitsResources filter checks whether a candidate Node has enough available resources to meet a Pod's specific resource requests. After this step, the node list contains any suitable Nodes; often, there will be more than one. If the list is empty, that Pod isn't (yet) schedulable.In the scoring step, the scheduler ranks the remaining nodes to choose the most suitable Pod placement. The scheduler assigns a score to each Node that survived filtering, basing this score on the active scoring rules.Finally, kube-scheduler assigns the Pod to the Node with the highest ranking. If there is more than one node with equal scores, kube-scheduler selects one of these at random.There are two supported ways to configure the filtering and scoring behavior of the scheduler:Scheduling Policies allow you to configure Predicates for filtering and Priorities for scoring.Scheduling Profiles allow you to configure Plugins that implement different scheduling stages, including: QueueSort, Filter, Score, Bind, Reserve, Permit, and others. You can also configure the kube-scheduler to run different profiles.What's nextRead about scheduler performance tuningRead about Pod topology spread constraintsRead the reference documentation for kube-schedulerRead the kube-scheduler config (v1) referenceLearn about configuring multiple schedulersLearn about topology management policiesLearn about Pod OverheadLearn about scheduling of Pods that use volumes in:Volume Topology SupportStorage Capacity TrackingNode-specific Volume Limits3.10.2 - Assigning Pods to NodesYou can constrain a Pod so that it is restricted to run on particular node(s), or to prefer to run on particular nodes. There are several ways to do this and the recommended approaches all use label selectors to facilitate the selection. Often, you do not need to set any such constraints; the scheduler will automatically do a reasonable placement (for example, spreading your Pods across nodes so as not place Pods on a node with insufficient free resources). However, there are some circumstances where you may want to control which node the Pod deploys to, for example, to ensure that a Pod ends up on a node with an SSD attached to it, or to co-locate Pods from two different services that communicate a lot into the same availability zone.You can use any of the following methods to choose where Kubernetes schedules specific Pods:nodeSelector field matching against node labelsAffinity and anti-affinitynodeName fieldPod topology spread constraintsNode labelsLike many other Kubernetes objects, nodes have labels. You can attach labels manually. Kubernetes also populates a standard set of labels on all nodes in a cluster.Note:The value of these labels is cloud provider specific and is not guaranteed to be reliable. For example, the value of kubernetes.io/hostname may be the same as the node name in some environments and a different value in other environments.Node isolation/restrictionAdding labels to nodes allows you to target Pods for scheduling on specific nodes or groups of nodes. You can use this functionality to ensure that specific Pods only run on nodes with certain isolation, security, or regulatory properties.If you use labels for node isolation, choose label keys that the kubelet cannot modify. This prevents a compromised node from setting those labels on itself so that the scheduler schedules workloads onto the compromised node.The NodeRestriction admission plugin prevents the kubelet from setting or modifying labels with a node-restriction.kubernetes.io/ prefix.To make use of that label prefix for node isolation:Ensure you are using the Node authorizer and have enabled the NodeRestriction admission plugin.Add labels with the node-restriction.kubernetes.io/ prefix to your nodes, and use those labels in your node selectors. For example, example.com.node-restriction.kubernetes.io/fips=true or example.com.node-restriction.kubernetes.io/pci-dss=true.nodeSelectornodeSelector is the simplest recommended form of node selection constraint. You can add the nodeSelector field to your Pod specification and specify the node labels you want the target node to have. Kubernetes only schedules the Pod onto nodes that have each of the labels you specify.See Assign Pods to Nodes for more information.Affinity and anti-affinitynodeSelector is the simplest way to constrain Pods to nodes with specific labels. Affinity and anti-affinity expands the types of constraints you can define. Some of the benefits of affinity and anti-affinity include:The affinity/anti-affinity language is more expressive. nodeSelector only selects nodes with all the specified labels. Affinity/anti-affinity gives you more control over the selection logic.You can indicate that a rule is soft or preferred, so that the scheduler still schedules the Pod even if it can't find a matching node.You can constrain a Pod using labels on other Pods running on the node (or other topological domain), instead of just node labels, which allows you to define rules for which Pods can be co-located on a node.The affinity feature consists of two types of affinity:Node affinity functions like the nodeSelector field but is more expressive and allows you to specify soft rules.Inter-pod affinity/anti-affinity allows you to constrain Pods against labels on other Pods.Node affinityNode affinity is conceptually similar to nodeSelector, allowing you to constrain which nodes your Pod can be scheduled on based on node labels. There are two types of node affinity:requiredDuringSchedulingIgnoredDuringExecution: The scheduler can't schedule the Pod unless the rule is met. This functions like nodeSelector, but with a more expressive syntax.preferredDuringSchedulingIgnoredDuringExecution: The scheduler tries to find a node that meets the rule. If a matching node is not available, the scheduler still schedules the Pod.Note:In the preceding types, IgnoredDuringExecution means that if the node labels change after Kubernetes schedules the Pod, the Pod continues to run.You can specify node affinities using the .spec.affinity.nodeAffinity field in your Pod spec.For example, consider the following Pod spec:pods/pod-with-node-affinity.yaml apiVersion: v1 kind: Pod metadata: name: with-node-affinity spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: \- matchExpressions: \- key: topology.kubernetes.io/zone operator: In values: \- antarctica-east1 \- antarctica-west1 preferredDuringSchedulingIgnoredDuringExecution: \- weight: 1 preference: matchExpressions: \- key: another-node-label-key operator: In values: \- another-node-label-value containers: \- name: with-node-affinity image: registry.k8s.io/pause:3.8In this example, the following rules apply:The node must have a label with the key topology.kubernetes.io/zone and the value of that label must be either antarctica-east1 or antarctica-west1.The node preferably has a label with the key another-node-label-key and the value another-node-label-value.You can use the operator field to specify a logical operator for Kubernetes to use when interpreting the rules. You can use In, NotIn, Exists, DoesNotExist, Gt and Lt.Read Operators to learn more about how these work.NotIn and DoesNotExist allow you to define node anti-affinity behavior. Alternatively, you can use node taints to repel Pods from specific nodes.Note:If you specify both nodeSelector and nodeAffinity, both must be satisfied for the Pod to be scheduled onto a node.If you specify multiple terms in nodeSelectorTerms associated with nodeAffinity types, then the Pod can be scheduled onto a node if one of the specified terms can be satisfied (terms are ORed).If you specify multiple expressions in a single matchExpressions field associated with a term in nodeSelectorTerms, then the Pod can be scheduled onto a node only if all the expressions are satisfied (expressions are ANDed).See Assign Pods to Nodes using Node Affinity for more information.Node affinity weightYou can specify a weight between 1 and 100 for each instance of the preferredDuringSchedulingIgnoredDuringExecution affinity type. When the scheduler finds nodes that meet all the other scheduling requirements of the Pod, the scheduler iterates through every preferred rule that the node satisfies and adds the value of the weight for that expression to a sum.The final sum is added to the score of other priority functions for the node. Nodes with the highest total score are prioritized when the scheduler makes a scheduling decision for the Pod.For example, consider the following Pod spec:pods/pod-with-affinity-preferred-weight.yaml apiVersion: v1 kind: Pod metadata: name: with-affinity-preferred-weight spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: \- matchExpressions: \- key: kubernetes.io/os operator: In values: \- linux preferredDuringSchedulingIgnoredDuringExecution: \- weight: 1 preference: matchExpressions: \- key: label-1 operator: In values: \- key-1 \- weight: 50 preference: matchExpressions: \- key: label-2 operator: In values: \- key-2 containers: \- name: with-node-affinity image: registry.k8s.io/pause:3.8 If there are two possible nodes that match the preferredDuringSchedulingIgnoredDuringExecution rule, one with the label-1:key-1 label and another with the label-2:key-2 label, the scheduler considers the weight of each node and adds the weight to the other scores for that node, and schedules the Pod onto the node with the highest final score.Note:If you want Kubernetes to successfully schedule the Pods in this example, you must have existing nodes with the kubernetes.io/os=linux label.Node affinity per scheduling profileFEATURE STATE: Kubernetes v1.20 [beta]When configuring multiple scheduling profiles, you can associate a profile with a node affinity, which is useful if a profile only applies to a specific set of nodes. To do so, add an addedAffinity to the args field of the NodeAffinity plugin in the scheduler configuration. For example:apiVersion: kubescheduler.config.k8s.io/v1 kind: KubeSchedulerConfiguration profiles: \- schedulerName: default-scheduler \- schedulerName: foo-scheduler pluginConfig: \- name: NodeAffinity args: addedAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: \- matchExpressions: \- key: scheduler-profile operator: In values: \- foo The addedAffinity is applied to all Pods that set .spec.schedulerName to foo-scheduler, in addition to the NodeAffinity specified in the PodSpec. That is, in order to match the Pod, nodes need to satisfy addedAffinity and the Pod's .spec.NodeAffinity.Since the addedAffinity is not visible to end users, its behavior might be unexpected to them. Use node labels that have a clear correlation to the scheduler profile name.Note:The DaemonSet controller, which creates Pods for DaemonSets, does not support scheduling profiles. When the DaemonSet controller creates Pods, the default Kubernetes scheduler places those Pods and honors any nodeAffinity rules in the DaemonSet controller.Inter-pod affinity and anti-affinityInter-pod affinity and anti-affinity allow you to constrain which nodes your Pods can be scheduled on based on the labels of Pods already running on that node, instead of the node labels.Inter-pod affinity and anti-affinity rules take the form "this Pod should (or, in the case of anti-affinity, should not) run in an X if that X is already running one or more Pods that meet rule Y", where X is a topology domain like node, rack, cloud provider zone or region, or similar and Y is the rule Kubernetes tries to satisfy.You express these rules (Y) as label selectors with an optional associated list of namespaces. Pods are namespaced objects in Kubernetes, so Pod labels also implicitly have namespaces. Any label selectors for Pod labels should specify the namespaces in which Kubernetes should look for those labels.You express the topology domain (X) using a topologyKey, which is the key for the node label that the system uses to denote the domain. For examples, see Well-Known Labels, Annotations and Taints.Note:Inter-pod affinity and anti-affinity require substantial amounts of processing which can slow down scheduling in large clusters significantly. We do not recommend using them in clusters larger than several hundred nodes.Note:Pod anti-affinity requires nodes to be consistently labeled, in other words, every node in the cluster must have an appropriate label matching topologyKey. If some or all nodes are missing the specified topologyKey label, it can lead to unintended behavior.Types of inter-pod affinity and anti-affinitySimilar to node affinity are two types of Pod affinity and anti-affinity as follows:requiredDuringSchedulingIgnoredDuringExecutionpreferredDuringSchedulingIgnoredDuringExecutionFor example, you could use requiredDuringSchedulingIgnoredDuringExecution affinity to tell the scheduler to co-locate Pods of two services in the same cloud provider zone because they communicate with each other a lot. Similarly, you could use preferredDuringSchedulingIgnoredDuringExecution anti-affinity to spread Pods from a service across multiple cloud provider zones.To use inter-pod affinity, use the affinity.podAffinity field in the Pod spec. For inter-pod anti-affinity, use the affinity.podAntiAffinity field in the Pod spec.Scheduling a group of pods with inter-pod affinity to themselvesIf the current Pod being scheduled is the first in a series that have affinity to themselves, it is allowed to be scheduled if it passes all other affinity checks. This is determined by verifying that no other pod in the cluster matches the namespace and selector of this pod, that the pod matches its own terms, and the chosen node matches all requested topologies. This ensures that there will not be a deadlock even if all the pods have inter-pod affinity specified.Pod affinity exampleConsider the following Pod spec:pods/pod-with-pod-affinity.yaml apiVersion: v1 kind: Pod metadata: name: with-pod-affinity spec: affinity: podAffinity: requiredDuringSchedulingIgnoredDuringExecution: \- labelSelector: matchExpressions: \- key: security operator: In values: \- S1 topologyKey: topology.kubernetes.io/zone podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: \- weight: 100 podAffinityTerm: labelSelector: matchExpressions: \- key: security operator: In values: \- S2 topologyKey: topology.kubernetes.io/zone containers: \- name: with-pod-affinity image: registry.k8s.io/pause:3.8 This example defines one Pod affinity rule and one Pod anti-affinity rule. The Pod affinity rule uses the "hard" requiredDuringSchedulingIgnoredDuringExecution, while the anti-affinity rule uses the "soft" preferredDuringSchedulingIgnoredDuringExecution.The affinity rule specifies that the scheduler is allowed to place the example Pod on a node only if that node belongs to a specific zone where other Pods have been labeled with security=S1. For instance, if we have a cluster with a designated zone, let's call it "Zone V," consisting of nodes labeled with topology.kubernetes.io/zone=V, the scheduler can assign the Pod to any node within Zone V, as long as there is at least one Pod within Zone V already labeled with security=S1. Conversely, if there are no Pods with security=S1 labels in Zone V, the scheduler will not assign the example Pod to any node in that zone.The anti-affinity rule specifies that the scheduler should try to avoid scheduling the Pod on a node if that node belongs to a specific zone where other Pods have been labeled with security=S2. For instance, if we have a cluster with a designated zone, let's call it "Zone R," consisting of nodes labeled with topology.kubernetes.io/zone=R, the scheduler should avoid assigning the Pod to any node within Zone R, as long as there is at least one Pod within Zone R already labeled with security=S2. Conversely, the anti-affinity rule does not impact scheduling into Zone R if there are no Pods with security=S2 labels.To get yourself more familiar with the examples of Pod affinity and anti-affinity, refer to the design proposal.You can use the In, NotIn, Exists and DoesNotExist values in the operator field for Pod affinity and anti-affinity.Read Operators to learn more about how these work.In principle, the topologyKey can be any allowed label key with the following exceptions for performance and security reasons:For Pod affinity and anti-affinity, an empty topologyKey field is not allowed in both requiredDuringSchedulingIgnoredDuringExecution and preferredDuringSchedulingIgnoredDuringExecution.For requiredDuringSchedulingIgnoredDuringExecution Pod anti-affinity rules, the admission controller LimitPodHardAntiAffinityTopology limits topologyKey to kubernetes.io/hostname. You can modify or disable the admission controller if you want to allow custom topologies.In addition to labelSelector and topologyKey, you can optionally specify a list of namespaces which the labelSelector should match against using the namespaces field at the same level as labelSelector and topologyKey. If omitted or empty, namespaces defaults to the namespace of the Pod where the affinity/anti-affinity definition appears.Namespace selectorFEATURE STATE: Kubernetes v1.24 [stable]You can also select matching namespaces using namespaceSelector, which is a label query over the set of namespaces. The affinity term is applied to namespaces selected by both namespaceSelector and the namespaces field. Note that an empty namespaceSelector ({}) matches all namespaces, while a null or empty namespaces list and null namespaceSelector matches the namespace of the Pod where the rule is defined.matchLabelKeysFEATURE STATE: Kubernetes v1.33 [stable] (enabled by default: true)Note:The matchLabelKeys field is a beta-level field and is enabled by default in Kubernetes 1.33. When you want to disable it, you have to disable it explicitly via the MatchLabelKeysInPodAffinity feature gate.Kubernetes includes an optional matchLabelKeys field for Pod affinity or anti-affinity. The field specifies keys for the labels that should match with the incoming Pod's labels, when satisfying the Pod (anti)affinity.The keys are used to look up values from the pod labels; those key-value labels are combined (using AND) with the match restrictions defined using the labelSelector field. The combined filtering selects the set of existing pods that will be taken into Pod (anti)affinity calculation.Caution:It's not recommended to use matchLabelKeys with labels that might be updated directly on pods. Even if you edit the pod's label that is specified at matchLabelKeys directly, (that is, not via a deployment), kube-apiserver doesn't reflect the label update onto the merged labelSelector.A common use case is to use matchLabelKeys with pod-template-hash (set on Pods managed as part of a Deployment, where the value is unique for each revision). Using pod-template-hash in matchLabelKeys allows you to target the Pods that belong to the same revision as the incoming Pod, so that a rolling upgrade won't break affinity.apiVersion: apps/v1 kind: Deployment metadata: name: application-server ... spec: template: spec: affinity: podAffinity: requiredDuringSchedulingIgnoredDuringExecution: \- labelSelector: matchExpressions: \- key: app operator: In values: \- database topologyKey: topology.kubernetes.io/zone # Only Pods from a given rollout are taken into consideration when calculating pod affinity. # If you update the Deployment, the replacement Pods follow their own affinity rules # (if there are any defined in the new Pod template) matchLabelKeys: \- pod-template-hash mismatchLabelKeysFEATURE STATE: Kubernetes v1.33 [stable] (enabled by default: true)Note:The mismatchLabelKeys field is a beta-level field and is enabled by default in Kubernetes 1.33. When you want to disable it, you have to disable it explicitly via the MatchLabelKeysInPodAffinity feature gate.Kubernetes includes an optional mismatchLabelKeys field for Pod affinity or anti-affinity. The field specifies keys for the labels that should not match with the incoming Pod's labels, when satisfying the Pod (anti)affinity.Caution:It's not recommended to use mismatchLabelKeys with labels that might be updated directly on pods. Even if you edit the pod's label that is specified at mismatchLabelKeys directly, (that is, not via a deployment), kube-apiserver doesn't reflect the label update onto the merged labelSelector.One example use case is to ensure Pods go to the topology domain (node, zone, etc) where only Pods from the same tenant or team are scheduled in. In other words, you want to avoid running Pods from two different tenants on the same topology domain at the same time.apiVersion: v1 kind: Pod metadata: labels: # Assume that all relevant Pods have a "tenant" label set tenant: tenant-a ... spec: affinity: podAffinity: requiredDuringSchedulingIgnoredDuringExecution: # ensure that pods associated with this tenant land on the correct node pool \- matchLabelKeys: \- tenant topologyKey: node-pool podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: # ensure that pods associated with this tenant can't schedule to nodes used for another tenant \- mismatchLabelKeys: \- tenant # whatever the value of the "tenant" label for this Pod, prevent # scheduling to nodes in any pool where any Pod from a different # tenant is running. labelSelector: # We have to have the labelSelector which selects only Pods with the tenant label, # otherwise this Pod would have Pods from daemonsets as well, for example, # which aren't supposed to have the tenant label. matchExpressions: \- key: tenant operator: Exists topologyKey: node-pool More practical use-casesInter-pod affinity and anti-affinity can be even more useful when they are used with higher level collections such as ReplicaSets, StatefulSets, Deployments, etc. These rules allow you to configure that a set of workloads should be co-located in the same defined topology; for example, preferring to place two related Pods onto the same node.For example: imagine a three-node cluster. You use the cluster to run a web application and also an in-memory cache (such as Redis). For this example, also assume that latency between the web application and the memory cache should be as low as is practical. You could use inter-pod affinity and anti-affinity to co-locate the web servers with the cache as much as possible.In the following example Deployment for the Redis cache, the replicas get the label app=store. The podAntiAffinity rule tells the scheduler to avoid placing multiple replicas with the app=store label on a single node. This creates each cache in a separate node.apiVersion: apps/v1 kind: Deployment metadata: name: redis-cache spec: selector: matchLabels: app: store replicas: 3 template: metadata: labels: app: store spec: affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: \- labelSelector: matchExpressions: \- key: app operator: In values: \- store topologyKey: "kubernetes.io/hostname" containers: \- name: redis-server image: redis:3.2-alpine The following example Deployment for the web servers creates replicas with the label app=web-store. The Pod affinity rule tells the scheduler to place each replica on a node that has a Pod with the label app=store. The Pod anti-affinity rule tells the scheduler never to place multiple app=web-store servers on a single node.apiVersion: apps/v1 kind: Deployment metadata: name: web-server spec: selector: matchLabels: app: web-store replicas: 3 template: metadata: labels: app: web-store spec: affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: \- labelSelector: matchExpressions: \- key: app operator: In values: \- web-store topologyKey: "kubernetes.io/hostname" podAffinity: requiredDuringSchedulingIgnoredDuringExecution: \- labelSelector: matchExpressions: \- key: app operator: In values: \- store topologyKey: "kubernetes.io/hostname" containers: \- name: web-app image: nginx:1.16-alpine Creating the two preceding Deployments results in the following cluster layout, where each web server is co-located with a cache, on three separate nodes.node-1node-2node-3webserver-1webserver-2webserver-3cache-1cache-2cache-3The overall effect is that each cache instance is likely to be accessed by a single client that is running on the same node. This approach aims to minimize both skew (imbalanced load) and latency.You might have other reasons to use Pod anti-affinity. See the ZooKeeper tutorial for an example of a StatefulSet configured with anti-affinity for high availability, using the same technique as this example.nodeNamenodeName is a more direct form of node selection than affinity or nodeSelector. nodeName is a field in the Pod spec. If the nodeName field is not empty, the scheduler ignores the Pod and the kubelet on the named node tries to place the Pod on that node. Using nodeName overrules using nodeSelector or affinity and anti-affinity rules.Some of the limitations of using nodeName to select nodes are:If the named node does not exist, the Pod will not run, and in some cases may be automatically deleted.If the named node does not have the resources to accommodate the Pod, the Pod will fail and its reason will indicate why, for example OutOfmemory or OutOfcpu.Node names in cloud environments are not always predictable or stable.Warning:nodeName is intended for use by custom schedulers or advanced use cases where you need to bypass any configured schedulers. Bypassing the schedulers might lead to failed Pods if the assigned Nodes get oversubscribed. You can use node affinity or the nodeSelector field to assign a Pod to a specific Node without bypassing the schedulers.Here is an example of a Pod spec using the nodeName field:apiVersion: v1 kind: Pod metadata: name: nginx spec: containers: \- name: nginx image: nginx nodeName: kube-01 The above Pod will only run on the node kube-01.Pod topology spread constraintsYou can use topology spread constraints to control how Pods are spread across your cluster among failure-domains such as regions, zones, nodes, or among any other topology domains that you define. You might do this to improve performance, expected availability, or overall utilization.Read Pod topology spread constraints to learn more about how these work.OperatorsThe following are all the logical operators that you can use in the operator field for nodeAffinity and podAffinity mentioned above.OperatorBehaviorInThe label value is present in the supplied set of stringsNotInThe label value is not contained in the supplied set of stringsExistsA label with this key exists on the objectDoesNotExistNo label with this key exists on the objectThe following operators can only be used with nodeAffinity.OperatorBehaviorGtThe field value will be parsed as an integer, and that integer is less than the integer that results from parsing the value of a label named by this selectorLtThe field value will be parsed as an integer, and that integer is greater than the integer that results from parsing the value of a label named by this selectorNote:Gt and Lt operators will not work with non-integer values. If the given value doesn't parse as an integer, the pod will fail to get scheduled. Also, Gt and Lt are not available for podAffinity.What's nextRead more about taints and tolerations.Read the design docs for node affinity and for inter-pod affinity/anti-affinity.Learn about how the topology manager takes part in node-level resource allocation decisions.Learn how to use nodeSelector.Learn how to use affinity and anti-affinity.3.10.3 - Pod OverheadFEATURE STATE: Kubernetes v1.24 [stable]When you run a Pod on a Node, the Pod itself takes an amount of system resources. These resources are additional to the resources needed to run the container(s) inside the Pod. In Kubernetes, Pod Overhead is a way to account for the resources consumed by the Pod infrastructure on top of the container requests & limits.In Kubernetes, the Pod's overhead is set at admission time according to the overhead associated with the Pod's RuntimeClass.A pod's overhead is considered in addition to the sum of container resource requests when scheduling a Pod. Similarly, the kubelet will include the Pod overhead when sizing the Pod cgroup, and when carrying out Pod eviction ranking.Configuring Pod overheadYou need to make sure a RuntimeClass is utilized which defines the overhead field.Usage exampleTo work with Pod overhead, you need a RuntimeClass that defines the overhead field. As an example, you could use the following RuntimeClass definition with a virtualization container runtime (in this example, Kata Containers combined with the Firecracker virtual machine monitor) that uses around 120MiB per Pod for the virtual machine and the guest OS:# You need to change this example to match the actual runtime name, and per-Pod # resource overhead, that the container runtime is adding in your cluster. apiVersion: node.k8s.io/v1 kind: RuntimeClass metadata: name: kata-fc handler: kata-fc overhead: podFixed: memory: "120Mi" cpu: "250m" Workloads which are created which specify the kata-fc RuntimeClass handler will take the memory and cpu overheads into account for resource quota calculations, node scheduling, as well as Pod cgroup sizing.Consider running the given example workload, test-pod:apiVersion: v1 kind: Pod metadata: name: test-pod spec: runtimeClassName: kata-fc containers: \- name: busybox-ctr image: busybox:1.28 stdin: true tty: true resources: limits: cpu: 500m memory: 100Mi \- name: nginx-ctr image: nginx resources: limits: cpu: 1500m memory: 100Mi Note:If only limits are specified in the pod definition, kubelet will deduce requests from those limits and set them to be the same as the defined limits.At admission time the RuntimeClass admission controller updates the workload's PodSpec to include the overhead as described in the RuntimeClass. If the PodSpec already has this field defined, the Pod will be rejected. In the given example, since only the RuntimeClass name is specified, the admission controller mutates the Pod to include an overhead.After the RuntimeClass admission controller has made modifications, you can check the updated Pod overhead value:kubectl get pod test-pod -o jsonpath='{.spec.overhead}' The output is:map[cpu:250m memory:120Mi] If a ResourceQuota is defined, the sum of container requests as well as the overhead field are counted.When the kube-scheduler is deciding which node should run a new Pod, the scheduler considers that Pod's overhead as well as the sum of container requests for that Pod. For this example, the scheduler adds the requests and the overhead, then looks for a node that has 2.25 CPU and 320 MiB of memory available.Once a Pod is scheduled to a node, the kubelet on that node creates a new cgroup for the Pod. It is within this pod that the underlying container runtime will create containers.If the resource has a limit defined for each container (Guaranteed QoS or Burstable QoS with limits defined), the kubelet will set an upper limit for the pod cgroup associated with that resource (cpu.cfs_quota_us for CPU and memory.limit_in_bytes memory). This upper limit is based on the sum of the container limits plus the overhead defined in the PodSpec.For CPU, if the Pod is Guaranteed or Burstable QoS, the kubelet will set cpu.shares based on the sum of container requests plus the overhead defined in the PodSpec.Looking at our example, verify the container requests for the workload:kubectl get pod test-pod -o jsonpath='{.spec.containers[*].resources.limits}' The total container requests are 2000m CPU and 200MiB of memory:map[cpu: 500m memory:100Mi] map[cpu:1500m memory:100Mi] Check this against what is observed by the node:kubectl describe node | grep test-pod -B2 The output shows requests for 2250m CPU, and for 320MiB of memory. The requests include Pod overhead: Namespace Name CPU Requests CPU Limits Memory Requests Memory Limits AGE \--------- ---- ------------ ---------- --------------- ------------- --- default test-pod 2250m (56%) 2250m (56%) 320Mi (1%) 320Mi (1%) 36m Verify Pod cgroup limitsCheck the Pod's memory cgroups on the node where the workload is running. In the following example, crictl is used on the node, which provides a CLI for CRI-compatible container runtimes. This is an advanced example to show Pod overhead behavior, and it is not expected that users should need to check cgroups directly on the node.First, on the particular node, determine the Pod identifier:# Run this on the node where the Pod is scheduled POD_ID="$(sudo crictl pods --name test-pod -q)" From this, you can determine the cgroup path for the Pod:# Run this on the node where the Pod is scheduled sudo crictl inspectp -o=json $POD_ID | grep cgroupsPath The resulting cgroup path includes the Pod's pause container. The Pod level cgroup is one directory above. "cgroupsPath": "/kubepods/podd7f4b509-cf94-4951-9417-d1087c92a5b2/7ccf55aee35dd16aca4189c952d83487297f3cd760f1bbf09620e206e7d0c27a" In this specific case, the pod cgroup path is kubepods/podd7f4b509-cf94-4951-9417-d1087c92a5b2. Verify the Pod level cgroup setting for memory:# Run this on the node where the Pod is scheduled. # Also, change the name of the cgroup to match the cgroup allocated for your pod. cat /sys/fs/cgroup/memory/kubepods/podd7f4b509-cf94-4951-9417-d1087c92a5b2/memory.limit_in_bytes This is 320 MiB, as expected:335544320 ObservabilitySome kube_pod_overhead_* metrics are available in kube-state-metrics to help identify when Pod overhead is being utilized and to help observe stability of workloads running with a defined overhead.What's nextLearn more about RuntimeClassRead the PodOverhead Design enhancement proposal for extra context3.10.4 - Pod Scheduling ReadinessFEATURE STATE: Kubernetes v1.30 [stable]Pods were considered ready for scheduling once created. Kubernetes scheduler does its due diligence to find nodes to place all pending Pods. However, in a real-world case, some Pods may stay in a "miss-essential-resources" state for a long period. These Pods actually churn the scheduler (and downstream integrators like Cluster AutoScaler) in an unnecessary manner.By specifying/removing a Pod's .spec.schedulingGates, you can control when a Pod is ready to be considered for scheduling.Configuring Pod schedulingGatesThe schedulingGates field contains a list of strings, and each string literal is perceived as a criteria that Pod should be satisfied before considered schedulable. This field can be initialized only when a Pod is created (either by the client, or mutated during admission). After creation, each schedulingGate can be removed in arbitrary order, but addition of a new scheduling gate is disallowed.Figure. Pod SchedulingGatesUsage exampleTo mark a Pod not-ready for scheduling, you can create it with one or more scheduling gates like this:pods/pod-with-scheduling-gates.yaml apiVersion: v1 kind: Pod metadata: name: test-pod spec: schedulingGates: \- name: example.com/foo \- name: example.com/bar containers: \- name: pause image: registry.k8s.io/pause:3.6 After the Pod's creation, you can check its state using:kubectl get pod test-pod The output reveals it's in SchedulingGated state:NAME READY STATUS RESTARTS AGE test-pod 0/1 SchedulingGated 0 7s You can also check its schedulingGates field by running:kubectl get pod test-pod -o jsonpath='{.spec.schedulingGates}' The output is:[{"name":"example.com/foo"},{"name":"example.com/bar"}] To inform scheduler this Pod is ready for scheduling, you can remove its schedulingGates entirely by reapplying a modified manifest:pods/pod-without-scheduling-gates.yaml apiVersion: v1 kind: Pod metadata: name: test-pod spec: containers: \- name: pause image: registry.k8s.io/pause:3.6 You can check if the schedulingGates is cleared by running:kubectl get pod test-pod -o jsonpath='{.spec.schedulingGates}' The output is expected to be empty. And you can check its latest status by running:kubectl get pod test-pod -o wide Given the test-pod doesn't request any CPU/memory resources, it's expected that this Pod's state get transited from previous SchedulingGated to Running:NAME READY STATUS RESTARTS AGE IP NODE test-pod 1/1 Running 0 15s 10.0.0.4 node-2 ObservabilityThe metric scheduler_pending_pods comes with a new label "gated" to distinguish whether a Pod has been tried scheduling but claimed as unschedulable, or explicitly marked as not ready for scheduling. You can use scheduler_pending_pods{queue="gated"} to check the metric result.Mutable Pod scheduling directivesYou can mutate scheduling directives of Pods while they have scheduling gates, with certain constraints. At a high level, you can only tighten the scheduling directives of a Pod. In other words, the updated directives would cause the Pods to only be able to be scheduled on a subset of the nodes that it would previously match. More concretely, the rules for updating a Pod's scheduling directives are as follows:For .spec.nodeSelector, only additions are allowed. If absent, it will be allowed to be set.For spec.affinity.nodeAffinity, if nil, then setting anything is allowed.If NodeSelectorTerms was empty, it will be allowed to be set. If not empty, then only additions of NodeSelectorRequirements to matchExpressions or fieldExpressions are allowed, and no changes to existing matchExpressions and fieldExpressions will be allowed. This is because the terms in .requiredDuringSchedulingIgnoredDuringExecution.NodeSelectorTerms, are ORed while the expressions in nodeSelectorTerms[].matchExpressions and nodeSelectorTerms[].fieldExpressions are ANDed.For .preferredDuringSchedulingIgnoredDuringExecution, all updates are allowed. This is because preferred terms are not authoritative, and so policy controllers don't validate those terms.What's nextRead the PodSchedulingReadiness KEP for more details3.10.5 - Pod Topology Spread ConstraintsYou can use topology spread constraints to control how Pods are spread across your cluster among failure-domains such as regions, zones, nodes, and other user-defined topology domains. This can help to achieve high availability as well as efficient resource utilization.You can set cluster-level constraints as a default, or configure topology spread constraints for individual workloads.MotivationImagine that you have a cluster of up to twenty nodes, and you want to run a workload that automatically scales how many replicas it uses. There could be as few as two Pods or as many as fifteen. When there are only two Pods, you'd prefer not to have both of those Pods run on the same node: you would run the risk that a single node failure takes your workload offline.In addition to this basic usage, there are some advanced usage examples that enable your workloads to benefit on high availability and cluster utilization.As you scale up and run more Pods, a different concern becomes important. Imagine that you have three nodes running five Pods each. The nodes have enough capacity to run that many replicas; however, the clients that interact with this workload are split across three different datacenters (or infrastructure zones). Now you have less concern about a single node failure, but you notice that latency is higher than you'd like, and you are paying for network costs associated with sending network traffic between the different zones.You decide that under normal operation you'd prefer to have a similar number of replicas scheduled into each infrastructure zone, and you'd like the cluster to self-heal in the case that there is a problem.Pod topology spread constraints offer you a declarative way to configure that.topologySpreadConstraints fieldThe Pod API includes a field, spec.topologySpreadConstraints. The usage of this field looks like the following:--- apiVersion: v1 kind: Pod metadata: name: example-pod spec: # Configure a topology spread constraint topologySpreadConstraints: \- maxSkew:  minDomains:  # optional topologyKey:  whenUnsatisfiable:  labelSelector:  matchLabelKeys:  # optional; beta since v1.27 nodeAffinityPolicy: [Honor|Ignore] # optional; beta since v1.26 nodeTaintsPolicy: [Honor|Ignore] # optional; beta since v1.26 ### other Pod fields go here You can read more about this field by running kubectl explain Pod.spec.topologySpreadConstraints or refer to the scheduling section of the API reference for Pod.Spread constraint definitionYou can define one or multiple topologySpreadConstraints entries to instruct the kube-scheduler how to place each incoming Pod in relation to the existing Pods across your cluster. Those fields are:maxSkew describes the degree to which Pods may be unevenly distributed. You must specify this field and the number must be greater than zero. Its semantics differ according to the value of whenUnsatisfiable:if you select whenUnsatisfiable: DoNotSchedule, then maxSkew defines the maximum permitted difference between the number of matching pods in the target topology and the global minimum (the minimum number of matching pods in an eligible domain or zero if the number of eligible domains is less than MinDomains). For example, if you have 3 zones with 2, 2 and 1 matching pods respectively, MaxSkew is set to 1 then the global minimum is 1.if you select whenUnsatisfiable: ScheduleAnyway, the scheduler gives higher precedence to topologies that would help reduce the skew.minDomains indicates a minimum number of eligible domains. This field is optional. A domain is a particular instance of a topology. An eligible domain is a domain whose nodes match the node selector.Note:Before Kubernetes v1.30, the minDomains field was only available if the MinDomainsInPodTopologySpread feature gate was enabled (default since v1.28). In older Kubernetes clusters it might be explicitly disabled or the field might not be available.The value of minDomains must be greater than 0, when specified. You can only specify minDomains in conjunction with whenUnsatisfiable: DoNotSchedule.When the number of eligible domains with match topology keys is less than minDomains, Pod topology spread treats global minimum as 0, and then the calculation of skew is performed. The global minimum is the minimum number of matching Pods in an eligible domain, or zero if the number of eligible domains is less than minDomains.When the number of eligible domains with matching topology keys equals or is greater than minDomains, this value has no effect on scheduling.If you do not specify minDomains, the constraint behaves as if minDomains is 1.topologyKey is the key of node labels. Nodes that have a label with this key and identical values are considered to be in the same topology. We call each instance of a topology (in other words, a  pair) a domain. The scheduler will try to put a balanced number of pods into each domain. Also, we define an eligible domain as a domain whose nodes meet the requirements of nodeAffinityPolicy and nodeTaintsPolicy.whenUnsatisfiable indicates how to deal with a Pod if it doesn't satisfy the spread constraint:DoNotSchedule (default) tells the scheduler not to schedule it.ScheduleAnyway tells the scheduler to still schedule it while prioritizing nodes that minimize the skew.labelSelector is used to find matching Pods. Pods that match this label selector are counted to determine the number of Pods in their corresponding topology domain. See Label Selectors for more details.matchLabelKeys is a list of pod label keys to select the pods over which spreading will be calculated. The keys are used to lookup values from the pod labels, those key-value labels are ANDed with labelSelector to select the group of existing pods over which spreading will be calculated for the incoming pod. The same key is forbidden to exist in both matchLabelKeys and labelSelector. matchLabelKeys cannot be set when labelSelector isn't set. Keys that don't exist in the pod labels will be ignored. A null or empty list means only match against the labelSelector.With matchLabelKeys, you don't need to update the pod.spec between different revisions. The controller/operator just needs to set different values to the same label key for different revisions. The scheduler will assume the values automatically based on matchLabelKeys. For example, if you are configuring a Deployment, you can use the label keyed with pod-template-hash, which is added automatically by the Deployment controller, to distinguish between different revisions in a single Deployment. topologySpreadConstraints: \- maxSkew: 1 topologyKey: kubernetes.io/hostname whenUnsatisfiable: DoNotSchedule labelSelector: matchLabels: app: foo matchLabelKeys: \- pod-template-hash Note:The matchLabelKeys field is a beta-level field and enabled by default in 1.27. You can disable it by disabling the MatchLabelKeysInPodTopologySpread feature gate.nodeAffinityPolicy indicates how we will treat Pod's nodeAffinity/nodeSelector when calculating pod topology spread skew. Options are:Honor: only nodes matching nodeAffinity/nodeSelector are included in the calculations.Ignore: nodeAffinity/nodeSelector are ignored. All nodes are included in the calculations.If this value is null, the behavior is equivalent to the Honor policy.Note:The nodeAffinityPolicy became beta in 1.26 and graduated to GA in 1.33. It's enabled by default in beta, you can disable it by disabling the NodeInclusionPolicyInPodTopologySpread feature gate.nodeTaintsPolicy indicates how we will treat node taints when calculating pod topology spread skew. Options are:Honor: nodes without taints, along with tainted nodes for which the incoming pod has a toleration, are included.Ignore: node taints are ignored. All nodes are included.If this value is null, the behavior is equivalent to the Ignore policy.Note:The nodeTaintsPolicy became beta in 1.26 and graduated to GA in 1.33. It's enabled by default in beta, you can disable it by disabling the NodeInclusionPolicyInPodTopologySpread feature gate.When a Pod defines more than one topologySpreadConstraint, those constraints are combined using a logical AND operation: the kube-scheduler looks for a node for the incoming Pod that satisfies all the configured constraints.Node labelsTopology spread constraints rely on node labels to identify the topology domain(s) that each node is in. For example, a node might have labels: region: us-east-1 zone: us-east-1a Note:For brevity, this example doesn't use the well-known label keys topology.kubernetes.io/zone and topology.kubernetes.io/region. However, those registered label keys are nonetheless recommended rather than the private (unqualified) label keys region and zone that are used here.You can't make a reliable assumption about the meaning of a private label key between different contexts.Suppose you have a 4-node cluster with the following labels:NAME STATUS ROLES AGE VERSION LABELS node1 Ready  4m26s v1.16.0 node=node1,zone=zoneA node2 Ready  3m58s v1.16.0 node=node2,zone=zoneA node3 Ready  3m17s v1.16.0 node=node3,zone=zoneB node4 Ready  2m43s v1.16.0 node=node4,zone=zoneB Then the cluster is logically viewed as below:graph TB subgraph "zoneB" n3(Node3) n4(Node4) end subgraph "zoneA" n1(Node1) n2(Node2) end classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000; classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff; classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5; class n1,n2,n3,n4 k8s; class zoneA,zoneB cluster;JavaScript must be enabled to view this contentConsistencyYou should set the same Pod topology spread constraints on all pods in a group.Usually, if you are using a workload controller such as a Deployment, the pod template takes care of this for you. If you mix different spread constraints then Kubernetes follows the API definition of the field; however, the behavior is more likely to become confusing and troubleshooting is less straightforward.You need a mechanism to ensure that all the nodes in a topology domain (such as a cloud provider region) are labeled consistently. To avoid you needing to manually label nodes, most clusters automatically populate well-known labels such as kubernetes.io/hostname. Check whether your cluster supports this.Topology spread constraint examplesExample: one topology spread constraintSuppose you have a 4-node cluster where 3 Pods labeled foo: bar are located in node1, node2 and node3 respectively:graph BT subgraph "zoneB" p3(Pod) --> n3(Node3) n4(Node4) end subgraph "zoneA" p1(Pod) --> n1(Node1) p2(Pod) --> n2(Node2) end classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000; classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff; classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5; class n1,n2,n3,n4,p1,p2,p3 k8s; class zoneA,zoneB cluster;JavaScript must be enabled to view this contentIf you want an incoming Pod to be evenly spread with existing Pods across zones, you can use a manifest similar to:pods/topology-spread-constraints/one-constraint.yaml kind: Pod apiVersion: v1 metadata: name: mypod labels: foo: bar spec: topologySpreadConstraints: \- maxSkew: 1 topologyKey: zone whenUnsatisfiable: DoNotSchedule labelSelector: matchLabels: foo: bar containers: \- name: pause image: registry.k8s.io/pause:3.1From that manifest, topologyKey: zone implies the even distribution will only be applied to nodes that are labeled zone:  (nodes that don't have a zone label are skipped). The field whenUnsatisfiable: DoNotSchedule tells the scheduler to let the incoming Pod stay pending if the scheduler can't find a way to satisfy the constraint.If the scheduler placed this incoming Pod into zone A, the distribution of Pods would become [3, 1]. That means the actual skew is then 2 (calculated as 3 - 1), which violates maxSkew: 1. To satisfy the constraints and context for this example, the incoming Pod can only be placed onto a node in zone B:graph BT subgraph "zoneB" p3(Pod) --> n3(Node3) p4(mypod) --> n4(Node4) end subgraph "zoneA" p1(Pod) --> n1(Node1) p2(Pod) --> n2(Node2) end classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000; classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff; classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5; class n1,n2,n3,n4,p1,p2,p3 k8s; class p4 plain; class zoneA,zoneB cluster;JavaScript must be enabled to view this contentORgraph BT subgraph "zoneB" p3(Pod) --> n3(Node3) p4(mypod) --> n3 n4(Node4) end subgraph "zoneA" p1(Pod) --> n1(Node1) p2(Pod) --> n2(Node2) end classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000; classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff; classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5; class n1,n2,n3,n4,p1,p2,p3 k8s; class p4 plain; class zoneA,zoneB cluster;JavaScript must be enabled to view this contentYou can tweak the Pod spec to meet various kinds of requirements:Change maxSkew to a bigger value - such as 2 - so that the incoming Pod can be placed into zone A as well.Change topologyKey to node so as to distribute the Pods evenly across nodes instead of zones. In the above example, if maxSkew remains 1, the incoming Pod can only be placed onto the node node4.Change whenUnsatisfiable: DoNotSchedule to whenUnsatisfiable: ScheduleAnyway to ensure the incoming Pod to be always schedulable (suppose other scheduling APIs are satisfied). However, it's preferred to be placed into the topology domain which has fewer matching Pods. (Be aware that this preference is jointly normalized with other internal scheduling priorities such as resource usage ratio).Example: multiple topology spread constraintsThis builds upon the previous example. Suppose you have a 4-node cluster where 3 existing Pods labeled foo: bar are located on node1, node2 and node3 respectively:graph BT subgraph "zoneB" p3(Pod) --> n3(Node3) n4(Node4) end subgraph "zoneA" p1(Pod) --> n1(Node1) p2(Pod) --> n2(Node2) end classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000; classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff; classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5; class n1,n2,n3,n4,p1,p2,p3 k8s; class p4 plain; class zoneA,zoneB cluster;JavaScript must be enabled to view this contentYou can combine two topology spread constraints to control the spread of Pods both by node and by zone:pods/topology-spread-constraints/two-constraints.yaml kind: Pod apiVersion: v1 metadata: name: mypod labels: foo: bar spec: topologySpreadConstraints: \- maxSkew: 1 topologyKey: zone whenUnsatisfiable: DoNotSchedule labelSelector: matchLabels: foo: bar \- maxSkew: 1 topologyKey: node whenUnsatisfiable: DoNotSchedule labelSelector: matchLabels: foo: bar containers: \- name: pause image: registry.k8s.io/pause:3.1In this case, to match the first constraint, the incoming Pod can only be placed onto nodes in zone B; while in terms of the second constraint, the incoming Pod can only be scheduled to the node node4. The scheduler only considers options that satisfy all defined constraints, so the only valid placement is onto node node4.Example: conflicting topology spread constraintsMultiple constraints can lead to conflicts. Suppose you have a 3-node cluster across 2 zones:graph BT subgraph "zoneB" p4(Pod) --> n3(Node3) p5(Pod) --> n3 end subgraph "zoneA" p1(Pod) --> n1(Node1) p2(Pod) --> n1 p3(Pod) --> n2(Node2) end classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000; classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff; classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5; class n1,n2,n3,n4,p1,p2,p3,p4,p5 k8s; class zoneA,zoneB cluster;JavaScript must be enabled to view this contentIf you were to apply two-constraints.yaml (the manifest from the previous example) to this cluster, you would see that the Pod mypod stays in the Pending state. This happens because: to satisfy the first constraint, the Pod mypod can only be placed into zone B; while in terms of the second constraint, the Pod mypod can only schedule to node node2. The intersection of the two constraints returns an empty set, and the scheduler cannot place the Pod.To overcome this situation, you can either increase the value of maxSkew or modify one of the constraints to use whenUnsatisfiable: ScheduleAnyway. Depending on circumstances, you might also decide to delete an existing Pod manually - for example, if you are troubleshooting why a bug-fix rollout is not making progress.Interaction with node affinity and node selectorsThe scheduler will skip the non-matching nodes from the skew calculations if the incoming Pod has spec.nodeSelector or spec.affinity.nodeAffinity defined.Example: topology spread constraints with node affinitySuppose you have a 5-node cluster ranging across zones A to C:graph BT subgraph "zoneB" p3(Pod) --> n3(Node3) n4(Node4) end subgraph "zoneA" p1(Pod) --> n1(Node1) p2(Pod) --> n2(Node2) end classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000; classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff; classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5; class n1,n2,n3,n4,p1,p2,p3 k8s; class p4 plain; class zoneA,zoneB cluster;JavaScript must be enabled to view this contentgraph BT subgraph "zoneC" n5(Node5) end classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000; classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff; classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5; class n5 k8s; class zoneC cluster;JavaScript must be enabled to view this contentand you know that zone C must be excluded. In this case, you can compose a manifest as below, so that Pod mypod will be placed into zone B instead of zone C. Similarly, Kubernetes also respects spec.nodeSelector.pods/topology-spread-constraints/one-constraint-with-nodeaffinity.yaml kind: Pod apiVersion: v1 metadata: name: mypod labels: foo: bar spec: topologySpreadConstraints: \- maxSkew: 1 topologyKey: zone whenUnsatisfiable: DoNotSchedule labelSelector: matchLabels: foo: bar affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: \- matchExpressions: \- key: zone operator: NotIn values: \- zoneC containers: \- name: pause image: registry.k8s.io/pause:3.1Implicit conventionsThere are some implicit conventions worth noting here:Only the Pods holding the same namespace as the incoming Pod can be matching candidates.The scheduler only considers nodes that have all topologySpreadConstraints[*].topologyKey present at the same time. Nodes missing any of these topologyKeys are bypassed. This implies that:any Pods located on those bypassed nodes do not impact maxSkew calculation - in the above example, suppose the node node1 does not have a label "zone", then the 2 Pods will be disregarded, hence the incoming Pod will be scheduled into zone A.the incoming Pod has no chances to be scheduled onto this kind of nodes - in the above example, suppose a node node5 has the mistyped label zone-typo: zoneC (and no zone label set). After node node5 joins the cluster, it will be bypassed and Pods for this workload aren't scheduled there.Be aware of what will happen if the incoming Pod's topologySpreadConstraints[*].labelSelector doesn't match its own labels. In the above example, if you remove the incoming Pod's labels, it can still be placed onto nodes in zone B, since the constraints are still satisfied. However, after that placement, the degree of imbalance of the cluster remains unchanged - it's still zone A having 2 Pods labeled as foo: bar, and zone B having 1 Pod labeled as foo: bar. If this is not what you expect, update the workload's topologySpreadConstraints[*].labelSelector to match the labels in the pod template.Cluster-level default constraintsIt is possible to set default topology spread constraints for a cluster. Default topology spread constraints are applied to a Pod if, and only if:It doesn't define any constraints in its .spec.topologySpreadConstraints.It belongs to a Service, ReplicaSet, StatefulSet or ReplicationController.Default constraints can be set as part of the PodTopologySpread plugin arguments in a scheduling profile. The constraints are specified with the same API above, except that labelSelector must be empty. The selectors are calculated from the Services, ReplicaSets, StatefulSets or ReplicationControllers that the Pod belongs to.An example configuration might look like follows:apiVersion: kubescheduler.config.k8s.io/v1 kind: KubeSchedulerConfiguration profiles: \- schedulerName: default-scheduler pluginConfig: \- name: PodTopologySpread args: defaultConstraints: \- maxSkew: 1 topologyKey: topology.kubernetes.io/zone whenUnsatisfiable: ScheduleAnyway defaultingType: List Built-in default constraintsFEATURE STATE: Kubernetes v1.24 [stable]If you don't configure any cluster-level default constraints for pod topology spreading, then kube-scheduler acts as if you specified the following default topology constraints:defaultConstraints: \- maxSkew: 3 topologyKey: "kubernetes.io/hostname" whenUnsatisfiable: ScheduleAnyway \- maxSkew: 5 topologyKey: "topology.kubernetes.io/zone" whenUnsatisfiable: ScheduleAnyway Also, the legacy SelectorSpread plugin, which provides an equivalent behavior, is disabled by default.Note:The PodTopologySpread plugin does not score the nodes that don't have the topology keys specified in the spreading constraints. This might result in a different default behavior compared to the legacy SelectorSpread plugin when using the default topology constraints.If your nodes are not expected to have both kubernetes.io/hostname and topology.kubernetes.io/zone labels set, define your own constraints instead of using the Kubernetes defaults.If you don't want to use the default Pod spreading constraints for your cluster, you can disable those defaults by setting defaultingType to List and leaving empty defaultConstraints in the PodTopologySpread plugin configuration:apiVersion: kubescheduler.config.k8s.io/v1 kind: KubeSchedulerConfiguration profiles: \- schedulerName: default-scheduler pluginConfig: \- name: PodTopologySpread args: defaultConstraints: [] defaultingType: List Comparison with podAffinity and podAntiAffinityIn Kubernetes, inter-Pod affinity and anti-affinity control how Pods are scheduled in relation to one another - either more packed or more scattered.podAffinityattracts Pods; you can try to pack any number of Pods into qualifying topology domain(s).podAntiAffinityrepels Pods. If you set this to requiredDuringSchedulingIgnoredDuringExecution mode then only a single Pod can be scheduled into a single topology domain; if you choose preferredDuringSchedulingIgnoredDuringExecution then you lose the ability to enforce the constraint.For finer control, you can specify topology spread constraints to distribute Pods across different topology domains - to achieve either high availability or cost-saving. This can also help on rolling update workloads and scaling out replicas smoothly.For more context, see the Motivation section of the enhancement proposal about Pod topology spread constraints.Known limitationsThere's no guarantee that the constraints remain satisfied when Pods are removed. For example, scaling down a Deployment may result in imbalanced Pods distribution.You can use a tool such as the Descheduler to rebalance the Pods distribution.Pods matched on tainted nodes are respected. See Issue 80921.The scheduler doesn't have prior knowledge of all the zones or other topology domains that a cluster has. They are determined from the existing nodes in the cluster. This could lead to a problem in autoscaled clusters, when a node pool (or node group) is scaled to zero nodes, and you're expecting the cluster to scale up, because, in this case, those topology domains won't be considered until there is at least one node in them.You can work around this by using a Node autoscaler that is aware of Pod topology spread constraints and is also aware of the overall set of topology domains.What's nextThe blog article Introducing PodTopologySpread explains maxSkew in some detail, as well as covering some advanced usage examples.Read the scheduling section of the API reference for Pod.3.10.6 - Taints and TolerationsNode affinity is a property of Pods that attracts them to a set of nodes (either as a preference or a hard requirement). Taints are the opposite -- they allow a node to repel a set of pods.Tolerations are applied to pods. Tolerations allow the scheduler to schedule pods with matching taints. Tolerations allow scheduling but don't guarantee scheduling: the scheduler also evaluates other parameters as part of its function.Taints and tolerations work together to ensure that pods are not scheduled onto inappropriate nodes. One or more taints are applied to a node; this marks that the node should not accept any pods that do not tolerate the taints.ConceptsYou add a taint to a node using kubectl taint. For example,kubectl taint nodes node1 key1=value1:NoSchedule places a taint on node node1. The taint has key key1, value value1, and taint effect NoSchedule. This means that no pod will be able to schedule onto node1 unless it has a matching toleration.To remove the taint added by the command above, you can run:kubectl taint nodes node1 key1=value1:NoSchedule- You specify a toleration for a pod in the PodSpec. Both of the following tolerations "match" the taint created by the kubectl taint line above, and thus a pod with either toleration would be able to schedule onto node1:tolerations: \- key: "key1" operator: "Equal" value: "value1" effect: "NoSchedule" tolerations: \- key: "key1" operator: "Exists" effect: "NoSchedule" The default Kubernetes scheduler takes taints and tolerations into account when selecting a node to run a particular Pod. However, if you manually specify the .spec.nodeName for a Pod, that action bypasses the scheduler; the Pod is then bound onto the node where you assigned it, even if there are NoSchedule taints on that node that you selected. If this happens and the node also has a NoExecute taint set, the kubelet will eject the Pod unless there is an appropriate tolerance set.Here's an example of a pod that has some tolerations defined:pods/pod-with-toleration.yaml apiVersion: v1 kind: Pod metadata: name: nginx labels: env: test spec: containers: \- name: nginx image: nginx imagePullPolicy: IfNotPresent tolerations: \- key: "example-key" operator: "Exists" effect: "NoSchedule" The default value for operator is Equal.A toleration "matches" a taint if the keys are the same and the effects are the same, and:the operator is Exists (in which case no value should be specified), orthe operator is Equal and the values should be equal.Note:There are two special cases:If the key is empty, then the operator must be Exists, which matches all keys and values. Note that the effect still needs to be matched at the same time.An empty effect matches all effects with key key1.The above example used the effect of NoSchedule. Alternatively, you can use the effect of PreferNoSchedule.The allowed values for the effect field are:NoExecuteThis affects pods that are already running on the node as follows:Pods that do not tolerate the taint are evicted immediatelyPods that tolerate the taint without specifying tolerationSeconds in their toleration specification remain bound foreverPods that tolerate the taint with a specified tolerationSeconds remain bound for the specified amount of time. After that time elapses, the node lifecycle controller evicts the Pods from the node.NoScheduleNo new Pods will be scheduled on the tainted node unless they have a matching toleration. Pods currently running on the node are not evicted.PreferNoSchedulePreferNoSchedule is a "preference" or "soft" version of NoSchedule. The control plane will try to avoid placing a Pod that does not tolerate the taint on the node, but it is not guaranteed.You can put multiple taints on the same node and multiple tolerations on the same pod. The way Kubernetes processes multiple taints and tolerations is like a filter: start with all of a node's taints, then ignore the ones for which the pod has a matching toleration; the remaining un-ignored taints have the indicated effects on the pod. In particular,if there is at least one un-ignored taint with effect NoSchedule then Kubernetes will not schedule the pod onto that nodeif there is no un-ignored taint with effect NoSchedule but there is at least one un-ignored taint with effect PreferNoSchedule then Kubernetes will try to not schedule the pod onto the nodeif there is at least one un-ignored taint with effect NoExecute then the pod will be evicted from the node (if it is already running on the node), and will not be scheduled onto the node (if it is not yet running on the node).For example, imagine you taint a node like thiskubectl taint nodes node1 key1=value1:NoSchedule kubectl taint nodes node1 key1=value1:NoExecute kubectl taint nodes node1 key2=value2:NoSchedule And a pod has two tolerations:tolerations: \- key: "key1" operator: "Equal" value: "value1" effect: "NoSchedule" \- key: "key1" operator: "Equal" value: "value1" effect: "NoExecute" In this case, the pod will not be able to schedule onto the node, because there is no toleration matching the third taint. But it will be able to continue running if it is already running on the node when the taint is added, because the third taint is the only one of the three that is not tolerated by the pod.Normally, if a taint with effect NoExecute is added to a node, then any pods that do not tolerate the taint will be evicted immediately, and pods that do tolerate the taint will never be evicted. However, a toleration with NoExecute effect can specify an optional tolerationSeconds field that dictates how long the pod will stay bound to the node after the taint is added. For example,tolerations: \- key: "key1" operator: "Equal" value: "value1" effect: "NoExecute" tolerationSeconds: 3600 means that if this pod is running and a matching taint is added to the node, then the pod will stay bound to the node for 3600 seconds, and then be evicted. If the taint is removed before that time, the pod will not be evicted.Example Use CasesTaints and tolerations are a flexible way to steer pods away from nodes or evict pods that shouldn't be running. A few of the use cases areDedicated Nodes: If you want to dedicate a set of nodes for exclusive use by a particular set of users, you can add a taint to those nodes (say, kubectl taint nodes nodename dedicated=groupName:NoSchedule) and then add a corresponding toleration to their pods (this would be done most easily by writing a custom admission controller). The pods with the tolerations will then be allowed to use the tainted (dedicated) nodes as well as any other nodes in the cluster. If you want to dedicate the nodes to them and ensure they only use the dedicated nodes, then you should additionally add a label similar to the taint to the same set of nodes (e.g. dedicated=groupName), and the admission controller should additionally add a node affinity to require that the pods can only schedule onto nodes labeled with dedicated=groupName.Nodes with Special Hardware: In a cluster where a small subset of nodes have specialized hardware (for example GPUs), it is desirable to keep pods that don't need the specialized hardware off of those nodes, thus leaving room for later-arriving pods that do need the specialized hardware. This can be done by tainting the nodes that have the specialized hardware (e.g. kubectl taint nodes nodename special=true:NoSchedule or kubectl taint nodes nodename special=true:PreferNoSchedule) and adding a corresponding toleration to pods that use the special hardware. As in the dedicated nodes use case, it is probably easiest to apply the tolerations using a custom admission controller. For example, it is recommended to use Extended Resources to represent the special hardware, taint your special hardware nodes with the extended resource name and run the ExtendedResourceToleration admission controller. Now, because the nodes are tainted, no pods without the toleration will schedule on them. But when you submit a pod that requests the extended resource, the ExtendedResourceToleration admission controller will automatically add the correct toleration to the pod and that pod will schedule on the special hardware nodes. This will make sure that these special hardware nodes are dedicated for pods requesting such hardware and you don't have to manually add tolerations to your pods.Taint based Evictions: A per-pod-configurable eviction behavior when there are node problems, which is described in the next section.Taint based EvictionsFEATURE STATE: Kubernetes v1.18 [stable]The node controller automatically taints a Node when certain conditions are true. The following taints are built in:node.kubernetes.io/not-ready: Node is not ready. This corresponds to the NodeCondition Ready being "False".node.kubernetes.io/unreachable: Node is unreachable from the node controller. This corresponds to the NodeCondition Ready being "Unknown".node.kubernetes.io/memory-pressure: Node has memory pressure.node.kubernetes.io/disk-pressure: Node has disk pressure.node.kubernetes.io/pid-pressure: Node has PID pressure.node.kubernetes.io/network-unavailable: Node's network is unavailable.node.kubernetes.io/unschedulable: Node is unschedulable.node.cloudprovider.kubernetes.io/uninitialized: When the kubelet is started with an "external" cloud provider, this taint is set on a node to mark it as unusable. After a controller from the cloud-controller-manager initializes this node, the kubelet removes this taint.In case a node is to be drained, the node controller or the kubelet adds relevant taints with NoExecute effect. This effect is added by default for the node.kubernetes.io/not-ready and node.kubernetes.io/unreachable taints. If the fault condition returns to normal, the kubelet or node controller can remove the relevant taint(s).In some cases when the node is unreachable, the API server is unable to communicate with the kubelet on the node. The decision to delete the pods cannot be communicated to the kubelet until communication with the API server is re-established. In the meantime, the pods that are scheduled for deletion may continue to run on the partitioned node.Note:The control plane limits the rate of adding new taints to nodes. This rate limiting manages the number of evictions that are triggered when many nodes become unreachable at once (for example: if there is a network disruption).You can specify tolerationSeconds for a Pod to define how long that Pod stays bound to a failing or unresponsive Node.For example, you might want to keep an application with a lot of local state bound to node for a long time in the event of network partition, hoping that the partition will recover and thus the pod eviction can be avoided. The toleration you set for that Pod might look like:tolerations: \- key: "node.kubernetes.io/unreachable" operator: "Exists" effect: "NoExecute" tolerationSeconds: 6000 Note:Kubernetes automatically adds a toleration for node.kubernetes.io/not-ready and node.kubernetes.io/unreachable with tolerationSeconds=300, unless you, or a controller, set those tolerations explicitly.These automatically-added tolerations mean that Pods remain bound to Nodes for 5 minutes after one of these problems is detected.DaemonSet pods are created with NoExecute tolerations for the following taints with no tolerationSeconds:node.kubernetes.io/unreachablenode.kubernetes.io/not-readyThis ensures that DaemonSet pods are never evicted due to these problems.Taint Nodes by ConditionThe control plane, using the node controller, automatically creates taints with a NoSchedule effect for node conditions.The scheduler checks taints, not node conditions, when it makes scheduling decisions. This ensures that node conditions don't directly affect scheduling. For example, if the DiskPressure node condition is active, the control plane adds the node.kubernetes.io/disk-pressure taint and does not schedule new pods onto the affected node. If the MemoryPressure node condition is active, the control plane adds the node.kubernetes.io/memory-pressure taint.You can ignore node conditions for newly created pods by adding the corresponding Pod tolerations. The control plane also adds the node.kubernetes.io/memory-pressure toleration on pods that have a QoS class other than BestEffort. This is because Kubernetes treats pods in the Guaranteed or Burstable QoS classes (even pods with no memory request set) as if they are able to cope with memory pressure, while new BestEffort pods are not scheduled onto the affected node.The DaemonSet controller automatically adds the following NoSchedule tolerations to all daemons, to prevent DaemonSets from breaking.node.kubernetes.io/memory-pressurenode.kubernetes.io/disk-pressurenode.kubernetes.io/pid-pressure (1.14 or later)node.kubernetes.io/unschedulable (1.10 or later)node.kubernetes.io/network-unavailable (host network only)Adding these tolerations ensures backward compatibility. You can also add arbitrary tolerations to DaemonSets.Device taints and tolerationsInstead of tainting entire nodes, administrators can also taint individual devices when the cluster uses dynamic resource allocation to manage special hardware. The advantage is that tainting can be targeted towards exactly the hardware that is faulty or needs maintenance. Tolerations are also supported and can be specified when requesting devices. Like taints they apply to all pods which share the same allocated device.What's nextRead about Node-pressure Eviction and how you can configure itRead about Pod PriorityRead about device taints and tolerations3.10.7 - Scheduling FrameworkFEATURE STATE: Kubernetes v1.19 [stable]The scheduling framework is a pluggable architecture for the Kubernetes scheduler. It consists of a set of "plugin" APIs that are compiled directly into the scheduler. These APIs allow most scheduling features to be implemented as plugins, while keeping the scheduling "core" lightweight and maintainable. Refer to the design proposal of the scheduling framework for more technical information on the design of the framework.Framework workflowThe Scheduling Framework defines a few extension points. Scheduler plugins register to be invoked at one or more extension points. Some of these plugins can change the scheduling decisions and some are informational only.Each attempt to schedule one Pod is split into two phases, the scheduling cycle and the binding cycle.Scheduling cycle & binding cycleThe scheduling cycle selects a node for the Pod, and the binding cycle applies that decision to the cluster. Together, a scheduling cycle and binding cycle are referred to as a "scheduling context".Scheduling cycles are run serially, while binding cycles may run concurrently.A scheduling or binding cycle can be aborted if the Pod is determined to be unschedulable or if there is an internal error. The Pod will be returned to the queue and retried.InterfacesThe following picture shows the scheduling context of a Pod and the interfaces that the scheduling framework exposes.One plugin may implement multiple interfaces to perform more complex or stateful tasks.Some interfaces match the scheduler extension points which can be configured through Scheduler Configuration.Scheduling framework extension pointsPreEnqueueThese plugins are called prior to adding Pods to the internal active queue, where Pods are marked as ready for scheduling.Only when all PreEnqueue plugins return Success, the Pod is allowed to enter the active queue. Otherwise, it's placed in the internal unschedulable Pods list, and doesn't get an Unschedulable condition.For more details about how internal scheduler queues work, read Scheduling queue in kube-scheduler.EnqueueExtensionEnqueueExtension is the interface where the plugin can control whether to retry scheduling of Pods rejected by this plugin, based on changes in the cluster. Plugins that implement PreEnqueue, PreFilter, Filter, Reserve or Permit should implement this interface.QueueingHintFEATURE STATE: Kubernetes v1.32 [beta]QueueingHint is a callback function for deciding whether a Pod can be requeued to the active queue or backoff queue. It's executed every time a certain kind of event or change happens in the cluster. When the QueueingHint finds that the event might make the Pod schedulable, the Pod is put into the active queue or the backoff queue so that the scheduler will retry the scheduling of the Pod.Note:QueueingHint evaluation during scheduling is a beta-level feature. In Kubernetes 1.33, this feature gate is enabled by default, and you can disable it via the SchedulerQueueingHints feature gate.QueueSortThese plugins are used to sort Pods in the scheduling queue. A queue sort plugin essentially provides a Less(Pod1, Pod2) function. Only one queue sort plugin may be enabled at a time.PreFilterThese plugins are used to pre-process info about the Pod, or to check certain conditions that the cluster or the Pod must meet. If a PreFilter plugin returns an error, the scheduling cycle is aborted.FilterThese plugins are used to filter out nodes that cannot run the Pod. For each node, the scheduler will call filter plugins in their configured order. If any filter plugin marks the node as infeasible, the remaining plugins will not be called for that node. Nodes may be evaluated concurrently.PostFilterThese plugins are called after the Filter phase, but only when no feasible nodes were found for the pod. Plugins are called in their configured order. If any postFilter plugin marks the node as Schedulable, the remaining plugins will not be called. A typical PostFilter implementation is preemption, which tries to make the pod schedulable by preempting other Pods.PreScoreThese plugins are used to perform "pre-scoring" work, which generates a sharable state for Score plugins to use. If a PreScore plugin returns an error, the scheduling cycle is aborted.ScoreThese plugins are used to rank nodes that have passed the filtering phase. The scheduler will call each scoring plugin for each node. There will be a well defined range of integers representing the minimum and maximum scores. After the NormalizeScore phase, the scheduler will combine node scores from all plugins according to the configured plugin weights.Capacity scoringFEATURE STATE: Kubernetes v1.33 [alpha] (enabled by default: false)The feature gate VolumeCapacityPriority was used in v1.32 to support storage that are statically provisioned. Starting from v1.33, the new feature gate StorageCapacityScoring replaces the old VolumeCapacityPriority gate with added support to dynamically provisioned storage. When StorageCapacityScoring is enabled, the VolumeBinding plugin in the kube-scheduler is extended to score Nodes based on the storage capacity on each of them. This feature is applicable to CSI volumes that supported Storage Capacity, including local storage backed by a CSI driver.NormalizeScoreThese plugins are used to modify scores before the scheduler computes a final ranking of Nodes. A plugin that registers for this extension point will be called with the Score results from the same plugin. This is called once per plugin per scheduling cycle.For example, suppose a plugin BlinkingLightScorer ranks Nodes based on how many blinking lights they have.func ScoreNode(_ *v1.pod, n *v1.Node) (int, error) { return getBlinkingLightCount(n) } However, the maximum count of blinking lights may be small compared to NodeScoreMax. To fix this, BlinkingLightScorer should also register for this extension point.func NormalizeScores(scores map[string]int) { highest := 0 for _, score := range scores { highest = max(highest, score) } for node, score := range scores { scores[node] = score*NodeScoreMax/highest } } If any NormalizeScore plugin returns an error, the scheduling cycle is aborted.Note:Plugins wishing to perform "pre-reserve" work should use the NormalizeScore extension point.ReserveA plugin that implements the Reserve interface has two methods, namely Reserve and Unreserve, that back two informational scheduling phases called Reserve and Unreserve, respectively. Plugins which maintain runtime state (aka "stateful plugins") should use these phases to be notified by the scheduler when resources on a node are being reserved and unreserved for a given Pod.The Reserve phase happens before the scheduler actually binds a Pod to its designated node. It exists to prevent race conditions while the scheduler waits for the bind to succeed. The Reserve method of each Reserve plugin may succeed or fail; if one Reserve method call fails, subsequent plugins are not executed and the Reserve phase is considered to have failed. If the Reserve method of all plugins succeed, the Reserve phase is considered to be successful and the rest of the scheduling cycle and the binding cycle are executed.The Unreserve phase is triggered if the Reserve phase or a later phase fails. When this happens, the Unreserve method of all Reserve plugins will be executed in the reverse order of Reserve method calls. This phase exists to clean up the state associated with the reserved Pod.Caution:The implementation of the Unreserve method in Reserve plugins must be idempotent and may not fail.PermitPermit plugins are invoked at the end of the scheduling cycle for each Pod, to prevent or delay the binding to the candidate node. A permit plugin can do one of the three things:approveOnce all Permit plugins approve a Pod, it is sent for binding.denyIf any Permit plugin denies a Pod, it is returned to the scheduling queue. This will trigger the Unreserve phase in Reserve plugins.wait (with a timeout)If a Permit plugin returns "wait", then the Pod is kept in an internal "waiting" Pods list, and the binding cycle of this Pod starts but directly blocks until it gets approved. If a timeout occurs, wait becomes deny and the Pod is returned to the scheduling queue, triggering the Unreserve phase in Reserve plugins.Note:While any plugin can access the list of "waiting" Pods and approve them (see FrameworkHandle), we expect only the permit plugins to approve binding of reserved Pods that are in "waiting" state. Once a Pod is approved, it is sent to the PreBind phase.PreBindThese plugins are used to perform any work required before a Pod is bound. For example, a pre-bind plugin may provision a network volume and mount it on the target node before allowing the Pod to run there.If any PreBind plugin returns an error, the Pod is rejected and returned to the scheduling queue.BindThese plugins are used to bind a Pod to a Node. Bind plugins will not be called until all PreBind plugins have completed. Each bind plugin is called in the configured order. A bind plugin may choose whether or not to handle the given Pod. If a bind plugin chooses to handle a Pod, the remaining bind plugins are skipped.PostBindThis is an informational interface. Post-bind plugins are called after a Pod is successfully bound. This is the end of a binding cycle, and can be used to clean up associated resources.Plugin APIThere are two steps to the plugin API. First, plugins must register and get configured, then they use the extension point interfaces. Extension point interfaces have the following form.type Plugin interface { Name() string } type QueueSortPlugin interface { Plugin Less(*v1.pod, *v1.pod) bool } type PreFilterPlugin interface { Plugin PreFilter(context.Context, *framework.CycleState, *v1.pod) error } // ... Plugin configurationYou can enable or disable plugins in the scheduler configuration. If you are using Kubernetes v1.18 or later, most scheduling plugins are in use and enabled by default.In addition to default plugins, you can also implement your own scheduling plugins and get them configured along with default plugins. You can visit scheduler-plugins for more details.If you are using Kubernetes v1.18 or later, you can configure a set of plugins as a scheduler profile and then define multiple profiles to fit various kinds of workload. Learn more at multiple profiles.3.10.8 - Dynamic Resource AllocationFEATURE STATE: Kubernetes v1.32 [beta] (enabled by default: false)Dynamic resource allocation is an API for requesting and sharing resources between pods and containers inside a pod. It is a generalization of the persistent volumes API for generic resources. Typically those resources are devices like GPUs.Third-party resource drivers are responsible for tracking and preparing resources, with allocation of resources handled by Kubernetes via structured parameters (introduced in Kubernetes 1.30). Different kinds of resources support arbitrary parameters for defining requirements and initialization.Kubernetes v1.26 through to 1.31 included an (alpha) implementation of classic DRA, which is no longer supported. This documentation, which is for Kubernetes v1.33, explains the current approach to dynamic resource allocation within Kubernetes.Before you beginKubernetes v1.33 includes cluster-level API support for dynamic resource allocation, but it needs to be enabled explicitly. You also must install a resource driver for specific resources that are meant to be managed using this API. If you are not running Kubernetes v1.33, check the documentation for that version of Kubernetes.APIThe resource.k8s.io/v1beta1 and resource.k8s.io/v1beta2 API groups provide these types:ResourceClaimDescribes a request for access to resources in the cluster, for use by workloads. For example, if a workload needs an accelerator device with specific properties, this is how that request is expressed. The status stanza tracks whether this claim has been satisfied and what specific resources have been allocated.ResourceClaimTemplateDefines the spec and some metadata for creating ResourceClaims. Created by a user when deploying a workload. The per-Pod ResourceClaims are then created and removed by Kubernetes automatically.DeviceClassContains pre-defined selection criteria for certain devices and configuration for them. DeviceClasses are created by a cluster administrator when installing a resource driver. Each request to allocate a device in a ResourceClaim must reference exactly one DeviceClass.ResourceSliceUsed by DRA drivers to publish information about resources (typically devices) that are available in the cluster.DeviceTaintRuleUsed by admins or control plane components to add device taints to the devices described in ResourceSlices.All parameters that select devices are defined in the ResourceClaim and DeviceClass with in-tree types. Configuration parameters can be embedded there. Which configuration parameters are valid depends on the DRA driver -- Kubernetes only passes them through without interpreting them.The core/v1 PodSpec defines ResourceClaims that are needed for a Pod in a resourceClaims field. Entries in that list reference either a ResourceClaim or a ResourceClaimTemplate. When referencing a ResourceClaim, all Pods using this PodSpec (for example, inside a Deployment or StatefulSet) share the same ResourceClaim instance. When referencing a ResourceClaimTemplate, each Pod gets its own instance.The resources.claims list for container resources defines whether a container gets access to these resource instances, which makes it possible to share resources between one or more containers.Here is an example for a fictional resource driver. Two ResourceClaim objects will get created for this Pod and each container gets access to one of them.apiVersion: resource.k8s.io/v1beta2 kind: DeviceClass metadata: name: resource.example.com spec: selectors: \- cel: expression: device.driver == "resource-driver.example.com" \--- apiVersion: resource.k8s.io/v1beta2 kind: ResourceClaimTemplate metadata: name: large-black-cat-claim-template spec: spec: devices: requests: \- name: req-0 exactly: deviceClassName: resource.example.com selectors: \- cel: expression: |- device.attributes["resource-driver.example.com"].color == "black" && device.attributes["resource-driver.example.com"].size == "large" \--- apiVersion: v1 kind: Pod metadata: name: pod-with-cats spec: containers: \- name: container0 image: ubuntu:20.04 command: ["sleep", "9999"] resources: claims: \- name: cat-0 \- name: container1 image: ubuntu:20.04 command: ["sleep", "9999"] resources: claims: \- name: cat-1 resourceClaims: \- name: cat-0 resourceClaimTemplateName: large-black-cat-claim-template \- name: cat-1 resourceClaimTemplateName: large-black-cat-claim-template SchedulingThe scheduler is responsible for allocating resources to a ResourceClaim whenever a pod needs them. It does so by retrieving the full list of available resources from ResourceSlice objects, tracking which of those resources have already been allocated to existing ResourceClaims, and then selecting from those resources that remain.The only kind of supported resources at the moment are devices. A device instance has a name and several attributes and capacities. Devices get selected through CEL expressions which check those attributes and capacities. In addition, the set of selected devices also can be restricted to sets which meet certain constraints.The chosen resource is recorded in the ResourceClaim status together with any vendor-specific configuration, so when a pod is about to start on a node, the resource driver on the node has all the information it needs to prepare the resource.By using structured parameters, the scheduler is able to reach a decision without communicating with any DRA resource drivers. It is also able to schedule multiple pods quickly by keeping information about ResourceClaim allocations in memory and writing this information to the ResourceClaim objects in the background while concurrently binding the pod to a node.Monitoring resourcesThe kubelet provides a gRPC service to enable discovery of dynamic resources of running Pods. For more information on the gRPC endpoints, see the resource allocation reporting.Pre-scheduled PodsWhen you - or another API client - create a Pod with spec.nodeName already set, the scheduler gets bypassed. If some ResourceClaim needed by that Pod does not exist yet, is not allocated or not reserved for the Pod, then the kubelet will fail to run the Pod and re-check periodically because those requirements might still get fulfilled later.Such a situation can also arise when support for dynamic resource allocation was not enabled in the scheduler at the time when the Pod got scheduled (version skew, configuration, feature gate, etc.). kube-controller-manager detects this and tries to make the Pod runnable by reserving the required ResourceClaims. However, this only works if those were allocated by the scheduler for some other pod.It is better to avoid bypassing the scheduler because a Pod that is assigned to a node blocks normal resources (RAM, CPU) that then cannot be used for other Pods while the Pod is stuck. To make a Pod run on a specific node while still going through the normal scheduling flow, create the Pod with a node selector that exactly matches the desired node:apiVersion: v1 kind: Pod metadata: name: pod-with-cats spec: nodeSelector: kubernetes.io/hostname: name-of-the-intended-node ... You may also be able to mutate the incoming Pod, at admission time, to unset the .spec.nodeName field and to use a node selector instead.Admin accessFEATURE STATE: Kubernetes v1.32 [alpha] (enabled by default: false)You can mark a request in a ResourceClaim or ResourceClaimTemplate as having privileged features for maintenance and troubleshooting tasks. A request with admin access grants access to in-use devices and may enable additional permissions when making the device available in a container:apiVersion: resource.k8s.io/v1beta2 kind: ResourceClaimTemplate metadata: name: large-black-cat-claim-template spec: spec: devices: requests: \- name: req-0 exactly: deviceClassName: resource.example.com allocationMode: All adminAccess: true If this feature is disabled, the adminAccess field will be removed automatically when creating such a ResourceClaim.Admin access is a privileged mode and should not be granted to regular users in multi-tenant clusters. Starting with Kubernetes v1.33, only users authorized to create ResourceClaim or ResourceClaimTemplate objects in namespaces labeled with resource.k8s.io/admin-access: "true" (case-sensitive) can use the adminAccess field. This ensures that non-admin users cannot misuse the feature.ResourceClaim Device StatusFEATURE STATE: Kubernetes v1.33 [beta] (enabled by default: true)The drivers can report driver-specific device status data for each allocated device in a resource claim. For example, IPs assigned to a network interface device can be reported in the ResourceClaim status.The drivers setting the status, the accuracy of the information depends on the implementation of those DRA Drivers. Therefore, the reported status of the device may not always reflect the real time changes of the state of the device.When the feature is disabled, that field automatically gets cleared when storing the ResourceClaim.A ResourceClaim device status is supported when it is possible, from a DRA driver, to update an existing ResourceClaim where the status.devices field is set.Prioritized ListFEATURE STATE: Kubernetes v1.33 [alpha] (enabled by default: false)You can provide a prioritized list of subrequests for requests in a ResourceClaim. The scheduler will then select the first subrequest that can be allocated. This allows users to specify alternative devices that can be used by the workload if the primary choice is not available.In the example below, the ResourceClaimTemplate requested a device with the color black and the size large. If a device with those attributes are not available, the pod can not be scheduled. With the priotized list feature, a second alternative can be specified, which requests two devices with the color white and size small. The large black device will be allocated if it is available. But if it is not and two small white devices are available, the pod will still be able to run.apiVersion: resource.k8s.io/v1beta2 kind: ResourceClaimTemplate metadata: name: prioritized-list-claim-template spec: spec: devices: requests: \- name: req-0 firstAvailable: \- name: large-black deviceClassName: resource.example.com selectors: \- cel: expression: |- device.attributes["resource-driver.example.com"].color == "black" && device.attributes["resource-driver.example.com"].size == "large" \- name: small-white deviceClassName: resource.example.com selectors: \- cel: expression: |- device.attributes["resource-driver.example.com"].color == "white" && device.attributes["resource-driver.example.com"].size == "small" count: 2 Partitionable DevicesFEATURE STATE: Kubernetes v1.33 [alpha] (enabled by default: false)Devices represented in DRA don't necessarily have to be a single unit connected to a single machine, but can also be a logical device comprised of multiple devices connected to multiple machines. These devices might consume overlapping resources of the underlying phyical devices, meaning that when one logical device is allocated other devices will no longer be available.In the ResourceSlice API, this is represented as a list of named CounterSets, each of which contains a set of named counters. The counters represent the resources available on the physical device that are used by the logical devices advertised through DRA.Logical devices can specify the ConsumesCounters list. Each entry contains a reference to a CounterSet and a set of named counters with the amounts they will consume. So for a device to be allocatable, the referenced counter sets must have sufficient quantity for the counters referenced by the device.Here is an example of two devices, each consuming 6Gi of memory from the a shared counter with 8Gi of memory. Thus, only one of the devices can be allocated at any point in time. The scheduler handles this and it is transparent to the consumer as the ResourceClaim API is not affected.kind: ResourceSlice apiVersion: resource.k8s.io/v1beta2 metadata: name: resourceslice spec: nodeName: worker-1 pool: name: pool generation: 1 resourceSliceCount: 1 driver: dra.example.com sharedCounters: \- name: gpu-1-counters counters: memory: value: 8Gi devices: \- name: device-1 consumesCounters: \- counterSet: gpu-1-counters counters: memory: value: 6Gi \- name: device-2 consumesCounters: \- counterSet: gpu-1-counters counters: memory: value: 6Gi Device taints and tolerationsFEATURE STATE: Kubernetes v1.33 [alpha] (enabled by default: false)Device taints are similar to node taints: a taint has a string key, a string value, and an effect. The effect is applied to the ResourceClaim which is using a tainted device and to all Pods referencing that ResourceClaim. The "NoSchedule" effect prevents scheduling those Pods. Tainted devices are ignored when trying to allocate a ResourceClaim because using them would prevent scheduling of Pods.The "NoExecute" effect implies "NoSchedule" and in addition causes eviction of all Pods which have been scheduled already. This eviction is implemented in the device taint eviction controller in kube-controller-manager by deleting affected Pods.ResourceClaims can tolerate taints. If a taint is tolerated, its effect does not apply. An empty toleration matches all taints. A toleration can be limited to certain effects and/or match certain key/value pairs. A toleration can check that a certain key exists, regardless which value it has, or it can check for specific values of a key. For more information on this matching see the node taint concepts.Eviction can be delayed by tolerating a taint for a certain duration. That delay starts at the time when a taint gets added to a device, which is recorded in a field of the taint.Taints apply as described above also to ResourceClaims allocating "all" devices on a node. All devices must be untainted or all of their taints must be tolerated. Allocating a device with admin access (described above) is not exempt either. An admin using that mode must explicitly tolerate all taints to access tainted devices.Taints can be added to devices in two different ways:Taints set by the driverA DRA driver can add taints to the device information that it publishes in ResourceSlices. Consult the documentation of a DRA driver to learn whether the driver uses taints and what their keys and values are.Taints set by an adminAn admin or a control plane component can taint devices without having to tell the DRA driver to include taints in its device information in ResourceSlices. They do that by creating DeviceTaintRules. Each DeviceTaintRule adds one taint to devices which match the device selector. Without such a selector, no devices are tainted. This makes it harder to accidentally evict all pods using ResourceClaims when leaving out the selector by mistake.Devices can be selected by giving the name of a DeviceClass, driver, pool, and/or device. The DeviceClass selects all devices that are selected by the selectors in that DeviceClass. With just the driver name, an admin can taint all devices managed by that driver, for example while doing some kind of maintenance of that driver across the entire cluster. Adding a pool name can limit the taint to a single node, if the driver manages node-local devices.Finally, adding the device name can select one specific device. The device name and pool name can also be used alone, if desired. For example, drivers for node-local devices are encouraged to use the node name as their pool name. Then tainting with that pool name automatically taints all devices on a node.Drivers might use stable names like "gpu-0" that hide which specific device is currently assigned to that name. To support tainting a specific hardware instance, CEL selectors can be used in a DeviceTaintRule to match a vendor-specific unique ID attribute, if the driver supports one for its hardware.The taint applies as long as the DeviceTaintRule exists. It can be modified and and removed at any time. Here is one example of a DeviceTaintRule for a fictional DRA driver:apiVersion: resource.k8s.io/v1alpha3 kind: DeviceTaintRule metadata: name: example spec: # The entire hardware installation for this # particular driver is broken. # Evict all pods and don't schedule new ones. deviceSelector: driver: dra.example.com taint: key: dra.example.com/unhealthy value: Broken effect: NoExecute Enabling dynamic resource allocationDynamic resource allocation is a beta feature which is off by default and only enabled when the DynamicResourceAllocation feature gate and the resource.k8s.io/v1beta1 and resource.k8s.io/v1beta2 API groups are enabled. For details on that, see the --feature-gates and --runtime-config kube-apiserver parameters. kube-scheduler, kube-controller-manager and kubelet also need the feature gate.When a resource driver reports the status of the devices, then the DRAResourceClaimDeviceStatus feature gate has to be enabled in addition to DynamicResourceAllocation.A quick check whether a Kubernetes cluster supports the feature is to list DeviceClass objects with:kubectl get deviceclasses If your cluster supports dynamic resource allocation, the response is either a list of DeviceClass objects or:No resources found If not supported, this error is printed instead:error: the server doesn't have a resource type "deviceclasses" The default configuration of kube-scheduler enables the "DynamicResources" plugin if and only if the feature gate is enabled and when using the v1 configuration API. Custom configurations may have to be modified to include it.In addition to enabling the feature in the cluster, a resource driver also has to be installed. Please refer to the driver's documentation for details.Enabling admin accessAdmin access is an alpha feature and only enabled when the DRAAdminAccess feature gate is enabled in the kube-apiserver and kube-scheduler.Enabling Device StatusResourceClaim Device Status is an alpha feature and only enabled when the DRAResourceClaimDeviceStatus feature gate is enabled in the kube-apiserver.Enabling Prioritized ListPrioritized List) is an alpha feature and only enabled when the DRAPrioritizedList feature gate is enabled in the kube-apiserver and kube-scheduler. It also requires that the DynamicResourceAllocation feature gate is enabled.Enabling Partitionable DevicesPartitionable Devices is an alpha feature and only enabled when the DRAPartitionableDevices feature gate is enabled in the kube-apiserver and kube-scheduler.Enabling device taints and tolerationsDevice taints and tolerations is an alpha feature and only enabled when the DRADeviceTaints feature gate is enabled in the kube-apiserver, kube-controller-manager and kube-scheduler. To use DeviceTaintRules, the resource.k8s.io/v1alpha3 API version must be enabled.What's nextFor more information on the design, see the Dynamic Resource Allocation with Structured Parameters KEP.3.10.9 - Scheduler Performance TuningFEATURE STATE: Kubernetes v1.14 [beta]kube-scheduler is the Kubernetes default scheduler. It is responsible for placement of Pods on Nodes in a cluster.Nodes in a cluster that meet the scheduling requirements of a Pod are called feasible Nodes for the Pod. The scheduler finds feasible Nodes for a Pod and then runs a set of functions to score the feasible Nodes, picking a Node with the highest score among the feasible ones to run the Pod. The scheduler then notifies the API server about this decision in a process called Binding.This page explains performance tuning optimizations that are relevant for large Kubernetes clusters.In large clusters, you can tune the scheduler's behaviour balancing scheduling outcomes between latency (new Pods are placed quickly) and accuracy (the scheduler rarely makes poor placement decisions).You configure this tuning setting via kube-scheduler setting percentageOfNodesToScore. This KubeSchedulerConfiguration setting determines a threshold for scheduling nodes in your cluster.Setting the thresholdThe percentageOfNodesToScore option accepts whole numeric values between 0 and 100. The value 0 is a special number which indicates that the kube-scheduler should use its compiled-in default. If you set percentageOfNodesToScore above 100, kube-scheduler acts as if you had set a value of 100.To change the value, edit the kube-scheduler configuration file and then restart the scheduler. In many cases, the configuration file can be found at /etc/kubernetes/config/kube-scheduler.yaml.After you have made this change, you can runkubectl get pods -n kube-system | grep kube-scheduler to verify that the kube-scheduler component is healthy.Node scoring thresholdTo improve scheduling performance, the kube-scheduler can stop looking for feasible nodes once it has found enough of them. In large clusters, this saves time compared to a naive approach that would consider every node.You specify a threshold for how many nodes are enough, as a whole number percentage of all the nodes in your cluster. The kube-scheduler converts this into an integer number of nodes. During scheduling, if the kube-scheduler has identified enough feasible nodes to exceed the configured percentage, the kube-scheduler stops searching for more feasible nodes and moves on to the scoring phase.How the scheduler iterates over Nodes describes the process in detail.Default thresholdIf you don't specify a threshold, Kubernetes calculates a figure using a linear formula that yields 50% for a 100-node cluster and yields 10% for a 5000-node cluster. The lower bound for the automatic value is 5%.This means that the kube-scheduler always scores at least 5% of your cluster no matter how large the cluster is, unless you have explicitly set percentageOfNodesToScore to be smaller than 5.If you want the scheduler to score all nodes in your cluster, set percentageOfNodesToScore to 100.ExampleBelow is an example configuration that sets percentageOfNodesToScore to 50%.apiVersion: kubescheduler.config.k8s.io/v1alpha1 kind: KubeSchedulerConfiguration algorithmSource: provider: DefaultProvider ... percentageOfNodesToScore: 50 Tuning percentageOfNodesToScorepercentageOfNodesToScore must be a value between 1 and 100 with the default value being calculated based on the cluster size. There is also a hardcoded minimum value of 100 nodes.Note:In clusters with less than 100 feasible nodes, the scheduler still checks all the nodes because there are not enough feasible nodes to stop the scheduler's search early.In a small cluster, if you set a low value for percentageOfNodesToScore, your change will have no or little effect, for a similar reason.If your cluster has several hundred Nodes or fewer, leave this configuration option at its default value. Making changes is unlikely to improve the scheduler's performance significantly.An important detail to consider when setting this value is that when a smaller number of nodes in a cluster are checked for feasibility, some nodes are not sent to be scored for a given Pod. As a result, a Node which could possibly score a higher value for running the given Pod might not even be passed to the scoring phase. This would result in a less than ideal placement of the Pod.You should avoid setting percentageOfNodesToScore very low so that kube-scheduler does not make frequent, poor Pod placement decisions. Avoid setting the percentage to anything below 10%, unless the scheduler's throughput is critical for your application and the score of nodes is not important. In other words, you prefer to run the Pod on any Node as long as it is feasible.How the scheduler iterates over NodesThis section is intended for those who want to understand the internal details of this feature.In order to give all the Nodes in a cluster a fair chance of being considered for running Pods, the scheduler iterates over the nodes in a round robin fashion. You can imagine that Nodes are in an array. The scheduler starts from the start of the array and checks feasibility of the nodes until it finds enough Nodes as specified by percentageOfNodesToScore. For the next Pod, the scheduler continues from the point in the Node array that it stopped at when checking feasibility of Nodes for the previous Pod.If Nodes are in multiple zones, the scheduler iterates over Nodes in various zones to ensure that Nodes from different zones are considered in the feasibility checks. As an example, consider six nodes in two zones:Zone 1: Node 1, Node 2, Node 3, Node 4 Zone 2: Node 5, Node 6 The Scheduler evaluates feasibility of the nodes in this order:Node 1, Node 5, Node 2, Node 6, Node 3, Node 4 After going over all the Nodes, it goes back to Node 1.What's nextCheck the kube-scheduler configuration reference (v1)3.10.10 - Resource Bin PackingIn the scheduling-plugin NodeResourcesFit of kube-scheduler, there are two scoring strategies that support the bin packing of resources: MostAllocated and RequestedToCapacityRatio.Enabling bin packing using MostAllocated strategyThe MostAllocated strategy scores the nodes based on the utilization of resources, favoring the ones with higher allocation. For each resource type, you can set a weight to modify its influence in the node score.To set the MostAllocated strategy for the NodeResourcesFit plugin, use a scheduler configuration similar to the following:apiVersion: kubescheduler.config.k8s.io/v1 kind: KubeSchedulerConfiguration profiles: \- pluginConfig: \- args: scoringStrategy: resources: \- name: cpu weight: 1 \- name: memory weight: 1 \- name: intel.com/foo weight: 3 \- name: intel.com/bar weight: 3 type: MostAllocated name: NodeResourcesFit To learn more about other parameters and their default configuration, see the API documentation for NodeResourcesFitArgs.Enabling bin packing using RequestedToCapacityRatioThe RequestedToCapacityRatio strategy allows the users to specify the resources along with weights for each resource to score nodes based on the request to capacity ratio. This allows users to bin pack extended resources by using appropriate parameters to improve the utilization of scarce resources in large clusters. It favors nodes according to a configured function of the allocated resources. The behavior of the RequestedToCapacityRatio in the NodeResourcesFit score function can be controlled by the scoringStrategy field. Within the scoringStrategy field, you can configure two parameters: requestedToCapacityRatio and resources. The shape in the requestedToCapacityRatio parameter allows the user to tune the function as least requested or most requested based on utilization and score values. The resources parameter comprises both the name of the resource to be considered during scoring and its corresponding weight, which specifies the weight of each resource.Below is an example configuration that sets the bin packing behavior for extended resources intel.com/foo and intel.com/bar using the requestedToCapacityRatio field.apiVersion: kubescheduler.config.k8s.io/v1 kind: KubeSchedulerConfiguration profiles: \- pluginConfig: \- args: scoringStrategy: resources: \- name: intel.com/foo weight: 3 \- name: intel.com/bar weight: 3 requestedToCapacityRatio: shape: \- utilization: 0 score: 0 \- utilization: 100 score: 10 type: RequestedToCapacityRatio name: NodeResourcesFit Referencing the KubeSchedulerConfiguration file with the kube-scheduler flag --config=/path/to/config/file will pass the configuration to the scheduler.To learn more about other parameters and their default configuration, see the API documentation for NodeResourcesFitArgs.Tuning the score functionshape is used to specify the behavior of the RequestedToCapacityRatio function.shape: \- utilization: 0 score: 0 \- utilization: 100 score: 10 The above arguments give the node a score of 0 if utilization is 0% and 10 for utilization 100%, thus enabling bin packing behavior. To enable least requested the score value must be reversed as follows.shape: \- utilization: 0 score: 10 \- utilization: 100 score: 0 resources is an optional parameter which defaults to:resources: \- name: cpu weight: 1 \- name: memory weight: 1 It can be used to add extended resources as follows:resources: \- name: intel.com/foo weight: 5 \- name: cpu weight: 3 \- name: memory weight: 1 The weight parameter is optional and is set to 1 if not specified. Also, the weight cannot be set to a negative value.Node scoring for capacity allocationThis section is intended for those who want to understand the internal details of this feature. Below is an example of how the node score is calculated for a given set of values.Requested resources:intel.com/foo : 2 memory: 256MB cpu: 2 Resource weights:intel.com/foo : 5 memory: 1 cpu: 3 FunctionShapePoint {{0, 0}, {100, 10}}Node 1 spec:Available: intel.com/foo: 4 memory: 1 GB cpu: 8 Used: intel.com/foo: 1 memory: 256MB cpu: 1 Node score:intel.com/foo = resourceScoringFunction((2+1),4) = (100 - ((4-3)*100/4) = (100 - 25) = 75 # requested + used = 75% * available = rawScoringFunction(75) = 7 # floor(75/10) memory = resourceScoringFunction((256+256),1024) = (100 -((1024-512)*100/1024)) = 50 # requested + used = 50% * available = rawScoringFunction(50) = 5 # floor(50/10) cpu = resourceScoringFunction((2+1),8) = (100 -((8-3)*100/8)) = 37.5 # requested + used = 37.5% * available = rawScoringFunction(37.5) = 3 # floor(37.5/10) NodeScore = ((7 * 5) + (5 * 1) + (3 * 3)) / (5 + 1 + 3) = 5 Node 2 spec:Available: intel.com/foo: 8 memory: 1GB cpu: 8 Used: intel.com/foo: 2 memory: 512MB cpu: 6 Node score:intel.com/foo = resourceScoringFunction((2+2),8) = (100 - ((8-4)*100/8) = (100 - 50) = 50 = rawScoringFunction(50) = 5 memory = resourceScoringFunction((256+512),1024) = (100 -((1024-768)*100/1024)) = 75 = rawScoringFunction(75) = 7 cpu = resourceScoringFunction((2+6),8) = (100 -((8-8)*100/8)) = 100 = rawScoringFunction(100) = 10 NodeScore = ((5 * 5) + (7 * 1) + (10 * 3)) / (5 + 1 + 3) = 7 What's nextRead more about the scheduling frameworkRead more about scheduler configuration3.10.11 - Pod Priority and PreemptionFEATURE STATE: Kubernetes v1.14 [stable]Pods can have priority. Priority indicates the importance of a Pod relative to other Pods. If a Pod cannot be scheduled, the scheduler tries to preempt (evict) lower priority Pods to make scheduling of the pending Pod possible.Warning:In a cluster where not all users are trusted, a malicious user could create Pods at the highest possible priorities, causing other Pods to be evicted/not get scheduled. An administrator can use ResourceQuota to prevent users from creating pods at high priorities.See limit Priority Class consumption by default for details.How to use priority and preemptionTo use priority and preemption:Add one or more PriorityClasses.Create Pods withpriorityClassName set to one of the added PriorityClasses. Of course you do not need to create the Pods directly; normally you would add priorityClassName to the Pod template of a collection object like a Deployment.Keep reading for more information about these steps.Note:Kubernetes already ships with two PriorityClasses: system-cluster-critical and system-node-critical. These are common classes and are used to ensure that critical components are always scheduled first.PriorityClassA PriorityClass is a non-namespaced object that defines a mapping from a priority class name to the integer value of the priority. The name is specified in the name field of the PriorityClass object's metadata. The value is specified in the required value field. The higher the value, the higher the priority. The name of a PriorityClass object must be a valid DNS subdomain name, and it cannot be prefixed with system-.A PriorityClass object can have any 32-bit integer value smaller than or equal to 1 billion. This means that the range of values for a PriorityClass object is from -2147483648 to 1000000000 inclusive. Larger numbers are reserved for built-in PriorityClasses that represent critical system Pods. A cluster admin should create one PriorityClass object for each such mapping that they want.PriorityClass also has two optional fields: globalDefault and description. The globalDefault field indicates that the value of this PriorityClass should be used for Pods without a priorityClassName. Only one PriorityClass with globalDefault set to true can exist in the system. If there is no PriorityClass with globalDefault set, the priority of Pods with no priorityClassName is zero.The description field is an arbitrary string. It is meant to tell users of the cluster when they should use this PriorityClass.Notes about PodPriority and existing clustersIf you upgrade an existing cluster without this feature, the priority of your existing Pods is effectively zero.Addition of a PriorityClass with globalDefault set to true does not change the priorities of existing Pods. The value of such a PriorityClass is used only for Pods created after the PriorityClass is added.If you delete a PriorityClass, existing Pods that use the name of the deleted PriorityClass remain unchanged, but you cannot create more Pods that use the name of the deleted PriorityClass.Example PriorityClassapiVersion: scheduling.k8s.io/v1 kind: PriorityClass metadata: name: high-priority value: 1000000 globalDefault: false description: "This priority class should be used for XYZ service pods only." Non-preempting PriorityClassFEATURE STATE: Kubernetes v1.24 [stable]Pods with preemptionPolicy: Never will be placed in the scheduling queue ahead of lower-priority pods, but they cannot preempt other pods. A non-preempting pod waiting to be scheduled will stay in the scheduling queue, until sufficient resources are free, and it can be scheduled. Non-preempting pods, like other pods, are subject to scheduler back-off. This means that if the scheduler tries these pods and they cannot be scheduled, they will be retried with lower frequency, allowing other pods with lower priority to be scheduled before them.Non-preempting pods may still be preempted by other, high-priority pods.preemptionPolicy defaults to PreemptLowerPriority, which will allow pods of that PriorityClass to preempt lower-priority pods (as is existing default behavior). If preemptionPolicy is set to Never, pods in that PriorityClass will be non-preempting.An example use case is for data science workloads. A user may submit a job that they want to be prioritized above other workloads, but do not wish to discard existing work by preempting running pods. The high priority job with preemptionPolicy: Never will be scheduled ahead of other queued pods, as soon as sufficient cluster resources "naturally" become free.Example Non-preempting PriorityClassapiVersion: scheduling.k8s.io/v1 kind: PriorityClass metadata: name: high-priority-nonpreempting value: 1000000 preemptionPolicy: Never globalDefault: false description: "This priority class will not cause other pods to be preempted." Pod priorityAfter you have one or more PriorityClasses, you can create Pods that specify one of those PriorityClass names in their specifications. The priority admission controller uses the priorityClassName field and populates the integer value of the priority. If the priority class is not found, the Pod is rejected.The following YAML is an example of a Pod configuration that uses the PriorityClass created in the preceding example. The priority admission controller checks the specification and resolves the priority of the Pod to 1000000.apiVersion: v1 kind: Pod metadata: name: nginx labels: env: test spec: containers: \- name: nginx image: nginx imagePullPolicy: IfNotPresent priorityClassName: high-priority Effect of Pod priority on scheduling orderWhen Pod priority is enabled, the scheduler orders pending Pods by their priority and a pending Pod is placed ahead of other pending Pods with lower priority in the scheduling queue. As a result, the higher priority Pod may be scheduled sooner than Pods with lower priority if its scheduling requirements are met. If such Pod cannot be scheduled, the scheduler will continue and try to schedule other lower priority Pods.PreemptionWhen Pods are created, they go to a queue and wait to be scheduled. The scheduler picks a Pod from the queue and tries to schedule it on a Node. If no Node is found that satisfies all the specified requirements of the Pod, preemption logic is triggered for the pending Pod. Let's call the pending Pod P. Preemption logic tries to find a Node where removal of one or more Pods with lower priority than P would enable P to be scheduled on that Node. If such a Node is found, one or more lower priority Pods get evicted from the Node. After the Pods are gone, P can be scheduled on the Node.User exposed informationWhen Pod P preempts one or more Pods on Node N, nominatedNodeName field of Pod P's status is set to the name of Node N. This field helps the scheduler track resources reserved for Pod P and also gives users information about preemptions in their clusters.Please note that Pod P is not necessarily scheduled to the "nominated Node". The scheduler always tries the "nominated Node" before iterating over any other nodes. After victim Pods are preempted, they get their graceful termination period. If another node becomes available while scheduler is waiting for the victim Pods to terminate, scheduler may use the other node to schedule Pod P. As a result nominatedNodeName and nodeName of Pod spec are not always the same. Also, if the scheduler preempts Pods on Node N, but then a higher priority Pod than Pod P arrives, the scheduler may give Node N to the new higher priority Pod. In such a case, scheduler clears nominatedNodeName of Pod P. By doing this, scheduler makes Pod P eligible to preempt Pods on another Node.Limitations of preemptionGraceful termination of preemption victimsWhen Pods are preempted, the victims get their graceful termination period. They have that much time to finish their work and exit. If they don't, they are killed. This graceful termination period creates a time gap between the point that the scheduler preempts Pods and the time when the pending Pod (P) can be scheduled on the Node (N). In the meantime, the scheduler keeps scheduling other pending Pods. As victims exit or get terminated, the scheduler tries to schedule Pods in the pending queue. Therefore, there is usually a time gap between the point that scheduler preempts victims and the time that Pod P is scheduled. In order to minimize this gap, one can set graceful termination period of lower priority Pods to zero or a small number.PodDisruptionBudget is supported, but not guaranteedA PodDisruptionBudget (PDB) allows application owners to limit the number of Pods of a replicated application that are down simultaneously from voluntary disruptions. Kubernetes supports PDB when preempting Pods, but respecting PDB is best effort. The scheduler tries to find victims whose PDB are not violated by preemption, but if no such victims are found, preemption will still happen, and lower priority Pods will be removed despite their PDBs being violated.Inter-Pod affinity on lower-priority PodsA Node is considered for preemption only when the answer to this question is yes: "If all the Pods with lower priority than the pending Pod are removed from the Node, can the pending Pod be scheduled on the Node?"Note:Preemption does not necessarily remove all lower-priority Pods. If the pending Pod can be scheduled by removing fewer than all lower-priority Pods, then only a portion of the lower-priority Pods are removed. Even so, the answer to the preceding question must be yes. If the answer is no, the Node is not considered for preemption.If a pending Pod has inter-pod affinity to one or more of the lower-priority Pods on the Node, the inter-Pod affinity rule cannot be satisfied in the absence of those lower-priority Pods. In this case, the scheduler does not preempt any Pods on the Node. Instead, it looks for another Node. The scheduler might find a suitable Node or it might not. There is no guarantee that the pending Pod can be scheduled.Our recommended solution for this problem is to create inter-Pod affinity only towards equal or higher priority Pods.Cross node preemptionSuppose a Node N is being considered for preemption so that a pending Pod P can be scheduled on N. P might become feasible on N only if a Pod on another Node is preempted. Here's an example:Pod P is being considered for Node N.Pod Q is running on another Node in the same Zone as Node N.Pod P has Zone-wide anti-affinity with Pod Q (topologyKey: topology.kubernetes.io/zone).There are no other cases of anti-affinity between Pod P and other Pods in the Zone.In order to schedule Pod P on Node N, Pod Q can be preempted, but scheduler does not perform cross-node preemption. So, Pod P will be deemed unschedulable on Node N.If Pod Q were removed from its Node, the Pod anti-affinity violation would be gone, and Pod P could possibly be scheduled on Node N.We may consider adding cross Node preemption in future versions if there is enough demand and if we find an algorithm with reasonable performance.TroubleshootingPod priority and preemption can have unwanted side effects. Here are some examples of potential problems and ways to deal with them.Pods are preempted unnecessarilyPreemption removes existing Pods from a cluster under resource pressure to make room for higher priority pending Pods. If you give high priorities to certain Pods by mistake, these unintentionally high priority Pods may cause preemption in your cluster. Pod priority is specified by setting the priorityClassName field in the Pod's specification. The integer value for priority is then resolved and populated to the priority field of podSpec.To address the problem, you can change the priorityClassName for those Pods to use lower priority classes, or leave that field empty. An empty priorityClassName is resolved to zero by default.When a Pod is preempted, there will be events recorded for the preempted Pod. Preemption should happen only when a cluster does not have enough resources for a Pod. In such cases, preemption happens only when the priority of the pending Pod (preemptor) is higher than the victim Pods. Preemption must not happen when there is no pending Pod, or when the pending Pods have equal or lower priority than the victims. If preemption happens in such scenarios, please file an issue.Pods are preempted, but the preemptor is not scheduledWhen pods are preempted, they receive their requested graceful termination period, which is by default 30 seconds. If the victim Pods do not terminate within this period, they are forcibly terminated. Once all the victims go away, the preemptor Pod can be scheduled.While the preemptor Pod is waiting for the victims to go away, a higher priority Pod may be created that fits on the same Node. In this case, the scheduler will schedule the higher priority Pod instead of the preemptor.This is expected behavior: the Pod with the higher priority should take the place of a Pod with a lower priority.Higher priority Pods are preempted before lower priority podsThe scheduler tries to find nodes that can run a pending Pod. If no node is found, the scheduler tries to remove Pods with lower priority from an arbitrary node in order to make room for the pending pod. If a node with low priority Pods is not feasible to run the pending Pod, the scheduler may choose another node with higher priority Pods (compared to the Pods on the other node) for preemption. The victims must still have lower priority than the preemptor Pod.When there are multiple nodes available for preemption, the scheduler tries to choose the node with a set of Pods with lowest priority. However, if such Pods have PodDisruptionBudget that would be violated if they are preempted then the scheduler may choose another node with higher priority Pods.When multiple nodes exist for preemption and none of the above scenarios apply, the scheduler chooses a node with the lowest priority.Interactions between Pod priority and quality of servicePod priority and QoS class are two orthogonal features with few interactions and no default restrictions on setting the priority of a Pod based on its QoS classes. The scheduler's preemption logic does not consider QoS when choosing preemption targets. Preemption considers Pod priority and attempts to choose a set of targets with the lowest priority. Higher-priority Pods are considered for preemption only if the removal of the lowest priority Pods is not sufficient to allow the scheduler to schedule the preemptor Pod, or if the lowest priority Pods are protected by PodDisruptionBudget.The kubelet uses Priority to determine pod order for node-pressure eviction. You can use the QoS class to estimate the order in which pods are most likely to get evicted. The kubelet ranks pods for eviction based on the following factors:Whether the starved resource usage exceeds requestsPod PriorityAmount of resource usage relative to requestsSee Pod selection for kubelet eviction for more details.kubelet node-pressure eviction does not evict Pods when their usage does not exceed their requests. If a Pod with lower priority is not exceeding its requests, it won't be evicted. Another Pod with higher priority that exceeds its requests may be evicted.What's nextRead about using ResourceQuotas in connection with PriorityClasses: limit Priority Class consumption by defaultLearn about Pod DisruptionLearn about API-initiated EvictionLearn about Node-pressure Eviction3.10.12 - Node-pressure EvictionNode-pressure eviction is the process by which the kubelet proactively terminates pods to reclaim resources on nodes.FEATURE STATE: Kubernetes v1.31 [beta] (enabled by default: true)Note:The split image filesystem feature, which enables support for the containerfs filesystem, adds several new eviction signals, thresholds and metrics. To use containerfs, the Kubernetes release v1.33 requires the KubeletSeparateDiskGC feature gate to be enabled. Currently, only CRI-O (v1.29 or higher) offers the containerfs filesystem support.The kubelet monitors resources like memory, disk space, and filesystem inodes on your cluster's nodes. When one or more of these resources reach specific consumption levels, the kubelet can proactively fail one or more pods on the node to reclaim resources and prevent starvation.During a node-pressure eviction, the kubelet sets the phase for the selected pods to Failed, and terminates the Pod.Node-pressure eviction is not the same as API-initiated eviction.The kubelet does not respect your configured PodDisruptionBudget or the pod's terminationGracePeriodSeconds. If you use soft eviction thresholds, the kubelet respects your configured eviction-max-pod-grace-period. If you use hard eviction thresholds, the kubelet uses a 0s grace period (immediate shutdown) for termination.Self healing behaviorThe kubelet attempts to reclaim node-level resources before it terminates end-user pods. For example, it removes unused container images when disk resources are starved.If the pods are managed by a workload management object (such as StatefulSet or Deployment) that replaces failed pods, the control plane (kube-controller-manager) creates new pods in place of the evicted pods.Self healing for static podsIf you are running a static pod on a node that is under resource pressure, the kubelet may evict that static Pod. The kubelet then tries to create a replacement, because static Pods always represent an intent to run a Pod on that node.The kubelet takes the priority of the static pod into account when creating a replacement. If the static pod manifest specifies a low priority, and there are higher-priority Pods defined within the cluster's control plane, and the node is under resource pressure, the kubelet may not be able to make room for that static pod. The kubelet continues to attempt to run all static pods even when there is resource pressure on a node.Eviction signals and thresholdsThe kubelet uses various parameters to make eviction decisions, like the following:Eviction signalsEviction thresholdsMonitoring intervalsEviction signalsEviction signals are the current state of a particular resource at a specific point in time. The kubelet uses eviction signals to make eviction decisions by comparing the signals to eviction thresholds, which are the minimum amount of the resource that should be available on the node.The kubelet uses the following eviction signals:Eviction SignalDescriptionLinux Onlymemory.availablememory.available := node.status.capacity[memory] - node.stats.memory.workingSetnodefs.availablenodefs.available := node.stats.fs.availablenodefs.inodesFreenodefs.inodesFree := node.stats.fs.inodesFree•imagefs.availableimagefs.available := node.stats.runtime.imagefs.availableimagefs.inodesFreeimagefs.inodesFree := node.stats.runtime.imagefs.inodesFree•containerfs.availablecontainerfs.available := node.stats.runtime.containerfs.availablecontainerfs.inodesFreecontainerfs.inodesFree := node.stats.runtime.containerfs.inodesFree•pid.availablepid.available := node.stats.rlimit.maxpid - node.stats.rlimit.curproc•In this table, the Description column shows how kubelet gets the value of the signal. Each signal supports either a percentage or a literal value. The kubelet calculates the percentage value relative to the total capacity associated with the signal.Memory signalsOn Linux nodes, the value for memory.available is derived from the cgroupfs instead of tools like free -m. This is important because free -m does not work in a container, and if users use the node allocatable feature, out of resource decisions are made local to the end user Pod part of the cgroup hierarchy as well as the root node. This script or cgroupv2 script reproduces the same set of steps that the kubelet performs to calculate memory.available. The kubelet excludes inactive_file (the number of bytes of file-backed memory on the inactive LRU list) from its calculation, as it assumes that memory is reclaimable under pressure.On Windows nodes, the value for memory.available is derived from the node's global memory commit levels (queried through the GetPerformanceInfo() system call) by subtracting the node's global CommitTotal from the node's CommitLimit. Please note that CommitLimit can change if the node's page-file size changes!Filesystem signalsThe kubelet recognizes three specific filesystem identifiers that can be used with eviction signals (.inodesFree or .available):nodefs: The node's main filesystem, used for local disk volumes, emptyDir volumes not backed by memory, log storage, ephemeral storage, and more. For example, nodefs contains /var/lib/kubelet.imagefs: An optional filesystem that container runtimes can use to store container images (which are the read-only layers) and container writable layers.containerfs: An optional filesystem that container runtime can use to store the writeable layers. Similar to the main filesystem (see nodefs), it's used to store local disk volumes, emptyDir volumes not backed by memory, log storage, and ephemeral storage, except for the container images. When containerfs is used, the imagefs filesystem can be split to only store images (read-only layers) and nothing else.As such, kubelet generally allows three options for container filesystems:Everything is on the single nodefs, also referred to as "rootfs" or simply "root", and there is no dedicated image filesystem.Container storage (see nodefs) is on a dedicated disk, and imagefs (writable and read-only layers) is separate from the root filesystem. This is often referred to as "split disk" (or "separate disk") filesystem.Container filesystem containerfs (same as nodefs plus writable layers) is on root and the container images (read-only layers) are stored on separate imagefs. This is often referred to as "split image" filesystem.The kubelet will attempt to auto-discover these filesystems with their current configuration directly from the underlying container runtime and will ignore other local node filesystems.The kubelet does not support other container filesystems or storage configurations, and it does not currently support multiple filesystems for images and containers.Deprecated kubelet garbage collection featuresSome kubelet garbage collection features are deprecated in favor of eviction:Existing FlagRationale--maximum-dead-containersdeprecated once old logs are stored outside of container's context--maximum-dead-containers-per-containerdeprecated once old logs are stored outside of container's context--minimum-container-ttl-durationdeprecated once old logs are stored outside of container's contextEviction thresholdsYou can specify custom eviction thresholds for the kubelet to use when it makes eviction decisions. You can configure soft and hard eviction thresholds.Eviction thresholds have the form [eviction-signal][operator][quantity], where:eviction-signal is the eviction signal to use.operator is the relational operator you want, such as < (less than).quantity is the eviction threshold amount, such as 1Gi. The value of quantity must match the quantity representation used by Kubernetes. You can use either literal values or percentages (%).For example, if a node has 10GiB of total memory and you want trigger eviction if the available memory falls below 1GiB, you can define the eviction threshold as either memory.available<10% or memory.available<1Gi (you cannot use both).Soft eviction thresholdsA soft eviction threshold pairs an eviction threshold with a required administrator-specified grace period. The kubelet does not evict pods until the grace period is exceeded. The kubelet returns an error on startup if you do not specify a grace period.You can specify both a soft eviction threshold grace period and a maximum allowed pod termination grace period for kubelet to use during evictions. If you specify a maximum allowed grace period and the soft eviction threshold is met, the kubelet uses the lesser of the two grace periods. If you do not specify a maximum allowed grace period, the kubelet kills evicted pods immediately without graceful termination.You can use the following flags to configure soft eviction thresholds:eviction-soft: A set of eviction thresholds like memory.available<1.5Gi that can trigger pod eviction if held over the specified grace period.eviction-soft-grace-period: A set of eviction grace periods like memory.available=1m30s that define how long a soft eviction threshold must hold before triggering a Pod eviction.eviction-max-pod-grace-period: The maximum allowed grace period (in seconds) to use when terminating pods in response to a soft eviction threshold being met.Hard eviction thresholdsA hard eviction threshold has no grace period. When a hard eviction threshold is met, the kubelet kills pods immediately without graceful termination to reclaim the starved resource.You can use the eviction-hard flag to configure a set of hard eviction thresholds like memory.available<1Gi.The kubelet has the following default hard eviction thresholds:memory.available<100Mi (Linux nodes)memory.available<500Mi (Windows nodes)nodefs.available<10%imagefs.available<15%nodefs.inodesFree<5% (Linux nodes)imagefs.inodesFree<5% (Linux nodes)These default values of hard eviction thresholds will only be set if none of the parameters is changed. If you change the value of any parameter, then the values of other parameters will not be inherited as the default values and will be set to zero. In order to provide custom values, you should provide all the thresholds respectively. You can also set the kubelet config MergeDefaultEvictionSettings to true in the kubelet configuration file. If set to true and any paramater is changed, then the other parameters will inherit their default values instead of 0.The containerfs.available and containerfs.inodesFree (Linux nodes) default eviction thresholds will be set as follows:If a single filesystem is used for everything, then containerfs thresholds are set the same as nodefs.If separate filesystems are configured for both images and containers, then containerfs thresholds are set the same as imagefs.Setting custom overrides for thresholds related to containersfs is currently not supported, and a warning will be issued if an attempt to do so is made; any provided custom values will, as such, be ignored.Eviction monitoring intervalThe kubelet evaluates eviction thresholds based on its configured housekeeping-interval, which defaults to 10s.Node conditionsThe kubelet reports node conditions to reflect that the node is under pressure because hard or soft eviction threshold is met, independent of configured grace periods.The kubelet maps eviction signals to node conditions as follows:Node ConditionEviction SignalDescriptionMemoryPressurememory.availableAvailable memory on the node has satisfied an eviction thresholdDiskPressurenodefs.available, nodefs.inodesFree, imagefs.available, imagefs.inodesFree, containerfs.available, or containerfs.inodesFreeAvailable disk space and inodes on either the node's root filesystem, image filesystem, or container filesystem has satisfied an eviction thresholdPIDPressurepid.availableAvailable processes identifiers on the (Linux) node has fallen below an eviction thresholdThe control plane also maps these node conditions to taints.The kubelet updates the node conditions based on the configured \--node-status-update-frequency, which defaults to 10s.Node condition oscillationIn some cases, nodes oscillate above and below soft eviction thresholds without holding for the defined grace periods. This causes the reported node condition to constantly switch between true and false, leading to bad eviction decisions.To protect against oscillation, you can use the eviction-pressure-transition-period flag, which controls how long the kubelet must wait before transitioning a node condition to a different state. The transition period has a default value of 5m.Reclaiming node level resourcesThe kubelet tries to reclaim node-level resources before it evicts end-user pods.When a DiskPressure node condition is reported, the kubelet reclaims node-level resources based on the filesystems on the node.Without imagefs or containerfsIf the node only has a nodefs filesystem that meets eviction thresholds, the kubelet frees up disk space in the following order:Garbage collect dead pods and containers.Delete unused images.With imagefsIf the node has a dedicated imagefs filesystem for container runtimes to use, the kubelet does the following:If the nodefs filesystem meets the eviction thresholds, the kubelet garbage collects dead pods and containers.If the imagefs filesystem meets the eviction thresholds, the kubelet deletes all unused images.With imagefs and containerfsIf the node has a dedicated containerfs alongside the imagefs filesystem configured for the container runtimes to use, then kubelet will attempt to reclaim resources as follows:If the containerfs filesystem meets the eviction thresholds, the kubelet garbage collects dead pods and containers.If the imagefs filesystem meets the eviction thresholds, the kubelet deletes all unused images.Pod selection for kubelet evictionIf the kubelet's attempts to reclaim node-level resources don't bring the eviction signal below the threshold, the kubelet begins to evict end-user pods.The kubelet uses the following parameters to determine the pod eviction order:Whether the pod's resource usage exceeds requestsPod PriorityThe pod's resource usage relative to requestsAs a result, kubelet ranks and evicts pods in the following order:BestEffort or Burstable pods where the usage exceeds requests. These pods are evicted based on their Priority and then by how much their usage level exceeds the request.Guaranteed pods and Burstable pods where the usage is less than requests are evicted last, based on their Priority.Note:The kubelet does not use the pod's QoS class to determine the eviction order. You can use the QoS class to estimate the most likely pod eviction order when reclaiming resources like memory. QoS classification does not apply to EphemeralStorage requests, so the above scenario will not apply if the node is, for example, under DiskPressure.Guaranteed pods are guaranteed only when requests and limits are specified for all the containers and they are equal. These pods will never be evicted because of another pod's resource consumption. If a system daemon (such as kubelet and journald) is consuming more resources than were reserved via system-reserved or kube-reserved allocations, and the node only has Guaranteed or Burstable pods using less resources than requests left on it, then the kubelet must choose to evict one of these pods to preserve node stability and to limit the impact of resource starvation on other pods. In this case, it will choose to evict pods of lowest Priority first.If you are running a static pod and want to avoid having it evicted under resource pressure, set the priority field for that Pod directly. Static pods do not support the priorityClassName field.When the kubelet evicts pods in response to inode or process ID starvation, it uses the Pods' relative priority to determine the eviction order, because inodes and PIDs have no requests.The kubelet sorts pods differently based on whether the node has a dedicated imagefs or containerfs filesystem:Without imagefs or containerfs (nodefs and imagefs use the same filesystem)If nodefs triggers evictions, the kubelet sorts pods based on their total disk usage (local volumes + logs and a writable layer of all containers).With imagefs (nodefs and imagefs filesystems are separate)If nodefs triggers evictions, the kubelet sorts pods based on nodefs usage (local volumes + logs of all containers).If imagefs triggers evictions, the kubelet sorts pods based on the writable layer usage of all containers.With imagesfs and containerfs (imagefs and containerfs have been split)If containerfs triggers evictions, the kubelet sorts pods based on containerfs usage (local volumes + logs and a writable layer of all containers).If imagefs triggers evictions, the kubelet sorts pods based on the storage of images rank, which represents the disk usage of a given image.Minimum eviction reclaimNote:As of Kubernetes v1.33, you cannot set a custom value for the containerfs.available metric. The configuration for this specific metric will be set automatically to reflect values set for either the nodefs or imagefs, depending on the configuration.In some cases, pod eviction only reclaims a small amount of the starved resource. This can lead to the kubelet repeatedly hitting the configured eviction thresholds and triggering multiple evictions.You can use the --eviction-minimum-reclaim flag or a kubelet config file to configure a minimum reclaim amount for each resource. When the kubelet notices that a resource is starved, it continues to reclaim that resource until it reclaims the quantity you specify.For example, the following configuration sets minimum reclaim amounts:apiVersion: kubelet.config.k8s.io/v1beta1 kind: KubeletConfiguration evictionHard: memory.available: "500Mi" nodefs.available: "1Gi" imagefs.available: "100Gi" evictionMinimumReclaim: memory.available: "0Mi" nodefs.available: "500Mi" imagefs.available: "2Gi" In this example, if the nodefs.available signal meets the eviction threshold, the kubelet reclaims the resource until the signal reaches the threshold of 1GiB, and then continues to reclaim the minimum amount of 500MiB, until the available nodefs storage value reaches 1.5GiB.Similarly, the kubelet tries to reclaim the imagefs resource until the imagefs.available value reaches 102Gi, representing 102 GiB of available container image storage. If the amount of storage that the kubelet could reclaim is less than 2GiB, the kubelet doesn't reclaim anything.The default eviction-minimum-reclaim is 0 for all resources.Node out of memory behaviorIf the node experiences an out of memory (OOM) event prior to the kubelet being able to reclaim memory, the node depends on the oom_killer to respond.The kubelet sets an oom_score_adj value for each container based on the QoS for the pod.Quality of Serviceoom_score_adjGuaranteed-997BestEffort1000Burstablemin(max(2, 1000 - (1000 × memoryRequestBytes) / machineMemoryCapacityBytes), 999)Note:The kubelet also sets an oom_score_adj value of -997 for any containers in Pods that have system-node-critical Priority.If the kubelet can't reclaim memory before a node experiences OOM, the oom_killer calculates an oom_score based on the percentage of memory it's using on the node, and then adds the oom_score_adj to get an effective oom_score for each container. It then kills the container with the highest score.This means that containers in low QoS pods that consume a large amount of memory relative to their scheduling requests are killed first.Unlike pod eviction, if a container is OOM killed, the kubelet can restart it based on its restartPolicy.Good practicesThe following sections describe good practice for eviction configuration.Schedulable resources and eviction policiesWhen you configure the kubelet with an eviction policy, you should make sure that the scheduler will not schedule pods if they will trigger eviction because they immediately induce memory pressure.Consider the following scenario:Node memory capacity: 10GiBOperator wants to reserve 10% of memory capacity for system daemons (kernel, kubelet, etc.)Operator wants to evict Pods at 95% memory utilization to reduce incidence of system OOM.For this to work, the kubelet is launched as follows:--eviction-hard=memory.available<500Mi \--system-reserved=memory=1.5Gi In this configuration, the --system-reserved flag reserves 1.5GiB of memory for the system, which is 10% of the total memory + the eviction threshold amount.The node can reach the eviction threshold if a pod is using more than its request, or if the system is using more than 1GiB of memory, which makes the memory.available signal fall below 500MiB and triggers the threshold.DaemonSets and node-pressure evictionPod priority is a major factor in making eviction decisions. If you do not want the kubelet to evict pods that belong to a DaemonSet, give those pods a high enough priority by specifying a suitable priorityClassName in the pod spec. You can also use a lower priority, or the default, to only allow pods from that DaemonSet to run when there are enough resources.Known issuesThe following sections describe known issues related to out of resource handling.kubelet may not observe memory pressure right awayBy default, the kubelet polls cAdvisor to collect memory usage stats at a regular interval. If memory usage increases within that window rapidly, the kubelet may not observe MemoryPressure fast enough, and the OOM killer will still be invoked.You can use the --kernel-memcg-notification flag to enable the memcg notification API on the kubelet to get notified immediately when a threshold is crossed.If you are not trying to achieve extreme utilization, but a sensible measure of overcommit, a viable workaround for this issue is to use the --kube-reserved and --system-reserved flags to allocate memory for the system.active_file memory is not considered as available memoryOn Linux, the kernel tracks the number of bytes of file-backed memory on active least recently used (LRU) list as the active_file statistic. The kubelet treats active_file memory areas as not reclaimable. For workloads that make intensive use of block-backed local storage, including ephemeral local storage, kernel-level caches of file and block data means that many recently accessed cache pages are likely to be counted as active_file. If enough of these kernel block buffers are on the active LRU list, the kubelet is liable to observe this as high resource use and taint the node as experiencing memory pressure - triggering pod eviction.For more details, see https://github.com/kubernetes/kubernetes/issues/43916You can work around that behavior by setting the memory limit and memory request the same for containers likely to perform intensive I/O activity. You will need to estimate or measure an optimal memory limit value for that container.What's nextLearn about API-initiated EvictionLearn about Pod Priority and PreemptionLearn about PodDisruptionBudgetsLearn about Quality of Service (QoS)Check out the Eviction API3.10.13 - API-initiated EvictionAPI-initiated eviction is the process by which you use the Eviction API to create an Eviction object that triggers graceful pod termination.You can request eviction by calling the Eviction API directly, or programmatically using a client of the API server, like the kubectl drain command. This creates an Eviction object, which causes the API server to terminate the Pod.API-initiated evictions respect your configured PodDisruptionBudgets and terminationGracePeriodSeconds.Using the API to create an Eviction object for a Pod is like performing a policy-controlled DELETE operation on the Pod.Calling the Eviction APIYou can use a Kubernetes language client to access the Kubernetes API and create an Eviction object. To do this, you POST the attempted operation, similar to the following example:policy/v1policy/v1beta1Note:policy/v1 Eviction is available in v1.22+. Use policy/v1beta1 with prior releases.{ "apiVersion": "policy/v1", "kind": "Eviction", "metadata": { "name": "quux", "namespace": "default" } } Note:Deprecated in v1.22 in favor of policy/v1{ "apiVersion": "policy/v1beta1", "kind": "Eviction", "metadata": { "name": "quux", "namespace": "default" } } Alternatively, you can attempt an eviction operation by accessing the API using curl or wget, similar to the following example:curl -v -H 'Content-type: application/json' https://your-cluster-api-endpoint.example/api/v1/namespaces/default/pods/quux/eviction -d @eviction.json How API-initiated eviction worksWhen you request an eviction using the API, the API server performs admission checks and responds in one of the following ways:200 OK: the eviction is allowed, the Eviction subresource is created, and the Pod is deleted, similar to sending a DELETE request to the Pod URL.429 Too Many Requests: the eviction is not currently allowed because of the configured PodDisruptionBudget. You may be able to attempt the eviction again later. You might also see this response because of API rate limiting.500 Internal Server Error: the eviction is not allowed because there is a misconfiguration, like if multiple PodDisruptionBudgets reference the same Pod.If the Pod you want to evict isn't part of a workload that has a PodDisruptionBudget, the API server always returns 200 OK and allows the eviction.If the API server allows the eviction, the Pod is deleted as follows:The Pod resource in the API server is updated with a deletion timestamp, after which the API server considers the Pod resource to be terminated. The Pod resource is also marked with the configured grace period.The kubelet on the node where the local Pod is running notices that the Pod resource is marked for termination and starts to gracefully shut down the local Pod.While the kubelet is shutting the Pod down, the control plane removes the Pod from EndpointSlice objects. As a result, controllers no longer consider the Pod as a valid object.After the grace period for the Pod expires, the kubelet forcefully terminates the local Pod.The kubelet tells the API server to remove the Pod resource.The API server deletes the Pod resource.Troubleshooting stuck evictionsIn some cases, your applications may enter a broken state, where the Eviction API will only return 429 or 500 responses until you intervene. This can happen if, for example, a ReplicaSet creates pods for your application but new pods do not enter a Ready state. You may also notice this behavior in cases where the last evicted Pod had a long termination grace period.If you notice stuck evictions, try one of the following solutions:Abort or pause the automated operation causing the issue. Investigate the stuck application before you restart the operation.Wait a while, then directly delete the Pod from your cluster control plane instead of using the Eviction API.What's nextLearn how to protect your applications with a Pod Disruption Budget.Learn about Node-pressure Eviction.Learn about Pod Priority and Preemption.3.11 - Cluster AdministrationLower-level detail relevant to creating or administering a Kubernetes cluster.The cluster administration overview is for anyone creating or administering a Kubernetes cluster. It assumes some familiarity with core Kubernetes concepts.Planning a clusterSee the guides in Setup for examples of how to plan, set up, and configure Kubernetes clusters. The solutions listed in this article are called distros.Note:Not all distros are actively maintained. Choose distros which have been tested with a recent version of Kubernetes.Before choosing a guide, here are some considerations:Do you want to try out Kubernetes on your computer, or do you want to build a high-availability, multi-node cluster? Choose distros best suited for your needs.Will you be using a hosted Kubernetes cluster, such as Google Kubernetes Engine, or hosting your own cluster?Will your cluster be on-premises, or in the cloud (IaaS)? Kubernetes does not directly support hybrid clusters. Instead, you can set up multiple clusters.If you are configuring Kubernetes on-premises, consider which networking model fits best.Will you be running Kubernetes on "bare metal" hardware or on virtual machines (VMs)?Do you want to run a cluster, or do you expect to do active development of Kubernetes project code? If the latter, choose an actively-developed distro. Some distros only use binary releases, but offer a greater variety of choices.Familiarize yourself with the components needed to run a cluster.Managing a clusterLearn how to manage nodes.Read about Node autoscaling.Learn how to set up and manage the resource quota for shared clusters.Securing a clusterGenerate Certificates describes the steps to generate certificates using different tool chains.Kubernetes Container Environment describes the environment for Kubelet managed containers on a Kubernetes node.Controlling Access to the Kubernetes API describes how Kubernetes implements access control for its own API.Authenticating explains authentication in Kubernetes, including the various authentication options.Authorization is separate from authentication, and controls how HTTP calls are handled.Using Admission Controllers explains plug-ins which intercepts requests to the Kubernetes API server after authentication and authorization.Admission Webhook Good Practices provides good practices and considerations when designing mutating admission webhooks and validating admission webhooks.Using Sysctls in a Kubernetes Cluster describes to an administrator how to use the sysctl command-line tool to set kernel parameters .Auditing describes how to interact with Kubernetes' audit logs.Securing the kubeletControl Plane-Node communicationTLS bootstrappingKubelet authentication/authorizationOptional Cluster ServicesDNS Integration describes how to resolve a DNS name directly to a Kubernetes service.Logging and Monitoring Cluster Activity explains how logging in Kubernetes works and how to implement it.3.11.1 - Node ShutdownsIn a Kubernetes cluster, a node can be shut down in a planned graceful way or unexpectedly because of reasons such as a power outage or something else external. A node shutdown could lead to workload failure if the node is not drained before the shutdown. A node shutdown can be either graceful or non-graceful.Graceful node shutdownFEATURE STATE: Kubernetes v1.21 [beta] (enabled by default: true)The kubelet attempts to detect node system shutdown and terminates pods running on the node.kubelet ensures that pods follow the normal pod termination process during the node shutdown. During node shutdown, the kubelet does not accept new Pods (even if those Pods are already bound to the node).The graceful node shutdown feature depends on systemd since it takes advantage of systemd inhibitor locks to delay the node shutdown with a given duration.Graceful node shutdown is controlled with the GracefulNodeShutdown feature gate which is enabled by default in 1.21.Note that by default, both configuration options described below, shutdownGracePeriod and shutdownGracePeriodCriticalPods, are set to zero, thus not activating the graceful node shutdown functionality. To activate the feature, both options should be configured appropriately and set to non-zero values.Once systemd detects or is notified of a node shutdown, the kubelet sets a NotReady condition on the Node, with the reason set to "node is shutting down". The kube-scheduler honors this condition and does not schedule any Pods onto the affected node; other third-party schedulers are expected to follow the same logic. This means that new Pods won't be scheduled onto that node and therefore none will start.The kubelet also rejects Pods during the PodAdmission phase if an ongoing node shutdown has been detected, so that even Pods with a toleration for node.kubernetes.io/not-ready:NoSchedule do not start there.When kubelet is setting that condition on its Node via the API, the kubelet also begins terminating any Pods that are running locally.During a graceful shutdown, kubelet terminates pods in two phases:Terminate regular pods running on the node.Terminate critical pods running on the node.The graceful node shutdown feature is configured with two KubeletConfiguration options:shutdownGracePeriod:Specifies the total duration that the node should delay the shutdown by. This is the total grace period for pod termination for both regular and critical pods.shutdownGracePeriodCriticalPods:Specifies the duration used to terminate critical pods during a node shutdown. This value should be less than shutdownGracePeriod.Note:There are cases when Node termination was cancelled by the system (or perhaps manually by an administrator). In either of those situations the Node will return to the Ready state. However, Pods which already started the process of termination will not be restored by kubelet and will need to be re-scheduled.For example, if shutdownGracePeriod=30s, and shutdownGracePeriodCriticalPods=10s, kubelet will delay the node shutdown by 30 seconds. During the shutdown, the first 20 (30-10) seconds would be reserved for gracefully terminating normal pods, and the last 10 seconds would be reserved for terminating critical pods.Note:When pods were evicted during the graceful node shutdown, they are marked as shutdown. Running kubectl get pods shows the status of the evicted pods as Terminated. And kubectl describe pod indicates that the pod was evicted because of node shutdown:Reason: Terminated Message: Pod was terminated in response to imminent node shutdown. Pod Priority based graceful node shutdownFEATURE STATE: Kubernetes v1.24 [beta] (enabled by default: true)To provide more flexibility during graceful node shutdown around the ordering of pods during shutdown, graceful node shutdown honors the PriorityClass for Pods, provided that you enabled this feature in your cluster. The feature allows cluster administrators to explicitly define the ordering of pods during graceful node shutdown based on priority classes.The Graceful Node Shutdown feature, as described above, shuts down pods in two phases, non-critical pods, followed by critical pods. If additional flexibility is needed to explicitly define the ordering of pods during shutdown in a more granular way, pod priority based graceful shutdown can be used.When graceful node shutdown honors pod priorities, this makes it possible to do graceful node shutdown in multiple phases, each phase shutting down a particular priority class of pods. The kubelet can be configured with the exact phases and shutdown time per phase.Assuming the following custom pod priority classes in a cluster,Pod priority class namePod priority class valuecustom-class-a100000custom-class-b10000custom-class-c1000regular/unset0Within the kubelet configuration the settings for shutdownGracePeriodByPodPriority could look like:Pod priority class valueShutdown period10000010 seconds10000180 seconds1000120 seconds060 secondsThe corresponding kubelet config YAML configuration would be:shutdownGracePeriodByPodPriority: \- priority: 100000 shutdownGracePeriodSeconds: 10 \- priority: 10000 shutdownGracePeriodSeconds: 180 \- priority: 1000 shutdownGracePeriodSeconds: 120 \- priority: 0 shutdownGracePeriodSeconds: 60 The above table implies that any pod with priority value >= 100000 will get just 10 seconds to shut down, any pod with value >= 10000 and < 100000 will get 180 seconds to shut down, any pod with value >= 1000 and < 10000 will get 120 seconds to shut down. Finally, all other pods will get 60 seconds to shut down.One doesn't have to specify values corresponding to all of the classes. For example, you could instead use these settings:Pod priority class valueShutdown period100000300 seconds1000120 seconds060 secondsIn the above case, the pods with custom-class-b will go into the same bucket as custom-class-c for shutdown.If there are no pods in a particular range, then the kubelet does not wait for pods in that priority range. Instead, the kubelet immediately skips to the next priority class value range.If this feature is enabled and no configuration is provided, then no ordering action will be taken.Using this feature requires enabling the GracefulNodeShutdownBasedOnPodPriority feature gate, and setting ShutdownGracePeriodByPodPriority in the kubelet config to the desired configuration containing the pod priority class values and their respective shutdown periods.Note:The ability to take Pod priority into account during graceful node shutdown was introduced as an Alpha feature in Kubernetes v1.23. In Kubernetes 1.33 the feature is Beta and is enabled by default.Metrics graceful_shutdown_start_time_seconds and graceful_shutdown_end_time_seconds are emitted under the kubelet subsystem to monitor node shutdowns.Non-graceful node shutdown handlingFEATURE STATE: Kubernetes v1.28 [stable] (enabled by default: true)A node shutdown action may not be detected by kubelet's Node Shutdown Manager, either because the command does not trigger the inhibitor locks mechanism used by kubelet or because of a user error, i.e., the ShutdownGracePeriod and ShutdownGracePeriodCriticalPods are not configured properly. Please refer to above section Graceful Node Shutdown for more details.When a node is shutdown but not detected by kubelet's Node Shutdown Manager, the pods that are part of a StatefulSet will be stuck in terminating status on the shutdown node and cannot move to a new running node. This is because kubelet on the shutdown node is not available to delete the pods so the StatefulSet cannot create a new pod with the same name. If there are volumes used by the pods, the VolumeAttachments will not be deleted from the original shutdown node so the volumes used by these pods cannot be attached to a new running node. As a result, the application running on the StatefulSet cannot function properly. If the original shutdown node comes up, the pods will be deleted by kubelet and new pods will be created on a different running node. If the original shutdown node does not come up, these pods will be stuck in terminating status on the shutdown node forever.To mitigate the above situation, a user can manually add the taint node.kubernetes.io/out-of-service with either NoExecute or NoSchedule effect to a Node marking it out-of-service. If a Node is marked out-of-service with this taint, the pods on the node will be forcefully deleted if there are no matching tolerations on it and volume detach operations for the pods terminating on the node will happen immediately. This allows the Pods on the out-of-service node to recover quickly on a different node.During a non-graceful shutdown, Pods are terminated in the two phases:Force delete the Pods that do not have matching out-of-service tolerations.Immediately perform detach volume operation for such pods.Note:Before adding the taint node.kubernetes.io/out-of-service, it should be verified that the node is already in shutdown or power off state (not in the middle of restarting).The user is required to manually remove the out-of-service taint after the pods are moved to a new node and the user has checked that the shutdown node has been recovered since the user was the one who originally added the taint.Forced storage detach on timeoutIn any situation where a pod deletion has not succeeded for 6 minutes, kubernetes will force detach volumes being unmounted if the node is unhealthy at that instant. Any workload still running on the node that uses a force-detached volume will cause a violation of the CSI specification, which states that ControllerUnpublishVolume "must be called after all NodeUnstageVolume and NodeUnpublishVolume on the volume are called and succeed". In such circumstances, volumes on the node in question might encounter data corruption.The forced storage detach behaviour is optional; users might opt to use the "Non-graceful node shutdown" feature instead.Force storage detach on timeout can be disabled by setting the disable-force-detach-on-timeout config field in kube-controller-manager. Disabling the force detach on timeout feature means that a volume that is hosted on a node that is unhealthy for more than 6 minutes will not have its associated VolumeAttachment deleted.After this setting has been applied, unhealthy pods still attached to volumes must be recovered via the Non-Graceful Node Shutdown procedure mentioned above.Note:Caution must be taken while using the Non-Graceful Node Shutdown procedure.Deviation from the steps documented above can result in data corruption.Windows Graceful node shutdownFEATURE STATE: Kubernetes v1.32 [alpha] (enabled by default: false)The Windows graceful node shutdown feature depends on kubelet running as a Windows service, it will then have a registered service control handler to delay the preshutdown event with a given duration.Windows graceful node shutdown is controlled with the WindowsGracefulNodeShutdown feature gate which is introduced in 1.32 as an alpha feature.Windows graceful node shutdown can not be cancelled.If kubelet is not running as a Windows service, it will not be able to set and monitor the Preshutdown event, the node will have to go through the Non-Graceful Node Shutdown procedure mentioned above.In the case where the Windows graceful node shutdown feature is enabled, but the kubelet is not running as a Windows service, the kubelet will continue running instead of failing. However, it will log an error indicating that it needs to be run as a Windows service.What's nextLearn more about the following:Blog: Non-Graceful Node Shutdown.Cluster Architecture: Nodes.3.11.2 - Node AutoscalingAutomatically provision and consolidate the Nodes in your cluster to adapt to demand and optimize cost.In order to run workloads in your cluster, you need Nodes. Nodes in your cluster can be autoscaled - dynamically provisioned, or consolidated to provide needed capacity while optimizing cost. Autoscaling is performed by Node autoscalers.Node provisioningIf there are Pods in a cluster that can't be scheduled on existing Nodes, new Nodes can be automatically added to the cluster—provisioned—to accommodate the Pods. This is especially useful if the number of Pods changes over time, for example as a result of combining horizontal workload with Node autoscaling.Autoscalers provision the Nodes by creating and deleting cloud provider resources backing them. Most commonly, the resources backing the Nodes are Virtual Machines.The main goal of provisioning is to make all Pods schedulable. This goal is not always attainable because of various limitations, including reaching configured provisioning limits, provisioning configuration not being compatible with a particular set of pods, or the lack of cloud provider capacity. While provisioning, Node autoscalers often try to achieve additional goals (for example minimizing the cost of the provisioned Nodes or balancing the number of Nodes between failure domains).There are two main inputs to a Node autoscaler when determining Nodes to provision—Pod scheduling constraints, and Node constraints imposed by autoscaler configuration.Autoscaler configuration may also include other Node provisioning triggers (for example the number of Nodes falling below a configured minimum limit).Note:Provisioning was formerly known as scale-up in Cluster Autoscaler.Pod scheduling constraintsPods can express scheduling constraints to impose limitations on the kind of Nodes they can be scheduled on. Node autoscalers take these constraints into account to ensure that the pending Pods can be scheduled on the provisioned Nodes.The most common kind of scheduling constraints are the resource requests specified by Pod containers. Autoscalers will make sure that the provisioned Nodes have enough resources to satisfy the requests. However, they don't directly take into account the real resource usage of the Pods after they start running. In order to autoscale Nodes based on actual workload resource usage, you can combine horizontal workload autoscaling with Node autoscaling.Other common Pod scheduling constraints include Node affinity, inter-Pod affinity, or a requirement for a particular storage volume.Node constraints imposed by autoscaler configurationThe specifics of the provisioned Nodes (for example the amount of resources, the presence of a given label) depend on autoscaler configuration. Autoscalers can either choose them from a pre-defined set of Node configurations, or use auto-provisioning.Auto-provisioningNode auto-provisioning is a mode of provisioning in which a user doesn't have to fully configure the specifics of the Nodes that can be provisioned. Instead, the autoscaler dynamically chooses the Node configuration based on the pending Pods it's reacting to, as well as pre-configured constraints (for example, the minimum amount of resources or the need for a given label).Node consolidationThe main consideration when running a cluster is ensuring that all schedulable pods are running, whilst keeping the cost of the cluster as low as possible. To achieve this, the Pods' resource requests should utilize as much of the Nodes' resources as possible. From this perspective, the overall Node utilization in a cluster can be used as a proxy for how cost-effective the cluster is.Note:Correctly setting the resource requests of your Pods is as important to the overall cost-effectiveness of a cluster as optimizing Node utilization. Combining Node autoscaling with vertical workload autoscaling can help you achieve this.Nodes in your cluster can be automatically consolidated in order to improve the overall Node utilization, and in turn the cost-effectiveness of the cluster. Consolidation happens through removing a set of underutilized Nodes from the cluster. Optionally, a different set of Nodes can be provisioned to replace them.Consolidation, like provisioning, only considers Pod resource requests and not real resource usage when making decisions.For the purpose of consolidation, a Node is considered empty if it only has DaemonSet and static Pods running on it. Removing empty Nodes during consolidation is more straightforward than non-empty ones, and autoscalers often have optimizations designed specifically for consolidating empty Nodes.Removing non-empty Nodes during consolidation is disruptive—the Pods running on them are terminated, and possibly have to be recreated (for example by a Deployment). However, all such recreated Pods should be able to schedule on existing Nodes in the cluster, or the replacement Nodes provisioned as part of consolidation. No Pods should normally become pending as a result of consolidation.Note:Autoscalers predict how a recreated Pod will likely be scheduled after a Node is provisioned or consolidated, but they don't control the actual scheduling. Because of this, some Pods might become pending as a result of consolidation - if for example a completely new Pod appears while consolidation is being performed.Autoscaler configuration may also enable triggering consolidation by other conditions (for example, the time elapsed since a Node was created), in order to optimize different properties (for example, the maximum lifespan of Nodes in a cluster).The details of how consolidation is performed depend on the configuration of a given autoscaler.Note:Consolidation was formerly known as scale-down in Cluster Autoscaler.AutoscalersThe functionalities described in previous sections are provided by Node autoscalers. In addition to the Kubernetes API, autoscalers also need to interact with cloud provider APIs to provision and consolidate Nodes. This means that they need to be explicitly integrated with each supported cloud provider. The performance and feature set of a given autoscaler can differ between cloud provider integrations.graph TD na[Node autoscaler] k8s[Kubernetes] cp[Cloud Provider] k8s --> |get Pods/Nodes|na na --> |drain Nodes|k8s na --> |create/remove resources backing Nodes|cp cp --> |get resources backing Nodes|na classDef white_on_blue fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff; classDef blue_on_white fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5; class na blue_on_white; class k8s,cp white_on_blue;JavaScript must be enabled to view this contentAutoscaler implementationsCluster Autoscaler and Karpenter are the two Node autoscalers currently sponsored by SIG Autoscaling.From the perspective of a cluster user, both autoscalers should provide a similar Node autoscaling experience. Both will provision new Nodes for unschedulable Pods, and both will consolidate the Nodes that are no longer optimally utilized.Different autoscalers may also provide features outside the Node autoscaling scope described on this page, and those additional features may differ between them.Consult the sections below, and the linked documentation for the individual autoscalers to decide which autoscaler fits your use case better.Cluster AutoscalerCluster Autoscaler adds or removes Nodes to pre-configured Node groups. Node groups generally map to some sort of cloud provider resource group (most commonly a Virtual Machine group). A single instance of Cluster Autoscaler can simultaneously manage multiple Node groups. When provisioning, Cluster Autoscaler will add Nodes to the group that best fits the requests of pending Pods. When consolidating, Cluster Autoscaler always selects specific Nodes to remove, as opposed to just resizing the underlying cloud provider resource group.Additional context:Documentation overviewCloud provider integrationsCluster Autoscaler FAQContactKarpenterKarpenter auto-provisions Nodes based on NodePool configurations provided by the cluster operator. Karpenter handles all aspects of node lifecycle, not just autoscaling. This includes automatically refreshing Nodes once they reach a certain lifetime, and auto-upgrading Nodes when new worker Node images are released. It works directly with individual cloud provider resources (most commonly individual Virtual Machines), and doesn't rely on cloud provider resource groups.Additional context:DocumentationCloud provider integrationsKarpenter FAQContactImplementation comparisonMain differences between Cluster Autoscaler and Karpenter:Cluster Autoscaler provides features related to just Node autoscaling. Karpenter has a wider scope, and also provides features intended for managing Node lifecycle altogether (for example, utilizing disruption to auto-recreate Nodes once they reach a certain lifetime, or auto-upgrade them to new versions).Cluster Autoscaler doesn't support auto-provisioning, the Node groups it can provision from have to be pre-configured. Karpenter supports auto-provisioning, so the user only has to configure a set of constraints for the provisioned Nodes, instead of fully configuring homogenous groups.Cluster Autoscaler provides cloud provider integrations directly, which means that they're a part of the Kubernetes project. For Karpenter, the Kubernetes project publishes Karpenter as a library that cloud providers can integrate with to build a Node autoscaler.Cluster Autoscaler provides integrations with numerous cloud providers, including smaller and less popular providers. There are fewer cloud providers that integrate with Karpenter, including AWS, and Azure.Combine workload and Node autoscalingHorizontal workload autoscalingNode autoscaling usually works in response to Pods—it provisions new Nodes to accommodate unschedulable Pods, and then consolidates the Nodes once they're no longer needed.Horizontal workload autoscaling automatically scales the number of workload replicas to maintain a desired average resource utilization across the replicas. In other words, it automatically creates new Pods in response to application load, and then removes the Pods once the load decreases.You can use Node autoscaling together with horizontal workload autoscaling to autoscale the Nodes in your cluster based on the average real resource utilization of your Pods.If the application load increases, the average utilization of its Pods should also increase, prompting workload autoscaling to create new Pods. Node autoscaling should then provision new Nodes to accommodate the new Pods.Once the application load decreases, workload autoscaling should remove unnecessary Pods. Node autoscaling should, in turn, consolidate the Nodes that are no longer needed.If configured correctly, this pattern ensures that your application always has the Node capacity to handle load spikes if needed, but you don't have to pay for the capacity when it's not needed.Vertical workload autoscalingWhen using Node autoscaling, it's important to set Pod resource requests correctly. If the requests of a given Pod are too low, provisioning a new Node for it might not help the Pod actually run. If the requests of a given Pod are too high, it might incorrectly prevent consolidating its Node.Vertical workload autoscaling automatically adjusts the resource requests of your Pods based on their historical resource usage.You can use Node autoscaling together with vertical workload autoscaling in order to adjust the resource requests of your Pods while preserving Node autoscaling capabilities in your cluster.Caution:When using Node autoscaling, it's not recommended to set up vertical workload autoscaling for DaemonSet Pods. Autoscalers have to predict what DaemonSet Pods on a new Node will look like in order to predict available Node resources. Vertical workload autoscaling might make these predictions unreliable, leading to incorrect scaling decisions.Related componentsThis section describes components providing functionality related to Node autoscaling.DeschedulerThe descheduler is a component providing Node consolidation functionality based on custom policies, as well as other features related to optimizing Nodes and Pods (for example deleting frequently restarting Pods).Workload autoscalers based on cluster sizeCluster Proportional Autoscaler and Cluster Proportional Vertical Autoscaler provide horizontal, and vertical workload autoscaling based on the number of Nodes in the cluster. You can read more in autoscaling based on cluster size.What's nextRead about workload-level autoscaling3.11.3 - CertificatesTo learn how to generate certificates for your cluster, see Certificates.3.11.4 - Cluster NetworkingNetworking is a central part of Kubernetes, but it can be challenging to understand exactly how it is expected to work. There are 4 distinct networking problems to address:Highly-coupled container-to-container communications: this is solved by Pods and localhost communications.Pod-to-Pod communications: this is the primary focus of this document.Pod-to-Service communications: this is covered by Services.External-to-Service communications: this is also covered by Services.Kubernetes is all about sharing machines among applications. Typically, sharing machines requires ensuring that two applications do not try to use the same ports. Coordinating ports across multiple developers is very difficult to do at scale and exposes users to cluster-level issues outside of their control.Dynamic port allocation brings a lot of complications to the system - every application has to take ports as flags, the API servers have to know how to insert dynamic port numbers into configuration blocks, services have to know how to find each other, etc. Rather than deal with this, Kubernetes takes a different approach.To learn about the Kubernetes networking model, see here.Kubernetes IP address rangesKubernetes clusters require to allocate non-overlapping IP addresses for Pods, Services and Nodes, from a range of available addresses configured in the following components:The network plugin is configured to assign IP addresses to Pods.The kube-apiserver is configured to assign IP addresses to Services.The kubelet or the cloud-controller-manager is configured to assign IP addresses to Nodes.Cluster networking typesKubernetes clusters, attending to the IP families configured, can be categorized into:IPv4 only: The network plugin, kube-apiserver and kubelet/cloud-controller-manager are configured to assign only IPv4 addresses.IPv6 only: The network plugin, kube-apiserver and kubelet/cloud-controller-manager are configured to assign only IPv6 addresses.IPv4/IPv6 or IPv6/IPv4 dual-stack:The network plugin is configured to assign IPv4 and IPv6 addresses.The kube-apiserver is configured to assign IPv4 and IPv6 addresses.The kubelet or cloud-controller-manager is configured to assign IPv4 and IPv6 address.All components must agree on the configured primary IP family.Kubernetes clusters only consider the IP families present on the Pods, Services and Nodes objects, independently of the existing IPs of the represented objects. Per example, a server or a pod can have multiple IP addresses on its interfaces, but only the IP addresses in node.status.addresses or pod.status.ips are considered for implementing the Kubernetes network model and defining the type of the cluster.How to implement the Kubernetes network modelThe network model is implemented by the container runtime on each node. The most common container runtimes use Container Network Interface (CNI) plugins to manage their network and security capabilities. Many different CNI plugins exist from many different vendors. Some of these provide only basic features of adding and removing network interfaces, while others provide more sophisticated solutions, such as integration with other container orchestration systems, running multiple CNI plugins, advanced IPAM features etc.See this page for a non-exhaustive list of networking addons supported by Kubernetes.What's nextThe early design of the networking model and its rationale are described in more detail in the networking design document. For future plans and some on-going efforts that aim to improve Kubernetes networking, please refer to the SIG-Network KEPs.3.11.5 - Admission Webhook Good PracticesRecommendations for designing and deploying admission webhooks in Kubernetes.This page provides good practices and considerations when designing admission webhooks in Kubernetes. This information is intended for cluster operators who run admission webhook servers or third-party applications that modify or validate your API requests.Before reading this page, ensure that you're familiar with the following concepts:Admission controllersAdmission webhooksImportance of good webhook designAdmission control occurs when any create, update, or delete request is sent to the Kubernetes API. Admission controllers intercept requests that match specific criteria that you define. These requests are then sent to mutating admission webhooks or validating admission webhooks. These webhooks are often written to ensure that specific fields in object specifications exist or have specific allowed values.Webhooks are a powerful mechanism to extend the Kubernetes API. Badly-designed webhooks often result in workload disruptions because of how much control the webhooks have over objects in the cluster. Like other API extension mechanisms, webhooks are challenging to test at scale for compatibility with all of your workloads, other webhooks, add-ons, and plugins.Additionally, with every release, Kubernetes adds or modifies the API with new features, feature promotions to beta or stable status, and deprecations. Even stable Kubernetes APIs are likely to change. For example, the Pod API changed in v1.29 to add the Sidecar containers feature. While it's rare for a Kubernetes object to enter a broken state because of a new Kubernetes API, webhooks that worked as expected with earlier versions of an API might not be able to reconcile more recent changes to that API. This can result in unexpected behavior after you upgrade your clusters to newer versions.This page describes common webhook failure scenarios and how to avoid them by cautiously and thoughtfully designing and implementing your webhooks.Identify whether you use admission webhooksEven if you don't run your own admission webhooks, some third-party applications that you run in your clusters might use mutating or validating admission webhooks.To check whether your cluster has any mutating admission webhooks, run the following command:kubectl get mutatingwebhookconfigurations The output lists any mutating admission controllers in the cluster.To check whether your cluster has any validating admission webhooks, run the following command:kubectl get validatingwebhookconfigurations The output lists any validating admission controllers in the cluster.Choose an admission control mechanismKubernetes includes multiple admission control and policy enforcement options. Knowing when to use a specific option can help you to improve latency and performance, reduce management overhead, and avoid issues during version upgrades. The following table describes the mechanisms that let you mutate or validate resources during admission:Mutating and validating admission control in KubernetesMechanismDescriptionUse casesMutating admission webhookIntercept API requests before admission and modify as needed using custom logic.Make critical modifications that must happen before resource admission.Make complex modifications that require advanced logic, like calling external APIs.Mutating admission policyIntercept API requests before admission and modify as needed using Common Expression Language (CEL) expressions.Make critical modifications that must happen before resource admission.Make simple modifications, such as adjusting labels or replica counts.Validating admission webhookIntercept API requests before admission and validate against complex policy declarations.Validate critical configurations before resource admission.Enforce complex policy logic before admission.Validating admission policyIntercept API requests before admission and validate against CEL expressions.Validate critical configurations before resource admission.Enforce policy logic using CEL expressions.In general, use webhook admission control when you want an extensible way to declare or configure the logic. Use built-in CEL-based admission control when you want to declare simpler logic without the overhead of running a webhook server. The Kubernetes project recommends that you use CEL-based admission control when possible.Use built-in validation and defaulting for CustomResourceDefinitionsIf you use CustomResourceDefinitions, don't use admission webhooks to validate values in CustomResource specifications or to set default values for fields. Kubernetes lets you define validation rules and default field values when you create CustomResourceDefinitions.To learn more, see the following resources:Validation rulesDefaultingPerformance and latencyThis section describes recommendations for improving performance and reducing latency. In summary, these are as follows:Consolidate webhooks and limit the number of API calls per webhook.Use audit logs to check for webhooks that repeatedly do the same action.Use load balancing for webhook availability.Set a small timeout value for each webhook.Consider cluster availability needs during webhook design.Design admission webhooks for low latencyMutating admission webhooks are called in sequence. Depending on the mutating webhook setup, some webhooks might be called multiple times. Every mutating webhook call adds latency to the admission process. This is unlike validating webhooks, which get called in parallel.When designing your mutating webhooks, consider your latency requirements and tolerance. The more mutating webhooks there are in your cluster, the greater the chance of latency increases.Consider the following to reduce latency:Consolidate webhooks that perform a similar mutation on different objects.Reduce the number of API calls made in the mutating webhook server logic.Limit the match conditions of each mutating webhook to reduce how many webhooks are triggered by a specific API request.Consolidate small webhooks into one server and configuration to help with ordering and organization.Prevent loops caused by competing controllersConsider any other components that run in your cluster that might conflict with the mutations that your webhook makes. For example, if your webhook adds a label that a different controller removes, your webhook gets called again. This leads to a loop.To detect these loops, try the following:Update your cluster audit policy to log audit events. Use the following parameters:level: RequestResponseverbs: ["patch"]omitStages: RequestReceivedSet the audit rule to create events for the specific resources that your webhook mutates.Check your audit events for webhooks being reinvoked multiple times with the same patch being applied to the same object, or for an object having a field updated and reverted multiple times.Set a small timeout valueAdmission webhooks should evaluate as quickly as possible (typically in milliseconds), since they add to API request latency. Use a small timeout for webhooks.For details, see Timeouts.Use a load balancer to ensure webhook availabilityAdmission webhooks should leverage some form of load-balancing to provide high availability and performance benefits. If a webhook is running within the cluster, you can run multiple webhook backends behind a Service of type ClusterIP.Use a high-availability deployment modelConsider your cluster's availability requirements when designing your webhook. For example, during node downtime or zonal outages, Kubernetes marks Pods as NotReady to allow load balancers to reroute traffic to available zones and nodes. These updates to Pods might trigger your mutating webhooks. Depending on the number of affected Pods, the mutating webhook server has a risk of timing out or causing delays in Pod processing. As a result, traffic won't get rerouted as quickly as you need.Consider situations like the preceding example when writing your webhooks. Exclude operations that are a result of Kubernetes responding to unavoidable incidents.Request filteringThis section provides recommendations for filtering which requests trigger specific webhooks. In summary, these are as follows:Limit the webhook scope to avoid system components and read-only requests.Limit webhooks to specific namespaces.Use match conditions to perform fine-grained request filtering.Match all versions of an object.Limit the scope of each webhookAdmission webhooks are only called when an API request matches the corresponding webhook configuration. Limit the scope of each webhook to reduce unnecessary calls to the webhook server. Consider the following scope limitations:Avoid matching objects in the kube-system namespace. If you run your own Pods in the kube-system namespace, use an objectSelector to avoid mutating a critical workload.Don't mutate node leases, which exist as Lease objects in the kube-node-lease system namespace. Mutating node leases might result in failed node upgrades. Only apply validation controls to Lease objects in this namespace if you're confident that the controls won't put your cluster at risk.Don't mutate TokenReview or SubjectAccessReview objects. These are always read-only requests. Modifying these objects might break your cluster.Limit each webhook to a specific namespace by using a namespaceSelector.Filter for specific requests by using match conditionsAdmission controllers support multiple fields that you can use to match requests that meet specific criteria. For example, you can use a namespaceSelector to filter for requests that target a specific namespace.For more fine-grained request filtering, use the matchConditions field in your webhook configuration. This field lets you write multiple CEL expressions that must evaluate to true for a request to trigger your admission webhook. Using matchConditions might significantly reduce the number of calls to your webhook server.For details, see Matching requests: matchConditions.Match all versions of an APIBy default, admission webhooks run on any API versions that affect a specified resource. The matchPolicy field in the webhook configuration controls this behavior. Specify a value of Equivalent in the matchPolicy field or omit the field to allow the webhook to run on any API version.For details, see Matching requests: matchPolicy.Mutation scope and field considerationsThis section provides recommendations for the scope of mutations and any special considerations for object fields. In summary, these are as follows:Patch only the fields that you need to patch.Don't overwrite array values.Avoid side effects in mutations when possible.Avoid self-mutations.Fail open and validate the final state.Plan for future field updates in later versions.Prevent webhooks from self-triggering.Don't change immutable objects.Patch only required fieldsAdmission webhook servers send HTTP responses to indicate what to do with a specific Kubernetes API request. This response is an AdmissionReview object. A mutating webhook can add specific fields to mutate before allowing admission by using the patchType field and the patch field in the response. Ensure that you only modify the fields that require a change.For example, consider a mutating webhook that's configured to ensure that web-server Deployments have at least three replicas. When a request to create a Deployment object matches your webhook configuration, the webhook should only update the value in the spec.replicas field.Don't overwrite array valuesFields in Kubernetes object specifications might include arrays. Some arrays contain key:value pairs (like the envVar field in a container specification), while other arrays are unkeyed (like the readinessGates field in a Pod specification). The order of values in an array field might matter in some situations. For example, the order of arguments in the args field of a container specification might affect the container.Consider the following when modifying arrays:Whenever possible, use the add JSONPatch operation instead of replace to avoid accidentally replacing a required value.Treat arrays that don't use key:value pairs as sets.Ensure that the values in the field that you modify aren't required to be in a specific order.Don't overwrite existing key:value pairs unless absolutely necessary.Use caution when modifying label fields. An accidental modification might cause label selectors to break, resulting in unintended behavior.Avoid side effectsEnsure that your webhooks operate only on the content of the AdmissionReview that's sent to them, and do not make out-of-band changes. These additional changes, called side effects, might cause conflicts during admission if they aren't reconciled properly. The .webhooks[].sideEffects field should be set to None if a webhook doesn't have any side effect.If side effects are required during the admission evaluation, they must be suppressed when processing an AdmissionReview object with dryRun set to true, and the .webhooks[].sideEffects field should be set to NoneOnDryRun.For details, see Side effects.Avoid self-mutationsA webhook running inside the cluster might cause deadlocks for its own deployment if it is configured to intercept resources required to start its own Pods.For example, a mutating admission webhook is configured to admit create Pod requests only if a certain label is set in the Pod (such as env: prod). The webhook server runs in a Deployment that doesn't set the env label.When a node that runs the webhook server Pods becomes unhealthy, the webhook Deployment tries to reschedule the Pods to another node. However, the existing webhook server rejects the requests since the env label is unset. As a result, the migration cannot happen.Exclude the namespace where your webhook is running with a namespaceSelector.Avoid dependency loopsDependency loops can occur in scenarios like the following:Two webhooks check each other's Pods. If both webhooks become unavailable at the same time, neither webhook can start.Your webhook intercepts cluster add-on components, such as networking plugins or storage plugins, that your webhook depends on. If both the webhook and the dependent add-on become unavailable, neither component can function.To avoid these dependency loops, try the following:Use ValidatingAdmissionPolicies to avoid introducing dependencies.Prevent webhooks from validating or mutating other webhooks. Consider excluding specific namespaces from triggering your webhook.Prevent your webhooks from acting on dependent add-ons by using an objectSelector.Fail open and validate the final stateMutating admission webhooks support the failurePolicy configuration field. This field indicates whether the API server should admit or reject the request if the webhook fails. Webhook failures might occur because of timeouts or errors in the server logic.By default, admission webhooks set the failurePolicy field to Fail. The API server rejects a request if the webhook fails. However, rejecting requests by default might result in compliant requests being rejected during webhook downtime.Let your mutating webhooks "fail open" by setting the failurePolicy field to Ignore. Use a validating controller to check the state of requests to ensure that they comply with your policies.This approach has the following benefits:Mutating webhook downtime doesn't affect compliant resources from deploying.Policy enforcement occurs during validating admission control.Mutating webhooks don't interfere with other controllers in the cluster.Plan for future updates to fieldsIn general, design your webhooks under the assumption that Kubernetes APIs might change in a later version. Don't write a server that takes the stability of an API for granted. For example, the release of sidecar containers in Kubernetes added a restartPolicy field to the Pod API.Prevent your webhook from triggering itselfMutating webhooks that respond to a broad range of API requests might unintentionally trigger themselves. For example, consider a webhook that responds to all requests in the cluster. If you configure the webhook to create Event objects for every mutation, it'll respond to its own Event object creation requests.To avoid this, consider setting a unique label in any resources that your webhook creates. Exclude this label from your webhook match conditions.Don't change immutable objectsSome Kubernetes objects in the API server can't change. For example, when you deploy a static Pod, the kubelet on the node creates a mirror Pod in the API server to track the static Pod. However, changes to the mirror Pod don't propagate to the static Pod.Don't attempt to mutate these objects during admission. All mirror Pods have the kubernetes.io/config.mirror annotation. To exclude mirror Pods while reducing the security risk of ignoring an annotation, allow static Pods to only run in specific namespaces.Mutating webhook ordering and idempotenceThis section provides recommendations for webhook order and designing idempotent webhooks. In summary, these are as follows:Don't rely on a specific order of execution.Validate mutations before admission.Check for mutations being overwritten by other controllers.Ensure that the set of mutating webhooks is idempotent, not just the individual webhooks.Don't rely on mutating webhook invocation orderMutating admission webhooks don't run in a consistent order. Various factors might change when a specific webhook is called. Don't rely on your webhook running at a specific point in the admission process. Other webhooks could still mutate your modified object.The following recommendations might help to minimize the risk of unintended changes:Validate mutations before admissionUse a reinvocation policy to observe changes to an object by other plugins and re-run the webhook as needed. For details, see Reinvocation policy.Ensure that the mutating webhooks in your cluster are idempotentEvery mutating admission webhook should be idempotent. The webhook should be able to run on an object that it already modifed without making additional changes beyond the original change.Additionally, all of the mutating webhooks in your cluster should, as a collection, be idempotent. After the mutation phase of admission control ends, every individual mutating webhook should be able to run on an object without making additional changes to the object.Depending on your environment, ensuring idempotence at scale might be challenging. The following recommendations might help:Use validating admission controllers to verify the final state of critical workloads.Test your deployments in a staging cluster to see if any objects get modified multiple times by the same webhook.Ensure that the scope of each mutating webhook is specific and limited.The following examples show idempotent mutation logic:For a create Pod request, set the field .spec.securityContext.runAsNonRoot of the Pod to true.For a create Pod request, if the field .spec.containers[].resources.limits of a container is not set, set default resource limits.For a create Pod request, inject a sidecar container with name foo-sidecar if no container with the name foo-sidecar already exists.In these cases, the webhook can be safely reinvoked, or admit an object that already has the fields set.The following examples show non-idempotent mutation logic:For a create Pod request, inject a sidecar container with name foo-sidecar suffixed with the current timestamp (such as foo-sidecar-19700101-000000).Reinvoking the webhook can result in the same sidecar being injected multiple times to a Pod, each time with a different container name. Similarly, the webhook can inject duplicated containers if the sidecar already exists in a user-provided pod.For a create/update Pod request, reject if the Pod has label env set, otherwise add an env: prod label to the Pod.Reinvoking the webhook will result in the webhook failing on its own output.For a create Pod request, append a sidecar container named foo-sidecar without checking whether a foo-sidecar container exists.Reinvoking the webhook will result in duplicated containers in the Pod, which makes the request invalid and rejected by the API server.Mutation testing and validationThis section provides recommendations for testing your mutating webhooks and validating mutated objects. In summary, these are as follows:Test webhooks in staging environments.Avoid mutations that violate validations.Test minor version upgrades for regressions and conflicts.Validate mutated objects before admission.Test webhooks in staging environmentsRobust testing should be a core part of your release cycle for new or updated webhooks. If possible, test any changes to your cluster webhooks in a staging environment that closely resembles your production clusters. At the very least, consider using a tool like minikube or kind to create a small test cluster for webhook changes.Ensure that mutations don't violate validationsYour mutating webhooks shouldn't break any of the validations that apply to an object before admission. For example, consider a mutating webhook that sets the default CPU request of a Pod to a specific value. If the CPU limit of that Pod is set to a lower value than the mutated request, the Pod fails admission.Test every mutating webhook against the validations that run in your cluster.Test minor version upgrades to ensure consistent behaviorBefore upgrading your production clusters to a new minor version, test your webhooks and workloads in a staging environment. Compare the results to ensure that your webhooks continue to function as expected after the upgrade.Additionally, use the following resources to stay informed about API changes:Kubernetes release notesKubernetes blogValidate mutations before admissionMutating webhooks run to completion before any validating webhooks run. There is no stable order in which mutations are applied to objects. As a result, your mutations could get overwritten by a mutating webhook that runs at a later time.Add a validating admission controller like a ValidatingAdmissionWebhook or a ValidatingAdmissionPolicy to your cluster to ensure that your mutations are still present. For example, consider a mutating webhook that inserts the restartPolicy: Always field to specific init containers to make them run as sidecar containers. You could run a validating webhook to ensure that those init containers retained the restartPolicy: Always configuration after all mutations were completed.For details, see the following resources:Validating Admission PolicyValidatingAdmissionWebhooksMutating webhook deploymentThis section provides recommendations for deploying your mutating admission webhooks. In summary, these are as follows:Gradually roll out the webhook configuration and monitor for issues by namespace.Limit access to edit the webhook configuration resources.Limit access to the namespace that runs the webhook server, if the server is in-cluster.Install and enable a mutating webhookWhen you're ready to deploy your mutating webhook to a cluster, use the following order of operations:Install the webhook server and start it.Set the failurePolicy field in the MutatingWebhookConfiguration manifest to Ignore. This lets you avoid disruptions caused by misconfigured webhooks.Set the namespaceSelector field in the MutatingWebhookConfiguration manifest to a test namespace.Deploy the MutatingWebhookConfiguration to your cluster.Monitor the webhook in the test namespace to check for any issues, then roll the webhook out to other namespaces. If the webhook intercepts an API request that it wasn't meant to intercept, pause the rollout and adjust the scope of the webhook configuration.Limit edit access to mutating webhooksMutating webhooks are powerful Kubernetes controllers. Use RBAC or another authorization mechanism to limit access to your webhook configurations and servers. For RBAC, ensure that the following access is only available to trusted entities:Verbs: create, update, patch, delete, deletecollectionAPI group: admissionregistration.k8s.io/v1API kind: MutatingWebhookConfigurationsIf your mutating webhook server runs in the cluster, limit access to create or modify any resources in that namespace.Examples of good implementationsNote: This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the content guide before submitting a change. More information.The following projects are examples of "good" custom webhook server implementations. You can use them as a starting point when designing your own webhooks. Don't use these examples as-is; use them as a starting point and design your webhooks to run well in your specific environment.cert-managerGatekeeper Open Policy Agent (OPA)What's nextUse webhooks for authentication and authorizationLearn about MutatingAdmissionPoliciesLearn about ValidatingAdmissionPolicies3.11.6 - Logging ArchitectureApplication logs can help you understand what is happening inside your application. The logs are particularly useful for debugging problems and monitoring cluster activity. Most modern applications have some kind of logging mechanism. Likewise, container engines are designed to support logging. The easiest and most adopted logging method for containerized applications is writing to standard output and standard error streams.However, the native functionality provided by a container engine or runtime is usually not enough for a complete logging solution.For example, you may want to access your application's logs if a container crashes, a pod gets evicted, or a node dies.In a cluster, logs should have a separate storage and lifecycle independent of nodes, pods, or containers. This concept is called cluster-level logging.Cluster-level logging architectures require a separate backend to store, analyze, and query logs. Kubernetes does not provide a native storage solution for log data. Instead, there are many logging solutions that integrate with Kubernetes. The following sections describe how to handle and store logs on nodes.Pod and container logsKubernetes captures logs from each container in a running Pod.This example uses a manifest for a Pod with a container that writes text to the standard output stream, once per second.debug/counter-pod.yaml apiVersion: v1 kind: Pod metadata: name: counter spec: containers: \- name: count image: busybox:1.28 args: [/bin/sh, -c, 'i=0; while true; do echo "$i: $(date)"; i=$((i+1)); sleep 1; done'] To run this pod, use the following command:kubectl apply -f https://k8s.io/examples/debug/counter-pod.yaml The output is:pod/counter created To fetch the logs, use the kubectl logs command, as follows:kubectl logs counter The output is similar to:0: Fri Apr 1 11:42:23 UTC 2022 1: Fri Apr 1 11:42:24 UTC 2022 2: Fri Apr 1 11:42:25 UTC 2022 You can use kubectl logs --previous to retrieve logs from a previous instantiation of a container. If your pod has multiple containers, specify which container's logs you want to access by appending a container name to the command, with a -c flag, like so:kubectl logs counter -c count Container log streamsFEATURE STATE: Kubernetes v1.32 [alpha] (enabled by default: false)As an alpha feature, the kubelet can split out the logs from the two standard streams produced by a container: standard output and standard error. To use this behavior, you must enable the PodLogsQuerySplitStreams feature gate. With that feature gate enabled, Kubernetes 1.33 allows access to these log streams directly via the Pod API. You can fetch a specific stream by specifying the stream name (either Stdout or Stderr), using the stream query string. You must have access to read the log subresource of that Pod.To demonstrate this feature, you can create a Pod that periodically writes text to both the standard output and error stream.debug/counter-pod-err.yaml apiVersion: v1 kind: Pod metadata: name: counter-err spec: containers: \- name: count image: busybox:1.28 args: [/bin/sh, -c, 'i=0; while true; do echo "$i: $(date)"; echo "$i: err" >&2 ; i=$((i+1)); sleep 1; done'] To run this pod, use the following command:kubectl apply -f https://k8s.io/examples/debug/counter-pod-err.yaml To fetch only the stderr log stream, you can run:kubectl get --raw "/api/v1/namespaces/default/pods/counter-err/log?stream=Stderr" See the kubectl logs documentation for more details.How nodes handle container logsA container runtime handles and redirects any output generated to a containerized application's stdout and stderr streams. Different container runtimes implement this in different ways; however, the integration with the kubelet is standardized as the CRI logging format.By default, if a container restarts, the kubelet keeps one terminated container with its logs. If a pod is evicted from the node, all corresponding containers are also evicted, along with their logs.The kubelet makes logs available to clients via a special feature of the Kubernetes API. The usual way to access this is by running kubectl logs.Log rotationFEATURE STATE: Kubernetes v1.21 [stable]The kubelet is responsible for rotating container logs and managing the logging directory structure. The kubelet sends this information to the container runtime (using CRI), and the runtime writes the container logs to the given location.You can configure two kubelet configuration settings, containerLogMaxSize (default 10Mi) and containerLogMaxFiles (default 5), using the kubelet configuration file. These settings let you configure the maximum size for each log file and the maximum number of files allowed for each container respectively.In order to perform an efficient log rotation in clusters where the volume of the logs generated by the workload is large, kubelet also provides a mechanism to tune how the logs are rotated in terms of how many concurrent log rotations can be performed and the interval at which the logs are monitored and rotated as required. You can configure two kubelet configuration settings, containerLogMaxWorkers and containerLogMonitorInterval using the kubelet configuration file.When you run kubectl logs as in the basic logging example, the kubelet on the node handles the request and reads directly from the log file. The kubelet returns the content of the log file.Note:Only the contents of the latest log file are available through kubectl logs.For example, if a Pod writes 40 MiB of logs and the kubelet rotates logs after 10 MiB, running kubectl logs returns at most 10MiB of data.System component logsThere are two types of system components: those that typically run in a container, and those components directly involved in running containers. For example:The kubelet and container runtime do not run in containers. The kubelet runs your containers (grouped together in pods)The Kubernetes scheduler, controller manager, and API server run within pods (usually static Pods). The etcd component runs in the control plane, and most commonly also as a static pod. If your cluster uses kube-proxy, you typically run this as a DaemonSet.Log locationsThe way that the kubelet and container runtime write logs depends on the operating system that the node uses:LinuxWindowsOn Linux nodes that use systemd, the kubelet and container runtime write to journald by default. You use journalctl to read the systemd journal; for example: journalctl -u kubelet.If systemd is not present, the kubelet and container runtime write to .log files in the /var/log directory. If you want to have logs written elsewhere, you can indirectly run the kubelet via a helper tool, kube-log-runner, and use that tool to redirect kubelet logs to a directory that you choose.By default, kubelet directs your container runtime to write logs into directories within /var/log/pods.For more information on kube-log-runner, read System Logs.By default, the kubelet writes logs to files within the directory C:\var\logs (notice that this is not C:\var\log).Although C:\var\log is the Kubernetes default location for these logs, several cluster deployment tools set up Windows nodes to log to C:\var\log\kubelet instead.If you want to have logs written elsewhere, you can indirectly run the kubelet via a helper tool, kube-log-runner, and use that tool to redirect kubelet logs to a directory that you choose.However, by default, kubelet directs your container runtime to write logs within the directory C:\var\log\pods.For more information on kube-log-runner, read System Logs.For Kubernetes cluster components that run in pods, these write to files inside the /var/log directory, bypassing the default logging mechanism (the components do not write to the systemd journal). You can use Kubernetes' storage mechanisms to map persistent storage into the container that runs the component.Kubelet allows changing the pod logs directory from default /var/log/pods to a custom path. This adjustment can be made by configuring the podLogsDir parameter in the kubelet's configuration file.Caution:It's important to note that the default location /var/log/pods has been in use for an extended period and certain processes might implicitly assume this path. Therefore, altering this parameter must be approached with caution and at your own risk.Another caveat to keep in mind is that the kubelet supports the location being on the same disk as /var. Otherwise, if the logs are on a separate filesystem from /var, then the kubelet will not track that filesystem's usage, potentially leading to issues if it fills up.For details about etcd and its logs, view the etcd documentation. Again, you can use Kubernetes' storage mechanisms to map persistent storage into the container that runs the component.Note:If you deploy Kubernetes cluster components (such as the scheduler) to log to a volume shared from the parent node, you need to consider and ensure that those logs are rotated. Kubernetes does not manage that log rotation.Your operating system may automatically implement some log rotation - for example, if you share the directory /var/log into a static Pod for a component, node-level log rotation treats a file in that directory the same as a file written by any component outside Kubernetes.Some deploy tools account for that log rotation and automate it; others leave this as your responsibility.Cluster-level logging architecturesWhile Kubernetes does not provide a native solution for cluster-level logging, there are several common approaches you can consider. Here are some options:Use a node-level logging agent that runs on every node.Include a dedicated sidecar container for logging in an application pod.Push logs directly to a backend from within an application.Using a node logging agentYou can implement cluster-level logging by including a node-level logging agent on each node. The logging agent is a dedicated tool that exposes logs or pushes logs to a backend. Commonly, the logging agent is a container that has access to a directory with log files from all of the application containers on that node.Because the logging agent must run on every node, it is recommended to run the agent as a DaemonSet.Node-level logging creates only one agent per node and doesn't require any changes to the applications running on the node.Containers write to stdout and stderr, but with no agreed format. A node-level agent collects these logs and forwards them for aggregation.Using a sidecar container with the logging agentYou can use a sidecar container in one of the following ways:The sidecar container streams application logs to its own stdout.The sidecar container runs a logging agent, which is configured to pick up logs from an application container.Streaming sidecar containerBy having your sidecar containers write to their own stdout and stderr streams, you can take advantage of the kubelet and the logging agent that already run on each node. The sidecar containers read logs from a file, a socket, or journald. Each sidecar container prints a log to its own stdout or stderr stream.This approach allows you to separate several log streams from different parts of your application, some of which can lack support for writing to stdout or stderr. The logic behind redirecting logs is minimal, so it's not a significant overhead. Additionally, because stdout and stderr are handled by the kubelet, you can use built-in tools like kubectl logs.For example, a pod runs a single container, and the container writes to two different log files using two different formats. Here's a manifest for the Pod:admin/logging/two-files-counter-pod.yaml apiVersion: v1 kind: Pod metadata: name: counter spec: containers: \- name: count image: busybox:1.28 args: \- /bin/sh \- -c \- > i=0; while true; do echo "$i: $(date)" >> /var/log/1.log; echo "$(date) INFO $i" >> /var/log/2.log; i=$((i+1)); sleep 1; done volumeMounts: \- name: varlog mountPath: /var/log volumes: \- name: varlog emptyDir: {} It is not recommended to write log entries with different formats to the same log stream, even if you managed to redirect both components to the stdout stream of the container. Instead, you can create two sidecar containers. Each sidecar container could tail a particular log file from a shared volume and then redirect the logs to its own stdout stream.Here's a manifest for a pod that has two sidecar containers:admin/logging/two-files-counter-pod-streaming-sidecar.yaml apiVersion: v1 kind: Pod metadata: name: counter spec: containers: \- name: count image: busybox:1.28 args: \- /bin/sh \- -c \- > i=0; while true; do echo "$i: $(date)" >> /var/log/1.log; echo "$(date) INFO $i" >> /var/log/2.log; i=$((i+1)); sleep 1; done volumeMounts: \- name: varlog mountPath: /var/log \- name: count-log-1 image: busybox:1.28 args: [/bin/sh, -c, 'tail -n+1 -F /var/log/1.log'] volumeMounts: \- name: varlog mountPath: /var/log \- name: count-log-2 image: busybox:1.28 args: [/bin/sh, -c, 'tail -n+1 -F /var/log/2.log'] volumeMounts: \- name: varlog mountPath: /var/log volumes: \- name: varlog emptyDir: {} Now when you run this pod, you can access each log stream separately by running the following commands:kubectl logs counter count-log-1 The output is similar to:0: Fri Apr 1 11:42:26 UTC 2022 1: Fri Apr 1 11:42:27 UTC 2022 2: Fri Apr 1 11:42:28 UTC 2022 ... kubectl logs counter count-log-2 The output is similar to:Fri Apr 1 11:42:29 UTC 2022 INFO 0 Fri Apr 1 11:42:30 UTC 2022 INFO 0 Fri Apr 1 11:42:31 UTC 2022 INFO 0 ... If you installed a node-level agent in your cluster, that agent picks up those log streams automatically without any further configuration. If you like, you can configure the agent to parse log lines depending on the source container.Even for Pods that only have low CPU and memory usage (order of a couple of millicores for cpu and order of several megabytes for memory), writing logs to a file and then streaming them to stdout can double how much storage you need on the node. If you have an application that writes to a single file, it's recommended to set /dev/stdout as the destination rather than implement the streaming sidecar container approach.Sidecar containers can also be used to rotate log files that cannot be rotated by the application itself. An example of this approach is a small container running logrotate periodically. However, it's more straightforward to use stdout and stderr directly, and leave rotation and retention policies to the kubelet.Sidecar container with a logging agentIf the node-level logging agent is not flexible enough for your situation, you can create a sidecar container with a separate logging agent that you have configured specifically to run with your application.Note:Using a logging agent in a sidecar container can lead to significant resource consumption. Moreover, you won't be able to access those logs using kubectl logs because they are not controlled by the kubelet.Here are two example manifests that you can use to implement a sidecar container with a logging agent. The first manifest contains a ConfigMap to configure fluentd.admin/logging/fluentd-sidecar-config.yaml apiVersion: v1 kind: ConfigMap metadata: name: fluentd-config data: fluentd.conf: |  type tail format none path /var/log/1.log pos_file /var/log/1.log.pos tag count.format1  type tail format none path /var/log/2.log pos_file /var/log/2.log.pos tag count.format2  type google_cloud  Note:In the sample configurations, you can replace fluentd with any logging agent, reading from any source inside an application container.The second manifest describes a pod that has a sidecar container running fluentd. The pod mounts a volume where fluentd can pick up its configuration data.admin/logging/two-files-counter-pod-agent-sidecar.yaml apiVersion: v1 kind: Pod metadata: name: counter spec: containers: \- name: count image: busybox:1.28 args: \- /bin/sh \- -c \- > i=0; while true; do echo "$i: $(date)" >> /var/log/1.log; echo "$(date) INFO $i" >> /var/log/2.log; i=$((i+1)); sleep 1; done volumeMounts: \- name: varlog mountPath: /var/log \- name: count-agent image: registry.k8s.io/fluentd-gcp:1.30 env: \- name: FLUENTD_ARGS value: -c /etc/fluentd-config/fluentd.conf volumeMounts: \- name: varlog mountPath: /var/log \- name: config-volume mountPath: /etc/fluentd-config volumes: \- name: varlog emptyDir: {} \- name: config-volume configMap: name: fluentd-config Exposing logs directly from the applicationCluster-logging that exposes or pushes logs directly from every application is outside the scope of Kubernetes.What's nextRead about Kubernetes system logsLearn about Traces For Kubernetes System ComponentsLearn how to customise the termination message that Kubernetes records when a Pod fails3.11.7 - Compatibility Version For Kubernetes Control Plane ComponentsSince release v1.32, we introduced configurable version compatibility and emulation options to Kubernetes control plane components to make upgrades safer by providing more control and increasing the granularity of steps available to cluster administrators.Emulated VersionThe emulation option is set by the --emulated-version flag of control plane components. It allows the component to emulate the behavior (APIs, features, ...) of an earlier version of Kubernetes.When used, the capabilities available will match the emulated version:Any capabilities present in the binary version that were introduced after the emulation version will be unavailable.Any capabilities removed after the emulation version will be available.This enables a binary from a particular Kubernetes release to emulate the behavior of a previous version with sufficient fidelity that interoperability with other system components can be defined in terms of the emulated version.The --emulated-version must be <= binaryVersion. See the help message of the --emulated-version flag for supported range of emulated versions.3.11.8 - Metrics For Kubernetes System ComponentsSystem component metrics can give a better look into what is happening inside them. Metrics are particularly useful for building dashboards and alerts.Kubernetes components emit metrics in Prometheus format. This format is structured plain text, designed so that people and machines can both read it.Metrics in KubernetesIn most cases metrics are available on /metrics endpoint of the HTTP server. For components that don't expose endpoint by default, it can be enabled using --bind-address flag.Examples of those components:kube-controller-managerkube-proxykube-apiserverkube-schedulerkubeletIn a production environment you may want to configure Prometheus Server or some other metrics scraper to periodically gather these metrics and make them available in some kind of time series database.Note that kubelet also exposes metrics in /metrics/cadvisor, /metrics/resource and /metrics/probes endpoints. Those metrics do not have the same lifecycle.If your cluster uses RBAC, reading metrics requires authorization via a user, group or ServiceAccount with a ClusterRole that allows accessing /metrics. For example:apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: prometheus rules: \- nonResourceURLs: \- "/metrics" verbs: \- get Metric lifecycleAlpha metric → Stable metric → Deprecated metric → Hidden metric → Deleted metricAlpha metrics have no stability guarantees. These metrics can be modified or deleted at any time.Stable metrics are guaranteed to not change. This means:A stable metric without a deprecated signature will not be deleted or renamedA stable metric's type will not be modifiedDeprecated metrics are slated for deletion, but are still available for use. These metrics include an annotation about the version in which they became deprecated.For example:Before deprecation# HELP some_counter this counts things # TYPE some_counter counter some_counter 0 After deprecation# HELP some_counter (Deprecated since 1.15.0) this counts things # TYPE some_counter counter some_counter 0 Hidden metrics are no longer published for scraping, but are still available for use. To use a hidden metric, please refer to the Show hidden metrics section.Deleted metrics are no longer published and cannot be used.Show hidden metricsAs described above, admins can enable hidden metrics through a command-line flag on a specific binary. This intends to be used as an escape hatch for admins if they missed the migration of the metrics deprecated in the last release.The flag show-hidden-metrics-for-version takes a version for which you want to show metrics deprecated in that release. The version is expressed as x.y, where x is the major version, y is the minor version. The patch version is not needed even though a metrics can be deprecated in a patch release, the reason for that is the metrics deprecation policy runs against the minor release.The flag can only take the previous minor version as it's value. All metrics hidden in previous will be emitted if admins set the previous version to show-hidden-metrics-for-version. The too old version is not allowed because this violates the metrics deprecated policy.Take metric A as an example, here assumed that A is deprecated in 1.n. According to metrics deprecated policy, we can reach the following conclusion:In release 1.n, the metric is deprecated, and it can be emitted by default.In release 1.n+1, the metric is hidden by default and it can be emitted by command line show-hidden-metrics-for-version=1.n.In release 1.n+2, the metric should be removed from the codebase. No escape hatch anymore.If you're upgrading from release 1.12 to 1.13, but still depend on a metric A deprecated in 1.12, you should set hidden metrics via command line: --show-hidden-metrics=1.12 and remember to remove this metric dependency before upgrading to 1.14Component metricskube-controller-manager metricsController manager metrics provide important insight into the performance and health of the controller manager. These metrics include common Go language runtime metrics such as go_routine count and controller specific metrics such as etcd request latencies or Cloudprovider (AWS, GCE, OpenStack) API latencies that can be used to gauge the health of a cluster.Starting from Kubernetes 1.7, detailed Cloudprovider metrics are available for storage operations for GCE, AWS, Vsphere and OpenStack. These metrics can be used to monitor health of persistent volume operations.For example, for GCE these metrics are called:cloudprovider_gce_api_request_duration_seconds { request = "instance_list"} cloudprovider_gce_api_request_duration_seconds { request = "disk_insert"} cloudprovider_gce_api_request_duration_seconds { request = "disk_delete"} cloudprovider_gce_api_request_duration_seconds { request = "attach_disk"} cloudprovider_gce_api_request_duration_seconds { request = "detach_disk"} cloudprovider_gce_api_request_duration_seconds { request = "list_disk"} kube-scheduler metricsFEATURE STATE: Kubernetes v1.21 [beta]The scheduler exposes optional metrics that reports the requested resources and the desired limits of all running pods. These metrics can be used to build capacity planning dashboards, assess current or historical scheduling limits, quickly identify workloads that cannot schedule due to lack of resources, and compare actual usage to the pod's request.The kube-scheduler identifies the resource requests and limits configured for each Pod; when either a request or limit is non-zero, the kube-scheduler reports a metrics timeseries. The time series is labelled by:namespacepod namethe node where the pod is scheduled or an empty string if not yet scheduledprioritythe assigned scheduler for that podthe name of the resource (for example, cpu)the unit of the resource if known (for example, cores)Once a pod reaches completion (has a restartPolicy of Never or OnFailure and is in the Succeeded or Failed pod phase, or has been deleted and all containers have a terminated state) the series is no longer reported since the scheduler is now free to schedule other pods to run. The two metrics are called kube_pod_resource_request and kube_pod_resource_limit.The metrics are exposed at the HTTP endpoint /metrics/resources. They require authorization for the /metrics/resources endpoint, usually granted by a ClusterRole with the get verb for the /metrics/resources non-resource URL.On Kubernetes 1.21 you must use the --show-hidden-metrics-for-version=1.20 flag to expose these alpha stability metrics.kubelet Pressure Stall Information (PSI) metricsFEATURE STATE: Kubernetes v1.33 [alpha]As an alpha feature, Kubernetes lets you configure kubelet to collect Linux kernel Pressure Stall Information (PSI) for CPU, memory and IO usage. The information is collected at node, pod and container level. The metrics are exposed at the /metrics/cadvisor endpoint with the following names:container_pressure_cpu_stalled_seconds_total container_pressure_cpu_waiting_seconds_total container_pressure_memory_stalled_seconds_total container_pressure_memory_waiting_seconds_total container_pressure_io_stalled_seconds_total container_pressure_io_waiting_seconds_total You must enable the KubeletPSI feature gate to use this feature. The information is also exposed in the Summary API.RequirementsPressure Stall Information requires:Linux kernel versions 4.20 or later.cgroup v2Disabling metricsYou can explicitly turn off metrics via command line flag --disabled-metrics. This may be desired if, for example, a metric is causing a performance problem. The input is a list of disabled metrics (i.e. --disabled-metrics=metric1,metric2).Metric cardinality enforcementMetrics with unbounded dimensions could cause memory issues in the components they instrument. To limit resource use, you can use the --allow-metric-labels command line option to dynamically configure an allow-list of label values for a metric.In alpha stage, the flag can only take in a series of mappings as metric label allow-list. Each mapping is of the format ,= where  is a comma-separated list of acceptable label names.The overall format looks like:--allow-metric-labels ,=', ...', ,=', ...', ... Here is an example:--allow-metric-labels number_count_metric,odd_number='1,3,5', number_count_metric,even_number='2,4,6', date_gauge_metric,weekend='Saturday,Sunday' In addition to specifying this from the CLI, this can also be done within a configuration file. You can specify the path to that configuration file using the --allow-metric-labels-manifest command line argument to a component. Here's an example of the contents of that configuration file:"metric1,label2": "v1,v2,v3" "metric2,label1": "v1,v2,v3" Additionally, the cardinality_enforcement_unexpected_categorizations_total meta-metric records the count of unexpected categorizations during cardinality enforcement, that is, whenever a label value is encountered that is not allowed with respect to the allow-list constraints.What's nextRead about the Prometheus text format for metricsSee the list of stable Kubernetes metricsRead about the Kubernetes deprecation policy3.11.9 - Metrics for Kubernetes Object Stateskube-state-metrics, an add-on agent to generate and expose cluster-level metrics.The state of Kubernetes objects in the Kubernetes API can be exposed as metrics. An add-on agent called kube-state-metrics can connect to the Kubernetes API server and expose a HTTP endpoint with metrics generated from the state of individual objects in the cluster. It exposes various information about the state of objects like labels and annotations, startup and termination times, status or the phase the object currently is in. For example, containers running in pods create a kube_pod_container_info metric. This includes the name of the container, the name of the pod it is part of, the namespace the pod is running in, the name of the container image, the ID of the image, the image name from the spec of the container, the ID of the running container and the ID of the pod as labels.🛇 This item links to a third party project or product that is not part of Kubernetes itself. More informationAn external component that is able and capable to scrape the endpoint of kube-state-metrics (for example via Prometheus) can now be used to enable the following use cases.Example: using metrics from kube-state-metrics to query the cluster stateMetric series generated by kube-state-metrics are helpful to gather further insights into the cluster, as they can be used for querying.If you use Prometheus or another tool that uses the same query language, the following PromQL query returns the number of pods that are not ready:count(kube_pod_status_ready{condition="false"}) by (namespace, pod) Example: alerting based on from kube-state-metricsMetrics generated from kube-state-metrics also allow for alerting on issues in the cluster.If you use Prometheus or a similar tool that uses the same alert rule language, the following alert will fire if there are pods that have been in a Terminating state for more than 5 minutes:groups: \- name: Pod state rules: \- alert: PodsBlockedInTerminatingState expr: count(kube_pod_deletion_timestamp) by (namespace, pod) * count(kube_pod_status_reason{reason="NodeLost"} == 0) by (namespace, pod) > 0 for: 5m labels: severity: page annotations: summary: Pod {{$labels.namespace}}/{{$labels.pod}} blocked in Terminating state. 3.11.10 - System LogsSystem component logs record events happening in cluster, which can be very useful for debugging. You can configure log verbosity to see more or less detail. Logs can be as coarse-grained as showing errors within a component, or as fine-grained as showing step-by-step traces of events (like HTTP access logs, pod state changes, controller actions, or scheduler decisions).Warning:In contrast to the command line flags described here, the log output itself does not fall under the Kubernetes API stability guarantees: individual log entries and their formatting may change from one release to the next!Klogklog is the Kubernetes logging library. klog generates log messages for the Kubernetes system components.Kubernetes is in the process of simplifying logging in its components. The following klog command line flags are deprecated starting with Kubernetes v1.23 and removed in Kubernetes v1.26:--add-dir-header--alsologtostderr--log-backtrace-at--log-dir--log-file--log-file-max-size--logtostderr--one-output--skip-headers--skip-log-headers--stderrthresholdOutput will always be written to stderr, regardless of the output format. Output redirection is expected to be handled by the component which invokes a Kubernetes component. This can be a POSIX shell or a tool like systemd.In some cases, for example a distroless container or a Windows system service, those options are not available. Then the kube-log-runner binary can be used as wrapper around a Kubernetes component to redirect output. A prebuilt binary is included in several Kubernetes base images under its traditional name as /go-runner and as kube-log-runner in server and node release archives.This table shows how kube-log-runner invocations correspond to shell redirection:UsagePOSIX shell (such as bash)kube-log-runner  Merge stderr and stdout, write to stdout2>&1kube-log-runner (default behavior)Redirect both into log file1>>/tmp/log 2>&1kube-log-runner -log-file=/tmp/logCopy into log file and to stdout2>&1 | tee -a /tmp/logkube-log-runner -log-file=/tmp/log -also-stdoutRedirect only stdout into log file>/tmp/logkube-log-runner -log-file=/tmp/log -redirect-stderr=falseKlog outputAn example of the traditional klog native format:I1025 00:15:15.525108 1 httplog.go:79] GET /api/v1/namespaces/kube-system/pods/metrics-server-v0.3.1-57c75779f-9p8wg: (1.512ms) 200 [pod_nanny/v0.0.0 (linux/amd64) kubernetes/$Format 10.56.1.19:51756] The message string may contain line breaks:I1025 00:15:15.525108 1 example.go:79] This is a message which has a line break. Structured LoggingFEATURE STATE: Kubernetes v1.23 [beta]Warning:Migration to structured log messages is an ongoing process. Not all log messages are structured in this version. When parsing log files, you must also handle unstructured log messages.Log formatting and value serialization are subject to change.Structured logging introduces a uniform structure in log messages allowing for programmatic extraction of information. You can store and process structured logs with less effort and cost. The code which generates a log message determines whether it uses the traditional unstructured klog output or structured logging.The default formatting of structured log messages is as text, with a format that is backward compatible with traditional klog: "" ="" ="" ... Example:I1025 00:15:15.525108 1 controller_utils.go:116] "Pod status updated" pod="kube-system/kubedns" status="ready" Strings are quoted. Other values are formatted with %+v, which may cause log messages to continue on the next line depending on the data.I1025 00:15:15.525108 1 example.go:116] "Example" data="This is text with a line break\nand \"quotation marks\"." someInt=1 someFloat=0.1 someStruct={StringField: First line, second line.} Contextual LoggingFEATURE STATE: Kubernetes v1.30 [beta]Contextual logging builds on top of structured logging. It is primarily about how developers use logging calls: code based on that concept is more flexible and supports additional use cases as described in the Contextual Logging KEP.If developers use additional functions like WithValues or WithName in their components, then log entries contain additional information that gets passed into functions by their caller.For Kubernetes 1.33, this is gated behind the ContextualLogging feature gate and is enabled by default. The infrastructure for this was added in 1.24 without modifying components. The component-base/logs/example command demonstrates how to use the new logging calls and how a component behaves that supports contextual logging.$ cd $GOPATH/src/k8s.io/kubernetes/staging/src/k8s.io/component-base/logs/example/cmd/ $ go run . --help ... \--feature-gates mapStringBool A set of key=value pairs that describe feature gates for alpha/experimental features. Options are: AllAlpha=true|false (ALPHA - default=false) AllBeta=true|false (BETA - default=false) ContextualLogging=true|false (BETA - default=true) $ go run . --feature-gates ContextualLogging=true ... I0222 15:13:31.645988 197901 example.go:54] "runtime" logger="example.myname" foo="bar" duration="1m0s" I0222 15:13:31.646007 197901 example.go:55] "another runtime" logger="example" foo="bar" duration="1h0m0s" duration="1m0s" The logger key and foo="bar" were added by the caller of the function which logs the runtime message and duration="1m0s" value, without having to modify that function.With contextual logging disable, WithValues and WithName do nothing and log calls go through the global klog logger. Therefore this additional information is not in the log output anymore:$ go run . --feature-gates ContextualLogging=false ... I0222 15:14:40.497333 198174 example.go:54] "runtime" duration="1m0s" I0222 15:14:40.497346 198174 example.go:55] "another runtime" duration="1h0m0s" duration="1m0s" JSON log formatFEATURE STATE: Kubernetes v1.19 [alpha]Warning:JSON output does not support many standard klog flags. For list of unsupported klog flags, see the Command line tool reference.Not all logs are guaranteed to be written in JSON format (for example, during process start). If you intend to parse logs, make sure you can handle log lines that are not JSON as well.Field names and JSON serialization are subject to change.The --logging-format=json flag changes the format of logs from klog native format to JSON format. Example of JSON log format (pretty printed):{ "ts": 1580306777.04728, "v": 4, "msg": "Pod status updated", "pod":{ "name": "nginx-1", "namespace": "default" }, "status": "ready" } Keys with special meaning:ts - timestamp as Unix time (required, float)v - verbosity (only for info and not for error messages, int)err - error string (optional, string)msg - message (required, string)List of components currently supporting JSON format:kube-controller-managerkube-apiserverkube-schedulerkubeletLog verbosity levelThe -v flag controls log verbosity. Increasing the value increases the number of logged events. Decreasing the value decreases the number of logged events. Increasing verbosity settings logs increasingly less severe events. A verbosity setting of 0 logs only critical events.Log locationThere are two types of system components: those that run in a container and those that do not run in a container. For example:The Kubernetes scheduler and kube-proxy run in a container.The kubelet and container runtime do not run in containers.On machines with systemd, the kubelet and container runtime write to journald. Otherwise, they write to .log files in the /var/log directory. System components inside containers always write to .log files in the /var/log directory, bypassing the default logging mechanism. Similar to the container logs, you should rotate system component logs in the /var/log directory. In Kubernetes clusters created by the kube-up.sh script, log rotation is configured by the logrotate tool. The logrotate tool rotates logs daily, or once the log size is greater than 100MB.Log queryFEATURE STATE: Kubernetes v1.30 [beta] (enabled by default: false)To help with debugging issues on nodes, Kubernetes v1.27 introduced a feature that allows viewing logs of services running on the node. To use the feature, ensure that the NodeLogQuery feature gate is enabled for that node, and that the kubelet configuration options enableSystemLogHandler and enableSystemLogQuery are both set to true. On Linux the assumption is that service logs are available via journald. On Windows the assumption is that service logs are available in the application log provider. On both operating systems, logs are also available by reading files within /var/log/.Provided you are authorized to interact with node objects, you can try out this feature on all your nodes or just a subset. Here is an example to retrieve the kubelet service logs from a node:# Fetch kubelet logs from a node named node-1.example kubectl get --raw "/api/v1/nodes/node-1.example/proxy/logs/?query=kubelet" You can also fetch files, provided that the files are in a directory that the kubelet allows for log fetches. For example, you can fetch a log from /var/log on a Linux node:kubectl get --raw "/api/v1/nodes//proxy/logs/?query=/" The kubelet uses heuristics to retrieve logs. This helps if you are not aware whether a given system service is writing logs to the operating system's native logger like journald or to a log file in /var/log/. The heuristics first checks the native logger and if that is not available attempts to retrieve the first logs from /var/log/ or /var/log/.log or /var/log//.log.The complete list of options that can be used are:OptionDescriptionbootboot show messages from a specific system bootpatternpattern filters log entries by the provided PERL-compatible regular expressionqueryquery specifies services(s) or files from which to return logs (required)sinceTimean RFC3339 timestamp from which to show logs (inclusive)untilTimean RFC3339 timestamp until which to show logs (inclusive)tailLinesspecify how many lines from the end of the log to retrieve; the default is to fetch the whole logExample of a more complex query:# Fetch kubelet logs from a node named node-1.example that have the word "error" kubectl get --raw "/api/v1/nodes/node-1.example/proxy/logs/?query=kubelet&pattern;=error" What's nextRead about the Kubernetes Logging ArchitectureRead about Structured LoggingRead about Contextual LoggingRead about deprecation of klog flagsRead about the Conventions for logging severityRead about Log Query3.11.11 - Traces For Kubernetes System ComponentsFEATURE STATE: Kubernetes v1.27 [beta]System component traces record the latency of and relationships between operations in the cluster.Kubernetes components emit traces using the OpenTelemetry Protocol with the gRPC exporter and can be collected and routed to tracing backends using an OpenTelemetry Collector.Trace CollectionKubernetes components have built-in gRPC exporters for OTLP to export traces, either with an OpenTelemetry Collector, or without an OpenTelemetry Collector.For a complete guide to collecting traces and using the collector, see Getting Started with the OpenTelemetry Collector. However, there are a few things to note that are specific to Kubernetes components.By default, Kubernetes components export traces using the grpc exporter for OTLP on the IANA OpenTelemetry port, 4317. As an example, if the collector is running as a sidecar to a Kubernetes component, the following receiver configuration will collect spans and log them to standard output:receivers: otlp: protocols: grpc: exporters: # Replace this exporter with the exporter for your backend logging: logLevel: debug service: pipelines: traces: receivers: [otlp] exporters: [logging] To directly emit traces to a backend without utilizing a collector, specify the endpoint field in the Kubernetes tracing configuration file with the desired trace backend address. This method negates the need for a collector and simplifies the overall structure.For trace backend header configuration, including authentication details, environment variables can be used with OTEL_EXPORTER_OTLP_HEADERS, see OTLP Exporter Configuration.Additionally, for trace resource attribute configuration such as Kubernetes cluster name, namespace, Pod name, etc., environment variables can also be used with OTEL_RESOURCE_ATTRIBUTES, see OTLP Kubernetes Resource.Component traceskube-apiserver tracesThe kube-apiserver generates spans for incoming HTTP requests, and for outgoing requests to webhooks, etcd, and re-entrant requests. It propagates the W3C Trace Context with outgoing requests but does not make use of the trace context attached to incoming requests, as the kube-apiserver is often a public endpoint.Enabling tracing in the kube-apiserverTo enable tracing, provide the kube-apiserver with a tracing configuration file with --tracing-config-file=. This is an example config that records spans for 1 in 10000 requests, and uses the default OpenTelemetry endpoint:apiVersion: apiserver.config.k8s.io/v1beta1 kind: TracingConfiguration # default value #endpoint: localhost:4317 samplingRatePerMillion: 100 For more information about the TracingConfiguration struct, see API server config API (v1beta1).kubelet tracesFEATURE STATE: Kubernetes v1.27 [beta] (enabled by default: true)The kubelet CRI interface and authenticated http servers are instrumented to generate trace spans. As with the apiserver, the endpoint and sampling rate are configurable. Trace context propagation is also configured. A parent span's sampling decision is always respected. A provided tracing configuration sampling rate will apply to spans without a parent. Enabled without a configured endpoint, the default OpenTelemetry Collector receiver address of "localhost:4317" is set.Enabling tracing in the kubeletTo enable tracing, apply the tracing configuration. This is an example snippet of a kubelet config that records spans for 1 in 10000 requests, and uses the default OpenTelemetry endpoint:apiVersion: kubelet.config.k8s.io/v1beta1 kind: KubeletConfiguration featureGates: KubeletTracing: true tracing: # default value #endpoint: localhost:4317 samplingRatePerMillion: 100 If the samplingRatePerMillion is set to one million (1000000), then every span will be sent to the exporter.The kubelet in Kubernetes v1.33 collects spans from the garbage collection, pod synchronization routine as well as every gRPC method. The kubelet propagates trace context with gRPC requests so that container runtimes with trace instrumentation, such as CRI-O and containerd, can associate their exported spans with the trace context from the kubelet. The resulting traces will have parent-child links between kubelet and container runtime spans, providing helpful context when debugging node issues.Please note that exporting spans always comes with a small performance overhead on the networking and CPU side, depending on the overall configuration of the system. If there is any issue like that in a cluster which is running with tracing enabled, then mitigate the problem by either reducing the samplingRatePerMillion or disabling tracing completely by removing the configuration.StabilityTracing instrumentation is still under active development, and may change in a variety of ways. This includes span names, attached attributes, instrumented endpoints, etc. Until this feature graduates to stable, there are no guarantees of backwards compatibility for tracing instrumentation.What's nextRead about Getting Started with the OpenTelemetry CollectorRead about OTLP Exporter Configuration3.11.12 - Proxies in KubernetesThis page explains proxies used with Kubernetes.ProxiesThere are several different proxies you may encounter when using Kubernetes:The kubectl proxy:runs on a user's desktop or in a podproxies from a localhost address to the Kubernetes apiserverclient to proxy uses HTTPproxy to apiserver uses HTTPSlocates apiserveradds authentication headersThe apiserver proxy:is a bastion built into the apiserverconnects a user outside of the cluster to cluster IPs which otherwise might not be reachableruns in the apiserver processesclient to proxy uses HTTPS (or http if apiserver so configured)proxy to target may use HTTP or HTTPS as chosen by proxy using available informationcan be used to reach a Node, Pod, or Servicedoes load balancing when used to reach a ServiceThe kube proxy:runs on each nodeproxies UDP, TCP and SCTPdoes not understand HTTPprovides load balancingis only used to reach servicesA Proxy/Load-balancer in front of apiserver(s):existence and implementation varies from cluster to cluster (e.g. nginx)sits between all clients and one or more apiserversacts as load balancer if there are several apiservers.Cloud Load Balancers on external services:are provided by some cloud providers (e.g. AWS ELB, Google Cloud Load Balancer)are created automatically when the Kubernetes service has type LoadBalancerusually supports UDP/TCP onlySCTP support is up to the load balancer implementation of the cloud providerimplementation varies by cloud provider.Kubernetes users will typically not need to worry about anything other than the first two types. The cluster admin will typically ensure that the latter types are set up correctly.Requesting redirectsProxies have replaced redirect capabilities. Redirects have been deprecated.3.11.13 - API Priority and FairnessFEATURE STATE: Kubernetes v1.29 [stable]Controlling the behavior of the Kubernetes API server in an overload situation is a key task for cluster administrators. The kube-apiserver has some controls available (i.e. the --max-requests-inflight and --max-mutating-requests-inflight command-line flags) to limit the amount of outstanding work that will be accepted, preventing a flood of inbound requests from overloading and potentially crashing the API server, but these flags are not enough to ensure that the most important requests get through in a period of high traffic.The API Priority and Fairness feature (APF) is an alternative that improves upon aforementioned max-inflight limitations. APF classifies and isolates requests in a more fine-grained way. It also introduces a limited amount of queuing, so that no requests are rejected in cases of very brief bursts. Requests are dispatched from queues using a fair queuing technique so that, for example, a poorly-behaved controller need not starve others (even at the same priority level).This feature is designed to work well with standard controllers, which use informers and react to failures of API requests with exponential back-off, and other clients that also work this way.Caution:Some requests classified as "long-running"—such as remote command execution or log tailing—are not subject to the API Priority and Fairness filter. This is also true for the \--max-requests-inflight flag without the API Priority and Fairness feature enabled. API Priority and Fairness does apply to watch requests. When API Priority and Fairness is disabled, watch requests are not subject to the --max-requests-inflight limit.Enabling/Disabling API Priority and FairnessThe API Priority and Fairness feature is controlled by a command-line flag and is enabled by default. See Options for a general explanation of the available kube-apiserver command-line options and how to enable and disable them. The name of the command-line option for APF is "--enable-priority-and-fairness". This feature also involves an API Group with: (a) a stable v1 version, introduced in 1.29, and enabled by default (b) a v1beta3 version, enabled by default, and deprecated in v1.29. You can disable the API group beta version v1beta3 by adding the following command-line flags to your kube-apiserver invocation:kube-apiserver \ \--runtime-config=flowcontrol.apiserver.k8s.io/v1beta3=false \ # …and other flags as usual The command-line flag --enable-priority-and-fairness=false will disable the API Priority and Fairness feature.Recursive server scenariosAPI Priority and Fairness must be used carefully in recursive server scenarios. These are scenarios in which some server A, while serving a request, issues a subsidiary request to some server B. Perhaps server B might even make a further subsidiary call back to server A. In situations where Priority and Fairness control is applied to both the original request and some subsidiary ones(s), no matter how deep in the recursion, there is a danger of priority inversions and/or deadlocks.One example of recursion is when the kube-apiserver issues an admission webhook call to server B, and while serving that call, server B makes a further subsidiary request back to the kube-apiserver. Another example of recursion is when an APIService object directs the kube-apiserver to delegate requests about a certain API group to a custom external server B (this is one of the things called "aggregation").When the original request is known to belong to a certain priority level, and the subsidiary controlled requests are classified to higher priority levels, this is one possible solution. When the original requests can belong to any priority level, the subsidiary controlled requests have to be exempt from Priority and Fairness limitation. One way to do that is with the objects that configure classification and handling, discussed below. Another way is to disable Priority and Fairness on server B entirely, using the techniques discussed above. A third way, which is the simplest to use when server B is not kube-apisever, is to build server B with Priority and Fairness disabled in the code.ConceptsThere are several distinct features involved in the API Priority and Fairness feature. Incoming requests are classified by attributes of the request using FlowSchemas, and assigned to priority levels. Priority levels add a degree of isolation by maintaining separate concurrency limits, so that requests assigned to different priority levels cannot starve each other. Within a priority level, a fair-queuing algorithm prevents requests from different flows from starving each other, and allows for requests to be queued to prevent bursty traffic from causing failed requests when the average load is acceptably low.Priority LevelsWithout APF enabled, overall concurrency in the API server is limited by the kube-apiserver flags --max-requests-inflight and \--max-mutating-requests-inflight. With APF enabled, the concurrency limits defined by these flags are summed and then the sum is divided up among a configurable set of priority levels. Each incoming request is assigned to a single priority level, and each priority level will only dispatch as many concurrent requests as its particular limit allows.The default configuration, for example, includes separate priority levels for leader-election requests, requests from built-in controllers, and requests from Pods. This means that an ill-behaved Pod that floods the API server with requests cannot prevent leader election or actions by the built-in controllers from succeeding.The concurrency limits of the priority levels are periodically adjusted, allowing under-utilized priority levels to temporarily lend concurrency to heavily-utilized levels. These limits are based on nominal limits and bounds on how much concurrency a priority level may lend and how much it may borrow, all derived from the configuration objects mentioned below.Seats Occupied by a RequestThe above description of concurrency management is the baseline story. Requests have different durations but are counted equally at any given moment when comparing against a priority level's concurrency limit. In the baseline story, each request occupies one unit of concurrency. The word "seat" is used to mean one unit of concurrency, inspired by the way each passenger on a train or aircraft takes up one of the fixed supply of seats.But some requests take up more than one seat. Some of these are list requests that the server estimates will return a large number of objects. These have been found to put an exceptionally heavy burden on the server. For this reason, the server estimates the number of objects that will be returned and considers the request to take a number of seats that is proportional to that estimated number.Execution time tweaks for watch requestsAPI Priority and Fairness manages watch requests, but this involves a couple more excursions from the baseline behavior. The first concerns how long a watch request is considered to occupy its seat. Depending on request parameters, the response to a watch request may or may not begin with create notifications for all the relevant pre-existing objects. API Priority and Fairness considers a watch request to be done with its seat once that initial burst of notifications, if any, is over.The normal notifications are sent in a concurrent burst to all relevant watch response streams whenever the server is notified of an object create/update/delete. To account for this work, API Priority and Fairness considers every write request to spend some additional time occupying seats after the actual writing is done. The server estimates the number of notifications to be sent and adjusts the write request's number of seats and seat occupancy time to include this extra work.QueuingEven within a priority level there may be a large number of distinct sources of traffic. In an overload situation, it is valuable to prevent one stream of requests from starving others (in particular, in the relatively common case of a single buggy client flooding the kube-apiserver with requests, that buggy client would ideally not have much measurable impact on other clients at all). This is handled by use of a fair-queuing algorithm to process requests that are assigned the same priority level. Each request is assigned to a flow, identified by the name of the matching FlowSchema plus a flow distinguisher — which is either the requesting user, the target resource's namespace, or nothing — and the system attempts to give approximately equal weight to requests in different flows of the same priority level. To enable distinct handling of distinct instances, controllers that have many instances should authenticate with distinct usernamesAfter classifying a request into a flow, the API Priority and Fairness feature then may assign the request to a queue. This assignment uses a technique known as shuffle sharding, which makes relatively efficient use of queues to insulate low-intensity flows from high-intensity flows.The details of the queuing algorithm are tunable for each priority level, and allow administrators to trade off memory use, fairness (the property that independent flows will all make progress when total traffic exceeds capacity), tolerance for bursty traffic, and the added latency induced by queuing.Exempt requestsSome requests are considered sufficiently important that they are not subject to any of the limitations imposed by this feature. These exemptions prevent an improperly-configured flow control configuration from totally disabling an API server.ResourcesThe flow control API involves two kinds of resources. PriorityLevelConfigurations define the available priority levels, the share of the available concurrency budget that each can handle, and allow for fine-tuning queuing behavior. FlowSchemas are used to classify individual inbound requests, matching each to a single PriorityLevelConfiguration.PriorityLevelConfigurationA PriorityLevelConfiguration represents a single priority level. Each PriorityLevelConfiguration has an independent limit on the number of outstanding requests, and limitations on the number of queued requests.The nominal concurrency limit for a PriorityLevelConfiguration is not specified in an absolute number of seats, but rather in "nominal concurrency shares." The total concurrency limit for the API Server is distributed among the existing PriorityLevelConfigurations in proportion to these shares, to give each level its nominal limit in terms of seats. This allows a cluster administrator to scale up or down the total amount of traffic to a server by restarting kube-apiserver with a different value for --max-requests-inflight (or --max-mutating-requests-inflight), and all PriorityLevelConfigurations will see their maximum allowed concurrency go up (or down) by the same fraction.Caution:In the versions before v1beta3 the relevant PriorityLevelConfiguration field is named "assured concurrency shares" rather than "nominal concurrency shares". Also, in Kubernetes release 1.25 and earlier there were no periodic adjustments: the nominal/assured limits were always applied without adjustment.The bounds on how much concurrency a priority level may lend and how much it may borrow are expressed in the PriorityLevelConfiguration as percentages of the level's nominal limit. These are resolved to absolute numbers of seats by multiplying with the nominal limit / 100.0 and rounding. The dynamically adjusted concurrency limit of a priority level is constrained to lie between (a) a lower bound of its nominal limit minus its lendable seats and (b) an upper bound of its nominal limit plus the seats it may borrow. At each adjustment the dynamic limits are derived by each priority level reclaiming any lent seats for which demand recently appeared and then jointly fairly responding to the recent seat demand on the priority levels, within the bounds just described.Caution:With the Priority and Fairness feature enabled, the total concurrency limit for the server is set to the sum of --max-requests-inflight and \--max-mutating-requests-inflight. There is no longer any distinction made between mutating and non-mutating requests; if you want to treat them separately for a given resource, make separate FlowSchemas that match the mutating and non-mutating verbs respectively.When the volume of inbound requests assigned to a single PriorityLevelConfiguration is more than its permitted concurrency level, the type field of its specification determines what will happen to extra requests. A type of Reject means that excess traffic will immediately be rejected with an HTTP 429 (Too Many Requests) error. A type of Queue means that requests above the threshold will be queued, with the shuffle sharding and fair queuing techniques used to balance progress between request flows.The queuing configuration allows tuning the fair queuing algorithm for a priority level. Details of the algorithm can be read in the enhancement proposal, but in short:Increasing queues reduces the rate of collisions between different flows, at the cost of increased memory usage. A value of 1 here effectively disables the fair-queuing logic, but still allows requests to be queued.Increasing queueLengthLimit allows larger bursts of traffic to be sustained without dropping any requests, at the cost of increased latency and memory usage.Changing handSize allows you to adjust the probability of collisions between different flows and the overall concurrency available to a single flow in an overload situation.Note:A larger handSize makes it less likely for two individual flows to collide (and therefore for one to be able to starve the other), but more likely that a small number of flows can dominate the apiserver. A larger handSize also potentially increases the amount of latency that a single high-traffic flow can cause. The maximum number of queued requests possible from a single flow is handSize * queueLengthLimit.Following is a table showing an interesting collection of shuffle sharding configurations, showing for each the probability that a given mouse (low-intensity flow) is squished by the elephants (high-intensity flows) for an illustrative collection of numbers of elephants. See https://play.golang.org/p/Gi0PLgVHiUg , which computes this table.Example Shuffle Sharding ConfigurationsHandSizeQueues1 elephant4 elephants16 elephants12324.428838398950118e-090.114313488300991440.993508960765602410321.550093439632541e-080.06264798402235450.975310151902755410646.601827268370426e-120.000455713209903707760.499999291500893459643.6310049976037345e-110.000455012123041122730.42823148764548588642.25929199850899e-100.00048866970530404460.3593511468112307681286.994461389026097e-133.4055790161620863e-060.0274617313715506371281.0579122850901972e-116.960839379258192e-060.0240615738634014772567.597695465552631e-146.728547142019406e-080.000670966154253368262562.7134626662687968e-122.9516464018476436e-070.000889565464200034865124.116062922897309e-144.982983350480894e-092.26025764343413e-05610246.337324016514285e-168.09060164312957e-114.517408062903668e-07FlowSchemaA FlowSchema matches some inbound requests and assigns them to a priority level. Every inbound request is tested against FlowSchemas, starting with those with the numerically lowest matchingPrecedence and working upward. The first match wins.Caution:Only the first matching FlowSchema for a given request matters. If multiple FlowSchemas match a single inbound request, it will be assigned based on the one with the highest matchingPrecedence. If multiple FlowSchemas with equal matchingPrecedence match the same request, the one with lexicographically smaller name will win, but it's better not to rely on this, and instead to ensure that no two FlowSchemas have the same matchingPrecedence.A FlowSchema matches a given request if at least one of its rules matches. A rule matches if at least one of its subjects and at least one of its resourceRules or nonResourceRules (depending on whether the incoming request is for a resource or non-resource URL) match the request.For the name field in subjects, and the verbs, apiGroups, resources, namespaces, and nonResourceURLs fields of resource and non-resource rules, the wildcard * may be specified to match all values for the given field, effectively removing it from consideration.A FlowSchema's distinguisherMethod.type determines how requests matching that schema will be separated into flows. It may be ByUser, in which one requesting user will not be able to starve other users of capacity; ByNamespace, in which requests for resources in one namespace will not be able to starve requests for resources in other namespaces of capacity; or blank (or distinguisherMethod may be omitted entirely), in which all requests matched by this FlowSchema will be considered part of a single flow. The correct choice for a given FlowSchema depends on the resource and your particular environment.DefaultsEach kube-apiserver maintains two sorts of APF configuration objects: mandatory and suggested.Mandatory Configuration ObjectsThe four mandatory configuration objects reflect fixed built-in guardrail behavior. This is behavior that the servers have before those objects exist, and when those objects exist their specs reflect this behavior. The four mandatory objects are as follows.The mandatory exempt priority level is used for requests that are not subject to flow control at all: they will always be dispatched immediately. The mandatory exempt FlowSchema classifies all requests from the system:masters group into this priority level. You may define other FlowSchemas that direct other requests to this priority level, if appropriate.The mandatory catch-all priority level is used in combination with the mandatory catch-all FlowSchema to make sure that every request gets some kind of classification. Typically you should not rely on this catch-all configuration, and should create your own catch-all FlowSchema and PriorityLevelConfiguration (or use the suggested global-default priority level that is installed by default) as appropriate. Because it is not expected to be used normally, the mandatory catch-all priority level has a very small concurrency share and does not queue requests.Suggested Configuration ObjectsThe suggested FlowSchemas and PriorityLevelConfigurations constitute a reasonable default configuration. You can modify these and/or create additional configuration objects if you want. If your cluster is likely to experience heavy load then you should consider what configuration will work best.The suggested configuration groups requests into six priority levels:The node-high priority level is for health updates from nodes.The system priority level is for non-health requests from the system:nodes group, i.e. Kubelets, which must be able to contact the API server in order for workloads to be able to schedule on them.The leader-election priority level is for leader election requests from built-in controllers (in particular, requests for endpoints, configmaps, or leases coming from the system:kube-controller-manager or system:kube-scheduler users and service accounts in the kube-system namespace). These are important to isolate from other traffic because failures in leader election cause their controllers to fail and restart, which in turn causes more expensive traffic as the new controllers sync their informers.The workload-high priority level is for other requests from built-in controllers.The workload-low priority level is for requests from any other service account, which will typically include all requests from controllers running in Pods.The global-default priority level handles all other traffic, e.g. interactive kubectl commands run by nonprivileged users.The suggested FlowSchemas serve to steer requests into the above priority levels, and are not enumerated here.Maintenance of the Mandatory and Suggested Configuration ObjectsEach kube-apiserver independently maintains the mandatory and suggested configuration objects, using initial and periodic behavior. Thus, in a situation with a mixture of servers of different versions there may be thrashing as long as different servers have different opinions of the proper content of these objects.Each kube-apiserver makes an initial maintenance pass over the mandatory and suggested configuration objects, and after that does periodic maintenance (once per minute) of those objects.For the mandatory configuration objects, maintenance consists of ensuring that the object exists and, if it does, has the proper spec. The server refuses to allow a creation or update with a spec that is inconsistent with the server's guardrail behavior.Maintenance of suggested configuration objects is designed to allow their specs to be overridden. Deletion, on the other hand, is not respected: maintenance will restore the object. If you do not want a suggested configuration object then you need to keep it around but set its spec to have minimal consequences. Maintenance of suggested objects is also designed to support automatic migration when a new version of the kube-apiserver is rolled out, albeit potentially with thrashing while there is a mixed population of servers.Maintenance of a suggested configuration object consists of creating it --- with the server's suggested spec --- if the object does not exist. OTOH, if the object already exists, maintenance behavior depends on whether the kube-apiservers or the users control the object. In the former case, the server ensures that the object's spec is what the server suggests; in the latter case, the spec is left alone.The question of who controls the object is answered by first looking for an annotation with key apf.kubernetes.io/autoupdate-spec. If there is such an annotation and its value is true then the kube-apiservers control the object. If there is such an annotation and its value is false then the users control the object. If neither of those conditions holds then the metadata.generation of the object is consulted. If that is 1 then the kube-apiservers control the object. Otherwise the users control the object. These rules were introduced in release 1.22 and their consideration of metadata.generation is for the sake of migration from the simpler earlier behavior. Users who wish to control a suggested configuration object should set its apf.kubernetes.io/autoupdate-spec annotation to false.Maintenance of a mandatory or suggested configuration object also includes ensuring that it has an apf.kubernetes.io/autoupdate-spec annotation that accurately reflects whether the kube-apiservers control the object.Maintenance also includes deleting objects that are neither mandatory nor suggested but are annotated apf.kubernetes.io/autoupdate-spec=true.Health check concurrency exemptionThe suggested configuration gives no special treatment to the health check requests on kube-apiservers from their local kubelets --- which tend to use the secured port but supply no credentials. With the suggested config, these requests get assigned to the global-default FlowSchema and the corresponding global-default priority level, where other traffic can crowd them out.If you add the following additional FlowSchema, this exempts those requests from rate limiting.Caution:Making this change also allows any hostile party to then send health-check requests that match this FlowSchema, at any volume they like. If you have a web traffic filter or similar external security mechanism to protect your cluster's API server from general internet traffic, you can configure rules to block any health check requests that originate from outside your cluster.priority-and-fairness/health-for-strangers.yaml apiVersion: flowcontrol.apiserver.k8s.io/v1 kind: FlowSchema metadata: name: health-for-strangers spec: matchingPrecedence: 1000 priorityLevelConfiguration: name: exempt rules: \- nonResourceRules: \- nonResourceURLs: \- "/healthz" \- "/livez" \- "/readyz" verbs: \- "*" subjects: \- kind: Group group: name: "system:unauthenticated" ObservabilityMetricsNote:In versions of Kubernetes before v1.20, the labels flow_schema and priority_level were inconsistently named flowSchema and priorityLevel, respectively. If you're running Kubernetes versions v1.19 and earlier, you should refer to the documentation for your version.When you enable the API Priority and Fairness feature, the kube-apiserver exports additional metrics. Monitoring these can help you determine whether your configuration is inappropriately throttling important traffic, or find poorly-behaved workloads that may be harming system health.Maturity level BETAapiserver_flowcontrol_rejected_requests_total is a counter vector (cumulative since server start) of requests that were rejected, broken down by the labels flow_schema (indicating the one that matched the request), priority_level (indicating the one to which the request was assigned), and reason. The reason label will be one of the following values:queue-full, indicating that too many requests were already queued.concurrency-limit, indicating that the PriorityLevelConfiguration is configured to reject rather than queue excess requests.time-out, indicating that the request was still in the queue when its queuing time limit expired.cancelled, indicating that the request is not purge locked and has been ejected from the queue.apiserver_flowcontrol_dispatched_requests_total is a counter vector (cumulative since server start) of requests that began executing, broken down by flow_schema and priority_level.apiserver_flowcontrol_current_inqueue_requests is a gauge vector holding the instantaneous number of queued (not executing) requests, broken down by priority_level and flow_schema.apiserver_flowcontrol_current_executing_requests is a gauge vector holding the instantaneous number of executing (not waiting in a queue) requests, broken down by priority_level and flow_schema.apiserver_flowcontrol_current_executing_seats is a gauge vector holding the instantaneous number of occupied seats, broken down by priority_level and flow_schema.apiserver_flowcontrol_request_wait_duration_seconds is a histogram vector of how long requests spent queued, broken down by the labels flow_schema, priority_level, and execute. The execute label indicates whether the request has started executing.Note:Since each FlowSchema always assigns requests to a single PriorityLevelConfiguration, you can add the histograms for all the FlowSchemas for one priority level to get the effective histogram for requests assigned to that priority level.apiserver_flowcontrol_nominal_limit_seats is a gauge vector holding each priority level's nominal concurrency limit, computed from the API server's total concurrency limit and the priority level's configured nominal concurrency shares.Maturity level ALPHAapiserver_current_inqueue_requests is a gauge vector of recent high water marks of the number of queued requests, grouped by a label named request_kind whose value is mutating or readOnly. These high water marks describe the largest number seen in the one second window most recently completed. These complement the older apiserver_current_inflight_requests gauge vector that holds the last window's high water mark of number of requests actively being served.apiserver_current_inqueue_seats is a gauge vector of the sum over queued requests of the largest number of seats each will occupy, grouped by labels named flow_schema and priority_level.apiserver_flowcontrol_read_vs_write_current_requests is a histogram vector of observations, made at the end of every nanosecond, of the number of requests broken down by the labels phase (which takes on the values waiting and executing) and request_kind (which takes on the values mutating and readOnly). Each observed value is a ratio, between 0 and 1, of the number of requests divided by the corresponding limit on the number of requests (queue volume limit for waiting and concurrency limit for executing).apiserver_flowcontrol_request_concurrency_in_use is a gauge vector holding the instantaneous number of occupied seats, broken down by priority_level and flow_schema.apiserver_flowcontrol_priority_level_request_utilization is a histogram vector of observations, made at the end of each nanosecond, of the number of requests broken down by the labels phase (which takes on the values waiting and executing) and priority_level. Each observed value is a ratio, between 0 and 1, of a number of requests divided by the corresponding limit on the number of requests (queue volume limit for waiting and concurrency limit for executing).apiserver_flowcontrol_priority_level_seat_utilization is a histogram vector of observations, made at the end of each nanosecond, of the utilization of a priority level's concurrency limit, broken down by priority_level. This utilization is the fraction (number of seats occupied) / (concurrency limit). This metric considers all stages of execution (both normal and the extra delay at the end of a write to cover for the corresponding notification work) of all requests except WATCHes; for those it considers only the initial stage that delivers notifications of pre-existing objects. Each histogram in the vector is also labeled with phase: executing (there is no seat limit for the waiting phase).apiserver_flowcontrol_request_queue_length_after_enqueue is a histogram vector of queue lengths for the queues, broken down by priority_level and flow_schema, as sampled by the enqueued requests. Each request that gets queued contributes one sample to its histogram, reporting the length of the queue immediately after the request was added. Note that this produces different statistics than an unbiased survey would.Note:An outlier value in a histogram here means it is likely that a single flow (i.e., requests by one user or for one namespace, depending on configuration) is flooding the API server, and being throttled. By contrast, if one priority level's histogram shows that all queues for that priority level are longer than those for other priority levels, it may be appropriate to increase that PriorityLevelConfiguration's concurrency shares.apiserver_flowcontrol_request_concurrency_limit is the same as apiserver_flowcontrol_nominal_limit_seats. Before the introduction of concurrency borrowing between priority levels, this was always equal to apiserver_flowcontrol_current_limit_seats (which did not exist as a distinct metric).apiserver_flowcontrol_lower_limit_seats is a gauge vector holding the lower bound on each priority level's dynamic concurrency limit.apiserver_flowcontrol_upper_limit_seats is a gauge vector holding the upper bound on each priority level's dynamic concurrency limit.apiserver_flowcontrol_demand_seats is a histogram vector counting observations, at the end of every nanosecond, of each priority level's ratio of (seat demand) / (nominal concurrency limit). A priority level's seat demand is the sum, over both queued requests and those in the initial phase of execution, of the maximum of the number of seats occupied in the request's initial and final execution phases.apiserver_flowcontrol_demand_seats_high_watermark is a gauge vector holding, for each priority level, the maximum seat demand seen during the last concurrency borrowing adjustment period.apiserver_flowcontrol_demand_seats_average is a gauge vector holding, for each priority level, the time-weighted average seat demand seen during the last concurrency borrowing adjustment period.apiserver_flowcontrol_demand_seats_stdev is a gauge vector holding, for each priority level, the time-weighted population standard deviation of seat demand seen during the last concurrency borrowing adjustment period.apiserver_flowcontrol_demand_seats_smoothed is a gauge vector holding, for each priority level, the smoothed enveloped seat demand determined at the last concurrency adjustment.apiserver_flowcontrol_target_seats is a gauge vector holding, for each priority level, the concurrency target going into the borrowing allocation problem.apiserver_flowcontrol_seat_fair_frac is a gauge holding the fair allocation fraction determined in the last borrowing adjustment.apiserver_flowcontrol_current_limit_seats is a gauge vector holding, for each priority level, the dynamic concurrency limit derived in the last adjustment.apiserver_flowcontrol_request_execution_seconds is a histogram vector of how long requests took to actually execute, broken down by flow_schema and priority_level.apiserver_flowcontrol_watch_count_samples is a histogram vector of the number of active WATCH requests relevant to a given write, broken down by flow_schema and priority_level.apiserver_flowcontrol_work_estimated_seats is a histogram vector of the number of estimated seats (maximum of initial and final stage of execution) associated with requests, broken down by flow_schema and priority_level.apiserver_flowcontrol_request_dispatch_no_accommodation_total is a counter vector of the number of events that in principle could have led to a request being dispatched but did not, due to lack of available concurrency, broken down by flow_schema and priority_level.apiserver_flowcontrol_epoch_advance_total is a counter vector of the number of attempts to jump a priority level's progress meter backward to avoid numeric overflow, grouped by priority_level and success.Good practices for using API Priority and FairnessWhen a given priority level exceeds its permitted concurrency, requests can experience increased latency or be dropped with an HTTP 429 (Too Many Requests) error. To prevent these side effects of APF, you can modify your workload or tweak your APF settings to ensure there are sufficient seats available to serve your requests.To detect whether requests are being rejected due to APF, check the following metrics:apiserver_flowcontrol_rejected_requests_total: the total number of requests rejected per FlowSchema and PriorityLevelConfiguration.apiserver_flowcontrol_current_inqueue_requests: the current number of requests queued per FlowSchema and PriorityLevelConfiguration.apiserver_flowcontrol_request_wait_duration_seconds: the latency added to requests waiting in queues.apiserver_flowcontrol_priority_level_seat_utilization: the seat utilization per PriorityLevelConfiguration.Workload modificationsTo prevent requests from queuing and adding latency or being dropped due to APF, you can optimize your requests by:Reducing the rate at which requests are executed. A fewer number of requests over a fixed period will result in a fewer number of seats being needed at a given time.Avoid issuing a large number of expensive requests concurrently. Requests can be optimized to use fewer seats or have lower latency so that these requests hold those seats for a shorter duration. List requests can occupy more than 1 seat depending on the number of objects fetched during the request. Restricting the number of objects retrieved in a list request, for example by using pagination, will use less total seats over a shorter period. Furthermore, replacing list requests with watch requests will require lower total concurrency shares as watch requests only occupy 1 seat during its initial burst of notifications. If using streaming lists in versions 1.27 and later, watch requests will occupy the same number of seats as a list request for its initial burst of notifications because the entire state of the collection has to be streamed. Note that in both cases, a watch request will not hold any seats after this initial phase.Keep in mind that queuing or rejected requests from APF could be induced by either an increase in the number of requests or an increase in latency for existing requests. For example, if requests that normally take 1s to execute start taking 60s, it is possible that APF will start rejecting requests because requests are occupying seats for a longer duration than normal due to this increase in latency. If APF starts rejecting requests across multiple priority levels without a significant change in workload, it is possible there is an underlying issue with control plane performance rather than the workload or APF settings.Priority and fairness settingsYou can also modify the default FlowSchema and PriorityLevelConfiguration objects or create new objects of these types to better accommodate your workload.APF settings can be modified to:Give more seats to high priority requests.Isolate non-essential or expensive requests that would starve a concurrency level if it was shared with other flows.Give more seats to high priority requestsIf possible, the number of seats available across all priority levels for a particular kube-apiserver can be increased by increasing the values for the max-requests-inflight and max-mutating-requests-inflight flags. Alternatively, horizontally scaling the number of kube-apiserver instances will increase the total concurrency per priority level across the cluster assuming there is sufficient load balancing of requests.You can create a new FlowSchema which references a PriorityLevelConfiguration with a larger concurrency level. This new PriorityLevelConfiguration could be an existing level or a new level with its own set of nominal concurrency shares. For example, a new FlowSchema could be introduced to change the PriorityLevelConfiguration for your requests from global-default to workload-low to increase the number of seats available to your user. Creating a new PriorityLevelConfiguration will reduce the number of seats designated for existing levels. Recall that editing a default FlowSchema or PriorityLevelConfiguration will require setting the apf.kubernetes.io/autoupdate-spec annotation to false.You can also increase the NominalConcurrencyShares for the PriorityLevelConfiguration which is serving your high priority requests. Alternatively, for versions 1.26 and later, you can increase the LendablePercent for competing priority levels so that the given priority level has a higher pool of seats it can borrow.Isolate non-essential requests from starving other flowsFor request isolation, you can create a FlowSchema whose subject matches the user making these requests or create a FlowSchema that matches what the request is (corresponding to the resourceRules). Next, you can map this FlowSchema to a PriorityLevelConfiguration with a low share of seats.For example, suppose list event requests from Pods running in the default namespace are using 10 seats each and execute for 1 minute. To prevent these expensive requests from impacting requests from other Pods using the existing service-accounts FlowSchema, you can apply the following FlowSchema to isolate these list calls from other requests.Example FlowSchema object to isolate list event requests:priority-and-fairness/list-events-default-service-account.yaml apiVersion: flowcontrol.apiserver.k8s.io/v1 kind: FlowSchema metadata: name: list-events-default-service-account spec: distinguisherMethod: type: ByUser matchingPrecedence: 8000 priorityLevelConfiguration: name: catch-all rules: \- resourceRules: \- apiGroups: \- '*' namespaces: \- default resources: \- events verbs: \- list subjects: \- kind: ServiceAccount serviceAccount: name: default namespace: defaultThis FlowSchema captures all list event calls made by the default service account in the default namespace. The matching precedence 8000 is lower than the value of 9000 used by the existing service-accounts FlowSchema so these list event calls will match list-events-default-service-account rather than service-accounts.The catch-all PriorityLevelConfiguration is used to isolate these requests. The catch-all priority level has a very small concurrency share and does not queue requests.What's nextYou can visit flow control reference doc to learn more about troubleshooting.For background information on design details for API priority and fairness, see the enhancement proposal.You can make suggestions and feature requests via SIG API Machinery or the feature's slack channel.3.11.14 - Installing AddonsNote: This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the content guide before submitting a change. More information.Add-ons extend the functionality of Kubernetes.This page lists some of the available add-ons and links to their respective installation instructions. The list does not try to be exhaustive.Networking and Network PolicyACI provides integrated container networking and network security with Cisco ACI.Antrea operates at Layer 3/4 to provide networking and security services for Kubernetes, leveraging Open vSwitch as the networking data plane. Antrea is a CNCF project at the Sandbox level.Calico is a networking and network policy provider. Calico supports a flexible set of networking options so you can choose the most efficient option for your situation, including non-overlay and overlay networks, with or without BGP. Calico uses the same engine to enforce network policy for hosts, pods, and (if using Istio & Envoy) applications at the service mesh layer.Canal unites Flannel and Calico, providing networking and network policy.Cilium is a networking, observability, and security solution with an eBPF-based data plane. Cilium provides a simple flat Layer 3 network with the ability to span multiple clusters in either a native routing or overlay/encapsulation mode, and can enforce network policies on L3-L7 using an identity-based security model that is decoupled from network addressing. Cilium can act as a replacement for kube-proxy; it also offers additional, opt-in observability and security features. Cilium is a CNCF project at the Graduated level.CNI-Genie enables Kubernetes to seamlessly connect to a choice of CNI plugins, such as Calico, Canal, Flannel, or Weave. CNI-Genie is a CNCF project at the Sandbox level.Contiv provides configurable networking (native L3 using BGP, overlay using vxlan, classic L2, and Cisco-SDN/ACI) for various use cases and a rich policy framework. Contiv project is fully open sourced. The installer provides both kubeadm and non-kubeadm based installation options.Contrail, based on Tungsten Fabric, is an open source, multi-cloud network virtualization and policy management platform. Contrail and Tungsten Fabric are integrated with orchestration systems such as Kubernetes, OpenShift, OpenStack and Mesos, and provide isolation modes for virtual machines, containers/pods and bare metal workloads.Flannel is an overlay network provider that can be used with Kubernetes.Gateway API is an open source project managed by the SIG Network community and provides an expressive, extensible, and role-oriented API for modeling service networking.Knitter is a plugin to support multiple network interfaces in a Kubernetes pod.Multus is a Multi plugin for multiple network support in Kubernetes to support all CNI plugins (e.g. Calico, Cilium, Contiv, Flannel), in addition to SRIOV, DPDK, OVS-DPDK and VPP based workloads in Kubernetes.OVN-Kubernetes is a networking provider for Kubernetes based on OVN (Open Virtual Network), a virtual networking implementation that came out of the Open vSwitch (OVS) project. OVN-Kubernetes provides an overlay based networking implementation for Kubernetes, including an OVS based implementation of load balancing and network policy.Nodus is an OVN based CNI controller plugin to provide cloud native based Service function chaining(SFC).NSX-T Container Plug-in (NCP) provides integration between VMware NSX-T and container orchestrators such as Kubernetes, as well as integration between NSX-T and container-based CaaS/PaaS platforms such as Pivotal Container Service (PKS) and OpenShift.Nuage is an SDN platform that provides policy-based networking between Kubernetes Pods and non-Kubernetes environments with visibility and security monitoring.Romana is a Layer 3 networking solution for pod networks that also supports the NetworkPolicy API.Spiderpool is an underlay and RDMA networking solution for Kubernetes. Spiderpool is supported on bare metal, virtual machines, and public cloud environments.Weave Net provides networking and network policy, will carry on working on both sides of a network partition, and does not require an external database.Service DiscoveryCoreDNS is a flexible, extensible DNS server which can be installed as the in-cluster DNS for pods.Visualization & ControlDashboard is a dashboard web interface for Kubernetes.InfrastructureKubeVirt is an add-on to run virtual machines on Kubernetes. Usually run on bare-metal clusters.The node problem detector runs on Linux nodes and reports system issues as either Events or Node conditions.Instrumentationkube-state-metricsLegacy Add-onsThere are several other add-ons documented in the deprecated cluster/addons directory.Well-maintained ones should be linked to here. PRs welcome!3.11.15 - Coordinated Leader ElectionFEATURE STATE: Kubernetes v1.33 [beta] (enabled by default: false)Kubernetes 1.33 includes a beta feature that allows control plane components to deterministically select a leader via coordinated leader election. This is useful to satisfy Kubernetes version skew constraints during cluster upgrades. Currently, the only builtin selection strategy is OldestEmulationVersion, preferring the leader with the lowest emulation version, followed by binary version, followed by creation timestamp.Enabling coordinated leader electionEnsure that CoordinatedLeaderElection feature gate is enabled when you start the API Server: and that the coordination.k8s.io/v1beta1 API group is enabled.This can be done by setting flags --feature-gates="CoordinatedLeaderElection=true" and \--runtime-config="coordination.k8s.io/v1beta1=true".Component configurationProvided that you have enabled the CoordinatedLeaderElection feature gate andhave the coordination.k8s.io/v1beta1 API group enabled, compatible control planecomponents automatically use the LeaseCandidate and Lease APIs to elect a leaderas needed.For Kubernetes 1.33, two control plane components(kube-controller-manager and kube-scheduler) automatically use coordinatedleader election when the feature gate and API group are enabled.3.12 - Windows in KubernetesKubernetes supports nodes that run Microsoft Windows.Kubernetes supports worker nodes running either Linux or Microsoft Windows.🛇 This item links to a third party project or product that is not part of Kubernetes itself. More informationThe CNCF and its parent the Linux Foundation take a vendor-neutral approach towards compatibility. It is possible to join your Windows server as a worker node to a Kubernetes cluster.You can install and set up kubectl on Windows no matter what operating system you use within your cluster.If you are using Windows nodes, you can read:Networking On WindowsWindows Storage In KubernetesResource Management for Windows NodesConfigure RunAsUserName for Windows Pods and ContainersCreate A Windows HostProcess PodConfigure Group Managed Service Accounts for Windows Pods and ContainersSecurity For Windows NodesWindows Debugging TipsGuide for Scheduling Windows Containers in Kubernetesor, for an overview, read:3.12.1 - Windows containers in KubernetesWindows applications constitute a large portion of the services and applications that run in many organizations. Windows containers provide a way to encapsulate processes and package dependencies, making it easier to use DevOps practices and follow cloud native patterns for Windows applications.Organizations with investments in Windows-based applications and Linux-based applications don't have to look for separate orchestrators to manage their workloads, leading to increased operational efficiencies across their deployments, regardless of operating system.Windows nodes in KubernetesTo enable the orchestration of Windows containers in Kubernetes, include Windows nodes in your existing Linux cluster. Scheduling Windows containers in Pods on Kubernetes is similar to scheduling Linux-based containers.In order to run Windows containers, your Kubernetes cluster must include multiple operating systems. While you can only run the control plane on Linux, you can deploy worker nodes running either Windows or Linux.Windows nodes are supported provided that the operating system is Windows Server 2019 or Windows Server 2022.This document uses the term Windows containers to mean Windows containers with process isolation. Kubernetes does not support running Windows containers with Hyper-V isolation.Compatibility and limitationsSome node features are only available if you use a specific container runtime; others are not available on Windows nodes, including:HugePages: not supported for Windows containersPrivileged containers: not supported for Windows containers. HostProcess Containers offer similar functionality.TerminationGracePeriod: requires containerDNot all features of shared namespaces are supported. See API compatibility for more details.See Windows OS version compatibility for details on the Windows versions that Kubernetes is tested against.From an API and kubectl perspective, Windows containers behave in much the same way as Linux-based containers. However, there are some notable differences in key functionality which are outlined in this section.Comparison with LinuxKey Kubernetes elements work the same way in Windows as they do in Linux. This section refers to several key workload abstractions and how they map to Windows.PodsA Pod is the basic building block of Kubernetes–the smallest and simplest unit in the Kubernetes object model that you create or deploy. You may not deploy Windows and Linux containers in the same Pod. All containers in a Pod are scheduled onto a single Node where each Node represents a specific platform and architecture. The following Pod capabilities, properties and events are supported with Windows containers:Single or multiple containers per Pod with process isolation and volume sharingPod status fieldsReadiness, liveness, and startup probespostStart & preStop container lifecycle hooksConfigMap, Secrets: as environment variables or volumesemptyDir volumesNamed pipe host mountsResource limitsOS field:The .spec.os.name field should be set to windows to indicate that the current Pod uses Windows containers.If you set the .spec.os.name field to windows, you must not set the following fields in the .spec of that Pod:spec.hostPIDspec.hostIPCspec.securityContext.seLinuxOptionsspec.securityContext.seccompProfilespec.securityContext.fsGroupspec.securityContext.fsGroupChangePolicyspec.securityContext.sysctlsspec.shareProcessNamespacespec.securityContext.runAsUserspec.securityContext.runAsGroupspec.securityContext.supplementalGroupsspec.containers[*].securityContext.seLinuxOptionsspec.containers[*].securityContext.seccompProfilespec.containers[*].securityContext.capabilitiesspec.containers[*].securityContext.readOnlyRootFilesystemspec.containers[*].securityContext.privilegedspec.containers[*].securityContext.allowPrivilegeEscalationspec.containers[*].securityContext.procMountspec.containers[*].securityContext.runAsUserspec.containers[*].securityContext.runAsGroupIn the above list, wildcards (*) indicate all elements in a list. For example, spec.containers[*].securityContext refers to the SecurityContext object for all containers. If any of these fields is specified, the Pod will not be admitted by the API server.Workload resources including:ReplicaSetDeploymentStatefulSetDaemonSetJobCronJobReplicationControllerServices See Load balancing and Services for more details.Pods, workload resources, and Services are critical elements to managing Windows workloads on Kubernetes. However, on their own they are not enough to enable the proper lifecycle management of Windows workloads in a dynamic cloud native environment.kubectl execPod and container metricsHorizontal pod autoscalingResource quotasScheduler preemptionCommand line options for the kubeletSome kubelet command line options behave differently on Windows, as described below:The --windows-priorityclass lets you set the scheduling priority of the kubelet process (see CPU resource management)The --kube-reserved, --system-reserved , and --eviction-hard flags update NodeAllocatableEviction by using --enforce-node-allocable is not implementedWhen running on a Windows node the kubelet does not have memory or CPU restrictions. --kube-reserved and --system-reserved only subtract from NodeAllocatable and do not guarantee resource provided for workloads. See Resource Management for Windows nodes for more information.The PIDPressure Condition is not implementedThe kubelet does not take OOM eviction actionsAPI compatibilityThere are subtle differences in the way the Kubernetes APIs work for Windows due to the OS and container runtime. Some workload properties were designed for Linux, and fail to run on Windows.At a high level, these OS concepts are different:Identity - Linux uses userID (UID) and groupID (GID) which are represented as integer types. User and group names are not canonical - they are just an alias in /etc/groups or /etc/passwd back to UID+GID. Windows uses a larger binary security identifier (SID) which is stored in the Windows Security Access Manager (SAM) database. This database is not shared between the host and containers, or between containers.File permissions - Windows uses an access control list based on (SIDs), whereas POSIX systems such as Linux use a bitmask based on object permissions and UID+GID, plus optional access control lists.File paths - the convention on Windows is to use \ instead of /. The Go IO libraries typically accept both and just make it work, but when you're setting a path or command line that's interpreted inside a container, \ may be needed.Signals - Windows interactive apps handle termination differently, and can implement one or more of these:A UI thread handles well-defined messages including WM_CLOSE.Console apps handle Ctrl-C or Ctrl-break using a Control Handler.Services register a Service Control Handler function that can accept SERVICE_CONTROL_STOP control codes.Container exit codes follow the same convention where 0 is success, and nonzero is failure. The specific error codes may differ across Windows and Linux. However, exit codes passed from the Kubernetes components (kubelet, kube-proxy) are unchanged.Field compatibility for container specificationsThe following list documents differences between how Pod container specifications work between Windows and Linux:Huge pages are not implemented in the Windows container runtime, and are not available. They require asserting a user privilege that's not configurable for containers.requests.cpu and requests.memory - requests are subtracted from node available resources, so they can be used to avoid overprovisioning a node. However, they cannot be used to guarantee resources in an overprovisioned node. They should be applied to all containers as a best practice if the operator wants to avoid overprovisioning entirely.securityContext.allowPrivilegeEscalation - not possible on Windows; none of the capabilities are hooked upsecurityContext.capabilities - POSIX capabilities are not implemented on WindowssecurityContext.privileged - Windows doesn't support privileged containers, use HostProcess Containers insteadsecurityContext.procMount - Windows doesn't have a /proc filesystemsecurityContext.readOnlyRootFilesystem - not possible on Windows; write access is required for registry & system processes to run inside the containersecurityContext.runAsGroup - not possible on Windows as there is no GID supportsecurityContext.runAsNonRoot - this setting will prevent containers from running as ContainerAdministrator which is the closest equivalent to a root user on Windows.securityContext.runAsUser - use runAsUserName insteadsecurityContext.seLinuxOptions - not possible on Windows as SELinux is Linux-specificterminationMessagePath - this has some limitations in that Windows doesn't support mapping single files. The default value is /dev/termination-log, which does work because it does not exist on Windows by default.Field compatibility for Pod specificationsThe following list documents differences between how Pod specifications work between Windows and Linux:hostIPC and hostpid - host namespace sharing is not possible on WindowshostNetwork - host networking is not possible on WindowsdnsPolicy - setting the Pod dnsPolicy to ClusterFirstWithHostNet is not supported on Windows because host networking is not provided. Pods always run with a container network.podSecurityContext see belowshareProcessNamespace - this is a beta feature, and depends on Linux namespaces which are not implemented on Windows. Windows cannot share process namespaces or the container's root filesystem. Only the network can be shared.terminationGracePeriodSeconds - this is not fully implemented in Docker on Windows, see the GitHub issue. The behavior today is that the ENTRYPOINT process is sent CTRL_SHUTDOWN_EVENT, then Windows waits 5 seconds by default, and finally shuts down all processes using the normal Windows shutdown behavior. The 5 second default is actually in the Windows registry inside the container, so it can be overridden when the container is built.volumeDevices - this is a beta feature, and is not implemented on Windows. Windows cannot attach raw block devices to pods.volumesIf you define an emptyDir volume, you cannot set its volume source to memory.You cannot enable mountPropagation for volume mounts as this is not supported on Windows.Host network accessKubernetes v1.26 to v1.32 included alpha support for running Windows Pods in the host's network namespace.Kubernetes v1.33 does not include the WindowsHostNetwork feature gate or support for running Windows Pods in the host's network namespace.Field compatibility for Pod security contextOnly the securityContext.runAsNonRoot and securityContext.windowsOptions from the Pod securityContext fields work on Windows.Node problem detectorThe node problem detector (see Monitor Node Health) has preliminary support for Windows. For more information, visit the project's GitHub page.Pause containerIn a Kubernetes Pod, an infrastructure or “pause” container is first created to host the container. In Linux, the cgroups and namespaces that make up a pod need a process to maintain their continued existence; the pause process provides this. Containers that belong to the same pod, including infrastructure and worker containers, share a common network endpoint (same IPv4 and / or IPv6 address, same network port spaces). Kubernetes uses pause containers to allow for worker containers crashing or restarting without losing any of the networking configuration.Kubernetes maintains a multi-architecture image that includes support for Windows. For Kubernetes v1.33.0 the recommended pause image is registry.k8s.io/pause:3.6. The source code is available on GitHub.Microsoft maintains a different multi-architecture image, with Linux and Windows amd64 support, that you can find as mcr.microsoft.com/oss/kubernetes/pause:3.6. This image is built from the same source as the Kubernetes maintained image but all of the Windows binaries are authenticode signed by Microsoft. The Kubernetes project recommends using the Microsoft maintained image if you are deploying to a production or production-like environment that requires signed binaries.Container runtimesYou need to install a container runtime into each node in the cluster so that Pods can run there.The following container runtimes work with Windows:Note: This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the content guide before submitting a change. More information.ContainerDFEATURE STATE: Kubernetes v1.20 [stable]You can use ContainerD 1.4.0+ as the container runtime for Kubernetes nodes that run Windows.Learn how to install ContainerD on a Windows node.Note:There is a known limitation when using GMSA with containerd to access Windows network shares, which requires a kernel patch.Mirantis Container RuntimeMirantis Container Runtime (MCR) is available as a container runtime for all Windows Server 2019 and later versions.See Install MCR on Windows Servers for more information.Windows OS version compatibilityOn Windows nodes, strict compatibility rules apply where the host OS version must match the container base image OS version. Only Windows containers with a container operating system of Windows Server 2019 are fully supported.For Kubernetes v1.33, operating system compatibility for Windows nodes (and Pods) is as follows:Windows Server LTSC releaseWindows Server 2019Windows Server 2022Windows Server SAC releaseWindows Server version 20H2The Kubernetes version-skew policy also applies.Hardware recommendations and considerationsNote: This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the content guide before submitting a change. More information.Note:The following hardware specifications outlined here should be regarded as sensible default values. They are not intended to represent minimum requirements or specific recommendations for production environments. Depending on the requirements for your workload these values may need to be adjusted.64-bit processor 4 CPU cores or more, capable of supporting virtualization8GB or more of RAM50GB or more of free disk spaceRefer to Hardware requirements for Windows Server Microsoft documentation for the most up-to-date information on minimum hardware requirements. For guidance on deciding on resources for production worker nodes refer to Production worker nodes Kubernetes documentation.To optimize system resources, if a graphical user interface is not required, it may be preferable to use a Windows Server OS installation that excludes the Windows Desktop Experience installation option, as this configuration typically frees up more system resources.In assessing disk space for Windows worker nodes, take note that Windows container images are typically larger than Linux container images, with container image sizes ranging from 300MB to over 10GB for a single image. Additionally, take note that the C: drive in Windows containers represents a virtual free size of 20GB by default, which is not the actual consumed space, but rather the disk size for which a single container can grow to occupy when using local storage on the host. See Containers on Windows - Container Storage Documentation for more detail.Getting help and troubleshootingYour main source of help for troubleshooting your Kubernetes cluster should start with the Troubleshooting page.Some additional, Windows-specific troubleshooting help is included in this section. Logs are an important element of troubleshooting issues in Kubernetes. Make sure to include them any time you seek troubleshooting assistance from other contributors. Follow the instructions in the SIG Windows contributing guide on gathering logs.Reporting issues and feature requestsIf you have what looks like a bug, or you would like to make a feature request, please follow the SIG Windows contributing guide to create a new issue. You should first search the list of issues in case it was reported previously and comment with your experience on the issue and add additional logs. SIG Windows channel on the Kubernetes Slack is also a great avenue to get some initial support and troubleshooting ideas prior to creating a ticket.Validating the Windows cluster operabilityThe Kubernetes project provides a Windows Operational Readiness specification, accompanied by a structured test suite. This suite is split into two sets of tests, core and extended, each containing categories aimed at testing specific areas. It can be used to validate all the functionalities of a Windows and hybrid system (mixed with Linux nodes) with full coverage.To set up the project on a newly created cluster, refer to the instructions in the project guide.Deployment toolsThe kubeadm tool helps you to deploy a Kubernetes cluster, providing the control plane to manage the cluster it, and nodes to run your workloads.The Kubernetes cluster API project also provides means to automate deployment of Windows nodes.Windows distribution channelsFor a detailed explanation of Windows distribution channels see the Microsoft documentation.Information on the different Windows Server servicing channels including their support models can be found at Windows Server servicing channels.3.12.2 - Guide for Running Windows Containers in KubernetesThis page provides a walkthrough for some steps you can follow to run Windows containers using Kubernetes. The page also highlights some Windows specific functionality within Kubernetes.It is important to note that creating and deploying services and workloads on Kubernetes behaves in much the same way for Linux and Windows containers. The kubectl commands to interface with the cluster are identical. The examples in this page are provided to jumpstart your experience with Windows containers.ObjectivesConfigure an example deployment to run Windows containers on a Windows node.Before you beginYou should already have access to a Kubernetes cluster that includes a worker node running Windows Server.Getting Started: Deploying a Windows workloadThe example YAML file below deploys a simple webserver application running inside a Windows container.Create a manifest named win-webserver.yaml with the contents below:--- apiVersion: v1 kind: Service metadata: name: win-webserver labels: app: win-webserver spec: ports: # the port that this service should serve on \- port: 80 targetPort: 80 selector: app: win-webserver type: NodePort \--- apiVersion: apps/v1 kind: Deployment metadata: labels: app: win-webserver name: win-webserver spec: replicas: 2 selector: matchLabels: app: win-webserver template: metadata: labels: app: win-webserver name: win-webserver spec: containers: \- name: windowswebserver image: mcr.microsoft.com/windows/servercore:ltsc2019 command: \- powershell.exe \- -command \- "<#code used from https://gist.github.com/19WAS85/5424431#> ; $$listener = New-Object System.Net.HttpListener ; $$listener.Prefixes.Add('http://*:80/') ; $$listener.Start() ; $$callerCounts = @{} ; Write-Host('Listening at http://*:80/') ; while ($$listener.IsListening) { ;$$context = $$listener.GetContext() ;$$requestUrl = $$context.Request.Url ;$$clientIP = $$context.Request.RemoteEndPoint.Address ;$$response = $$context.Response ;Write-Host '' ;Write-Host('> {0}' -f $$requestUrl) ; ;$$count = 1 ;$$k=$$callerCounts.Get_Item($$clientIP) ;if ($$k -ne $$null) { $$count += $$k } ;$$callerCounts.Set_Item($$clientIP, $$count) ;$$ip=(Get-NetAdapter | Get-NetIpAddress); $$header='

# Windows Container Web Server

' ;$$callerCountsString='' ;$$callerCounts.Keys | % { $$callerCountsString+='

IP {0} callerCount {1} ' -f $$ip[1].IPAddress,$$callerCounts.Item($$_) } ;$$footer='' ;$$content='{0}{1}{2}' -f $$header,$$callerCountsString,$$footer ;Write-Output $$content ;$$buffer = [System.Text.Encoding]::UTF8.GetBytes($$content) ;$$response.ContentLength64 = $$buffer.Length ;$$response.OutputStream.Write($$buffer, 0, $$buffer.Length) ;$$response.Close() ;$$responseStatus = $$response.StatusCode ;Write-Host('< {0}' -f $$responseStatus) } ; " nodeSelector: kubernetes.io/os: windows Note:Port mapping is also supported, but for simplicity this example exposes port 80 of the container directly to the Service.Check that all nodes are healthy:kubectl get nodes Deploy the service and watch for pod updates:kubectl apply -f win-webserver.yaml kubectl get pods -o wide -w When the service is deployed correctly both Pods are marked as Ready. To exit the watch command, press Ctrl+C.Check that the deployment succeeded. To verify:Several pods listed from the Linux control plane node, use kubectl get podsNode-to-pod communication across the network, curl port 80 of your pod IPs from the Linux control plane node to check for a web server responsePod-to-pod communication, ping between pods (and across hosts, if you have more than one Windows node) using kubectl execService-to-pod communication, curl the virtual service IP (seen under kubectl get services) from the Linux control plane node and from individual podsService discovery, curl the service name with the Kubernetes default DNS suffixInbound connectivity, curl the NodePort from the Linux control plane node or machines outside of the clusterOutbound connectivity, curl external IPs from inside the pod using kubectl execNote:Windows container hosts are not able to access the IP of services scheduled on them due to current platform limitations of the Windows networking stack. Only Windows pods are able to access service IPs.ObservabilityCapturing logs from workloadsLogs are an important element of observability; they enable users to gain insights into the operational aspect of workloads and are a key ingredient to troubleshooting issues. Because Windows containers and workloads inside Windows containers behave differently from Linux containers, users had a hard time collecting logs, limiting operational visibility. Windows workloads for example are usually configured to log to ETW (Event Tracing for Windows) or push entries to the application event log. LogMonitor, an open source tool by Microsoft, is the recommended way to monitor configured log sources inside a Windows container. LogMonitor supports monitoring event logs, ETW providers, and custom application logs, piping them to STDOUT for consumption by kubectl logs .Follow the instructions in the LogMonitor GitHub page to copy its binaries and configuration files to all your containers and add the necessary entrypoints for LogMonitor to push your logs to STDOUT.Configuring container userUsing configurable Container usernamesWindows containers can be configured to run their entrypoints and processes with different usernames than the image defaults. Learn more about it here.Managing Workload Identity with Group Managed Service AccountsWindows container workloads can be configured to use Group Managed Service Accounts (GMSA). Group Managed Service Accounts are a specific type of Active Directory account that provide automatic password management, simplified service principal name (SPN) management, and the ability to delegate the management to other administrators across multiple servers. Containers configured with a GMSA can access external Active Directory Domain resources while carrying the identity configured with the GMSA. Learn more about configuring and using GMSA for Windows containers here.Taints and tolerationsUsers need to use some combination of taint and node selectors in order to schedule Linux and Windows workloads to their respective OS-specific nodes. The recommended approach is outlined below, with one of its main goals being that this approach should not break compatibility for existing Linux workloads.You can (and should) set .spec.os.name for each Pod, to indicate the operating system that the containers in that Pod are designed for. For Pods that run Linux containers, set .spec.os.name to linux. For Pods that run Windows containers, set .spec.os.name to windows.Note:If you are running a version of Kubernetes older than 1.24, you may need to enable the IdentifyPodOS feature gate to be able to set a value for .spec.pod.os.The scheduler does not use the value of .spec.os.name when assigning Pods to nodes. You should use normal Kubernetes mechanisms for assigning pods to nodes to ensure that the control plane for your cluster places pods onto nodes that are running the appropriate operating system.The .spec.os.name value has no effect on the scheduling of the Windows pods, so taints and tolerations (or node selectors) are still required to ensure that the Windows pods land onto appropriate Windows nodes.Ensuring OS-specific workloads land on the appropriate container hostUsers can ensure Windows containers can be scheduled on the appropriate host using taints and tolerations. All Kubernetes nodes running Kubernetes 1.33 have the following default labels:kubernetes.io/os = [windows|linux]kubernetes.io/arch = [amd64|arm64|...]If a Pod specification does not specify a nodeSelector such as "kubernetes.io/os": windows, it is possible the Pod can be scheduled on any host, Windows or Linux. This can be problematic since a Windows container can only run on Windows and a Linux container can only run on Linux. The best practice for Kubernetes 1.33 is to use a nodeSelector.However, in many cases users have a pre-existing large number of deployments for Linux containers, as well as an ecosystem of off-the-shelf configurations, such as community Helm charts, and programmatic Pod generation cases, such as with operators. In those situations, you may be hesitant to make the configuration change to add nodeSelector fields to all Pods and Pod templates. The alternative is to use taints. Because the kubelet can set taints during registration, it could easily be modified to automatically add a taint when running on Windows only.For example: --register-with-taints='os=windows:NoSchedule'By adding a taint to all Windows nodes, nothing will be scheduled on them (that includes existing Linux Pods). In order for a Windows Pod to be scheduled on a Windows node, it would need both the nodeSelector and the appropriate matching toleration to choose Windows.nodeSelector: kubernetes.io/os: windows node.kubernetes.io/windows-build: '10.0.17763' tolerations: \- key: "os" operator: "Equal" value: "windows" effect: "NoSchedule" Handling multiple Windows versions in the same clusterThe Windows Server version used by each pod must match that of the node. If you want to use multiple Windows Server versions in the same cluster, then you should set additional node labels and nodeSelector fields.Kubernetes automatically adds a label, node.kubernetes.io/windows-build to simplify this.This label reflects the Windows major, minor, and build number that need to match for compatibility. Here are values used for each Windows Server version:Product NameVersionWindows Server 201910.0.17763Windows Server 202210.0.20348Simplifying with RuntimeClassRuntimeClass can be used to simplify the process of using taints and tolerations. A cluster administrator can create a RuntimeClass object which is used to encapsulate these taints and tolerations.Save this file to runtimeClasses.yml. It includes the appropriate nodeSelector for the Windows OS, architecture, and version.--- apiVersion: node.k8s.io/v1 kind: RuntimeClass metadata: name: windows-2019 handler: example-container-runtime-handler scheduling: nodeSelector: kubernetes.io/os: 'windows' kubernetes.io/arch: 'amd64' node.kubernetes.io/windows-build: '10.0.17763' tolerations: \- effect: NoSchedule key: os operator: Equal value: "windows" Run kubectl create -f runtimeClasses.yml using as a cluster administratorAdd runtimeClassName: windows-2019 as appropriate to Pod specsFor example:--- apiVersion: apps/v1 kind: Deployment metadata: name: iis-2019 labels: app: iis-2019 spec: replicas: 1 template: metadata: name: iis-2019 labels: app: iis-2019 spec: runtimeClassName: windows-2019 containers: \- name: iis image: mcr.microsoft.com/windows/servercore/iis:windowsservercore-ltsc2019 resources: limits: cpu: 1 memory: 800Mi requests: cpu: .1 memory: 300Mi ports: \- containerPort: 80 selector: matchLabels: app: iis-2019 \--- apiVersion: v1 kind: Service metadata: name: iis spec: type: LoadBalancer ports: \- protocol: TCP port: 80 selector: app: iis-2019 3.13 - Extending KubernetesDifferent ways to change the behavior of your Kubernetes cluster.Kubernetes is highly configurable and extensible. As a result, there is rarely a need to fork or submit patches to the Kubernetes project code.This guide describes the options for customizing a Kubernetes cluster. It is aimed at cluster operators who want to understand how to adapt their Kubernetes cluster to the needs of their work environment. Developers who are prospective Platform Developers or Kubernetes Project Contributors will also find it useful as an introduction to what extension points and patterns exist, and their trade-offs and limitations.Customization approaches can be broadly divided into configuration, which only involves changing command line arguments, local configuration files, or API resources; and extensions, which involve running additional programs, additional network services, or both. This document is primarily about extensions.ConfigurationConfiguration files and command arguments are documented in the Reference section of the online documentation, with a page for each binary:kube-apiserverkube-controller-managerkube-schedulerkubeletkube-proxyCommand arguments and configuration files may not always be changeable in a hosted Kubernetes service or a distribution with managed installation. When they are changeable, they are usually only changeable by the cluster operator. Also, they are subject to change in future Kubernetes versions, and setting them may require restarting processes. For those reasons, they should be used only when there are no other options.Built-in policy APIs, such as ResourceQuota, NetworkPolicy and Role-based Access Control (RBAC), are built-in Kubernetes APIs that provide declaratively configured policy settings. APIs are typically usable even with hosted Kubernetes services and with managed Kubernetes installations. The built-in policy APIs follow the same conventions as other Kubernetes resources such as Pods. When you use a policy APIs that is stable, you benefit from a defined support policy like other Kubernetes APIs. For these reasons, policy APIs are recommended over configuration files and command arguments where suitable.ExtensionsExtensions are software components that extend and deeply integrate with Kubernetes. They adapt it to support new types and new kinds of hardware.Many cluster administrators use a hosted or distribution instance of Kubernetes. These clusters come with extensions pre-installed. As a result, most Kubernetes users will not need to install extensions and even fewer users will need to author new ones.Extension patternsKubernetes is designed to be automated by writing client programs. Any program that reads and/or writes to the Kubernetes API can provide useful automation. Automation can run on the cluster or off it. By following the guidance in this doc you can write highly available and robust automation. Automation generally works with any Kubernetes cluster, including hosted clusters and managed installations.There is a specific pattern for writing client programs that work well with Kubernetes called the controller pattern. Controllers typically read an object's .spec, possibly do things, and then update the object's .status.A controller is a client of the Kubernetes API. When Kubernetes is the client and calls out to a remote service, Kubernetes calls this a webhook. The remote service is called a webhook backend. As with custom controllers, webhooks do add a point of failure.Note:Outside of Kubernetes, the term “webhook” typically refers to a mechanism for asynchronous notifications, where the webhook call serves as a one-way notification to another system or component. In the Kubernetes ecosystem, even synchronous HTTP callouts are often described as “webhooks”.In the webhook model, Kubernetes makes a network request to a remote service. With the alternative binary Plugin model, Kubernetes executes a binary (program). Binary plugins are used by the kubelet (for example, CSI storage plugins and CNI network plugins), and by kubectl (see Extend kubectl with plugins).Extension pointsThis diagram shows the extension points in a Kubernetes cluster and the clients that access it.Kubernetes extension pointsKey to the figureUsers often interact with the Kubernetes API using kubectl. Plugins customise the behaviour of clients. There are generic extensions that can apply to different clients, as well as specific ways to extend kubectl.The API server handles all requests. Several types of extension points in the API server allow authenticating requests, or blocking them based on their content, editing content, and handling deletion. These are described in the API Access Extensions section.The API server serves various kinds of resources. Built-in resource kinds, such as pods, are defined by the Kubernetes project and can't be changed. Read API extensions to learn about extending the Kubernetes API.The Kubernetes scheduler decides which nodes to place pods on. There are several ways to extend scheduling, which are described in the Scheduling extensions section.Much of the behavior of Kubernetes is implemented by programs called controllers, that are clients of the API server. Controllers are often used in conjunction with custom resources. Read combining new APIs with automation and Changing built-in resources to learn more.The kubelet runs on servers (nodes), and helps pods appear like virtual servers with their own IPs on the cluster network. Network Plugins allow for different implementations of pod networking.You can use Device Plugins to integrate custom hardware or other special node-local facilities, and make these available to Pods running in your cluster. The kubelet includes support for working with device plugins.The kubelet also mounts and unmounts volume for pods and their containers. You can use Storage Plugins to add support for new kinds of storage and other volume types.Extension point choice flowchartIf you are unsure where to start, this flowchart can help. Note that some solutions may involve several types of extensions.Flowchart guide to select an extension approachClient extensionsPlugins for kubectl are separate binaries that add or replace the behavior of specific subcommands. The kubectl tool can also integrate with credential plugins These extensions only affect a individual user's local environment, and so cannot enforce site-wide policies.If you want to extend the kubectl tool, read Extend kubectl with plugins.API extensionsCustom resource definitionsConsider adding a Custom Resource to Kubernetes if you want to define new controllers, application configuration objects or other declarative APIs, and to manage them using Kubernetes tools, such as kubectl.For more about Custom Resources, see the Custom Resources concept guide.API aggregation layerYou can use Kubernetes' API Aggregation Layer to integrate the Kubernetes API with additional services such as for metrics.Combining new APIs with automationA combination of a custom resource API and a control loop is called the controllers pattern. If your controller takes the place of a human operator deploying infrastructure based on a desired state, then the controller may also be following the operator pattern. The Operator pattern is used to manage specific applications; usually, these are applications that maintain state and require care in how they are managed.You can also make your own custom APIs and control loops that manage other resources, such as storage, or to define policies (such as an access control restriction).Changing built-in resourcesWhen you extend the Kubernetes API by adding custom resources, the added resources always fall into a new API Groups. You cannot replace or change existing API groups. Adding an API does not directly let you affect the behavior of existing APIs (such as Pods), whereas API Access Extensions do.API access extensionsWhen a request reaches the Kubernetes API Server, it is first authenticated, then authorized, and is then subject to various types of admission control (some requests are in fact not authenticated, and get special treatment). See Controlling Access to the Kubernetes API for more on this flow.Each of the steps in the Kubernetes authentication / authorization flow offers extension points.AuthenticationAuthentication maps headers or certificates in all requests to a username for the client making the request.Kubernetes has several built-in authentication methods that it supports. It can also sit behind an authenticating proxy, and it can send a token from an Authorization: header to a remote service for verification (an authentication webhook) if those don't meet your needs.AuthorizationAuthorization determines whether specific users can read, write, and do other operations on API resources. It works at the level of whole resources -- it doesn't discriminate based on arbitrary object fields.If the built-in authorization options don't meet your needs, an authorization webhook allows calling out to custom code that makes an authorization decision.Dynamic admission controlAfter a request is authorized, if it is a write operation, it also goes through Admission Control steps. In addition to the built-in steps, there are several extensions:The Image Policy webhook restricts what images can be run in containers.To make arbitrary admission control decisions, a general Admission webhook can be used. Admission webhooks can reject creations or updates. Some admission webhooks modify the incoming request data before it is handled further by Kubernetes.Infrastructure extensionsDevice pluginsDevice plugins allow a node to discover new Node resources (in addition to the builtin ones like cpu and memory) via a Device Plugin.Storage pluginsContainer Storage Interface (CSI) plugins provide a way to extend Kubernetes with supports for new kinds of volumes. The volumes can be backed by durable external storage, or provide ephemeral storage, or they might offer a read-only interface to information using a filesystem paradigm.Kubernetes also includes support for FlexVolume plugins, which are deprecated since Kubernetes v1.23 (in favour of CSI).FlexVolume plugins allow users to mount volume types that aren't natively supported by Kubernetes. When you run a Pod that relies on FlexVolume storage, the kubelet calls a binary plugin to mount the volume. The archived FlexVolume design proposal has more detail on this approach.The Kubernetes Volume Plugin FAQ for Storage Vendors includes general information on storage plugins.Network pluginsYour Kubernetes cluster needs a network plugin in order to have a working Pod network and to support other aspects of the Kubernetes network model.Network Plugins allow Kubernetes to work with different networking topologies and technologies.Kubelet image credential provider pluginsFEATURE STATE: Kubernetes v1.26 [stable]Kubelet image credential providers are plugins for the kubelet to dynamically retrieve image registry credentials. The credentials are then used when pulling images from container image registries that match the configuration.The plugins can communicate with external services or use local files to obtain credentials. This way, the kubelet does not need to have static credentials for each registry, and can support various authentication methods and protocols.For plugin configuration details, see Configure a kubelet image credential provider.Scheduling extensionsThe scheduler is a special type of controller that watches pods, and assigns pods to nodes. The default scheduler can be replaced entirely, while continuing to use other Kubernetes components, or multiple schedulers can run at the same time.This is a significant undertaking, and almost all Kubernetes users find they do not need to modify the scheduler.You can control which scheduling plugins are active, or associate sets of plugins with different named scheduler profiles. You can also write your own plugin that integrates with one or more of the kube-scheduler's extension points.Finally, the built in kube-scheduler component supports a webhook that permits a remote HTTP backend (scheduler extension) to filter and / or prioritize the nodes that the kube-scheduler chooses for a pod.Note:You can only affect node filtering and node prioritization with a scheduler extender webhook; other extension points are not available through the webhook integration.What's nextLearn more about infrastructure extensionsDevice PluginsNetwork PluginsCSI storage pluginsLearn about kubectl pluginsLearn more about Custom ResourcesLearn more about Extension API ServersLearn about Dynamic admission controlLearn about the Operator pattern3.13.1 - Compute, Storage, and Networking ExtensionsThis section covers extensions to your cluster that do not come as part as Kubernetes itself. You can use these extensions to enhance the nodes in your cluster, or to provide the network fabric that links Pods together.CSI and FlexVolume storage pluginsContainer Storage Interface (CSI) plugins provide a way to extend Kubernetes with supports for new kinds of volumes. The volumes can be backed by durable external storage, or provide ephemeral storage, or they might offer a read-only interface to information using a filesystem paradigm.Kubernetes also includes support for FlexVolume plugins, which are deprecated since Kubernetes v1.23 (in favour of CSI).FlexVolume plugins allow users to mount volume types that aren't natively supported by Kubernetes. When you run a Pod that relies on FlexVolume storage, the kubelet calls a binary plugin to mount the volume. The archived FlexVolume design proposal has more detail on this approach.The Kubernetes Volume Plugin FAQ for Storage Vendors includes general information on storage plugins.Device pluginsDevice plugins allow a node to discover new Node facilities (in addition to the built-in node resources such as cpu and memory), and provide these custom node-local facilities to Pods that request them.Network pluginsNetwork plugins allow Kubernetes to work with different networking topologies and technologies. Your Kubernetes cluster needs a network plugin in order to have a working Pod network and to support other aspects of the Kubernetes network model.Kubernetes 1.33 is compatible with CNI network plugins.3.13.1.1 - Network PluginsKubernetes (version 1.3 through to the latest 1.33, and likely onwards) lets you use Container Network Interface (CNI) plugins for cluster networking. You must use a CNI plugin that is compatible with your cluster and that suits your needs. Different plugins are available (both open- and closed- source) in the wider Kubernetes ecosystem.A CNI plugin is required to implement the Kubernetes network model.You must use a CNI plugin that is compatible with the v0.4.0 or later releases of the CNI specification. The Kubernetes project recommends using a plugin that is compatible with the v1.0.0 CNI specification (plugins can be compatible with multiple spec versions).InstallationA Container Runtime, in the networking context, is a daemon on a node configured to provide CRI Services for kubelet. In particular, the Container Runtime must be configured to load the CNI plugins required to implement the Kubernetes network model.Note:Prior to Kubernetes 1.24, the CNI plugins could also be managed by the kubelet using the cni-bin-dir and network-plugin command-line parameters. These command-line parameters were removed in Kubernetes 1.24, with management of the CNI no longer in scope for kubelet.See Troubleshooting CNI plugin-related errors if you are facing issues following the removal of dockershim.For specific information about how a Container Runtime manages the CNI plugins, see the documentation for that Container Runtime, for example:containerdCRI-OFor specific information about how to install and manage a CNI plugin, see the documentation for that plugin or networking provider.Network Plugin RequirementsLoopback CNIIn addition to the CNI plugin installed on the nodes for implementing the Kubernetes network model, Kubernetes also requires the container runtimes to provide a loopback interface lo, which is used for each sandbox (pod sandboxes, vm sandboxes, ...). Implementing the loopback interface can be accomplished by re-using the CNI loopback plugin. or by developing your own code to achieve this (see this example from CRI-O).Support hostPortThe CNI networking plugin supports hostPort. You can use the official portmap plugin offered by the CNI plugin team or use your own plugin with portMapping functionality.If you want to enable hostPort support, you must specify portMappings capability in your cni-conf-dir. For example:{ "name": "k8s-pod-network", "cniVersion": "0.4.0", "plugins": [ { "type": "calico", "log_level": "info", "datastore_type": "kubernetes", "nodename": "127.0.0.1", "ipam": { "type": "host-local", "subnet": "usePodCidr" }, "policy": { "type": "k8s" }, "kubernetes": { "kubeconfig": "/etc/cni/net.d/calico-kubeconfig" } }, { "type": "portmap", "capabilities": {"portMappings": true}, "externalSetMarkChain": "KUBE-MARK-MASQ" } ] } Support traffic shapingExperimental FeatureThe CNI networking plugin also supports pod ingress and egress traffic shaping. You can use the official bandwidth plugin offered by the CNI plugin team or use your own plugin with bandwidth control functionality.If you want to enable traffic shaping support, you must add the bandwidth plugin to your CNI configuration file (default /etc/cni/net.d) and ensure that the binary is included in your CNI bin dir (default /opt/cni/bin).{ "name": "k8s-pod-network", "cniVersion": "0.4.0", "plugins": [ { "type": "calico", "log_level": "info", "datastore_type": "kubernetes", "nodename": "127.0.0.1", "ipam": { "type": "host-local", "subnet": "usePodCidr" }, "policy": { "type": "k8s" }, "kubernetes": { "kubeconfig": "/etc/cni/net.d/calico-kubeconfig" } }, { "type": "bandwidth", "capabilities": {"bandwidth": true} } ] } Now you can add the kubernetes.io/ingress-bandwidth and kubernetes.io/egress-bandwidth annotations to your Pod. For example:apiVersion: v1 kind: Pod metadata: annotations: kubernetes.io/ingress-bandwidth: 1M kubernetes.io/egress-bandwidth: 1M ... What's nextLearn more about Cluster NetworkingLearn more about Network PoliciesLearn about the Troubleshooting CNI plugin-related errors3.13.1.2 - Device PluginsDevice plugins let you configure your cluster with support for devices or resources that require vendor-specific setup, such as GPUs, NICs, FPGAs, or non-volatile main memory.FEATURE STATE: Kubernetes v1.26 [stable]Kubernetes provides a device plugin framework that you can use to advertise system hardware resources to the Kubelet.Instead of customizing the code for Kubernetes itself, vendors can implement a device plugin that you deploy either manually or as a DaemonSet. The targeted devices include GPUs, high-performance NICs, FPGAs, InfiniBand adapters, and other similar computing resources that may require vendor specific initialization and setup.Device plugin registrationThe kubelet exports a Registration gRPC service:service Registration { rpc Register(RegisterRequest) returns (Empty) {} } A device plugin can register itself with the kubelet through this gRPC service. During the registration, the device plugin needs to send:The name of its Unix socket.The Device Plugin API version against which it was built.The ResourceName it wants to advertise. Here ResourceName needs to follow the extended resource naming scheme as vendor-domain/resourcetype. (For example, an NVIDIA GPU is advertised as nvidia.com/gpu.)Following a successful registration, the device plugin sends the kubelet the list of devices it manages, and the kubelet is then in charge of advertising those resources to the API server as part of the kubelet node status update. For example, after a device plugin registers hardware-vendor.example/foo with the kubelet and reports two healthy devices on a node, the node status is updated to advertise that the node has 2 "Foo" devices installed and available.Then, users can request devices as part of a Pod specification (see container). Requesting extended resources is similar to how you manage requests and limits for other resources, with the following differences:Extended resources are only supported as integer resources and cannot be overcommitted.Devices cannot be shared between containers.ExampleSuppose a Kubernetes cluster is running a device plugin that advertises resource hardware-vendor.example/foo on certain nodes. Here is an example of a pod requesting this resource to run a demo workload:--- apiVersion: v1 kind: Pod metadata: name: demo-pod spec: containers: \- name: demo-container-1 image: registry.k8s.io/pause:3.8 resources: limits: hardware-vendor.example/foo: 2 # # This Pod needs 2 of the hardware-vendor.example/foo devices # and can only schedule onto a Node that's able to satisfy # that need. # # If the Node has more than 2 of those devices available, the # remainder would be available for other Pods to use. Device plugin implementationThe general workflow of a device plugin includes the following steps:Initialization. During this phase, the device plugin performs vendor-specific initialization and setup to make sure the devices are in a ready state.The plugin starts a gRPC service, with a Unix socket under the host path /var/lib/kubelet/device-plugins/, that implements the following interfaces:service DevicePlugin { // GetDevicePluginOptions returns options to be communicated with Device Manager. rpc GetDevicePluginOptions(Empty) returns (DevicePluginOptions) {} // ListAndWatch returns a stream of List of Devices // Whenever a Device state change or a Device disappears, ListAndWatch // returns the new list rpc ListAndWatch(Empty) returns (stream ListAndWatchResponse) {} // Allocate is called during container creation so that the Device // Plugin can run device specific operations and instruct Kubelet // of the steps to make the Device available in the container rpc Allocate(AllocateRequest) returns (AllocateResponse) {} // GetPreferredAllocation returns a preferred set of devices to allocate // from a list of available ones. The resulting preferred allocation is not // guaranteed to be the allocation ultimately performed by the // devicemanager. It is only designed to help the devicemanager make a more // informed allocation decision when possible. rpc GetPreferredAllocation(PreferredAllocationRequest) returns (PreferredAllocationResponse) {} // PreStartContainer is called, if indicated by Device Plugin during registration phase, // before each container start. Device plugin can run device specific operations // such as resetting the device before making devices available to the container. rpc PreStartContainer(PreStartContainerRequest) returns (PreStartContainerResponse) {} } Note:Plugins are not required to provide useful implementations for GetPreferredAllocation() or PreStartContainer(). Flags indicating the availability of these calls, if any, should be set in the DevicePluginOptions message sent back by a call to GetDevicePluginOptions(). The kubelet will always call GetDevicePluginOptions() to see which optional functions are available, before calling any of them directly.The plugin registers itself with the kubelet through the Unix socket at host path /var/lib/kubelet/device-plugins/kubelet.sock.Note:The ordering of the workflow is important. A plugin MUST start serving gRPC service before registering itself with kubelet for successful registration.After successfully registering itself, the device plugin runs in serving mode, during which it keeps monitoring device health and reports back to the kubelet upon any device state changes. It is also responsible for serving Allocate gRPC requests. During Allocate, the device plugin may do device-specific preparation; for example, GPU cleanup or QRNG initialization. If the operations succeed, the device plugin returns an AllocateResponse that contains container runtime configurations for accessing the allocated devices. The kubelet passes this information to the container runtime.An AllocateResponse contains zero or more ContainerAllocateResponse objects. In these, the device plugin defines modifications that must be made to a container's definition to provide access to the device. These modifications include:Annotationsdevice nodesenvironment variablesmountsfully-qualified CDI device namesNote:The processing of the fully-qualified CDI device names by the Device Manager requires that the DevicePluginCDIDevices feature gate is enabled for both the kubelet and the kube-apiserver. This was added as an alpha feature in Kubernetes v1.28, graduated to beta in v1.29 and to GA in v1.31.Handling kubelet restartsA device plugin is expected to detect kubelet restarts and re-register itself with the new kubelet instance. A new kubelet instance deletes all the existing Unix sockets under /var/lib/kubelet/device-plugins when it starts. A device plugin can monitor the deletion of its Unix socket and re-register itself upon such an event.Device plugin and unhealthy devicesThere are cases when devices fail or are shut down. The responsibility of the Device Plugin in this case is to notify the kubelet about the situation using the ListAndWatchResponse API.Once a device is marked as unhealthy, the kubelet will decrease the allocatable count for this resource on the Node to reflect how many devices can be used for scheduling new pods. Capacity count for the resource will not change.Pods that were assigned to the failed devices will continue be assigned to this device. It is typical that code relying on the device will start failing and Pod may get into Failed phase if restartPolicy for the Pod was not Always or enter the crash loop otherwise.Before Kubernetes v1.31, the way to know whether or not a Pod is associated with the failed device is to use the PodResources API.FEATURE STATE: Kubernetes v1.31 [alpha] (enabled by default: false)By enabling the feature gate ResourceHealthStatus, the field allocatedResourcesStatus will be added to each container status, within the .status for each Pod. The allocatedResourcesStatus field reports health information for each device assigned to the container.For a failed Pod, or or where you suspect a fault, you can use this status to understand whether the Pod behavior may be associated with device failure. For example, if an accelerator is reporting an over-temperature event, the allocatedResourcesStatus field may be able to report this.Device plugin deploymentYou can deploy a device plugin as a DaemonSet, as a package for your node's operating system, or manually.The canonical directory /var/lib/kubelet/device-plugins requires privileged access, so a device plugin must run in a privileged security context. If you're deploying a device plugin as a DaemonSet, /var/lib/kubelet/device-plugins must be mounted as a Volume in the plugin's PodSpec.If you choose the DaemonSet approach you can rely on Kubernetes to: place the device plugin's Pod onto Nodes, to restart the daemon Pod after failure, and to help automate upgrades.API compatibilityPreviously, the versioning scheme required the Device Plugin's API version to match exactly the Kubelet's version. Since the graduation of this feature to Beta in v1.12 this is no longer a hard requirement. The API is versioned and has been stable since Beta graduation of this feature. Because of this, kubelet upgrades should be seamless but there still may be changes in the API before stabilization making upgrades not guaranteed to be non-breaking.Note:Although the Device Manager component of Kubernetes is a generally available feature, the device plugin API is not stable. For information on the device plugin API and version compatibility, read Device Plugin API versions.As a project, Kubernetes recommends that device plugin developers:Watch for Device Plugin API changes in the future releases.Support multiple versions of the device plugin API for backward/forward compatibility.To run device plugins on nodes that need to be upgraded to a Kubernetes release with a newer device plugin API version, upgrade your device plugins to support both versions before upgrading these nodes. Taking that approach will ensure the continuous functioning of the device allocations during the upgrade.Monitoring device plugin resourcesFEATURE STATE: Kubernetes v1.28 [stable]In order to monitor resources provided by device plugins, monitoring agents need to be able to discover the set of devices that are in-use on the node and obtain metadata to describe which container the metric should be associated with. Prometheus metrics exposed by device monitoring agents should follow the Kubernetes Instrumentation Guidelines, identifying containers using pod, namespace, and container prometheus labels.The kubelet provides a gRPC service to enable discovery of in-use devices, and to provide metadata for these devices:// PodResourcesLister is a service provided by the kubelet that provides information about the // node resources consumed by pods and containers on the node service PodResourcesLister { rpc List(ListPodResourcesRequest) returns (ListPodResourcesResponse) {} rpc GetAllocatableResources(AllocatableResourcesRequest) returns (AllocatableResourcesResponse) {} rpc Get(GetPodResourcesRequest) returns (GetPodResourcesResponse) {} } List gRPC endpointThe List endpoint provides information on resources of running pods, with details such as the id of exclusively allocated CPUs, device id as it was reported by device plugins and id of the NUMA node where these devices are allocated. Also, for NUMA-based machines, it contains the information about memory and hugepages reserved for a container.Starting from Kubernetes v1.27, the List endpoint can provide information on resources of running pods allocated in ResourceClaims by the DynamicResourceAllocation API. To enable this feature kubelet must be started with the following flags:--feature-gates=DynamicResourceAllocation=true,KubeletPodResourcesDynamicResources=true // ListPodResourcesResponse is the response returned by List function message ListPodResourcesResponse { repeated PodResources pod_resources = 1; } // PodResources contains information about the node resources assigned to a pod message PodResources { string name = 1; string namespace = 2; repeated ContainerResources containers = 3; } // ContainerResources contains information about the resources assigned to a container message ContainerResources { string name = 1; repeated ContainerDevices devices = 2; repeated int64 cpu_ids = 3; repeated ContainerMemory memory = 4; repeated DynamicResource dynamic_resources = 5; } // ContainerMemory contains information about memory and hugepages assigned to a container message ContainerMemory { string memory_type = 1; uint64 size = 2; TopologyInfo topology = 3; } // Topology describes hardware topology of the resource message TopologyInfo { repeated NUMANode nodes = 1; } // NUMA representation of NUMA node message NUMANode { int64 ID = 1; } // ContainerDevices contains information about the devices assigned to a container message ContainerDevices { string resource_name = 1; repeated string device_ids = 2; TopologyInfo topology = 3; } // DynamicResource contains information about the devices assigned to a container by Dynamic Resource Allocation message DynamicResource { string class_name = 1; string claim_name = 2; string claim_namespace = 3; repeated ClaimResource claim_resources = 4; } // ClaimResource contains per-plugin resource information message ClaimResource { repeated CDIDevice cdi_devices = 1 [(gogoproto.customname) = "CDIDevices"]; } // CDIDevice specifies a CDI device information message CDIDevice { // Fully qualified CDI device name // for example: vendor.com/gpu=gpudevice1 // see more details in the CDI specification: // https://github.com/container-orchestrated-devices/container-device-interface/blob/main/SPEC.md string name = 1; } Note:cpu_ids in the ContainerResources in the List endpoint correspond to exclusive CPUs allocated to a particular container. If the goal is to evaluate CPUs that belong to the shared pool, the List endpoint needs to be used in conjunction with the GetAllocatableResources endpoint as explained below:Call GetAllocatableResources to get a list of all the allocatable CPUsCall GetCpuIds on all ContainerResources in the systemSubtract out all of the CPUs from the GetCpuIds calls from the GetAllocatableResources callGetAllocatableResources gRPC endpointFEATURE STATE: Kubernetes v1.28 [stable]GetAllocatableResources provides information on resources initially available on the worker node. It provides more information than kubelet exports to APIServer.Note:GetAllocatableResources should only be used to evaluate allocatable resources on a node. If the goal is to evaluate free/unallocated resources it should be used in conjunction with the List() endpoint. The result obtained by GetAllocatableResources would remain the same unless the underlying resources exposed to kubelet change. This happens rarely but when it does (for example: hotplug/hotunplug, device health changes), client is expected to call GetAlloctableResources endpoint.However, calling GetAllocatableResources endpoint is not sufficient in case of cpu and/or memory update and Kubelet needs to be restarted to reflect the correct resource capacity and allocatable.// AllocatableResourcesResponses contains information about all the devices known by the kubelet message AllocatableResourcesResponse { repeated ContainerDevices devices = 1; repeated int64 cpu_ids = 2; repeated ContainerMemory memory = 3; } ContainerDevices do expose the topology information declaring to which NUMA cells the device is affine. The NUMA cells are identified using a opaque integer ID, which value is consistent to what device plugins report when they register themselves to the kubelet.The gRPC service is served over a unix socket at /var/lib/kubelet/pod-resources/kubelet.sock. Monitoring agents for device plugin resources can be deployed as a daemon, or as a DaemonSet. The canonical directory /var/lib/kubelet/pod-resources requires privileged access, so monitoring agents must run in a privileged security context. If a device monitoring agent is running as a DaemonSet, /var/lib/kubelet/pod-resources must be mounted as a Volume in the device monitoring agent's PodSpec.Note:When accessing the /var/lib/kubelet/pod-resources/kubelet.sock from DaemonSet or any other app deployed as a container on the host, which is mounting socket as a volume, it is a good practice to mount directory /var/lib/kubelet/pod-resources/ instead of the /var/lib/kubelet/pod-resources/kubelet.sock. This will ensure that after kubelet restart, container will be able to re-connect to this socket.Container mounts are managed by inode referencing the socket or directory, depending on what was mounted. When kubelet restarts, socket is deleted and a new socket is created, while directory stays untouched. So the original inode for the socket become unusable. Inode to directory will continue working.Get gRPC endpointFEATURE STATE: Kubernetes v1.27 [alpha]The Get endpoint provides information on resources of a running Pod. It exposes information similar to those described in the List endpoint. The Get endpoint requires PodName and PodNamespace of the running Pod.// GetPodResourcesRequest contains information about the pod message GetPodResourcesRequest { string pod_name = 1; string pod_namespace = 2; } To enable this feature, you must start your kubelet services with the following flag:--feature-gates=KubeletPodResourcesGet=true The Get endpoint can provide Pod information related to dynamic resources allocated by the dynamic resource allocation API. To enable this feature, you must ensure your kubelet services are started with the following flags:--feature-gates=KubeletPodResourcesGet=true,DynamicResourceAllocation=true,KubeletPodResourcesDynamicResources=true Device plugin integration with the Topology ManagerFEATURE STATE: Kubernetes v1.27 [stable]The Topology Manager is a Kubelet component that allows resources to be co-ordinated in a Topology aligned manner. In order to do this, the Device Plugin API was extended to include a TopologyInfo struct.message TopologyInfo { repeated NUMANode nodes = 1; } message NUMANode { int64 ID = 1; } Device Plugins that wish to leverage the Topology Manager can send back a populated TopologyInfo struct as part of the device registration, along with the device IDs and the health of the device. The device manager will then use this information to consult with the Topology Manager and make resource assignment decisions.TopologyInfo supports setting a nodes field to either nil or a list of NUMA nodes. This allows the Device Plugin to advertise a device that spans multiple NUMA nodes.Setting TopologyInfo to nil or providing an empty list of NUMA nodes for a given device indicates that the Device Plugin does not have a NUMA affinity preference for that device.An example TopologyInfo struct populated for a device by a Device Plugin:pluginapi.Device{ID: "25102017", Health: pluginapi.Healthy, Topology:&pluginapi.TopologyInfo;{Nodes: []*pluginapi.NUMANode{&pluginapi.NUMANode;{ID: 0,},}}} Device plugin examplesNote: This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the content guide before submitting a change. More information.Here are some examples of device plugin implementations:Akri, which lets you easily expose heterogeneous leaf devices (such as IP cameras and USB devices).The AMD GPU device pluginThe generic device plugin for generic Linux devices and USB devicesThe HAMi for heterogeneous AI computing virtualization middleware (for example, NVIDIA, Cambricon, Hygon, Iluvatar, MThreads, Ascend, Metax)The Intel device plugins for Intel GPU, FPGA, QAT, VPU, SGX, DSA, DLB and IAA devicesThe KubeVirt device plugins for hardware-assisted virtualizationThe NVIDIA GPU device plugin, NVIDIA's official device plugin to expose NVIDIA GPUs and monitor GPU healthThe NVIDIA GPU device plugin for Container-Optimized OSThe RDMA device pluginThe SocketCAN device pluginThe Solarflare device pluginThe SR-IOV Network device pluginThe Xilinx FPGA device plugins for Xilinx FPGA devicesWhat's nextLearn about scheduling GPU resources using device pluginsLearn about advertising extended resources on a nodeLearn about the Topology ManagerRead about using hardware acceleration for TLS ingress with Kubernetes3.13.2 - Extending the Kubernetes APICustom resources are extensions of the Kubernetes API. Kubernetes provides two ways to add custom resources to your cluster:The CustomResourceDefinition (CRD) mechanism allows you to declaratively define a new custom API with an API group, kind, and schema that you specify. The Kubernetes control plane serves and handles the storage of your custom resource. CRDs allow you to create new types of resources for your cluster without writing and running a custom API server.The aggregation layer sits behind the primary API server, which acts as a proxy. This arrangement is called API Aggregation (AA), which allows you to provide specialized implementations for your custom resources by writing and deploying your own API server. The main API server delegates requests to your API server for the custom APIs that you specify, making them available to all of its clients.3.13.2.1 - Custom ResourcesCustom resources are extensions of the Kubernetes API. This page discusses when to add a custom resource to your Kubernetes cluster and when to use a standalone service. It describes the two methods for adding custom resources and how to choose between them.Custom resourcesA resource is an endpoint in the Kubernetes API that stores a collection of API objects of a certain kind; for example, the built-in pods resource contains a collection of Pod objects.A custom resource is an extension of the Kubernetes API that is not necessarily available in a default Kubernetes installation. It represents a customization of a particular Kubernetes installation. However, many core Kubernetes functions are now built using custom resources, making Kubernetes more modular.Custom resources can appear and disappear in a running cluster through dynamic registration, and cluster admins can update custom resources independently of the cluster itself. Once a custom resource is installed, users can create and access its objects using kubectl, just as they do for built-in resources like Pods.Custom controllersOn their own, custom resources let you store and retrieve structured data. When you combine a custom resource with a custom controller, custom resources provide a true declarative API.The Kubernetes declarative API enforces a separation of responsibilities. You declare the desired state of your resource. The Kubernetes controller keeps the current state of Kubernetes objects in sync with your declared desired state. This is in contrast to an imperative API, where you instruct a server what to do.You can deploy and update a custom controller on a running cluster, independently of the cluster's lifecycle. Custom controllers can work with any kind of resource, but they are especially effective when combined with custom resources. The Operator pattern combines custom resources and custom controllers. You can use custom controllers to encode domain knowledge for specific applications into an extension of the Kubernetes API.Should I add a custom resource to my Kubernetes cluster?When creating a new API, consider whether to aggregate your API with the Kubernetes cluster APIs or let your API stand alone.Consider API aggregation if:Prefer a stand-alone API if:Your API is Declarative.Your API does not fit the Declarative model.You want your new types to be readable and writable using kubectl.kubectl support is not requiredYou want to view your new types in a Kubernetes UI, such as dashboard, alongside built-in types.Kubernetes UI support is not required.You are developing a new API.You already have a program that serves your API and works well.You are willing to accept the format restriction that Kubernetes puts on REST resource paths, such as API Groups and Namespaces. (See the API Overview.)You need to have specific REST paths to be compatible with an already defined REST API.Your resources are naturally scoped to a cluster or namespaces of a cluster.Cluster or namespace scoped resources are a poor fit; you need control over the specifics of resource paths.You want to reuse Kubernetes API support features.You don't need those features.Declarative APIsIn a Declarative API, typically:Your API consists of a relatively small number of relatively small objects (resources).The objects define configuration of applications or infrastructure.The objects are updated relatively infrequently.Humans often need to read and write the objects.The main operations on the objects are CRUD-y (creating, reading, updating and deleting).Transactions across objects are not required: the API represents a desired state, not an exact state.Imperative APIs are not declarative. Signs that your API might not be declarative include:The client says "do this", and then gets a synchronous response back when it is done.The client says "do this", and then gets an operation ID back, and has to check a separate Operation object to determine completion of the request.You talk about Remote Procedure Calls (RPCs).Directly storing large amounts of data; for example, > a few kB per object, or > 1000s of objects.High bandwidth access (10s of requests per second sustained) needed.Store end-user data (such as images, PII, etc.) or other large-scale data processed by applications.The natural operations on the objects are not CRUD-y.The API is not easily modeled as objects.You chose to represent pending operations with an operation ID or an operation object.Should I use a ConfigMap or a custom resource?Use a ConfigMap if any of the following apply:There is an existing, well-documented configuration file format, such as a mysql.cnf or pom.xml.You want to put the entire configuration into one key of a ConfigMap.The main use of the configuration file is for a program running in a Pod on your cluster to consume the file to configure itself.Consumers of the file prefer to consume via file in a Pod or environment variable in a pod, rather than the Kubernetes API.You want to perform rolling updates via Deployment, etc., when the file is updated.Note:Use a Secret for sensitive data, which is similar to a ConfigMap but more secure.Use a custom resource (CRD or Aggregated API) if most of the following apply:You want to use Kubernetes client libraries and CLIs to create and update the new resource.You want top-level support from kubectl; for example, kubectl get my-object object-name.You want to build new automation that watches for updates on the new object, and then CRUD other objects, or vice versa.You want to write automation that handles updates to the object.You want to use Kubernetes API conventions like .spec, .status, and .metadata.You want the object to be an abstraction over a collection of controlled resources, or a summarization of other resources.Adding custom resourcesKubernetes provides two ways to add custom resources to your cluster:CRDs are simple and can be created without any programming.API Aggregation requires programming, but allows more control over API behaviors like how data is stored and conversion between API versions.Kubernetes provides these two options to meet the needs of different users, so that neither ease of use nor flexibility is compromised.Aggregated APIs are subordinate API servers that sit behind the primary API server, which acts as a proxy. This arrangement is called API Aggregation(AA). To users, the Kubernetes API appears extended.CRDs allow users to create new types of resources without adding another API server. You do not need to understand API Aggregation to use CRDs.Regardless of how they are installed, the new resources are referred to as Custom Resources to distinguish them from built-in Kubernetes resources (like pods).Note:Avoid using a Custom Resource as data storage for application, end user, or monitoring data: architecture designs that store application data within the Kubernetes API typically represent a design that is too closely coupled.Architecturally, cloud native application architectures favor loose coupling between components. If part of your workload requires a backing service for its routine operation, run that backing service as a component or consume it as an external service. This way, your workload does not rely on the Kubernetes API for its normal operation.CustomResourceDefinitionsThe CustomResourceDefinition API resource allows you to define custom resources. Defining a CRD object creates a new custom resource with a name and schema that you specify. The Kubernetes API serves and handles the storage of your custom resource. The name of the CRD object itself must be a valid DNS subdomain name derived from the defined resource name and its API group; see how to create a CRD for more details. Further, the name of an object whose kind/resource is defined by a CRD must also be a valid DNS subdomain name.This frees you from writing your own API server to handle the custom resource, but the generic nature of the implementation means you have less flexibility than with API server aggregation.Refer to the custom controller example for an example of how to register a new custom resource, work with instances of your new resource type, and use a controller to handle events.API server aggregationUsually, each resource in the Kubernetes API requires code that handles REST requests and manages persistent storage of objects. The main Kubernetes API server handles built-in resources like pods and services, and can also generically handle custom resources through CRDs.The aggregation layer allows you to provide specialized implementations for your custom resources by writing and deploying your own API server. The main API server delegates requests to your API server for the custom resources that you handle, making them available to all of its clients.Choosing a method for adding custom resourcesCRDs are easier to use. Aggregated APIs are more flexible. Choose the method that best meets your needs.Typically, CRDs are a good fit if:You have a handful of fieldsYou are using the resource within your company, or as part of a small open-source project (as opposed to a commercial product)Comparing ease of useCRDs are easier to create than Aggregated APIs.CRDsAggregated APIDo not require programming. Users can choose any language for a CRD controller.Requires programming and building binary and image.No additional service to run; CRDs are handled by API server.An additional service to create and that could fail.No ongoing support once the CRD is created. Any bug fixes are picked up as part of normal Kubernetes Master upgrades.May need to periodically pickup bug fixes from upstream and rebuild and update the Aggregated API server.No need to handle multiple versions of your API; for example, when you control the client for this resource, you can upgrade it in sync with the API.You need to handle multiple versions of your API; for example, when developing an extension to share with the world.Advanced features and flexibilityAggregated APIs offer more advanced API features and customization of other features; for example, the storage layer.FeatureDescriptionCRDsAggregated APIValidationHelp users prevent errors and allow you to evolve your API independently of your clients. These features are most useful when there are many clients who can't all update at the same time.Yes. Most validation can be specified in the CRD using OpenAPI v3.0 validation. CRDValidationRatcheting feature gate allows failing validations specified using OpenAPI also can be ignored if the failing part of the resource was unchanged. Any other validations supported by addition of a Validating Webhook.Yes, arbitrary validation checksDefaultingSee aboveYes, either via OpenAPI v3.0 validation default keyword (GA in 1.17), or via a Mutating Webhook (though this will not be run when reading from etcd for old objects).YesMulti-versioningAllows serving the same object through two API versions. Can help ease API changes like renaming fields. Less important if you control your client versions.YesYesCustom StorageIf you need storage with a different performance mode (for example, a time-series database instead of key-value store) or isolation for security (for example, encryption of sensitive information, etc.)NoYesCustom Business LogicPerform arbitrary checks or actions when creating, reading, updating or deleting an objectYes, using Webhooks.YesScale SubresourceAllows systems like HorizontalPodAutoscaler and PodDisruptionBudget interact with your new resourceYesYesStatus SubresourceAllows fine-grained access control where user writes the spec section and the controller writes the status section. Allows incrementing object Generation on custom resource data mutation (requires separate spec and status sections in the resource)YesYesOther SubresourcesAdd operations other than CRUD, such as "logs" or "exec".NoYesstrategic-merge-patchThe new endpoints support PATCH with Content-Type: application/strategic-merge-patch+json. Useful for updating objects that may be modified both locally, and by the server. For more information, see "Update API Objects in Place Using kubectl patch"NoYesProtocol BuffersThe new resource supports clients that want to use Protocol BuffersNoYesOpenAPI SchemaIs there an OpenAPI (swagger) schema for the types that can be dynamically fetched from the server? Is the user protected from misspelling field names by ensuring only allowed fields are set? Are types enforced (in other words, don't put an int in a string field?)Yes, based on the OpenAPI v3.0 validation schema (GA in 1.16).YesInstance NameDoes this extension mechanism impose any constraints on the names of objects whose kind/resource is defined this way?Yes, such an object's name must be a valid DNS subdomain name.NoCommon FeaturesWhen you create a custom resource, either via a CRD or an AA, you get many features for your API, compared to implementing it outside the Kubernetes platform:FeatureWhat it doesCRUDThe new endpoints support CRUD basic operations via HTTP and kubectlWatchThe new endpoints support Kubernetes Watch operations via HTTPDiscoveryClients like kubectl and dashboard automatically offer list, display, and field edit operations on your resourcesjson-patchThe new endpoints support PATCH with Content-Type: application/json-patch+jsonmerge-patchThe new endpoints support PATCH with Content-Type: application/merge-patch+jsonHTTPSThe new endpoints uses HTTPSBuilt-in AuthenticationAccess to the extension uses the core API server (aggregation layer) for authenticationBuilt-in AuthorizationAccess to the extension can reuse the authorization used by the core API server; for example, RBAC.FinalizersBlock deletion of extension resources until external cleanup happens.Admission WebhooksSet default values and validate extension resources during any create/update/delete operation.UI/CLI DisplayKubectl, dashboard can display extension resources.Unset versus EmptyClients can distinguish unset fields from zero-valued fields.Client Libraries GenerationKubernetes provides generic client libraries, as well as tools to generate type-specific client libraries.Labels and annotationsCommon metadata across objects that tools know how to edit for core and custom resources.Preparing to install a custom resourceThere are several points to be aware of before adding a custom resource to your cluster.Third party code and new points of failureWhile creating a CRD does not automatically add any new points of failure (for example, by causing third party code to run on your API server), packages (for example, Charts) or other installation bundles often include CRDs as well as a Deployment of third-party code that implements the business logic for a new custom resource.Installing an Aggregated API server always involves running a new Deployment.StorageCustom resources consume storage space in the same way that ConfigMaps do. Creating too many custom resources may overload your API server's storage space.Aggregated API servers may use the same storage as the main API server, in which case the same warning applies.Authentication, authorization, and auditingCRDs always use the same authentication, authorization, and audit logging as the built-in resources of your API server.If you use RBAC for authorization, most RBAC roles will not grant access to the new resources (except the cluster-admin role or any role created with wildcard rules). You'll need to explicitly grant access to the new resources. CRDs and Aggregated APIs often come bundled with new role definitions for the types they add.Aggregated API servers may or may not use the same authentication, authorization, and auditing as the primary API server.Accessing a custom resourceKubernetes client libraries can be used to access custom resources. Not all client libraries support custom resources. The Go and Python client libraries do.When you add a custom resource, you can access it using:kubectlThe Kubernetes dynamic client.A REST client that you write.A client generated using Kubernetes client generation tools (generating one is an advanced undertaking, but some projects may provide a client along with the CRD or AA).Custom resource field selectorsField Selectors let clients select custom resources based on the value of one or more resource fields.All custom resources support the metadata.name and metadata.namespace field selectors.Fields declared in a CustomResourceDefinition may also be used with field selectors when included in the spec.versions[*].selectableFields field of the CustomResourceDefinition.Selectable fields for custom resourcesFEATURE STATE: Kubernetes v1.32 [stable] (enabled by default: true)The spec.versions[*].selectableFields field of a CustomResourceDefinition may be used to declare which other fields in a custom resource may be used in field selectors.The following example adds the .spec.color and .spec.size fields as selectable fields.customresourcedefinition/shirt-resource-definition.yaml apiVersion: apiextensions.k8s.io/v1 kind: CustomResourceDefinition metadata: name: shirts.stable.example.com spec: group: stable.example.com scope: Namespaced names: plural: shirts singular: shirt kind: Shirt versions: \- name: v1 served: true storage: true schema: openAPIV3Schema: type: object properties: spec: type: object properties: color: type: string size: type: string selectableFields: \- jsonPath: .spec.color \- jsonPath: .spec.size additionalPrinterColumns: \- jsonPath: .spec.color name: Color type: string \- jsonPath: .spec.size name: Size type: string Field selectors can then be used to get only resources with a color of blue:kubectl get shirts.stable.example.com --field-selector spec.color=blue The output should be:NAME COLOR SIZE example1 blue S example2 blue M What's nextLearn how to Extend the Kubernetes API with the aggregation layer.Learn how to Extend the Kubernetes API with CustomResourceDefinition.3.13.2.2 - Kubernetes API Aggregation LayerThe aggregation layer allows Kubernetes to be extended with additional APIs, beyond what is offered by the core Kubernetes APIs. The additional APIs can either be ready-made solutions such as a metrics server, or APIs that you develop yourself.The aggregation layer is different from Custom Resource Definitions, which are a way to make the kube-apiserver recognise new kinds of object.Aggregation layerThe aggregation layer runs in-process with the kube-apiserver. Until an extension resource is registered, the aggregation layer will do nothing. To register an API, you add an APIService object, which "claims" the URL path in the Kubernetes API. At that point, the aggregation layer will proxy anything sent to that API path (e.g. /apis/myextension.mycompany.io/v1/…) to the registered APIService.The most common way to implement the APIService is to run an extension API server in Pod(s) that run in your cluster. If you're using the extension API server to manage resources in your cluster, the extension API server (also written as "extension-apiserver") is typically paired with one or more controllers. The apiserver-builder library provides a skeleton for both extension API servers and the associated controller(s).Response latencyExtension API servers should have low latency networking to and from the kube-apiserver. Discovery requests are required to round-trip from the kube-apiserver in five seconds or less.If your extension API server cannot achieve that latency requirement, consider making changes that let you meet it.What's nextTo get the aggregator working in your environment, configure the aggregation layer.Then, setup an extension api-server to work with the aggregation layer.Read about APIService in the API referenceLearn about Declarative Validation Concepts, an internal mechanism for defining validation rules that in the future will help support validation for extension API server development.Alternatively: learn how to extend the Kubernetes API using Custom Resource Definitions.3.13.3 - Operator patternOperators are software extensions to Kubernetes that make use of custom resources to manage applications and their components. Operators follow Kubernetes principles, notably the control loop.MotivationThe operator pattern aims to capture the key aim of a human operator who is managing a service or set of services. Human operators who look after specific applications and services have deep knowledge of how the system ought to behave, how to deploy it, and how to react if there are problems.People who run workloads on Kubernetes often like to use automation to take care of repeatable tasks. The operator pattern captures how you can write code to automate a task beyond what Kubernetes itself provides.Operators in KubernetesKubernetes is designed for automation. Out of the box, you get lots of built-in automation from the core of Kubernetes. You can use Kubernetes to automate deploying and running workloads, and you can automate how Kubernetes does that.Kubernetes' operator pattern concept lets you extend the cluster's behaviour without modifying the code of Kubernetes itself by linking controllers to one or more custom resources. Operators are clients of the Kubernetes API that act as controllers for a Custom Resource.An example operatorSome of the things that you can use an operator to automate include:deploying an application on demandtaking and restoring backups of that application's statehandling upgrades of the application code alongside related changes such as database schemas or extra configuration settingspublishing a Service to applications that don't support Kubernetes APIs to discover themsimulating failure in all or part of your cluster to test its resiliencechoosing a leader for a distributed application without an internal member election processWhat might an operator look like in more detail? Here's an example:A custom resource named SampleDB, that you can configure into the cluster.A Deployment that makes sure a Pod is running that contains the controller part of the operator.A container image of the operator code.Controller code that queries the control plane to find out what SampleDB resources are configured.The core of the operator is code to tell the API server how to make reality match the configured resources.If you add a new SampleDB, the operator sets up PersistentVolumeClaims to provide durable database storage, a StatefulSet to run SampleDB and a Job to handle initial configuration.If you delete it, the operator takes a snapshot, then makes sure that the StatefulSet and Volumes are also removed.The operator also manages regular database backups. For each SampleDB resource, the operator determines when to create a Pod that can connect to the database and take backups. These Pods would rely on a ConfigMap and / or a Secret that has database connection details and credentials.Because the operator aims to provide robust automation for the resource it manages, there would be additional supporting code. For this example, code checks to see if the database is running an old version and, if so, creates Job objects that upgrade it for you.Deploying operatorsThe most common way to deploy an operator is to add the Custom Resource Definition and its associated Controller to your cluster. The Controller will normally run outside of the control plane, much as you would run any containerized application. For example, you can run the controller in your cluster as a Deployment.Using an operatorOnce you have an operator deployed, you'd use it by adding, modifying or deleting the kind of resource that the operator uses. Following the above example, you would set up a Deployment for the operator itself, and then:kubectl get SampleDB # find configured databases kubectl edit SampleDB/example-database # manually change some settings …and that's it! The operator will take care of applying the changes as well as keeping the existing service in good shape.Writing your own operatorIf there isn't an operator in the ecosystem that implements the behavior you want, you can code your own.You also implement an operator (that is, a Controller) using any language / runtime that can act as a client for the Kubernetes API.Following are a few libraries and tools you can use to write your own cloud native operator.Note: This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the content guide before submitting a change. More information.Charmed Operator FrameworkJava Operator SDKKopf (Kubernetes Operator Pythonic Framework)kube-rs (Rust)kubebuilderKubeOps (.NET operator SDK)MastMetacontroller along with WebHooks that you implement yourselfOperator Frameworkshell-operatorWhat's nextRead the CNCF Operator White Paper.Learn more about Custom ResourcesFind ready-made operators on OperatorHub.io to suit your use casePublish your operator for other people to useRead CoreOS' original article that introduced the operator pattern (this is an archived version of the original article).Read an article from Google Cloud about best practices for building operators4 - TasksThis section of the Kubernetes documentation contains pages that show how to do individual tasks. A task page shows how to do a single thing, typically by giving a short sequence of steps.If you would like to write a task page, see Creating a Documentation Pull Request.4.1 - Install ToolsSet up Kubernetes tools on your computer.kubectlThe Kubernetes command-line tool, kubectl, allows you to run commands against Kubernetes clusters. You can use kubectl to deploy applications, inspect and manage cluster resources, and view logs. For more information including a complete list of kubectl operations, see the kubectl reference documentation.kubectl is installable on a variety of Linux platforms, macOS and Windows. Find your preferred operating system below.Install kubectl on LinuxInstall kubectl on macOSInstall kubectl on Windowskindkind lets you run Kubernetes on your local computer. This tool requires that you have either Docker or Podman installed.The kind Quick Start page shows you what you need to do to get up and running with kind.View kind Quick Start GuideminikubeLike kind, minikube is a tool that lets you run Kubernetes locally. minikube runs an all-in-one or a multi-node local Kubernetes cluster on your personal computer (including Windows, macOS and Linux PCs) so that you can try out Kubernetes, or for daily development work.You can follow the official Get Started! guide if your focus is on getting the tool installed.View minikube Get Started! GuideOnce you have minikube working, you can use it to run a sample application.kubeadmYou can use the kubeadm tool to create and manage Kubernetes clusters. It performs the actions necessary to get a minimum viable, secure cluster up and running in a user friendly way.Installing kubeadm shows you how to install kubeadm. Once installed, you can use it to create a cluster.View kubeadm Install Guide4.1.1 - Install and Set Up kubectl on LinuxBefore you beginYou must use a kubectl version that is within one minor version difference of your cluster. For example, a v1.33 client can communicate with v1.32, v1.33, and v1.34 control planes. Using the latest compatible version of kubectl helps avoid unforeseen issues.Install kubectl on LinuxThe following methods exist for installing kubectl on Linux:Install kubectl binary with curl on LinuxInstall using native package managementInstall using other package managementInstall kubectl binary with curl on LinuxDownload the latest release with the command:x86-64ARM64 curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl" curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/arm64/kubectl" Note:To download a specific version, replace the $(curl -L -s https://dl.k8s.io/release/stable.txt) portion of the command with the specific version.For example, to download version 1.33.0 on Linux x86-64, type:curl -LO https://dl.k8s.io/release/v1.33.0/bin/linux/amd64/kubectl And for Linux ARM64, type:curl -LO https://dl.k8s.io/release/v1.33.0/bin/linux/arm64/kubectl Validate the binary (optional)Download the kubectl checksum file:x86-64ARM64 curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl.sha256" curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/arm64/kubectl.sha256" Validate the kubectl binary against the checksum file:echo "$(cat kubectl.sha256) kubectl" | sha256sum --check If valid, the output is:kubectl: OK If the check fails, sha256 exits with nonzero status and prints output similar to:kubectl: FAILED sha256sum: WARNING: 1 computed checksum did NOT match Note:Download the same version of the binary and checksum.Install kubectlsudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl Note:If you do not have root access on the target system, you can still install kubectl to the ~/.local/bin directory:chmod +x kubectl mkdir -p ~/.local/bin mv ./kubectl ~/.local/bin/kubectl # and then append (or prepend) ~/.local/bin to $PATH Test to ensure the version you installed is up-to-date:kubectl version --client Or use this for detailed view of version:kubectl version --client --output=yaml Install using native package managementDebian-based distributionsRed Hat-based distributionsSUSE-based distributionsUpdate the apt package index and install packages needed to use the Kubernetes apt repository:sudo apt-get update # apt-transport-https may be a dummy package; if so, you can skip that package sudo apt-get install -y apt-transport-https ca-certificates curl gnupg Download the public signing key for the Kubernetes package repositories. The same signing key is used for all repositories so you can disregard the version in the URL:# If the folder `/etc/apt/keyrings` does not exist, it should be created before the curl command, read the note below. # sudo mkdir -p -m 755 /etc/apt/keyrings curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.33/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg sudo chmod 644 /etc/apt/keyrings/kubernetes-apt-keyring.gpg # allow unprivileged APT programs to read this keyring Note:In releases older than Debian 12 and Ubuntu 22.04, folder /etc/apt/keyrings does not exist by default, and it should be created before the curl command.Add the appropriate Kubernetes apt repository. If you want to use Kubernetes version different than v1.33, replace v1.33 with the desired minor version in the command below:# This overwrites any existing configuration in /etc/apt/sources.list.d/kubernetes.list echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.33/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list sudo chmod 644 /etc/apt/sources.list.d/kubernetes.list # helps tools such as command-not-found to work correctly Note:To upgrade kubectl to another minor release, you'll need to bump the version in /etc/apt/sources.list.d/kubernetes.list before running apt-get update and apt-get upgrade. This procedure is described in more detail in Changing The Kubernetes Package Repository.Update apt package index, then install kubectl:sudo apt-get update sudo apt-get install -y kubectl Add the Kubernetes yum repository. If you want to use Kubernetes version different than v1.33, replace v1.33 with the desired minor version in the command below.# This overwrites any existing configuration in /etc/yum.repos.d/kubernetes.repo cat < Key Algorithm: RSA 2048 Key Created: Thu 25 Aug 2022 01:21:11 PM -03 Key Expires: Sat 02 Nov 2024 01:21:11 PM -03 (expires in 85 days) Rpm Name: gpg-pubkey-9a296436-6307a177 Note: Signing data enables the recipient to verify that no modifications occurred after the data were signed. Accepting data with no, wrong or unknown signature can lead to a corrupted system and in extreme cases even to a system compromise. Note: A GPG pubkey is clearly identified by its fingerprint. Do not rely on the key's name. If you are not sure whether the presented key is authentic, ask the repository provider or check their web site. Many providers maintain a web page showing the fingerprints of the GPG keys they are using. Do you want to reject the key, trust temporarily, or trust always? [r/t/a/?] (r): a Install kubectl using zypper:sudo zypper install -y kubectl Install using other package managementSnapHomebrewIf you are on Ubuntu or another Linux distribution that supports the snap package manager, kubectl is available as a snap application.snap install kubectl --classic kubectl version --client If you are on Linux and using Homebrew package manager, kubectl is available for installation.brew install kubectl kubectl version --client Verify kubectl configurationIn order for kubectl to find and access a Kubernetes cluster, it needs a kubeconfig file, which is created automatically when you create a cluster using kube-up.sh or successfully deploy a Minikube cluster. By default, kubectl configuration is located at ~/.kube/config.Check that kubectl is properly configured by getting the cluster state:kubectl cluster-info If you see a URL response, kubectl is correctly configured to access your cluster.If you see a message similar to the following, kubectl is not configured correctly or is not able to connect to a Kubernetes cluster.The connection to the server  was refused - did you specify the right host or port? For example, if you are intending to run a Kubernetes cluster on your laptop (locally), you will need a tool like Minikube to be installed first and then re-run the commands stated above.If kubectl cluster-info returns the url response but you can't access your cluster, to check whether it is configured properly, use:kubectl cluster-info dump Troubleshooting the 'No Auth Provider Found' error messageIn Kubernetes 1.26, kubectl removed the built-in authentication for the following cloud providers' managed Kubernetes offerings. These providers have released kubectl plugins to provide the cloud-specific authentication. For instructions, refer to the following provider documentation:Azure AKS: kubelogin pluginGoogle Kubernetes Engine: gke-gcloud-auth-plugin(There could also be other reasons to see the same error message, unrelated to that change.)Optional kubectl configurations and pluginsEnable shell autocompletionkubectl provides autocompletion support for Bash, Zsh, Fish, and PowerShell, which can save you a lot of typing.Below are the procedures to set up autocompletion for Bash, Fish, and Zsh.BashFishZshIntroductionThe kubectl completion script for Bash can be generated with the command kubectl completion bash. Sourcing the completion script in your shell enables kubectl autocompletion.However, the completion script depends on bash-completion, which means that you have to install this software first (you can test if you have bash-completion already installed by running type _init_completion).Install bash-completionbash-completion is provided by many package managers (see here). You can install it with apt-get install bash-completion or yum install bash-completion, etc.The above commands create /usr/share/bash-completion/bash_completion, which is the main script of bash-completion. Depending on your package manager, you have to manually source this file in your ~/.bashrc file.To find out, reload your shell and run type _init_completion. If the command succeeds, you're already set, otherwise add the following to your ~/.bashrc file:source /usr/share/bash-completion/bash_completion Reload your shell and verify that bash-completion is correctly installed by typing type _init_completion.Enable kubectl autocompletionBashYou now need to ensure that the kubectl completion script gets sourced in all your shell sessions. There are two ways in which you can do this:UserSystem echo 'source <(kubectl completion bash)' >>~/.bashrc kubectl completion bash | sudo tee /etc/bash_completion.d/kubectl > /dev/null sudo chmod a+r /etc/bash_completion.d/kubectl If you have an alias for kubectl, you can extend shell completion to work with that alias:echo 'alias k=kubectl' >>~/.bashrc echo 'complete -o default -F __start_kubectl k' >>~/.bashrc Note:bash-completion sources all completion scripts in /etc/bash_completion.d.Both approaches are equivalent. After reloading your shell, kubectl autocompletion should be working. To enable bash autocompletion in current session of shell, source the ~/.bashrc file:source ~/.bashrc Note:Autocomplete for Fish requires kubectl 1.23 or later.The kubectl completion script for Fish can be generated with the command kubectl completion fish. Sourcing the completion script in your shell enables kubectl autocompletion.To do so in all your shell sessions, add the following line to your ~/.config/fish/config.fish file:kubectl completion fish | source After reloading your shell, kubectl autocompletion should be working.The kubectl completion script for Zsh can be generated with the command kubectl completion zsh. Sourcing the completion script in your shell enables kubectl autocompletion.To do so in all your shell sessions, add the following to your ~/.zshrc file:source <(kubectl completion zsh) If you have an alias for kubectl, kubectl autocompletion will automatically work with it.After reloading your shell, kubectl autocompletion should be working.If you get an error like 2: command not found: compdef, then add the following to the beginning of your ~/.zshrc file:autoload -Uz compinit compinit Install kubectl convert pluginA plugin for Kubernetes command-line tool kubectl, which allows you to convert manifests between different API versions. This can be particularly helpful to migrate manifests to a non-deprecated api version with newer Kubernetes release. For more info, visit migrate to non deprecated apisDownload the latest release with the command:x86-64ARM64 curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl-convert" curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/arm64/kubectl-convert" Validate the binary (optional)Download the kubectl-convert checksum file:x86-64ARM64 curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl-convert.sha256" curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/arm64/kubectl-convert.sha256" Validate the kubectl-convert binary against the checksum file:echo "$(cat kubectl-convert.sha256) kubectl-convert" | sha256sum --check If valid, the output is:kubectl-convert: OK If the check fails, sha256 exits with nonzero status and prints output similar to:kubectl-convert: FAILED sha256sum: WARNING: 1 computed checksum did NOT match Note:Download the same version of the binary and checksum.Install kubectl-convertsudo install -o root -g root -m 0755 kubectl-convert /usr/local/bin/kubectl-convert Verify plugin is successfully installedkubectl convert --help If you do not see an error, it means the plugin is successfully installed.After installing the plugin, clean up the installation files:rm kubectl-convert kubectl-convert.sha256 What's nextInstall MinikubeSee the getting started guides for more about creating clusters.Learn how to launch and expose your application.If you need access to a cluster you didn't create, see the Sharing Cluster Access document.Read the kubectl reference docs4.1.2 - Install and Set Up kubectl on macOSBefore you beginYou must use a kubectl version that is within one minor version difference of your cluster. For example, a v1.33 client can communicate with v1.32, v1.33, and v1.34 control planes. Using the latest compatible version of kubectl helps avoid unforeseen issues.Install kubectl on macOSThe following methods exist for installing kubectl on macOS:Install kubectl on macOSInstall kubectl binary with curl on macOSInstall with Homebrew on macOSInstall with Macports on macOSVerify kubectl configurationOptional kubectl configurations and pluginsEnable shell autocompletionInstall kubectl convert pluginInstall kubectl binary with curl on macOSDownload the latest release:IntelApple Silicon curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/darwin/amd64/kubectl" curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/darwin/arm64/kubectl" Note:To download a specific version, replace the $(curl -L -s https://dl.k8s.io/release/stable.txt) portion of the command with the specific version.For example, to download version 1.33.0 on Intel macOS, type:curl -LO "https://dl.k8s.io/release/v1.33.0/bin/darwin/amd64/kubectl" And for macOS on Apple Silicon, type:curl -LO "https://dl.k8s.io/release/v1.33.0/bin/darwin/arm64/kubectl" Validate the binary (optional)Download the kubectl checksum file:IntelApple Silicon curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/darwin/amd64/kubectl.sha256" curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/darwin/arm64/kubectl.sha256" Validate the kubectl binary against the checksum file:echo "$(cat kubectl.sha256) kubectl" | shasum -a 256 --check If valid, the output is:kubectl: OK If the check fails, shasum exits with nonzero status and prints output similar to:kubectl: FAILED shasum: WARNING: 1 computed checksum did NOT match Note:Download the same version of the binary and checksum.Make the kubectl binary executable.chmod +x ./kubectl Move the kubectl binary to a file location on your system PATH.sudo mv ./kubectl /usr/local/bin/kubectl sudo chown root: /usr/local/bin/kubectl Note:Make sure /usr/local/bin is in your PATH environment variable.Test to ensure the version you installed is up-to-date:kubectl version --client Or use this for detailed view of version:kubectl version --client --output=yaml After installing and validating kubectl, delete the checksum file:rm kubectl.sha256 Install with Homebrew on macOSIf you are on macOS and using Homebrew package manager, you can install kubectl with Homebrew.Run the installation command:brew install kubectl orbrew install kubernetes-cli Test to ensure the version you installed is up-to-date:kubectl version --client Install with Macports on macOSIf you are on macOS and using Macports package manager, you can install kubectl with Macports.Run the installation command:sudo port selfupdate sudo port install kubectl Test to ensure the version you installed is up-to-date:kubectl version --client Verify kubectl configurationIn order for kubectl to find and access a Kubernetes cluster, it needs a kubeconfig file, which is created automatically when you create a cluster using kube-up.sh or successfully deploy a Minikube cluster. By default, kubectl configuration is located at ~/.kube/config.Check that kubectl is properly configured by getting the cluster state:kubectl cluster-info If you see a URL response, kubectl is correctly configured to access your cluster.If you see a message similar to the following, kubectl is not configured correctly or is not able to connect to a Kubernetes cluster.The connection to the server  was refused - did you specify the right host or port? For example, if you are intending to run a Kubernetes cluster on your laptop (locally), you will need a tool like Minikube to be installed first and then re-run the commands stated above.If kubectl cluster-info returns the url response but you can't access your cluster, to check whether it is configured properly, use:kubectl cluster-info dump Troubleshooting the 'No Auth Provider Found' error messageIn Kubernetes 1.26, kubectl removed the built-in authentication for the following cloud providers' managed Kubernetes offerings. These providers have released kubectl plugins to provide the cloud-specific authentication. For instructions, refer to the following provider documentation:Azure AKS: kubelogin pluginGoogle Kubernetes Engine: gke-gcloud-auth-plugin(There could also be other reasons to see the same error message, unrelated to that change.)Optional kubectl configurations and pluginsEnable shell autocompletionkubectl provides autocompletion support for Bash, Zsh, Fish, and PowerShell which can save you a lot of typing.Below are the procedures to set up autocompletion for Bash, Fish, and Zsh.BashFishZshIntroductionThe kubectl completion script for Bash can be generated with kubectl completion bash. Sourcing this script in your shell enables kubectl completion.However, the kubectl completion script depends on bash-completion which you thus have to previously install.Warning:There are two versions of bash-completion, v1 and v2. V1 is for Bash 3.2 (which is the default on macOS), and v2 is for Bash 4.1+. The kubectl completion script doesn't work correctly with bash-completion v1 and Bash 3.2. It requires bash-completion v2 and Bash 4.1+. Thus, to be able to correctly use kubectl completion on macOS, you have to install and use Bash 4.1+ (instructions). The following instructions assume that you use Bash 4.1+ (that is, any Bash version of 4.1 or newer).Upgrade BashThe instructions here assume you use Bash 4.1+. You can check your Bash's version by running:echo $BASH_VERSION If it is too old, you can install/upgrade it using Homebrew:brew install bash Reload your shell and verify that the desired version is being used:echo $BASH_VERSION $SHELL Homebrew usually installs it at /usr/local/bin/bash.Install bash-completionNote:As mentioned, these instructions assume you use Bash 4.1+, which means you will install bash-completion v2 (in contrast to Bash 3.2 and bash-completion v1, in which case kubectl completion won't work).You can test if you have bash-completion v2 already installed with type _init_completion. If not, you can install it with Homebrew:brew install bash-completion@2 As stated in the output of this command, add the following to your ~/.bash_profile file:brew_etc="$(brew --prefix)/etc" && [[ -r "${brew_etc}/profile.d/bash_completion.sh" ]] && . "${brew_etc}/profile.d/bash_completion.sh" Reload your shell and verify that bash-completion v2 is correctly installed with type _init_completion.Enable kubectl autocompletionYou now have to ensure that the kubectl completion script gets sourced in all your shell sessions. There are multiple ways to achieve this:Source the completion script in your ~/.bash_profile file:echo 'source <(kubectl completion bash)' >>~/.bash_profile Add the completion script to the /usr/local/etc/bash_completion.d directory:kubectl completion bash >/usr/local/etc/bash_completion.d/kubectl If you have an alias for kubectl, you can extend shell completion to work with that alias:echo 'alias k=kubectl' >>~/.bash_profile echo 'complete -o default -F __start_kubectl k' >>~/.bash_profile If you installed kubectl with Homebrew (as explained here), then the kubectl completion script should already be in /usr/local/etc/bash_completion.d/kubectl. In that case, you don't need to do anything.Note:The Homebrew installation of bash-completion v2 sources all the files in the BASH_COMPLETION_COMPAT_DIR directory, that's why the latter two methods work.In any case, after reloading your shell, kubectl completion should be working.Note:Autocomplete for Fish requires kubectl 1.23 or later.The kubectl completion script for Fish can be generated with the command kubectl completion fish. Sourcing the completion script in your shell enables kubectl autocompletion.To do so in all your shell sessions, add the following line to your ~/.config/fish/config.fish file:kubectl completion fish | source After reloading your shell, kubectl autocompletion should be working.The kubectl completion script for Zsh can be generated with the command kubectl completion zsh. Sourcing the completion script in your shell enables kubectl autocompletion.To do so in all your shell sessions, add the following to your ~/.zshrc file:source <(kubectl completion zsh) If you have an alias for kubectl, kubectl autocompletion will automatically work with it.After reloading your shell, kubectl autocompletion should be working.If you get an error like 2: command not found: compdef, then add the following to the beginning of your ~/.zshrc file:autoload -Uz compinit compinit Install kubectl convert pluginA plugin for Kubernetes command-line tool kubectl, which allows you to convert manifests between different API versions. This can be particularly helpful to migrate manifests to a non-deprecated api version with newer Kubernetes release. For more info, visit migrate to non deprecated apisDownload the latest release with the command:IntelApple Silicon curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/darwin/amd64/kubectl-convert" curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/darwin/arm64/kubectl-convert" Validate the binary (optional)Download the kubectl-convert checksum file:IntelApple Silicon curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/darwin/amd64/kubectl-convert.sha256" curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/darwin/arm64/kubectl-convert.sha256" Validate the kubectl-convert binary against the checksum file:echo "$(cat kubectl-convert.sha256) kubectl-convert" | shasum -a 256 --check If valid, the output is:kubectl-convert: OK If the check fails, shasum exits with nonzero status and prints output similar to:kubectl-convert: FAILED shasum: WARNING: 1 computed checksum did NOT match Note:Download the same version of the binary and checksum.Make kubectl-convert binary executablechmod +x ./kubectl-convert Move the kubectl-convert binary to a file location on your system PATH.sudo mv ./kubectl-convert /usr/local/bin/kubectl-convert sudo chown root: /usr/local/bin/kubectl-convert Note:Make sure /usr/local/bin is in your PATH environment variable.Verify plugin is successfully installedkubectl convert --help If you do not see an error, it means the plugin is successfully installed.After installing the plugin, clean up the installation files:rm kubectl-convert kubectl-convert.sha256 Uninstall kubectl on macOSDepending on how you installed kubectl, use one of the following methods.Uninstall kubectl using the command-lineLocate the kubectl binary on your system:which kubectl Remove the kubectl binary:sudo rm  Replace  with the path to the kubectl binary from the previous step. For example, sudo rm /usr/local/bin/kubectl.Uninstall kubectl using homebrewIf you installed kubectl using Homebrew, run the following command:brew remove kubectl What's nextInstall MinikubeSee the getting started guides for more about creating clusters.Learn how to launch and expose your application.If you need access to a cluster you didn't create, see the Sharing Cluster Access document.Read the kubectl reference docs4.1.3 - Install and Set Up kubectl on WindowsBefore you beginYou must use a kubectl version that is within one minor version difference of your cluster. For example, a v1.33 client can communicate with v1.32, v1.33, and v1.34 control planes. Using the latest compatible version of kubectl helps avoid unforeseen issues.Install kubectl on WindowsThe following methods exist for installing kubectl on Windows:Install kubectl binary on Windows (via direct download or curl)Install on Windows using Chocolatey, Scoop, or wingetInstall kubectl binary on Windows (via direct download or curl)You have two options for installing kubectl on your Windows deviceDirect download:Download the latest 1.33 patch release binary directly for your specific architecture by visiting the Kubernetes release page. Be sure to select the correct binary for your architecture (e.g., amd64, arm64, etc.).Using curl:If you have curl installed, use this command:curl.exe -LO "https://dl.k8s.io/release/v1.33.0/bin/windows/amd64/kubectl.exe" Note:To find out the latest stable version (for example, for scripting), take a look at https://dl.k8s.io/release/stable.txt.Validate the binary (optional)Download the kubectl checksum file:curl.exe -LO "https://dl.k8s.io/v1.33.0/bin/windows/amd64/kubectl.exe.sha256" Validate the kubectl binary against the checksum file:Using Command Prompt to manually compare CertUtil's output to the checksum file downloaded:CertUtil -hashfile kubectl.exe SHA256 type kubectl.exe.sha256 Using PowerShell to automate the verification using the -eq operator to get a True or False result: $(Get-FileHash -Algorithm SHA256 .\kubectl.exe).Hash -eq $(Get-Content .\kubectl.exe.sha256) Append or prepend the kubectl binary folder to your PATH environment variable.Test to ensure the version of kubectl is the same as downloaded:kubectl version --client Or use this for detailed view of version:kubectl version --client --output=yaml Note:Docker Desktop for Windows adds its own version of kubectl to PATH. If you have installed Docker Desktop before, you may need to place your PATH entry before the one added by the Docker Desktop installer or remove the Docker Desktop's kubectl.Install on Windows using Chocolatey, Scoop, or wingetTo install kubectl on Windows you can use either Chocolatey package manager, Scoop command-line installer, or winget package manager.chocoscoopwingetchoco install kubernetes-cli scoop install kubectl winget install -e --id Kubernetes.kubectl Test to ensure the version you installed is up-to-date:kubectl version --client Navigate to your home directory:# If you're using cmd.exe, run: cd %USERPROFILE% cd ~ Create the .kube directory:mkdir .kube Change to the .kube directory you just created:cd .kube Configure kubectl to use a remote Kubernetes cluster:New-Item config -type file Note:Edit the config file with a text editor of your choice, such as Notepad.Verify kubectl configurationIn order for kubectl to find and access a Kubernetes cluster, it needs a kubeconfig file, which is created automatically when you create a cluster using kube-up.sh or successfully deploy a Minikube cluster. By default, kubectl configuration is located at ~/.kube/config.Check that kubectl is properly configured by getting the cluster state:kubectl cluster-info If you see a URL response, kubectl is correctly configured to access your cluster.If you see a message similar to the following, kubectl is not configured correctly or is not able to connect to a Kubernetes cluster.The connection to the server  was refused - did you specify the right host or port? For example, if you are intending to run a Kubernetes cluster on your laptop (locally), you will need a tool like Minikube to be installed first and then re-run the commands stated above.If kubectl cluster-info returns the url response but you can't access your cluster, to check whether it is configured properly, use:kubectl cluster-info dump Troubleshooting the 'No Auth Provider Found' error messageIn Kubernetes 1.26, kubectl removed the built-in authentication for the following cloud providers' managed Kubernetes offerings. These providers have released kubectl plugins to provide the cloud-specific authentication. For instructions, refer to the following provider documentation:Azure AKS: kubelogin pluginGoogle Kubernetes Engine: gke-gcloud-auth-plugin(There could also be other reasons to see the same error message, unrelated to that change.)Optional kubectl configurations and pluginsEnable shell autocompletionkubectl provides autocompletion support for Bash, Zsh, Fish, and PowerShell, which can save you a lot of typing.Below are the procedures to set up autocompletion for PowerShell.The kubectl completion script for PowerShell can be generated with the command kubectl completion powershell.To do so in all your shell sessions, add the following line to your $PROFILE file:kubectl completion powershell | Out-String | Invoke-Expression This command will regenerate the auto-completion script on every PowerShell start up. You can also add the generated script directly to your $PROFILE file.To add the generated script to your $PROFILE file, run the following line in your powershell prompt:kubectl completion powershell >> $PROFILE After reloading your shell, kubectl autocompletion should be working.Install kubectl convert pluginA plugin for Kubernetes command-line tool kubectl, which allows you to convert manifests between different API versions. This can be particularly helpful to migrate manifests to a non-deprecated api version with newer Kubernetes release. For more info, visit migrate to non deprecated apisDownload the latest release with the command:curl.exe -LO "https://dl.k8s.io/release/v1.33.0/bin/windows/amd64/kubectl-convert.exe" Validate the binary (optional).Download the kubectl-convert checksum file:curl.exe -LO "https://dl.k8s.io/v1.33.0/bin/windows/amd64/kubectl-convert.exe.sha256" Validate the kubectl-convert binary against the checksum file:Using Command Prompt to manually compare CertUtil's output to the checksum file downloaded:CertUtil -hashfile kubectl-convert.exe SHA256 type kubectl-convert.exe.sha256 Using PowerShell to automate the verification using the -eq operator to get a True or False result:$($(CertUtil -hashfile .\kubectl-convert.exe SHA256)[1] -replace " ", "") -eq $(type .\kubectl-convert.exe.sha256) Append or prepend the kubectl-convert binary folder to your PATH environment variable.Verify the plugin is successfully installed.kubectl convert --help If you do not see an error, it means the plugin is successfully installed.After installing the plugin, clean up the installation files:del kubectl-convert.exe del kubectl-convert.exe.sha256 What's nextInstall MinikubeSee the getting started guides for more about creating clusters.Learn how to launch and expose your application.If you need access to a cluster you didn't create, see the Sharing Cluster Access document.Read the kubectl reference docs4.2 - Administer a ClusterLearn common tasks for administering a cluster.4.2.1 - Administration with kubeadmIf you don't yet have a cluster, visit bootstrapping clusters with kubeadm.The tasks in this section are aimed at people administering an existing cluster:4.2.1.1 - Adding Linux worker nodesThis page explains how to add Linux worker nodes to a kubeadm cluster.Before you beginEach joining worker node has installed the required components from Installing kubeadm, such as, kubeadm, the kubelet and a container runtime.A running kubeadm cluster created by kubeadm init and following the steps in the document Creating a cluster with kubeadm.You need superuser access to the node.Adding Linux worker nodesTo add new Linux worker nodes to your cluster do the following for each machine:Connect to the machine by using SSH or another method.Run the command that was output by kubeadm init. For example:sudo kubeadm join --token  : \--discovery-token-ca-cert-hash sha256: Additional information for kubeadm joinNote:To specify an IPv6 tuple for :, IPv6 address must be enclosed in square brackets, for example: [2001:db8::101]:2073.If you do not have the token, you can get it by running the following command on the control plane node:# Run this on a control plane node sudo kubeadm token list The output is similar to this:TOKEN TTL EXPIRES USAGES DESCRIPTION EXTRA GROUPS 8ewj1p.9r9hcjoqgajrj4gi 23h 2018-06-12T02:51:28Z authentication, The default bootstrap system: signing token generated by bootstrappers: 'kubeadm init'. kubeadm: default-node-token By default, node join tokens expire after 24 hours. If you are joining a node to the cluster after the current token has expired, you can create a new token by running the following command on the control plane node:# Run this on a control plane node sudo kubeadm token create The output is similar to this:5didvk.d09sbcov8ph2amjw If you don't have the value of --discovery-token-ca-cert-hash, you can get it by running the following commands on the control plane node:# Run this on a control plane node sudo cat /etc/kubernetes/pki/ca.crt | openssl x509 -pubkey | openssl rsa -pubin -outform der 2>/dev/null | \ openssl dgst -sha256 -hex | sed 's/^.* //' The output is similar to:8cb2de97839780a412b93877f8507ad6c94f73add17d5d7058e91741c9d5ec78 The output of the kubeadm join command should look something like:[preflight] Running pre-flight checks ... (log output of join workflow) ... Node join complete: * Certificate signing request sent to control-plane and response received. * Kubelet informed of new secure connection details. Run 'kubectl get nodes' on control-plane to see this machine join. A few seconds later, you should notice this node in the output from kubectl get nodes. (for example, run kubectl on a control plane node).Note:As the cluster nodes are usually initialized sequentially, the CoreDNS Pods are likely to all run on the first control plane node. To provide higher availability, please rebalance the CoreDNS Pods with kubectl -n kube-system rollout restart deployment coredns after at least one new node is joined.What's nextSee how to add Windows worker nodes.4.2.1.2 - Adding Windows worker nodesFEATURE STATE: Kubernetes v1.18 [beta]This page explains how to add Windows worker nodes to a kubeadm cluster.Before you beginA running Windows Server 2022 (or higher) instance with administrative access.A running kubeadm cluster created by kubeadm init and following the steps in the document Creating a cluster with kubeadm.Adding Windows worker nodesNote:To facilitate the addition of Windows worker nodes to a cluster, PowerShell scripts from the repository https://sigs.k8s.io/sig-windows-tools are used.Do the following for each machine:Open a PowerShell session on the machine.Make sure you are Administrator or a privileged user.Then proceed with the steps outlined below.Install containerdNote: This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the content guide before submitting a change. More information.To install containerd, first run the following command:curl.exe -LO https://raw.githubusercontent.com/kubernetes-sigs/sig-windows-tools/master/hostprocess/Install-Containerd.ps1 Then run the following command, but first replace CONTAINERD_VERSION with a recent release from the containerd repository. The version must not have a v prefix. For example, use 1.7.22 instead of v1.7.22:.\Install-Containerd.ps1 -ContainerDVersion CONTAINERD_VERSION Adjust any other parameters for Install-Containerd.ps1 such as netAdapterName as you need them.Set skipHypervisorSupportCheck if your machine does not support Hyper-V and cannot host Hyper-V isolated containers.If you change the Install-Containerd.ps1 optional parameters CNIBinPath and/or CNIConfigPath you will need to configure the installed Windows CNI plugin with matching values.Install kubeadm and kubeletRun the following commands to install kubeadm and the kubelet:curl.exe -LO https://raw.githubusercontent.com/kubernetes-sigs/sig-windows-tools/master/hostprocess/PrepareNode.ps1 .\PrepareNode.ps1 -KubernetesVersion v1.33 Adjust the parameter KubernetesVersion of PrepareNode.ps1 if needed.Run kubeadm joinRun the command that was output by kubeadm init. For example:kubeadm join --token  : \--discovery-token-ca-cert-hash sha256: Additional information about kubeadm joinNote:To specify an IPv6 tuple for :, IPv6 address must be enclosed in square brackets, for example: [2001:db8::101]:2073.If you do not have the token, you can get it by running the following command on the control plane node:# Run this on a control plane node sudo kubeadm token list The output is similar to this:TOKEN TTL EXPIRES USAGES DESCRIPTION EXTRA GROUPS 8ewj1p.9r9hcjoqgajrj4gi 23h 2018-06-12T02:51:28Z authentication, The default bootstrap system: signing token generated by bootstrappers: 'kubeadm init'. kubeadm: default-node-token By default, node join tokens expire after 24 hours. If you are joining a node to the cluster after the current token has expired, you can create a new token by running the following command on the control plane node:# Run this on a control plane node sudo kubeadm token create The output is similar to this:5didvk.d09sbcov8ph2amjw If you don't have the value of --discovery-token-ca-cert-hash, you can get it by running the following commands on the control plane node:sudo cat /etc/kubernetes/pki/ca.crt | openssl x509 -pubkey | openssl rsa -pubin -outform der 2>/dev/null | \ openssl dgst -sha256 -hex | sed 's/^.* //' The output is similar to:8cb2de97839780a412b93877f8507ad6c94f73add17d5d7058e91741c9d5ec78 The output of the kubeadm join command should look something like:[preflight] Running pre-flight checks ... (log output of join workflow) ... Node join complete: * Certificate signing request sent to control-plane and response received. * Kubelet informed of new secure connection details. Run 'kubectl get nodes' on control-plane to see this machine join. A few seconds later, you should notice this node in the output from kubectl get nodes. (for example, run kubectl on a control plane node).Network configurationCNI setup on clusters mixed with Linux and Windows nodes requires more steps than just running kubectl apply on a manifest file. Additionally, the CNI plugin running on control plane nodes must be prepared to support the CNI plugin running on Windows worker nodes.Note: This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the content guide before submitting a change. More information.Only a few CNI plugins currently support Windows. Below you can find individual setup instructions for them:FlannelCalicoInstall kubectl for Windows (optional)See Install and Set Up kubectl on Windows.What's nextSee how to add Linux worker nodes.4.2.1.3 - Upgrading kubeadm clustersThis page explains how to upgrade a Kubernetes cluster created with kubeadm from version 1.32.x to version 1.33.x, and from version 1.33.x to 1.33.y (where y > x). Skipping MINOR versions when upgrading is unsupported. For more details, please visit Version Skew Policy.To see information about upgrading clusters created using older versions of kubeadm, please refer to following pages instead:Upgrading a kubeadm cluster from 1.31 to 1.32Upgrading a kubeadm cluster from 1.30 to 1.31Upgrading a kubeadm cluster from 1.29 to 1.30Upgrading a kubeadm cluster from 1.28 to 1.29The Kubernetes project recommends upgrading to the latest patch releases promptly, and to ensure that you are running a supported minor release of Kubernetes. Following this recommendation helps you to to stay secure.The upgrade workflow at high level is the following:Upgrade a primary control plane node.Upgrade additional control plane nodes.Upgrade worker nodes.Before you beginMake sure you read the release notes carefully.The cluster should use a static control plane and etcd pods or external etcd.Make sure to back up any important components, such as app-level state stored in a database. kubeadm upgrade does not touch your workloads, only components internal to Kubernetes, but backups are always a best practice.Swap must be disabled.Additional informationThe instructions below outline when to drain each node during the upgrade process. If you are performing a minor version upgrade for any kubelet, you must first drain the node (or nodes) that you are upgrading. In the case of control plane nodes, they could be running CoreDNS Pods or other critical workloads. For more information see Draining nodes.The Kubernetes project recommends that you match your kubelet and kubeadm versions. You can instead use a version of kubelet that is older than kubeadm, provided it is within the range of supported versions. For more details, please visit kubeadm's skew against the kubelet.All containers are restarted after upgrade, because the container spec hash value is changed.To verify that the kubelet service has successfully restarted after the kubelet has been upgraded, you can execute systemctl status kubelet or view the service logs with journalctl -xeu kubelet.kubeadm upgrade supports --config with a UpgradeConfiguration API type which can be used to configure the upgrade process.kubeadm upgrade does not support reconfiguration of an existing cluster. Follow the steps in Reconfiguring a kubeadm cluster instead.Considerations when upgrading etcdBecause the kube-apiserver static pod is running at all times (even if you have drained the node), when you perform a kubeadm upgrade which includes an etcd upgrade, in-flight requests to the server will stall while the new etcd static pod is restarting. As a workaround, it is possible to actively stop the kube-apiserver process a few seconds before starting the kubeadm upgrade apply command. This permits to complete in-flight requests and close existing connections, and minimizes the consequence of the etcd downtime. This can be done as follows on control plane nodes:killall -s SIGTERM kube-apiserver # trigger a graceful kube-apiserver shutdown sleep 20 # wait a little bit to permit completing in-flight requests kubeadm upgrade ... # execute a kubeadm upgrade command Changing the package repositoryIf you're using the community-owned package repositories (pkgs.k8s.io), you need to enable the package repository for the desired Kubernetes minor release. This is explained in Changing the Kubernetes package repository document.Note: The legacy package repositories (apt.kubernetes.io and yum.kubernetes.io) have been deprecated and frozen starting from September 13, 2023. Using the new package repositories hosted at pkgs.k8s.io is strongly recommended and required in order to install Kubernetes versions released after September 13, 2023. The deprecated legacy repositories, and their contents, might be removed at any time in the future and without a further notice period. The new package repositories provide downloads for Kubernetes versions starting with v1.24.0.Determine which version to upgrade toFind the latest patch release for Kubernetes 1.33 using the OS package manager:Ubuntu, Debian or HypriotOSCentOS, RHEL or Fedora# Find the latest 1.33 version in the list. # It should look like 1.33.x-*, where x is the latest patch. sudo apt update sudo apt-cache madison kubeadm # Find the latest 1.33 version in the list. # It should look like 1.33.x-*, where x is the latest patch. sudo yum list --showduplicates kubeadm --disableexcludes=kubernetes If you don't see the version you expect to upgrade to, verify if the Kubernetes package repositories are used.Upgrading control plane nodesThe upgrade procedure on control plane nodes should be executed one node at a time. Pick a control plane node that you wish to upgrade first. It must have the /etc/kubernetes/admin.conf file.Call "kubeadm upgrade"For the first control plane nodeUpgrade kubeadm:Ubuntu, Debian or HypriotOSCentOS, RHEL or Fedora# replace x in 1.33.x-* with the latest patch version sudo apt-mark unhold kubeadm && \ sudo apt-get update && sudo apt-get install -y kubeadm='1.33.x-*' && \ sudo apt-mark hold kubeadm # replace x in 1.33.x-* with the latest patch version sudo yum install -y kubeadm-'1.33.x-*' --disableexcludes=kubernetes Verify that the download works and has the expected version:kubeadm version Verify the upgrade plan:sudo kubeadm upgrade plan This command checks that your cluster can be upgraded, and fetches the versions you can upgrade to. It also shows a table with the component config version states.Note:kubeadm upgrade also automatically renews the certificates that it manages on this node. To opt-out of certificate renewal the flag --certificate-renewal=false can be used. For more information see the certificate management guide.Choose a version to upgrade to, and run the appropriate command. For example:# replace x with the patch version you picked for this upgrade sudo kubeadm upgrade apply v1.33.x Once the command finishes you should see:[upgrade/successful] SUCCESS! Your cluster was upgraded to "v1.33.x". Enjoy! [upgrade/kubelet] Now that your control plane is upgraded, please proceed with upgrading your kubelets if you haven't already done so. Note:For versions earlier than v1.28, kubeadm defaulted to a mode that upgrades the addons (including CoreDNS and kube-proxy) immediately during kubeadm upgrade apply, regardless of whether there are other control plane instances that have not been upgraded. This may cause compatibility problems. Since v1.28, kubeadm defaults to a mode that checks whether all the control plane instances have been upgraded before starting to upgrade the addons. You must perform control plane instances upgrade sequentially or at least ensure that the last control plane instance upgrade is not started until all the other control plane instances have been upgraded completely, and the addons upgrade will be performed after the last control plane instance is upgraded.Manually upgrade your CNI provider plugin.Your Container Network Interface (CNI) provider may have its own upgrade instructions to follow. Check the addons page to find your CNI provider and see whether additional upgrade steps are required.This step is not required on additional control plane nodes if the CNI provider runs as a DaemonSet.For the other control plane nodesSame as the first control plane node but use:sudo kubeadm upgrade node instead of:sudo kubeadm upgrade apply Also calling kubeadm upgrade plan and upgrading the CNI provider plugin is no longer needed.Drain the nodePrepare the node for maintenance by marking it unschedulable and evicting the workloads:# replace  with the name of your node you are draining kubectl drain  \--ignore-daemonsets Upgrade kubelet and kubectlUpgrade the kubelet and kubectl:Ubuntu, Debian or HypriotOSCentOS, RHEL or Fedora# replace x in 1.33.x-* with the latest patch version sudo apt-mark unhold kubelet kubectl && \ sudo apt-get update && sudo apt-get install -y kubelet='1.33.x-*' kubectl='1.33.x-*' && \ sudo apt-mark hold kubelet kubectl # replace x in 1.33.x-* with the latest patch version sudo yum install -y kubelet-'1.33.x-*' kubectl-'1.33.x-*' --disableexcludes=kubernetes Restart the kubelet:sudo systemctl daemon-reload sudo systemctl restart kubelet Uncordon the nodeBring the node back online by marking it schedulable:# replace  with the name of your node kubectl uncordon  Upgrade worker nodesThe upgrade procedure on worker nodes should be executed one node at a time or few nodes at a time, without compromising the minimum required capacity for running your workloads.The following pages show how to upgrade Linux and Windows worker nodes:Upgrade Linux nodesUpgrade Windows nodesVerify the status of the clusterAfter the kubelet is upgraded on all nodes verify that all nodes are available again by running the following command from anywhere kubectl can access the cluster:kubectl get nodes The STATUS column should show Ready for all your nodes, and the version number should be updated.Recovering from a failure stateIf kubeadm upgrade fails and does not roll back, for example because of an unexpected shutdown during execution, you can run kubeadm upgrade again. This command is idempotent and eventually makes sure that the actual state is the desired state you declare.To recover from a bad state, you can also run sudo kubeadm upgrade apply --force without changing the version that your cluster is running.During upgrade kubeadm writes the following backup folders under /etc/kubernetes/tmp:kubeadm-backup-etcd--kubeadm-backup-manifests--kubeadm-backup-etcd contains a backup of the local etcd member data for this control plane Node. In case of an etcd upgrade failure and if the automatic rollback does not work, the contents of this folder can be manually restored in /var/lib/etcd. In case external etcd is used this backup folder will be empty.kubeadm-backup-manifests contains a backup of the static Pod manifest files for this control plane Node. In case of a upgrade failure and if the automatic rollback does not work, the contents of this folder can be manually restored in /etc/kubernetes/manifests. If for some reason there is no difference between a pre-upgrade and post-upgrade manifest file for a certain component, a backup file for it will not be written.Note:After the cluster upgrade using kubeadm, the backup directory /etc/kubernetes/tmp will remain and these backup files will need to be cleared manually.How it workskubeadm upgrade apply does the following:Checks that your cluster is in an upgradeable state:The API server is reachableAll nodes are in the Ready stateThe control plane is healthyEnforces the version skew policies.Makes sure the control plane images are available or available to pull to the machine.Generates replacements and/or uses user supplied overwrites if component configs require version upgrades.Upgrades the control plane components or rollbacks if any of them fails to come up.Applies the new CoreDNS and kube-proxy manifests and makes sure that all necessary RBAC rules are created.Creates new certificate and key files of the API server and backs up old files if they're about to expire in 180 days.kubeadm upgrade node does the following on additional control plane nodes:Fetches the kubeadm ClusterConfiguration from the cluster.Optionally backups the kube-apiserver certificate.Upgrades the static Pod manifests for the control plane components.Upgrades the kubelet configuration for this node.kubeadm upgrade node does the following on worker nodes:Fetches the kubeadm ClusterConfiguration from the cluster.Upgrades the kubelet configuration for this node.4.2.1.4 - Upgrading Linux nodesThis page explains how to upgrade a Linux Worker Nodes created with kubeadm.Before you beginYou need to have shell access to all the nodes, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts.To check the version, enter kubectl version.Familiarize yourself with the process for upgrading the rest of your kubeadm cluster. You will want to upgrade the control plane nodes before upgrading your Linux Worker nodes.Changing the package repositoryIf you're using the community-owned package repositories (pkgs.k8s.io), you need to enable the package repository for the desired Kubernetes minor release. This is explained in Changing the Kubernetes package repository document.Note: The legacy package repositories (apt.kubernetes.io and yum.kubernetes.io) have been deprecated and frozen starting from September 13, 2023. Using the new package repositories hosted at pkgs.k8s.io is strongly recommended and required in order to install Kubernetes versions released after September 13, 2023. The deprecated legacy repositories, and their contents, might be removed at any time in the future and without a further notice period. The new package repositories provide downloads for Kubernetes versions starting with v1.24.0.Upgrading worker nodesUpgrade kubeadmUpgrade kubeadm:Ubuntu, Debian or HypriotOSCentOS, RHEL or Fedora# replace x in 1.33.x-* with the latest patch version sudo apt-mark unhold kubeadm && \ sudo apt-get update && sudo apt-get install -y kubeadm='1.33.x-*' && \ sudo apt-mark hold kubeadm # replace x in 1.33.x-* with the latest patch version sudo yum install -y kubeadm-'1.33.x-*' --disableexcludes=kubernetes Call "kubeadm upgrade"For worker nodes this upgrades the local kubelet configuration:sudo kubeadm upgrade node Drain the nodePrepare the node for maintenance by marking it unschedulable and evicting the workloads:# execute this command on a control plane node # replace  with the name of your node you are draining kubectl drain  \--ignore-daemonsets Upgrade kubelet and kubectlUpgrade the kubelet and kubectl:Ubuntu, Debian or HypriotOSCentOS, RHEL or Fedora# replace x in 1.33.x-* with the latest patch version sudo apt-mark unhold kubelet kubectl && \ sudo apt-get update && sudo apt-get install -y kubelet='1.33.x-*' kubectl='1.33.x-*' && \ sudo apt-mark hold kubelet kubectl # replace x in 1.33.x-* with the latest patch version sudo yum install -y kubelet-'1.33.x-*' kubectl-'1.33.x-*' --disableexcludes=kubernetes Restart the kubelet:sudo systemctl daemon-reload sudo systemctl restart kubelet Uncordon the nodeBring the node back online by marking it schedulable:# execute this command on a control plane node # replace  with the name of your node kubectl uncordon  What's nextSee how to Upgrade Windows nodes.4.2.1.5 - Upgrading Windows nodesFEATURE STATE: Kubernetes v1.18 [beta]This page explains how to upgrade a Windows node created with kubeadm.Before you beginYou need to have shell access to all the nodes, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts.Your Kubernetes server must be at or later than version 1.17.To check the version, enter kubectl version.Familiarize yourself with the process for upgrading the rest of your kubeadm cluster. You will want to upgrade the control plane nodes before upgrading your Windows nodes.Upgrading worker nodesUpgrade kubeadmFrom the Windows node, upgrade kubeadm:# replace 1.33.0 with your desired version curl.exe -Lo  "https://dl.k8s.io/v1.33.0/bin/windows/amd64/kubeadm.exe" Drain the nodeFrom a machine with access to the Kubernetes API, prepare the node for maintenance by marking it unschedulable and evicting the workloads:# replace  with the name of your node you are draining kubectl drain  \--ignore-daemonsets You should see output similar to this:node/ip-172-31-85-18 cordoned node/ip-172-31-85-18 drained Upgrade the kubelet configurationFrom the Windows node, call the following command to sync new kubelet configuration:kubeadm upgrade node Upgrade kubelet and kube-proxyFrom the Windows node, upgrade and restart the kubelet:stop-service kubelet curl.exe -Lo  "https://dl.k8s.io/v1.33.0/bin/windows/amd64/kubelet.exe" restart-service kubelet From the Windows node, upgrade and restart the kube-proxy.stop-service kube-proxy curl.exe -Lo  "https://dl.k8s.io/v1.33.0/bin/windows/amd64/kube-proxy.exe" restart-service kube-proxy Note:If you are running kube-proxy in a HostProcess container within a Pod, and not as a Windows Service, you can upgrade kube-proxy by applying a newer version of your kube-proxy manifests.Uncordon the nodeFrom a machine with access to the Kubernetes API, bring the node back online by marking it schedulable:# replace  with the name of your node kubectl uncordon  What's nextSee how to Upgrade Linux nodes.4.2.1.6 - Configuring a cgroup driverThis page explains how to configure the kubelet's cgroup driver to match the container runtime cgroup driver for kubeadm clusters.Before you beginYou should be familiar with the Kubernetes container runtime requirements.Configuring the container runtime cgroup driverThe Container runtimes page explains that the systemd driver is recommended for kubeadm based setups instead of the kubelet's default cgroupfs driver, because kubeadm manages the kubelet as a systemd service.The page also provides details on how to set up a number of different container runtimes with the systemd driver by default.Configuring the kubelet cgroup driverkubeadm allows you to pass a KubeletConfiguration structure during kubeadm init. This KubeletConfiguration can include the cgroupDriver field which controls the cgroup driver of the kubelet.Note:In v1.22 and later, if the user does not set the cgroupDriver field under KubeletConfiguration, kubeadm defaults it to systemd.In Kubernetes v1.28, you can enable automatic detection of the cgroup driver as an alpha feature. See systemd cgroup driver for more details.A minimal example of configuring the field explicitly:# kubeadm-config.yaml kind: ClusterConfiguration apiVersion: kubeadm.k8s.io/v1beta4 kubernetesVersion: v1.21.0 \--- kind: KubeletConfiguration apiVersion: kubelet.config.k8s.io/v1beta1 cgroupDriver: systemd Such a configuration file can then be passed to the kubeadm command:kubeadm init --config kubeadm-config.yaml Note:Kubeadm uses the same KubeletConfiguration for all nodes in the cluster. The KubeletConfiguration is stored in a ConfigMap object under the kube-system namespace.Executing the sub commands init, join and upgrade would result in kubeadm writing the KubeletConfiguration as a file under /var/lib/kubelet/config.yaml and passing it to the local node kubelet.Using the cgroupfs driverTo use cgroupfs and to prevent kubeadm upgrade from modifying the KubeletConfiguration cgroup driver on existing setups, you must be explicit about its value. This applies to a case where you do not wish future versions of kubeadm to apply the systemd driver by default.See the below section on "Modify the kubelet ConfigMap" for details on how to be explicit about the value.If you wish to configure a container runtime to use the cgroupfs driver, you must refer to the documentation of the container runtime of your choice.Migrating to the systemd driverTo change the cgroup driver of an existing kubeadm cluster from cgroupfs to systemd in-place, a similar procedure to a kubelet upgrade is required. This must include both steps outlined below.Note:Alternatively, it is possible to replace the old nodes in the cluster with new ones that use the systemd driver. This requires executing only the first step below before joining the new nodes and ensuring the workloads can safely move to the new nodes before deleting the old nodes.Modify the kubelet ConfigMapCall kubectl edit cm kubelet-config -n kube-system.Either modify the existing cgroupDriver value or add a new field that looks like this:cgroupDriver: systemd This field must be present under the kubelet: section of the ConfigMap.Update the cgroup driver on all nodesFor each node in the cluster:Drain the node using kubectl drain  \--ignore-daemonsetsStop the kubelet using systemctl stop kubeletStop the container runtimeModify the container runtime cgroup driver to systemdSet cgroupDriver: systemd in /var/lib/kubelet/config.yamlStart the container runtimeStart the kubelet using systemctl start kubeletUncordon the node using kubectl uncordon Execute these steps on nodes one at a time to ensure workloads have sufficient time to schedule on different nodes.Once the process is complete ensure that all nodes and workloads are healthy.4.2.1.7 - Certificate Management with kubeadmFEATURE STATE: Kubernetes v1.15 [stable]Client certificates generated by kubeadm expire after 1 year. This page explains how to manage certificate renewals with kubeadm. It also covers other tasks related to kubeadm certificate management.The Kubernetes project recommends upgrading to the latest patch releases promptly, and to ensure that you are running a supported minor release of Kubernetes. Following this recommendation helps you to to stay secure.Before you beginYou should be familiar with PKI certificates and requirements in Kubernetes.You should be familiar with how to pass a configuration file to the kubeadm commands.This guide covers the usage of the openssl command (used for manual certificate signing, if you choose that approach), but you can use your preferred tools.Some of the steps here use sudo for administrator access. You can use any equivalent tool.Using custom certificatesBy default, kubeadm generates all the certificates needed for a cluster to run. You can override this behavior by providing your own certificates.To do so, you must place them in whatever directory is specified by the \--cert-dir flag or the certificatesDir field of kubeadm's ClusterConfiguration. By default this is /etc/kubernetes/pki.If a given certificate and private key pair exists before running kubeadm init, kubeadm does not overwrite them. This means you can, for example, copy an existing CA into /etc/kubernetes/pki/ca.crt and /etc/kubernetes/pki/ca.key, and kubeadm will use this CA for signing the rest of the certificates.Choosing an encryption algorithmkubeadm allows you to choose an encryption algorithm that is used for creating public and private keys. That can be done by using the encryptionAlgorithm field of the kubeadm configuration:apiVersion: kubeadm.k8s.io/v1beta4 kind: ClusterConfiguration encryptionAlgorithm:  can be one of RSA-2048 (default), RSA-3072, RSA-4096 or ECDSA-P256.Choosing certificate validity periodkubeadm allows you to choose the validity period of CA and leaf certificates. That can be done by using the certificateValidityPeriod and caCertificateValidityPeriod fields of the kubeadm configuration:apiVersion: kubeadm.k8s.io/v1beta4 kind: ClusterConfiguration certificateValidityPeriod: 8760h # Default: 365 days × 24 hours = 1 year caCertificateValidityPeriod: 87600h # Default: 365 days × 24 hours * 10 = 10 years The values of the fields follow the accepted format for Go's time.Duration values, with the longest supported unit being h (hours).External CA modeIt is also possible to provide only the ca.crt file and not the ca.key file (this is only available for the root CA file, not other cert pairs). If all other certificates and kubeconfig files are in place, kubeadm recognizes this condition and activates the "External CA" mode. kubeadm will proceed without the CA key on disk.Instead, run the controller-manager standalone with --controllers=csrsigner and point to the CA certificate and key.There are various ways to prepare the component credentials when using external CA mode.Manual preparation of component credentialsPKI certificates and requirements includes information on how to prepare all the required by kubeadm component credentials manually.This guide covers the usage of the openssl command (used for manual certificate signing, if you choose that approach), but you can use your preferred tools.Preparation of credentials by signing CSRs generated by kubeadmkubeadm can generate CSR files that you can sign manually with tools like openssl and your external CA. These CSR files will include all the specification for credentials that components deployed by kubeadm require.Automated preparation of component credentials by using kubeadm phasesAlternatively, it is possible to use kubeadm phase commands to automate this process.Go to a host that you want to prepare as a kubeadm control plane node with external CA.Copy the external CA files ca.crt and ca.key that you have into /etc/kubernetes/pki on the node.Prepare a temporary kubeadm configuration file called config.yaml that can be used with kubeadm init. Make sure that this file includes any relevant cluster wide or host-specific information that could be included in certificates, such as, ClusterConfiguration.controlPlaneEndpoint, ClusterConfiguration.certSANs and InitConfiguration.APIEndpoint.On the same host execute the commands kubeadm init phase kubeconfig all --config config.yaml and kubeadm init phase certs all --config config.yaml. This will generate all required kubeconfig files and certificates under /etc/kubernetes/ and its pki sub directory.Inspect the generated files. Delete /etc/kubernetes/pki/ca.key, delete or move to a safe location the file /etc/kubernetes/super-admin.conf.On nodes where kubeadm join will be called also delete /etc/kubernetes/kubelet.conf. This file is only required on the first node where kubeadm init will be called.Note that some files such pki/sa.*, pki/front-proxy-ca.* and pki/etc/ca.* are shared between control plane nodes, You can generate them once and distribute them manually to nodes where kubeadm join will be called, or you can use the \--upload-certs functionality of kubeadm init and --certificate-key of kubeadm join to automate this distribution.Once the credentials are prepared on all nodes, call kubeadm init and kubeadm join for these nodes to join the cluster. kubeadm will use the existing kubeconfig and certificate files under /etc/kubernetes/ and its pki sub directory.Certificate expiry and managementNote:kubeadm cannot manage certificates signed by an external CA.You can use the check-expiration subcommand to check when certificates expire:kubeadm certs check-expiration The output is similar to this:CERTIFICATE EXPIRES RESIDUAL TIME CERTIFICATE AUTHORITY EXTERNALLY MANAGED admin.conf Dec 30, 2020 23:36 UTC 364d no apiserver Dec 30, 2020 23:36 UTC 364d ca no apiserver-etcd-client Dec 30, 2020 23:36 UTC 364d etcd-ca no apiserver-kubelet-client Dec 30, 2020 23:36 UTC 364d ca no controller-manager.conf Dec 30, 2020 23:36 UTC 364d no etcd-healthcheck-client Dec 30, 2020 23:36 UTC 364d etcd-ca no etcd-peer Dec 30, 2020 23:36 UTC 364d etcd-ca no etcd-server Dec 30, 2020 23:36 UTC 364d etcd-ca no front-proxy-client Dec 30, 2020 23:36 UTC 364d front-proxy-ca no scheduler.conf Dec 30, 2020 23:36 UTC 364d no CERTIFICATE AUTHORITY EXPIRES RESIDUAL TIME EXTERNALLY MANAGED ca Dec 28, 2029 23:36 UTC 9y no etcd-ca Dec 28, 2029 23:36 UTC 9y no front-proxy-ca Dec 28, 2029 23:36 UTC 9y no The command shows expiration/residual time for the client certificates in the /etc/kubernetes/pki folder and for the client certificate embedded in the kubeconfig files used by kubeadm (admin.conf, controller-manager.conf and scheduler.conf).Additionally, kubeadm informs the user if the certificate is externally managed; in this case, the user should take care of managing certificate renewal manually/using other tools.The kubelet.conf configuration file is not included in the list above because kubeadm configures kubelet for automatic certificate renewal with rotatable certificates under /var/lib/kubelet/pki. To repair an expired kubelet client certificate see Kubelet client certificate rotation fails.Note:On nodes created with kubeadm init from versions prior to kubeadm version 1.17, there is a bug where you manually have to modify the contents of kubelet.conf. After kubeadm init finishes, you should update kubelet.conf to point to the rotated kubelet client certificates, by replacing client-certificate-data and client-key-data with:client-certificate: /var/lib/kubelet/pki/kubelet-client-current.pem client-key: /var/lib/kubelet/pki/kubelet-client-current.pem Automatic certificate renewalkubeadm renews all the certificates during control plane upgrade.This feature is designed for addressing the simplest use cases; if you don't have specific requirements on certificate renewal and perform Kubernetes version upgrades regularly (less than 1 year in between each upgrade), kubeadm will take care of keeping your cluster up to date and reasonably secure.If you have more complex requirements for certificate renewal, you can opt out from the default behavior by passing --certificate-renewal=false to kubeadm upgrade apply or to kubeadm upgrade node.Manual certificate renewalYou can renew your certificates manually at any time with the kubeadm certs renew command, with the appropriate command line options. If you are running cluster with a replicated control plane, this command needs to be executed on all the control-plane nodes.This command performs the renewal using CA (or front-proxy-CA) certificate and key stored in /etc/kubernetes/pki.kubeadm certs renew uses the existing certificates as the authoritative source for attributes (Common Name, Organization, subject alternative name) and does not rely on the kubeadm-config ConfigMap. Even so, the Kubernetes project recommends keeping the served certificate and the associated values in that ConfigMap synchronized, to avoid any risk of confusion.After running the command you should restart the control plane Pods. This is required since dynamic certificate reload is currently not supported for all components and certificates. Static Pods are managed by the local kubelet and not by the API Server, thus kubectl cannot be used to delete and restart them. To restart a static Pod you can temporarily remove its manifest file from /etc/kubernetes/manifests/ and wait for 20 seconds (see the fileCheckFrequency value in KubeletConfiguration struct. The kubelet will terminate the Pod if it's no longer in the manifest directory. You can then move the file back and after another fileCheckFrequency period, the kubelet will recreate the Pod and the certificate renewal for the component can complete.kubeadm certs renew can renew any specific certificate or, with the subcommand all, it can renew all of them:# If you are running cluster with a replicated control plane, this command # needs to be executed on all the control-plane nodes. kubeadm certs renew all Copying the administrator certificate (optional)Clusters built with kubeadm often copy the admin.conf certificate into $HOME/.kube/config, as instructed in Creating a cluster with kubeadm. On such a system, to update the contents of $HOME/.kube/config after renewing the admin.conf, you could run the following commands:sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config Renew certificates with the Kubernetes certificates APIThis section provides more details about how to execute manual certificate renewal using the Kubernetes certificates API.Caution:These are advanced topics for users who need to integrate their organization's certificate infrastructure into a kubeadm-built cluster. If the default kubeadm configuration satisfies your needs, you should let kubeadm manage certificates instead.Set up a signerThe Kubernetes Certificate Authority does not work out of the box. You can configure an external signer such as cert-manager, or you can use the built-in signer.The built-in signer is part of kube-controller-manager.To activate the built-in signer, you must pass the --cluster-signing-cert-file and \--cluster-signing-key-file flags.If you're creating a new cluster, you can use a kubeadm configuration file:apiVersion: kubeadm.k8s.io/v1beta4 kind: ClusterConfiguration controllerManager: extraArgs: \- name: "cluster-signing-cert-file" value: "/etc/kubernetes/pki/ca.crt" \- name: "cluster-signing-key-file" value: "/etc/kubernetes/pki/ca.key" Create certificate signing requests (CSR)See Create CertificateSigningRequest for creating CSRs with the Kubernetes API.Renew certificates with external CAThis section provide more details about how to execute manual certificate renewal using an external CA.To better integrate with external CAs, kubeadm can also produce certificate signing requests (CSRs). A CSR represents a request to a CA for a signed certificate for a client. In kubeadm terms, any certificate that would normally be signed by an on-disk CA can be produced as a CSR instead. A CA, however, cannot be produced as a CSR.Renewal by using certificate signing requests (CSR)Renewal of ceritficates is possible by generating new CSRs and signing them with the external CA. For more details about working with CSRs generated by kubeadm see the section Signing certificate signing requests (CSR) generated by kubeadm.Certificate authority (CA) rotationKubeadm does not support rotation or replacement of CA certificates out of the box.For more information about manual rotation or replacement of CA, see manual rotation of CA certificates.Enabling signed kubelet serving certificatesBy default the kubelet serving certificate deployed by kubeadm is self-signed. This means a connection from external services like the metrics-server to a kubelet cannot be secured with TLS.To configure the kubelets in a new kubeadm cluster to obtain properly signed serving certificates you must pass the following minimal configuration to kubeadm init:apiVersion: kubeadm.k8s.io/v1beta4 kind: ClusterConfiguration \--- apiVersion: kubelet.config.k8s.io/v1beta1 kind: KubeletConfiguration serverTLSBootstrap: true If you have already created the cluster you must adapt it by doing the following:Find and edit the kubelet-config-1.33 ConfigMap in the kube-system namespace. In that ConfigMap, the kubelet key has a KubeletConfiguration document as its value. Edit the KubeletConfiguration document to set serverTLSBootstrap: true.On each node, add the serverTLSBootstrap: true field in /var/lib/kubelet/config.yaml and restart the kubelet with systemctl restart kubeletThe field serverTLSBootstrap: true will enable the bootstrap of kubelet serving certificates by requesting them from the certificates.k8s.io API. One known limitation is that the CSRs (Certificate Signing Requests) for these certificates cannot be automatically approved by the default signer in the kube-controller-manager - kubernetes.io/kubelet-serving. This will require action from the user or a third party controller.These CSRs can be viewed using:kubectl get csr NAME AGE SIGNERNAME REQUESTOR CONDITION csr-9wvgt 112s kubernetes.io/kubelet-serving system:node:worker-1 Pending csr-lz97v 1m58s kubernetes.io/kubelet-serving system:node:control-plane-1 Pending To approve them you can do the following:kubectl certificate approve  By default, these serving certificate will expire after one year. Kubeadm sets the KubeletConfiguration field rotateCertificates to true, which means that close to expiration a new set of CSRs for the serving certificates will be created and must be approved to complete the rotation. To understand more see Certificate Rotation.If you are looking for a solution for automatic approval of these CSRs it is recommended that you contact your cloud provider and ask if they have a CSR signer that verifies the node identity with an out of band mechanism.Note: This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the content guide before submitting a change. More information.Third party custom controllers can be used:kubelet-csr-approverSuch a controller is not a secure mechanism unless it not only verifies the CommonName in the CSR but also verifies the requested IPs and domain names. This would prevent a malicious actor that has access to a kubelet client certificate to create CSRs requesting serving certificates for any IP or domain name.Generating kubeconfig files for additional usersDuring cluster creation, kubeadm init signs the certificate in the super-admin.conf to have Subject: O = system:masters, CN = kubernetes-super-admin. system:masters is a break-glass, super user group that bypasses the authorization layer (for example, RBAC). The file admin.conf is also created by kubeadm on control plane nodes and it contains a certificate with Subject: O = kubeadm:cluster-admins, CN = kubernetes-admin. kubeadm:cluster-admins is a group logically belonging to kubeadm. If your cluster uses RBAC (the kubeadm default), the kubeadm:cluster-admins group is bound to the cluster-admin ClusterRole.Warning:Avoid sharing the super-admin.conf or admin.conf files. Instead, create least privileged access even for people who work as administrators and use that least privilege alternative for anything other than break-glass (emergency) access.You can use the kubeadm kubeconfig user command to generate kubeconfig files for additional users. The command accepts a mixture of command line flags and kubeadm configuration options. The generated kubeconfig will be written to stdout and can be piped to a file using kubeadm kubeconfig user ... > somefile.conf.Example configuration file that can be used with --config:# example.yaml apiVersion: kubeadm.k8s.io/v1beta4 kind: ClusterConfiguration # Will be used as the target "cluster" in the kubeconfig clusterName: "kubernetes" # Will be used as the "server" (IP or DNS name) of this cluster in the kubeconfig controlPlaneEndpoint: "some-dns-address:6443" # The cluster CA key and certificate will be loaded from this local directory certificatesDir: "/etc/kubernetes/pki" Make sure that these settings match the desired target cluster settings. To see the settings of an existing cluster use:kubectl get cm kubeadm-config -n kube-system -o=jsonpath="{.data.ClusterConfiguration}" The following example will generate a kubeconfig file with credentials valid for 24 hours for a new user johndoe that is part of the appdevs group:kubeadm kubeconfig user --config example.yaml --org appdevs --client-name johndoe --validity-period 24h The following example will generate a kubeconfig file with administrator credentials valid for 1 week:kubeadm kubeconfig user --config example.yaml --client-name admin --validity-period 168h Signing certificate signing requests (CSR) generated by kubeadmYou can create certificate signing requests with kubeadm certs generate-csr. Calling this command will generate .csr / .key file pairs for regular certificates. For certificates embedded in kubeconfig files, the command will generate a .csr / .conf pair where the key is already embedded in the .conf file.A CSR file contains all relevant information for a CA to sign a certificate. kubeadm uses a well defined specification for all its certificates and CSRs.The default certificate directory is /etc/kubernetes/pki, while the default directory for kubeconfig files is /etc/kubernetes. These defaults can be overridden with the flags --cert-dir and --kubeconfig-dir, respectively.To pass custom options to kubeadm certs generate-csr use the --config flag, which accepts a kubeadm configuration file, similarly to commands such as kubeadm init. Any specification such as extra SANs and custom IP addresses must be stored in the same configuration file and used for all relevant kubeadm commands by passing it as --config.Note:This guide uses the default Kubernetes directory /etc/kubernetes, which requires a super user. If you are following this guide and are using directories that you can write to (typically, this means running kubeadm with --cert-dir and --kubeconfig-dir) then you can omit the sudo command).You must then copy the files that you produced over to within the /etc/kubernetes directory so that kubeadm init or kubeadm join will find them.Preparing CA and service account filesOn the primary control plane node, where kubeadm init will be executed, call the following commands:sudo kubeadm init phase certs ca sudo kubeadm init phase certs etcd-ca sudo kubeadm init phase certs front-proxy-ca sudo kubeadm init phase certs sa This will populate the folders /etc/kubernetes/pki and /etc/kubernetes/pki/etcd with all self-signed CA files (certificates and keys) and service account (public and private keys) that kubeadm needs for a control plane node.Note:If you are using an external CA, you must generate the same files out of band and manually copy them to the primary control plane node in /etc/kubernetes.Once all CSRs are signed, you can delete the root CA key (ca.key) as noted in the External CA mode section.For secondary control plane nodes (kubeadm join --control-plane) there is no need to call the above commands. Depending on how you setup the High Availability cluster, you either have to manually copy the same files from the primary control plane node, or use the automated --upload-certs functionality of kubeadm init.Generate CSRsThe kubeadm certs generate-csr command generates CSRs for all known certificates managed by kubeadm. Once the command is done you must manually delete .csr, .conf or .key files that you don't need.Considerations for kubelet.confThis section applies to both control plane and worker nodes.If you have deleted the ca.key file from control plane nodes (External CA mode), the active kube-controller-manager in this cluster will not be able to sign kubelet client certificates. If no external method for signing these certificates exists in your setup (such as an external signer, you could manually sign the kubelet.conf.csr as explained in this guide.Note that this also means that the automatic kubelet client certificate rotation will be disabled. If so, close to certificate expiration, you must generate a new kubelet.conf.csr, sign the certificate, embed it in kubelet.conf and restart the kubelet.If this does not apply to your setup, you can skip processing the kubelet.conf.csr on secondary control plane and on workers nodes (all nodes that call kubeadm join ...). That is because the active kube-controller-manager will be responsible for signing new kubelet client certificates.Note:You must process the kubelet.conf.csr file on the primary control plane node (the host where you originally ran kubeadm init). This is because kubeadm considers that as the node that bootstraps the cluster, and a pre-populated kubelet.conf is needed.Control plane nodesExecute the following command on primary (kubeadm init) and secondary (kubeadm join --control-plane) control plane nodes to generate all CSR files:sudo kubeadm certs generate-csr If external etcd is to be used, follow the External etcd with kubeadm guide to understand what CSR files are needed on the kubeadm and etcd nodes. Other .csr and .key files under /etc/kubernetes/pki/etcd can be removed.Based on the explanation in Considerations for kubelet.conf keep or delete the kubelet.conf and kubelet.conf.csr files.Worker nodesBased on the explanation in Considerations for kubelet.conf, optionally call:sudo kubeadm certs generate-csr and keep only the kubelet.conf and kubelet.conf.csr files. Alternatively skip the steps for worker nodes entirely.Signing CSRs for all certificatesNote:If you are using external CA and already have CA serial number files (.srl) for openssl, you can copy such files to a kubeadm node where CSRs will be processed. The .srl files to copy are /etc/kubernetes/pki/ca.srl, /etc/kubernetes/pki/front-proxy-ca.srl and /etc/kubernetes/pki/etcd/ca.srl. The files can be then moved to a new node where CSR files will be processed.If a .srl file is missing for a CA on a node, the script below will generate a new SRL file with a random starting serial number.To read more about .srl files see the openssl documentation for the --CAserial flag.Repeat this step for all nodes that have CSR files.Write the following script in the /etc/kubernetes directory, navigate to the directory and execute the script. The script will generate certificates for all CSR files that are present in the /etc/kubernetes tree.#!/bin/bash # Set certificate expiration time in days DAYS=365 # Process all CSR files except those for front-proxy and etcd find ./ -name "*.csr" | grep -v "pki/etcd" | grep -v "front-proxy" | while read -r FILE; do echo "* Processing ${FILE} ..." FILE=${FILE%.*} # Trim the extension if [ -f "./pki/ca.srl" ]; then SERIAL_FLAG="-CAserial ./pki/ca.srl" else SERIAL_FLAG="-CAcreateserial" fi openssl x509 -req -days "${DAYS}" -CA ./pki/ca.crt -CAkey ./pki/ca.key ${SERIAL_FLAG} \ -in "${FILE}.csr" -out "${FILE}.crt" sleep 2 done # Process all etcd CSRs find ./pki/etcd -name "*.csr" | while read -r FILE; do echo "* Processing ${FILE} ..." FILE=${FILE%.*} # Trim the extension if [ -f "./pki/etcd/ca.srl" ]; then SERIAL_FLAG=-CAserial ./pki/etcd/ca.srl else SERIAL_FLAG=-CAcreateserial fi openssl x509 -req -days "${DAYS}" -CA ./pki/etcd/ca.crt -CAkey ./pki/etcd/ca.key ${SERIAL_FLAG} \ -in "${FILE}.csr" -out "${FILE}.crt" done # Process front-proxy CSRs echo "* Processing ./pki/front-proxy-client.csr ..." openssl x509 -req -days "${DAYS}" -CA ./pki/front-proxy-ca.crt -CAkey ./pki/front-proxy-ca.key -CAcreateserial \ -in ./pki/front-proxy-client.csr -out ./pki/front-proxy-client.crt Embedding certificates in kubeconfig filesRepeat this step for all nodes that have CSR files.Write the following script in the /etc/kubernetes directory, navigate to the directory and execute the script. The script will take the .crt files that were signed for kubeconfig files from CSRs in the previous step and will embed them in the kubeconfig files.#!/bin/bash CLUSTER=kubernetes find ./ -name "*.conf" | while read -r FILE; do echo "* Processing ${FILE} ..." KUBECONFIG="${FILE}" kubectl config set-cluster "${CLUSTER}" --certificate-authority ./pki/ca.crt --embed-certs USER=$(KUBECONFIG="${FILE}" kubectl config view -o jsonpath='{.users[0].name}') KUBECONFIG="${FILE}" kubectl config set-credentials "${USER}" --client-certificate "${FILE}.crt" --embed-certs done Performing cleanupPerform this step on all nodes that have CSR files.Write the following script in the /etc/kubernetes directory, navigate to the directory and execute the script.#!/bin/bash # Cleanup CSR files rm -f ./*.csr ./pki/*.csr ./pki/etcd/*.csr # Clean all CSR files # Cleanup CRT files that were already embedded in kubeconfig files rm -f ./*.crt Optionally, move .srl files to the next node to be processed.Optionally, if using external CA remove the /etc/kubernetes/pki/ca.key file, as explained in the External CA node section.kubeadm node initializationOnce CSR files have been signed and required certificates are in place on the hosts you want to use as nodes, you can use the commands kubeadm init and kubeadm join to create a Kubernetes cluster from these nodes. During init and join, kubeadm uses existing certificates, encryption keys and kubeconfig files that it finds in the /etc/kubernetes tree on the host's local filesystem.4.2.1.8 - Reconfiguring a kubeadm clusterkubeadm does not support automated ways of reconfiguring components that were deployed on managed nodes. One way of automating this would be by using a custom operator.To modify the components configuration you must manually edit associated cluster objects and files on disk.This guide shows the correct sequence of steps that need to be performed to achieve kubeadm cluster reconfiguration.Before you beginYou need a cluster that was deployed using kubeadmHave administrator credentials (/etc/kubernetes/admin.conf) and network connectivity to a running kube-apiserver in the cluster from a host that has kubectl installedHave a text editor installed on all hostsReconfiguring the clusterkubeadm writes a set of cluster wide component configuration options in ConfigMaps and other objects. These objects must be manually edited. The command kubectl edit can be used for that.The kubectl edit command will open a text editor where you can edit and save the object directly.You can use the environment variables KUBECONFIG and KUBE_EDITOR to specify the location of the kubectl consumed kubeconfig file and preferred text editor.For example:KUBECONFIG=/etc/kubernetes/admin.conf KUBE_EDITOR=nano kubectl edit  Note:Upon saving any changes to these cluster objects, components running on nodes may not be automatically updated. The steps below instruct you on how to perform that manually.Warning:Component configuration in ConfigMaps is stored as unstructured data (YAML string). This means that validation will not be performed upon updating the contents of a ConfigMap. You have to be careful to follow the documented API format for a particular component configuration and avoid introducing typos and YAML indentation mistakes.Applying cluster configuration changesUpdating the ClusterConfigurationDuring cluster creation and upgrade, kubeadm writes its ClusterConfiguration in a ConfigMap called kubeadm-config in the kube-system namespace.To change a particular option in the ClusterConfiguration you can edit the ConfigMap with this command:kubectl edit cm -n kube-system kubeadm-config The configuration is located under the data.ClusterConfiguration key.Note:The ClusterConfiguration includes a variety of options that affect the configuration of individual components such as kube-apiserver, kube-scheduler, kube-controller-manager, CoreDNS, etcd and kube-proxy. Changes to the configuration must be reflected on node components manually.Reflecting ClusterConfiguration changes on control plane nodeskubeadm manages the control plane components as static Pod manifests located in the directory /etc/kubernetes/manifests. Any changes to the ClusterConfiguration under the apiServer, controllerManager, scheduler or etcd keys must be reflected in the associated files in the manifests directory on a control plane node.Such changes may include:extraArgs - requires updating the list of flags passed to a component containerextraVolumes - requires updating the volume mounts for a component container*SANs - requires writing new certificates with updated Subject Alternative NamesBefore proceeding with these changes, make sure you have backed up the directory /etc/kubernetes/.To write new certificates you can use:kubeadm init phase certs  \--config  To write new manifest files in /etc/kubernetes/manifests you can use:# For Kubernetes control plane components kubeadm init phase control-plane  \--config  # For local etcd kubeadm init phase etcd local --config  The  contents must match the updated ClusterConfiguration. The  value must be a name of a Kubernetes control plane component (apiserver, controller-manager or scheduler).Note:Updating a file in /etc/kubernetes/manifests will tell the kubelet to restart the static Pod for the corresponding component. Try doing these changes one node at a time to leave the cluster without downtime.Applying kubelet configuration changesUpdating the KubeletConfigurationDuring cluster creation and upgrade, kubeadm writes its KubeletConfiguration in a ConfigMap called kubelet-config in the kube-system namespace.You can edit the ConfigMap with this command:kubectl edit cm -n kube-system kubelet-config The configuration is located under the data.kubelet key.Reflecting the kubelet changesTo reflect the change on kubeadm nodes you must do the following:Log in to a kubeadm nodeRun kubeadm upgrade node phase kubelet-config to download the latest kubelet-config ConfigMap contents into the local file /var/lib/kubelet/config.yamlEdit the file /var/lib/kubelet/kubeadm-flags.env to apply additional configuration with flagsRestart the kubelet service with systemctl restart kubeletNote:Do these changes one node at a time to allow workloads to be rescheduled properly.Note:During kubeadm upgrade, kubeadm downloads the KubeletConfiguration from the kubelet-config ConfigMap and overwrite the contents of /var/lib/kubelet/config.yaml. This means that node local configuration must be applied either by flags in /var/lib/kubelet/kubeadm-flags.env or by manually updating the contents of /var/lib/kubelet/config.yaml after kubeadm upgrade, and then restarting the kubelet.Applying kube-proxy configuration changesUpdating the KubeProxyConfigurationDuring cluster creation and upgrade, kubeadm writes its KubeProxyConfiguration in a ConfigMap in the kube-system namespace called kube-proxy.This ConfigMap is used by the kube-proxy DaemonSet in the kube-system namespace.To change a particular option in the KubeProxyConfiguration, you can edit the ConfigMap with this command:kubectl edit cm -n kube-system kube-proxy The configuration is located under the data.config.conf key.Reflecting the kube-proxy changesOnce the kube-proxy ConfigMap is updated, you can restart all kube-proxy Pods:Delete the Pods with:kubectl delete po -n kube-system -l k8s-app=kube-proxy New Pods that use the updated ConfigMap will be created.Note:Because kubeadm deploys kube-proxy as a DaemonSet, node specific configuration is unsupported.Applying CoreDNS configuration changesUpdating the CoreDNS Deployment and Servicekubeadm deploys CoreDNS as a Deployment called coredns and with a Service kube-dns, both in the kube-system namespace.To update any of the CoreDNS settings, you can edit the Deployment and Service objects:kubectl edit deployment -n kube-system coredns kubectl edit service -n kube-system kube-dns Reflecting the CoreDNS changesOnce the CoreDNS changes are applied you can restart the CoreDNS deployment:kubectl rollout restart deployment -n kube-system coredns Note:kubeadm does not allow CoreDNS configuration during cluster creation and upgrade. This means that if you execute kubeadm upgrade apply, your changes to the CoreDNS objects will be lost and must be reapplied.Persisting the reconfigurationDuring the execution of kubeadm upgrade on a managed node, kubeadm might overwrite configuration that was applied after the cluster was created (reconfiguration).Persisting Node object reconfigurationkubeadm writes Labels, Taints, CRI socket and other information on the Node object for a particular Kubernetes node. To change any of the contents of this Node object you can use:kubectl edit no  During kubeadm upgrade the contents of such a Node might get overwritten. If you would like to persist your modifications to the Node object after upgrade, you can prepare a kubectl patch and apply it to the Node object:kubectl patch no  \--patch-file  Persisting control plane component reconfigurationThe main source of control plane configuration is the ClusterConfiguration object stored in the cluster. To extend the static Pod manifests configuration, patches can be used.These patch files must remain as files on the control plane nodes to ensure that they can be used by the kubeadm upgrade ... --patches .If reconfiguration is done to the ClusterConfiguration and static Pod manifests on disk, the set of node specific patches must be updated accordingly.Persisting kubelet reconfigurationAny changes to the KubeletConfiguration stored in /var/lib/kubelet/config.yaml will be overwritten on kubeadm upgrade by downloading the contents of the cluster wide kubelet-config ConfigMap. To persist kubelet node specific configuration either the file /var/lib/kubelet/config.yaml has to be updated manually post-upgrade or the file /var/lib/kubelet/kubeadm-flags.env can include flags. The kubelet flags override the associated KubeletConfiguration options, but note that some of the flags are deprecated.A kubelet restart will be required after changing /var/lib/kubelet/config.yaml or /var/lib/kubelet/kubeadm-flags.env.What's nextUpgrading kubeadm clustersCustomizing components with the kubeadm APICertificate management with kubeadmFind more about kubeadm set-up4.2.1.9 - Changing The Kubernetes Package RepositoryThis page explains how to enable a package repository for the desired Kubernetes minor release upon upgrading a cluster. This is only needed for users of the community-owned package repositories hosted at pkgs.k8s.io. Unlike the legacy package repositories, the community-owned package repositories are structured in a way that there's a dedicated package repository for each Kubernetes minor version.Note:This guide only covers a part of the Kubernetes upgrade process. Please see the upgrade guide for more information about upgrading Kubernetes clusters.Note:This step is only needed upon upgrading a cluster to another minor release. If you're upgrading to another patch release within the same minor release (e.g. v1.33.5 to v1.33.7), you don't need to follow this guide. However, if you're still using the legacy package repositories, you'll need to migrate to the new community-owned package repositories before upgrading (see the next section for more details on how to do this).Before you beginThis document assumes that you're already using the community-owned package repositories (pkgs.k8s.io). If that's not the case, it's strongly recommended to migrate to the community-owned package repositories as described in the official announcement.Note: The legacy package repositories (apt.kubernetes.io and yum.kubernetes.io) have been deprecated and frozen starting from September 13, 2023. Using the new package repositories hosted at pkgs.k8s.io is strongly recommended and required in order to install Kubernetes versions released after September 13, 2023. The deprecated legacy repositories, and their contents, might be removed at any time in the future and without a further notice period. The new package repositories provide downloads for Kubernetes versions starting with v1.24.0.Verifying if the Kubernetes package repositories are usedIf you're unsure whether you're using the community-owned package repositories or the legacy package repositories, take the following steps to verify:Ubuntu, Debian or HypriotOSCentOS, RHEL or FedoraopenSUSE or SLESPrint the contents of the file that defines the Kubernetes apt repository:# On your system, this configuration file could have a different name pager /etc/apt/sources.list.d/kubernetes.list If you see a line similar to:deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.32/deb/ / You're using the Kubernetes package repositories and this guide applies to you. Otherwise, it's strongly recommended to migrate to the Kubernetes package repositories as described in the official announcement.Print the contents of the file that defines the Kubernetes yum repository:# On your system, this configuration file could have a different name cat /etc/yum.repos.d/kubernetes.repo If you see a baseurl similar to the baseurl in the output below:[kubernetes] name=Kubernetes baseurl=https://pkgs.k8s.io/core:/stable:/v1.32/rpm/ enabled=1 gpgcheck=1 gpgkey=https://pkgs.k8s.io/core:/stable:/v1.32/rpm/repodata/repomd.xml.key exclude=kubelet kubeadm kubectl You're using the Kubernetes package repositories and this guide applies to you. Otherwise, it's strongly recommended to migrate to the Kubernetes package repositories as described in the official announcement.Print the contents of the file that defines the Kubernetes zypper repository:# On your system, this configuration file could have a different name cat /etc/zypp/repos.d/kubernetes.repo If you see a baseurl similar to the baseurl in the output below:[kubernetes] name=Kubernetes baseurl=https://pkgs.k8s.io/core:/stable:/v1.32/rpm/ enabled=1 gpgcheck=1 gpgkey=https://pkgs.k8s.io/core:/stable:/v1.32/rpm/repodata/repomd.xml.key exclude=kubelet kubeadm kubectl You're using the Kubernetes package repositories and this guide applies to you. Otherwise, it's strongly recommended to migrate to the Kubernetes package repositories as described in the official announcement.Note:The URL used for the Kubernetes package repositories is not limited to pkgs.k8s.io, it can also be one of:pkgs.k8s.iopkgs.kubernetes.iopackages.kubernetes.ioSwitching to another Kubernetes package repositoryThis step should be done upon upgrading from one to another Kubernetes minor release in order to get access to the packages of the desired Kubernetes minor version.Ubuntu, Debian or HypriotOSCentOS, RHEL or FedoraOpen the file that defines the Kubernetes apt repository using a text editor of your choice:nano /etc/apt/sources.list.d/kubernetes.list You should see a single line with the URL that contains your current Kubernetes minor version. For example, if you're using v1.32, you should see this:deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.32/deb/ / Change the version in the URL to the next available minor release, for example:deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.33/deb/ / Save the file and exit your text editor. Continue following the relevant upgrade instructions.Open the file that defines the Kubernetes yum repository using a text editor of your choice:nano /etc/yum.repos.d/kubernetes.repo You should see a file with two URLs that contain your current Kubernetes minor version. For example, if you're using v1.32, you should see this:[kubernetes] name=Kubernetes baseurl=https://pkgs.k8s.io/core:/stable:/v1.32/rpm/ enabled=1 gpgcheck=1 gpgkey=https://pkgs.k8s.io/core:/stable:/v1.32/rpm/repodata/repomd.xml.key exclude=kubelet kubeadm kubectl cri-tools kubernetes-cni Change the version in these URLs to the next available minor release, for example:[kubernetes] name=Kubernetes baseurl=https://pkgs.k8s.io/core:/stable:/v1.33/rpm/ enabled=1 gpgcheck=1 gpgkey=https://pkgs.k8s.io/core:/stable:/v1.33/rpm/repodata/repomd.xml.key exclude=kubelet kubeadm kubectl cri-tools kubernetes-cni Save the file and exit your text editor. Continue following the relevant upgrade instructions.What's nextSee how to Upgrade Linux nodes.See how to Upgrade Windows nodes.4.2.2 - Overprovision Node Capacity For A ClusterThis page guides you through configuring Node overprovisioning in your Kubernetes cluster. Node overprovisioning is a strategy that proactively reserves a portion of your cluster's compute resources. This reservation helps reduce the time required to schedule new pods during scaling events, enhancing your cluster's responsiveness to sudden spikes in traffic or workload demands.By maintaining some unused capacity, you ensure that resources are immediately available when new pods are created, preventing them from entering a pending state while the cluster scales up.Before you beginYou need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster.You should already have a basic understanding of Deployments, Pod priority, and PriorityClasses.Your cluster must be set up with an autoscaler that manages nodes based on demand.Create a PriorityClassBegin by defining a PriorityClass for the placeholder Pods. First, create a PriorityClass with a negative priority value, that you will shortly assign to the placeholder pods. Later, you will set up a Deployment that uses this PriorityClasspriorityclass/low-priority-class.yaml apiVersion: scheduling.k8s.io/v1 kind: PriorityClass metadata: name: placeholder # these Pods represent placeholder capacity value: -1000 globalDefault: false description: "Negative priority for placeholder pods to enable overprovisioning."Then create the PriorityClass:kubectl apply -f https://k8s.io/examples/priorityclass/low-priority-class.yaml You will next define a Deployment that uses the negative-priority PriorityClass and runs a minimal container. When you add this to your cluster, Kubernetes runs those placeholder pods to reserve capacity. Any time there is a capacity shortage, the control plane will pick one these placeholder pods as the first candidate to preempt.Run Pods that request node capacityReview the sample manifest:deployments/deployment-with-capacity-reservation.yaml apiVersion: apps/v1 kind: Deployment metadata: name: capacity-reservation # You should decide what namespace to deploy this into spec: replicas: 1 selector: matchLabels: app.kubernetes.io/name: capacity-placeholder template: metadata: labels: app.kubernetes.io/name: capacity-placeholder annotations: kubernetes.io/description: "Capacity reservation" spec: priorityClassName: placeholder affinity: # Try to place these overhead Pods on different nodes # if possible podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: \- labelSelector: matchLabels: app: placeholder topologyKey: "kubernetes.io/hostname" containers: \- name: pause image: registry.k8s.io/pause:3.6 resources: requests: cpu: "50m" memory: "512Mi" limits: memory: "512Mi" Pick a namespace for the placeholder podsYou should select, or create, a namespace that the placeholder Pods will go into.Create the placeholder deploymentCreate a Deployment based on that manifest:# Change the namespace name "example" kubectl --namespace example apply -f https://k8s.io/examples/deployments/deployment-with-capacity-reservation.yaml Adjust placeholder resource requestsConfigure the resource requests and limits for the placeholder pods to define the amount of overprovisioned resources you want to maintain. This reservation ensures that a specific amount of CPU and memory is kept available for new pods.To edit the Deployment, modify the resources section in the Deployment manifest file to set appropriate requests and limits. You can download that file locally and then edit it with whichever text editor you prefer.You can also edit the Deployment using kubectl:kubectl edit deployment capacity-reservation For example, to reserve a total of a 0.5 CPU and 1GiB of memory across 5 placeholder pods, define the resource requests and limits for a single placeholder pod as follows: resources: requests: cpu: "100m" memory: "200Mi" limits: cpu: "100m" Set the desired replica countCalculate the total reserved resourcesFor example, with 5 replicas each reserving 0.1 CPU and 200MiB of memory:Total CPU reserved: 5 × 0.1 = 0.5 (in the Pod specification, you'll write the quantity 500m)Total memory reserved: 5 × 200MiB = 1GiB (in the Pod specification, you'll write 1 Gi)To scale the Deployment, adjust the number of replicas based on your cluster's size and expected workload:kubectl scale deployment capacity-reservation --replicas=5 Verify the scaling:kubectl get deployment capacity-reservation The output should reflect the updated number of replicas:NAME READY UP-TO-DATE AVAILABLE AGE capacity-reservation 5/5 5 5 2m Note:Some autoscalers, notably Karpenter, treat preferred affinity rules as hard rules when considering node scaling. If you use Karpenter or another node autoscaler that uses the same heuristic, the replica count you set here also sets a minimum node count for your cluster.What's nextLearn more about PriorityClasses and how they affect pod scheduling.Explore node autoscaling to dynamically adjust your cluster's size based on workload demands.Understand Pod preemption, a key mechanism for Kubernetes to handle resource contention. The same page covers eviction, which is less relevant to the placeholder Pod approach, but is also a mechanism for Kubernetes to react when resources are contended.4.2.3 - Migrating from dockershimThis section presents information you need to know when migrating from dockershim to other container runtimes.Since the announcement of dockershim deprecation in Kubernetes 1.20, there were questions on how this will affect various workloads and Kubernetes installations. Our Dockershim Removal FAQ is there to help you to understand the problem better.Dockershim was removed from Kubernetes with the release of v1.24. If you use Docker Engine via dockershim as your container runtime and wish to upgrade to v1.24, it is recommended that you either migrate to another runtime or find an alternative means to obtain Docker Engine support. Check out the container runtimes section to know your options.The version of Kubernetes with dockershim (1.23) is out of support and the v1.24 will run out of support soon. Make sure to report issues you encountered with the migration so the issues can be fixed in a timely manner and your cluster would be ready for dockershim removal. After v1.24 running out of support, you will need to contact your Kubernetes provider for support or upgrade multiple versions at a time if there are critical issues affecting your cluster.Your cluster might have more than one kind of node, although this is not a common configuration.These tasks will help you to migrate:Check whether Dockershim removal affects youMigrating telemetry and security agents from dockershimWhat's nextCheck out container runtimes to understand your options for an alternative.If you find a defect or other technical concern relating to migrating away from dockershim, you can report an issue to the Kubernetes project.4.2.3.1 - Changing the Container Runtime on a Node from Docker Engine to containerdThis task outlines the steps needed to update your container runtime to containerd from Docker. It is applicable for cluster operators running Kubernetes 1.23 or earlier. This also covers an example scenario for migrating from dockershim to containerd. Alternative container runtimes can be picked from this page.Before you beginNote: This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the content guide before submitting a change. More information.Install containerd. For more information see containerd's installation documentation and for specific prerequisite follow the containerd guide.Drain the nodekubectl drain  \--ignore-daemonsets Replace  with the name of your node you are draining.Stop the Docker daemonsystemctl stop kubelet systemctl disable docker.service --now Install ContainerdFollow the guide for detailed steps to install containerd.LinuxWindows (PowerShell)Install the containerd.io package from the official Docker repositories. Instructions for setting up the Docker repository for your respective Linux distribution and installing the containerd.io package can be found at Getting started with containerd.Configure containerd:sudo mkdir -p /etc/containerd containerd config default | sudo tee /etc/containerd/config.toml Restart containerd:sudo systemctl restart containerd Start a Powershell session, set $Version to the desired version (ex: $Version="1.4.3"), and then run the following commands:Download containerd:curl.exe -L https://github.com/containerd/containerd/releases/download/v$Version/containerd-$Version-windows-amd64.tar.gz -o containerd-windows-amd64.tar.gz tar.exe xvf .\containerd-windows-amd64.tar.gz Extract and configure:Copy-Item -Path ".\bin\" -Destination "$Env:ProgramFiles\containerd" -Recurse -Force cd $Env:ProgramFiles\containerd\ .\containerd.exe config default | Out-File config.toml -Encoding ascii # Review the configuration. Depending on setup you may want to adjust: # - the sandbox_image (Kubernetes pause image) # - cni bin_dir and conf_dir locations Get-Content config.toml # (Optional - but highly recommended) Exclude containerd from Windows Defender Scans Add-MpPreference -ExclusionProcess "$Env:ProgramFiles\containerd\containerd.exe" Start containerd:.\containerd.exe --register-service Start-Service containerd Configure the kubelet to use containerd as its container runtimeEdit the file /var/lib/kubelet/kubeadm-flags.env and add the containerd runtime to the flags; \--container-runtime-endpoint=unix:///run/containerd/containerd.sock.Users using kubeadm should be aware that the kubeadm tool stores the CRI socket for each host as an annotation in the Node object for that host. To change it you can execute the following command on a machine that has the kubeadm /etc/kubernetes/admin.conf file.kubectl edit no  This will start a text editor where you can edit the Node object. To choose a text editor you can set the KUBE_EDITOR environment variable.Change the value of kubeadm.alpha.kubernetes.io/cri-socket from /var/run/dockershim.sock to the CRI socket path of your choice (for example unix:///run/containerd/containerd.sock).Note that new CRI socket paths must be prefixed with unix:// ideally.Save the changes in the text editor, which will update the Node object.Restart the kubeletsystemctl start kubelet Verify that the node is healthyRun kubectl get nodes -o wide and containerd appears as the runtime for the node we just changed.Remove Docker EngineNote: This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the content guide before submitting a change. More information.If the node appears healthy, remove Docker.CentOSDebianFedoraUbuntusudo yum remove docker-ce docker-ce-cli sudo apt-get purge docker-ce docker-ce-cli sudo dnf remove docker-ce docker-ce-cli sudo apt-get purge docker-ce docker-ce-cli The preceding commands don't remove images, containers, volumes, or customized configuration files on your host. To delete them, follow Docker's instructions to Uninstall Docker Engine.Caution:Docker's instructions for uninstalling Docker Engine create a risk of deleting containerd. Be careful when executing commands.Uncordon the nodekubectl uncordon  Replace  with the name of your node you previously drained.4.2.3.2 - Find Out What Container Runtime is Used on a NodeThis page outlines steps to find out what container runtime the nodes in your cluster use.Depending on the way you run your cluster, the container runtime for the nodes may have been pre-configured or you need to configure it. If you're using a managed Kubernetes service, there might be vendor-specific ways to check what container runtime is configured for the nodes. The method described on this page should work whenever the execution of kubectl is allowed.Before you beginInstall and configure kubectl. See Install Tools section for details.Find out the container runtime used on a NodeUse kubectl to fetch and show node information:kubectl get nodes -o wide The output is similar to the following. The column CONTAINER-RUNTIME outputs the runtime and its version.For Docker Engine, the output is similar to this:NAME STATUS VERSION CONTAINER-RUNTIME node-1 Ready v1.16.15 docker://19.3.1 node-2 Ready v1.16.15 docker://19.3.1 node-3 Ready v1.16.15 docker://19.3.1 If your runtime shows as Docker Engine, you still might not be affected by the removal of dockershim in Kubernetes v1.24. Check the runtime endpoint to see if you use dockershim. If you don't use dockershim, you aren't affected.For containerd, the output is similar to this:NAME STATUS VERSION CONTAINER-RUNTIME node-1 Ready v1.19.6 containerd://1.4.1 node-2 Ready v1.19.6 containerd://1.4.1 node-3 Ready v1.19.6 containerd://1.4.1 Find out more information about container runtimes on Container Runtimes page.Find out what container runtime endpoint you useThe container runtime talks to the kubelet over a Unix socket using the CRI protocol, which is based on the gRPC framework. The kubelet acts as a client, and the runtime acts as the server. In some cases, you might find it useful to know which socket your nodes use. For example, with the removal of dockershim in Kubernetes v1.24 and later, you might want to know whether you use Docker Engine with dockershim.Note:If you currently use Docker Engine in your nodes with cri-dockerd, you aren't affected by the dockershim removal.You can check which socket you use by checking the kubelet configuration on your nodes.Read the starting commands for the kubelet process:tr \\\0 ' ' < /proc/"$(pgrep kubelet)"/cmdline If you don't have tr or pgrep, check the command line for the kubelet process manually.In the output, look for the --container-runtime flag and the \--container-runtime-endpoint flag.If your nodes use Kubernetes v1.23 and earlier and these flags aren't present or if the --container-runtime flag is not remote, you use the dockershim socket with Docker Engine. The --container-runtime command line argument is not available in Kubernetes v1.27 and later.If the --container-runtime-endpoint flag is present, check the socket name to find out which runtime you use. For example, unix:///run/containerd/containerd.sock is the containerd endpoint.If you want to change the Container Runtime on a Node from Docker Engine to containerd, you can find out more information on migrating from Docker Engine to containerd, or, if you want to continue using Docker Engine in Kubernetes v1.24 and later, migrate to a CRI-compatible adapter like cri-dockerd.4.2.3.3 - Troubleshooting CNI plugin-related errorsTo avoid CNI plugin-related errors, verify that you are using or upgrading to a container runtime that has been tested to work correctly with your version of Kubernetes.About the "Incompatible CNI versions" and "Failed to destroy network for sandbox" errorsService issues exist for pod CNI network setup and tear down in containerd v1.6.0-v1.6.3 when the CNI plugins have not been upgraded and/or the CNI config version is not declared in the CNI config files. The containerd team reports, "these issues are resolved in containerd v1.6.4."With containerd v1.6.0-v1.6.3, if you do not upgrade the CNI plugins and/or declare the CNI config version, you might encounter the following "Incompatible CNI versions" or "Failed to destroy network for sandbox" error conditions.Incompatible CNI versions errorIf the version of your CNI plugin does not correctly match the plugin version in the config because the config version is later than the plugin version, the containerd log will likely show an error message on startup of a pod similar to:incompatible CNI versions; config is \"1.0.0\", plugin supports [\"0.1.0\" \"0.2.0\" \"0.3.0\" \"0.3.1\" \"0.4.0\"]" To fix this issue, update your CNI plugins and CNI config files.Failed to destroy network for sandbox errorIf the version of the plugin is missing in the CNI plugin config, the pod may run. However, stopping the pod generates an error similar to:ERROR[2022-04-26T00:43:24.518165483Z] StopPodSandbox for "b" failed error="failed to destroy network for sandbox \"bbc85f891eaf060c5a879e27bba9b6b06450210161dfdecfbb2732959fb6500a\": invalid version \"\": the version is empty" This error leaves the pod in the not-ready state with a network namespace still attached. To recover from this problem, edit the CNI config file to add the missing version information. The next attempt to stop the pod should be successful.Updating your CNI plugins and CNI config filesIf you're using containerd v1.6.0-v1.6.3 and encountered "Incompatible CNI versions" or "Failed to destroy network for sandbox" errors, consider updating your CNI plugins and editing the CNI config files.Here's an overview of the typical steps for each node:Safely drain and cordon the node.After stopping your container runtime and kubelet services, perform the following upgrade operations:If you're running CNI plugins, upgrade them to the latest version.If you're using non-CNI plugins, replace them with CNI plugins. Use the latest version of the plugins.Update the plugin configuration file to specify or match a version of the CNI specification that the plugin supports, as shown in the following "An example containerd configuration file" section.For containerd, ensure that you have installed the latest version (v1.0.0 or later) of the CNI loopback plugin.Upgrade node components (for example, the kubelet) to Kubernetes v1.24Upgrade to or install the most current version of the container runtime.Bring the node back into your cluster by restarting your container runtime and kubelet. Uncordon the node (kubectl uncordon ).An example containerd configuration fileThe following example shows a configuration for containerd runtime v1.6.x, which supports a recent version of the CNI specification (v1.0.0).Please see the documentation from your plugin and networking provider for further instructions on configuring your system.On Kubernetes, containerd runtime adds a loopback interface, lo, to pods as a default behavior. The containerd runtime configures the loopback interface via a CNI plugin, loopback. The loopback plugin is distributed as part of the containerd release packages that have the cni designation. containerd v1.6.0 and later includes a CNI v1.0.0-compatible loopback plugin as well as other default CNI plugins. The configuration for the loopback plugin is done internally by containerd, and is set to use CNI v1.0.0. This also means that the version of the loopback plugin must be v1.0.0 or later when this newer version containerd is started.The following bash command generates an example CNI config. Here, the 1.0.0 value for the config version is assigned to the cniVersion field for use when containerd invokes the CNI bridge plugin.cat << EOF | tee /etc/cni/net.d/10-containerd-net.conflist { "cniVersion": "1.0.0", "name": "containerd-net", "plugins": [ { "type": "bridge", "bridge": "cni0", "isGateway": true, "ipMasq": true, "promiscMode": true, "ipam": { "type": "host-local", "ranges": [ [{ "subnet": "10.88.0.0/16" }], [{ "subnet": "2001:db8:4860::/64" }] ], "routes": [ { "dst": "0.0.0.0/0" }, { "dst": "::/0" } ] } }, { "type": "portmap", "capabilities": {"portMappings": true}, "externalSetMarkChain": "KUBE-MARK-MASQ" } ] } EOF Update the IP address ranges in the preceding example with ones that are based on your use case and network addressing plan.4.2.3.4 - Check whether dockershim removal affects youThe dockershim component of Kubernetes allows the use of Docker as a Kubernetes's container runtime. Kubernetes' built-in dockershim component was removed in release v1.24.This page explains how your cluster could be using Docker as a container runtime, provides details on the role that dockershim plays when in use, and shows steps you can take to check whether any workloads could be affected by dockershim removal.Finding if your app has a dependencies on DockerIf you are using Docker for building your application containers, you can still run these containers on any container runtime. This use of Docker does not count as a dependency on Docker as a container runtime.When alternative container runtime is used, executing Docker commands may either not work or yield unexpected output. This is how you can find whether you have a dependency on Docker:Make sure no privileged Pods execute Docker commands (like docker ps), restart the Docker service (commands such as systemctl restart docker.service), or modify Docker-specific files such as /etc/docker/daemon.json.Check for any private registries or image mirror settings in the Docker configuration file (like /etc/docker/daemon.json). Those typically need to be reconfigured for another container runtime.Check that scripts and apps running on nodes outside of your Kubernetes infrastructure do not execute Docker commands. It might be:SSH to nodes to troubleshoot;Node startup scripts;Monitoring and security agents installed on nodes directly.Third-party tools that perform above mentioned privileged operations. See Migrating telemetry and security agents from dockershim for more information.Make sure there are no indirect dependencies on dockershim behavior. This is an edge case and unlikely to affect your application. Some tooling may be configured to react to Docker-specific behaviors, for example, raise alert on specific metrics or search for a specific log message as part of troubleshooting instructions. If you have such tooling configured, test the behavior on a test cluster before migration.Dependency on Docker explainedA container runtime is software that can execute the containers that make up a Kubernetes pod. Kubernetes is responsible for orchestration and scheduling of Pods; on each node, the kubelet uses the container runtime interface as an abstraction so that you can use any compatible container runtime.In its earliest releases, Kubernetes offered compatibility with one container runtime: Docker. Later in the Kubernetes project's history, cluster operators wanted to adopt additional container runtimes. The CRI was designed to allow this kind of flexibility - and the kubelet began supporting CRI. However, because Docker existed before the CRI specification was invented, the Kubernetes project created an adapter component, dockershim. The dockershim adapter allows the kubelet to interact with Docker as if Docker were a CRI compatible runtime.You can read about it in Kubernetes Containerd integration goes GA blog post.Switching to Containerd as a container runtime eliminates the middleman. All the same containers can be run by container runtimes like Containerd as before. But now, since containers schedule directly with the container runtime, they are not visible to Docker. So any Docker tooling or fancy UI you might have used before to check on these containers is no longer available.You cannot get container information using docker ps or docker inspect commands. As you cannot list containers, you cannot get logs, stop containers, or execute something inside a container using docker exec.Note:If you're running workloads via Kubernetes, the best way to stop a container is through the Kubernetes API rather than directly through the container runtime (this advice applies for all container runtimes, not only Docker).You can still pull images or build them using docker build command. But images built or pulled by Docker would not be visible to container runtime and Kubernetes. They needed to be pushed to some registry to allow them to be used by Kubernetes.Known issuesSome filesystem metrics are missing and the metrics format is differentThe Kubelet /metrics/cadvisor endpoint provides Prometheus metrics, as documented in Metrics for Kubernetes system components. If you install a metrics collector that depends on that endpoint, you might see the following issues:The metrics format on the Docker node is k8s_____ but the format on other runtime is different. For example, on containerd node it is .Some filesystem metrics are missing, as follows:container_fs_inodes_free container_fs_inodes_total container_fs_io_current container_fs_io_time_seconds_total container_fs_io_time_weighted_seconds_total container_fs_limit_bytes container_fs_read_seconds_total container_fs_reads_merged_total container_fs_sector_reads_total container_fs_sector_writes_total container_fs_usage_bytes container_fs_write_seconds_total container_fs_writes_merged_total WorkaroundYou can mitigate this issue by using cAdvisor as a standalone daemonset.Find the latest cAdvisor release with the name pattern vX.Y.Z-containerd-cri (for example, v0.42.0-containerd-cri).Follow the steps in cAdvisor Kubernetes Daemonset to create the daemonset.Point the installed metrics collector to use the cAdvisor /metrics endpoint which provides the full set of Prometheus container metrics.Alternatives:Use alternative third party metrics collection solution.Collect metrics from the Kubelet summary API that is served at /stats/summary.What's nextRead Migrating from dockershim to understand your next stepsRead the dockershim deprecation FAQ article for more information.4.2.3.5 - Migrating telemetry and security agents from dockershimNote: This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the content guide before submitting a change. More information.Kubernetes' support for direct integration with Docker Engine is deprecated and has been removed. Most apps do not have a direct dependency on runtime hosting containers. However, there are still a lot of telemetry and monitoring agents that have a dependency on Docker to collect containers metadata, logs, and metrics. This document aggregates information on how to detect these dependencies as well as links on how to migrate these agents to use generic tools or alternative runtimes.Telemetry and security agentsWithin a Kubernetes cluster there are a few different ways to run telemetry or security agents. Some agents have a direct dependency on Docker Engine when they run as DaemonSets or directly on nodes.Why do some telemetry agents communicate with Docker Engine?Historically, Kubernetes was written to work specifically with Docker Engine. Kubernetes took care of networking and scheduling, relying on Docker Engine for launching and running containers (within Pods) on a node. Some information that is relevant to telemetry, such as a pod name, is only available from Kubernetes components. Other data, such as container metrics, is not the responsibility of the container runtime. Early telemetry agents needed to query the container runtime and Kubernetes to report an accurate picture. Over time, Kubernetes gained the ability to support multiple runtimes, and now supports any runtime that is compatible with the container runtime interface.Some telemetry agents rely specifically on Docker Engine tooling. For example, an agent might run a command such as docker ps or docker top to list containers and processes or docker logs to receive streamed logs. If nodes in your existing cluster use Docker Engine, and you switch to a different container runtime, these commands will not work any longer.Identify DaemonSets that depend on Docker EngineIf a pod wants to make calls to the dockerd running on the node, the pod must either:mount the filesystem containing the Docker daemon's privileged socket, as a volume; ormount the specific path of the Docker daemon's privileged socket directly, also as a volume.For example: on COS images, Docker exposes its Unix domain socket at /var/run/docker.sock This means that the pod spec will include a hostPath volume mount of /var/run/docker.sock.Here's a sample shell script to find Pods that have a mount directly mapping the Docker socket. This script outputs the namespace and name of the pod. You can remove the grep '/var/run/docker.sock' to review other mounts.kubectl get pods --all-namespaces \ -o=jsonpath='{range .items[*]}{"\n"}{.metadata.namespace}{":\t"}{.metadata.name}{":\t"}{range .spec.volumes[*]}{.hostPath.path}{", "}{end}{end}' \ | sort \ | grep '/var/run/docker.sock' Note:There are alternative ways for a pod to access Docker on the host. For instance, the parent directory /var/run may be mounted instead of the full path (like in this example). The script above only detects the most common uses.Detecting Docker dependency from node agentsIf your cluster nodes are customized and install additional security and telemetry agents on the node, check with the agent vendor to verify whether it has any dependency on Docker.Telemetry and security agent vendorsThis section is intended to aggregate information about various telemetry and security agents that may have a dependency on container runtimes.We keep the work in progress version of migration instructions for various telemetry and security agent vendors in Google doc. Please contact the vendor to get up to date instructions for migrating from dockershim.Migration from dockershimAquaNo changes are needed: everything should work seamlessly on the runtime switch.DatadogHow to migrate: Docker deprecation in Kubernetes The pod that accesses Docker Engine may have a name containing any of:datadog-agentdatadogdd-agentDynatraceHow to migrate: Migrating from Docker-only to generic container metrics in DynatraceContainerd support announcement: Get automated full-stack visibility into containerd-based Kubernetes environmentsCRI-O support announcement: Get automated full-stack visibility into your CRI-O Kubernetes containers (Beta)The pod accessing Docker may have name containing:dynatrace-oneagentFalcoHow to migrate:Migrate Falco from dockershim Falco supports any CRI-compatible runtime (containerd is used in the default configuration); the documentation explains all details. The pod accessing Docker may have name containing:falcoPrisma Cloud ComputeCheck documentation for Prisma Cloud, under the "Install Prisma Cloud on a CRI (non-Docker) cluster" section. The pod accessing Docker may be named like:twistlock-defender-dsSignalFx (Splunk)The SignalFx Smart Agent (deprecated) uses several different monitors for Kubernetes including kubernetes-cluster, kubelet-stats/kubelet-metrics, and docker-container-stats. The kubelet-stats monitor was previously deprecated by the vendor, in favor of kubelet-metrics. The docker-container-stats monitor is the one affected by dockershim removal. Do not use the docker-container-stats with container runtimes other than Docker Engine.How to migrate from dockershim-dependent agent:Remove docker-container-stats from the list of configured monitors. Note, keeping this monitor enabled with non-dockershim runtime will result in incorrect metrics being reported when docker is installed on node and no metrics when docker is not installed.Enable and configure kubelet-metrics monitor.Note:The set of collected metrics will change. Review your alerting rules and dashboards.The Pod accessing Docker may be named something like:signalfx-agentYahoo Kubectl FlameFlame does not support container runtimes other than Docker. See https://github.com/yahoo/kubectl-flame/issues/514.2.4 - Generate Certificates ManuallyWhen using client certificate authentication, you can generate certificates manually through easyrsa, openssl or cfssl.easyrsaeasyrsa can manually generate certificates for your cluster.Download, unpack, and initialize the patched version of easyrsa3.curl -LO https://dl.k8s.io/easy-rsa/easy-rsa.tar.gz tar xzf easy-rsa.tar.gz cd easy-rsa-master/easyrsa3 ./easyrsa init-pki Generate a new certificate authority (CA). --batch sets automatic mode; \--req-cn specifies the Common Name (CN) for the CA's new root certificate../easyrsa --batch "--req-cn=${MASTER_IP}@`date +%s`" build-ca nopass Generate server certificate and key.The argument --subject-alt-name sets the possible IPs and DNS names the API server will be accessed with. The MASTER_CLUSTER_IP is usually the first IP from the service CIDR that is specified as the --service-cluster-ip-range argument for both the API server and the controller manager component. The argument --days is used to set the number of days after which the certificate expires. The sample below also assumes that you are using cluster.local as the default DNS domain name../easyrsa --subject-alt-name="IP:${MASTER_IP},"\ "IP:${MASTER_CLUSTER_IP},"\ "DNS:kubernetes,"\ "DNS:kubernetes.default,"\ "DNS:kubernetes.default.svc,"\ "DNS:kubernetes.default.svc.cluster,"\ "DNS:kubernetes.default.svc.cluster.local" \ \--days=10000 \ build-server-full server nopass Copy pki/ca.crt, pki/issued/server.crt, and pki/private/server.key to your directory.Fill in and add the following parameters into the API server start parameters:--client-ca-file=/yourdirectory/ca.crt \--tls-cert-file=/yourdirectory/server.crt \--tls-private-key-file=/yourdirectory/server.key opensslopenssl can manually generate certificates for your cluster.Generate a ca.key with 2048bit:openssl genrsa -out ca.key 2048 According to the ca.key generate a ca.crt (use -days to set the certificate effective time):openssl req -x509 -new -nodes -key ca.key -subj "/CN=${MASTER_IP}" -days 10000 -out ca.crt Generate a server.key with 2048bit:openssl genrsa -out server.key 2048 Create a config file for generating a Certificate Signing Request (CSR).Be sure to substitute the values marked with angle brackets (e.g. ) with real values before saving this to a file (e.g. csr.conf). Note that the value for MASTER_CLUSTER_IP is the service cluster IP for the API server as described in previous subsection. The sample below also assumes that you are using cluster.local as the default DNS domain name.[ req ] default_bits = 2048 prompt = no default_md = sha256 req_extensions = req_ext distinguished_name = dn [ dn ] C =  ST =  L =  O =  OU =  CN =  [ req_ext ] subjectAltName = @alt_names [ alt_names ] DNS.1 = kubernetes DNS.2 = kubernetes.default DNS.3 = kubernetes.default.svc DNS.4 = kubernetes.default.svc.cluster DNS.5 = kubernetes.default.svc.cluster.local IP.1 =  IP.2 =  [ v3_ext ] authorityKeyIdentifier=keyid,issuer:always basicConstraints=CA:FALSE keyUsage=keyEncipherment,dataEncipherment extendedKeyUsage=serverAuth,clientAuth subjectAltName=@alt_names Generate the certificate signing request based on the config file:openssl req -new -key server.key -out server.csr -config csr.conf Generate the server certificate using the ca.key, ca.crt and server.csr:openssl x509 -req -in server.csr -CA ca.crt -CAkey ca.key \ -CAcreateserial -out server.crt -days 10000 \ -extensions v3_ext -extfile csr.conf -sha256 View the certificate signing request:openssl req -noout -text -in ./server.csr View the certificate:openssl x509 -noout -text -in ./server.crt Finally, add the same parameters into the API server start parameters.cfsslcfssl is another tool for certificate generation.Download, unpack and prepare the command line tools as shown below.Note that you may need to adapt the sample commands based on the hardware architecture and cfssl version you are using.curl -L https://github.com/cloudflare/cfssl/releases/download/v1.5.0/cfssl_1.5.0_linux_amd64 -o cfssl chmod +x cfssl curl -L https://github.com/cloudflare/cfssl/releases/download/v1.5.0/cfssljson_1.5.0_linux_amd64 -o cfssljson chmod +x cfssljson curl -L https://github.com/cloudflare/cfssl/releases/download/v1.5.0/cfssl-certinfo_1.5.0_linux_amd64 -o cfssl-certinfo chmod +x cfssl-certinfo Create a directory to hold the artifacts and initialize cfssl:mkdir cert cd cert ../cfssl print-defaults config > config.json ../cfssl print-defaults csr > csr.json Create a JSON config file for generating the CA file, for example, ca-config.json:{ "signing": { "default": { "expiry": "8760h" }, "profiles": { "kubernetes": { "usages": [ "signing", "key encipherment", "server auth", "client auth" ], "expiry": "8760h" } } } } Create a JSON config file for CA certificate signing request (CSR), for example, ca-csr.json. Be sure to replace the values marked with angle brackets with real values you want to use.{ "CN": "kubernetes", "key": { "algo": "rsa", "size": 2048 }, "names":[{ "C": "", "ST": "", "L": "", "O": "", "OU": "" }] } Generate CA key (ca-key.pem) and certificate (ca.pem):../cfssl gencert -initca ca-csr.json | ../cfssljson -bare ca Create a JSON config file for generating keys and certificates for the API server, for example, server-csr.json. Be sure to replace the values in angle brackets with real values you want to use. The  is the service cluster IP for the API server as described in previous subsection. The sample below also assumes that you are using cluster.local as the default DNS domain name.{ "CN": "kubernetes", "hosts": [ "127.0.0.1", "", "", "kubernetes", "kubernetes.default", "kubernetes.default.svc", "kubernetes.default.svc.cluster", "kubernetes.default.svc.cluster.local" ], "key": { "algo": "rsa", "size": 2048 }, "names": [{ "C": "", "ST": "", "L": "", "O": "", "OU": "" }] } Generate the key and certificate for the API server, which are by default saved into file server-key.pem and server.pem respectively:../cfssl gencert -ca=ca.pem -ca-key=ca-key.pem \ \--config=ca-config.json -profile=kubernetes \ server-csr.json | ../cfssljson -bare server Distributing Self-Signed CA CertificateA client node may refuse to recognize a self-signed CA certificate as valid. For a non-production deployment, or for a deployment that runs behind a company firewall, you can distribute a self-signed CA certificate to all clients and refresh the local list for valid certificates.On each client, perform the following operations:sudo cp ca.crt /usr/local/share/ca-certificates/kubernetes.crt sudo update-ca-certificates Updating certificates in /etc/ssl/certs... 1 added, 0 removed; done. Running hooks in /etc/ca-certificates/update.d.... done. Certificates APIYou can use the certificates.k8s.io API to provision x509 certificates to use for authentication as documented in the Managing TLS in a cluster task page.4.2.5 - Manage Memory, CPU, and API Resources4.2.5.1 - Configure Default Memory Requests and Limits for a NamespaceDefine a default memory resource limit for a namespace, so that every new Pod in that namespace has a memory resource limit configured.This page shows how to configure default memory requests and limits for a namespace.A Kubernetes cluster can be divided into namespaces. Once you have a namespace that has a default memory limit, and you then try to create a Pod with a container that does not specify its own memory limit, then the control plane assigns the default memory limit to that container.Kubernetes assigns a default memory request under certain conditions that are explained later in this topic.Before you beginYou need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesYou must have access to create namespaces in your cluster.Each node in your cluster must have at least 2 GiB of memory.Create a namespaceCreate a namespace so that the resources you create in this exercise are isolated from the rest of your cluster.kubectl create namespace default-mem-example Create a LimitRange and a PodHere's a manifest for an example LimitRange. The manifest specifies a default memory request and a default memory limit.admin/resource/memory-defaults.yaml apiVersion: v1 kind: LimitRange metadata: name: mem-limit-range spec: limits: \- default: memory: 512Mi defaultRequest: memory: 256Mi type: Container Create the LimitRange in the default-mem-example namespace:kubectl apply -f https://k8s.io/examples/admin/resource/memory-defaults.yaml --namespace=default-mem-example Now if you create a Pod in the default-mem-example namespace, and any container within that Pod does not specify its own values for memory request and memory limit, then the control plane applies default values: a memory request of 256MiB and a memory limit of 512MiB.Here's an example manifest for a Pod that has one container. The container does not specify a memory request and limit.admin/resource/memory-defaults-pod.yaml apiVersion: v1 kind: Pod metadata: name: default-mem-demo spec: containers: \- name: default-mem-demo-ctr image: nginx Create the Pod.kubectl apply -f https://k8s.io/examples/admin/resource/memory-defaults-pod.yaml --namespace=default-mem-example View detailed information about the Pod:kubectl get pod default-mem-demo --output=yaml --namespace=default-mem-example The output shows that the Pod's container has a memory request of 256 MiB and a memory limit of 512 MiB. These are the default values specified by the LimitRange.containers: \- image: nginx imagePullPolicy: Always name: default-mem-demo-ctr resources: limits: memory: 512Mi requests: memory: 256Mi Delete your Pod:kubectl delete pod default-mem-demo --namespace=default-mem-example What if you specify a container's limit, but not its request?Here's a manifest for a Pod that has one container. The container specifies a memory limit, but not a request:admin/resource/memory-defaults-pod-2.yaml apiVersion: v1 kind: Pod metadata: name: default-mem-demo-2 spec: containers: \- name: default-mem-demo-2-ctr image: nginx resources: limits: memory: "1Gi" Create the Pod:kubectl apply -f https://k8s.io/examples/admin/resource/memory-defaults-pod-2.yaml --namespace=default-mem-example View detailed information about the Pod:kubectl get pod default-mem-demo-2 --output=yaml --namespace=default-mem-example The output shows that the container's memory request is set to match its memory limit. Notice that the container was not assigned the default memory request value of 256Mi.resources: limits: memory: 1Gi requests: memory: 1Gi What if you specify a container's request, but not its limit?Here's a manifest for a Pod that has one container. The container specifies a memory request, but not a limit:admin/resource/memory-defaults-pod-3.yaml apiVersion: v1 kind: Pod metadata: name: default-mem-demo-3 spec: containers: \- name: default-mem-demo-3-ctr image: nginx resources: requests: memory: "128Mi" Create the Pod:kubectl apply -f https://k8s.io/examples/admin/resource/memory-defaults-pod-3.yaml --namespace=default-mem-example View the Pod's specification:kubectl get pod default-mem-demo-3 --output=yaml --namespace=default-mem-example The output shows that the container's memory request is set to the value specified in the container's manifest. The container is limited to use no more than 512MiB of memory, which matches the default memory limit for the namespace.resources: limits: memory: 512Mi requests: memory: 128Mi Note:A LimitRange does not check the consistency of the default values it applies. This means that a default value for the limit that is set by LimitRange may be less than the request value specified for the container in the spec that a client submits to the API server. If that happens, the final Pod will not be scheduleable. See Constraints on resource limits and requests for more details.Motivation for default memory limits and requestsIf your namespace has a memory resource quota configured, it is helpful to have a default value in place for memory limit. Here are three of the restrictions that a resource quota imposes on a namespace:For every Pod that runs in the namespace, the Pod and each of its containers must have a memory limit. (If you specify a memory limit for every container in a Pod, Kubernetes can infer the Pod-level memory limit by adding up the limits for its containers).Memory limits apply a resource reservation on the node where the Pod in question is scheduled. The total amount of memory reserved for all Pods in the namespace must not exceed a specified limit.The total amount of memory actually used by all Pods in the namespace must also not exceed a specified limit.When you add a LimitRange:If any Pod in that namespace that includes a container does not specify its own memory limit, the control plane applies the default memory limit to that container, and the Pod can be allowed to run in a namespace that is restricted by a memory ResourceQuota.Clean upDelete your namespace:kubectl delete namespace default-mem-example What's nextFor cluster administratorsConfigure Default CPU Requests and Limits for a NamespaceConfigure Minimum and Maximum Memory Constraints for a NamespaceConfigure Minimum and Maximum CPU Constraints for a NamespaceConfigure Memory and CPU Quotas for a NamespaceConfigure a Pod Quota for a NamespaceConfigure Quotas for API ObjectsFor app developersAssign Memory Resources to Containers and PodsAssign CPU Resources to Containers and PodsAssign Pod-level CPU and memory resourcesConfigure Quality of Service for Pods4.2.5.2 - Configure Default CPU Requests and Limits for a NamespaceDefine a default CPU resource limits for a namespace, so that every new Pod in that namespace has a CPU resource limit configured.This page shows how to configure default CPU requests and limits for a namespace.A Kubernetes cluster can be divided into namespaces. If you create a Pod within a namespace that has a default CPU limit, and any container in that Pod does not specify its own CPU limit, then the control plane assigns the default CPU limit to that container.Kubernetes assigns a default CPU request, but only under certain conditions that are explained later in this page.Before you beginYou need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesYou must have access to create namespaces in your cluster.If you're not already familiar with what Kubernetes means by 1.0 CPU, read meaning of CPU.Create a namespaceCreate a namespace so that the resources you create in this exercise are isolated from the rest of your cluster.kubectl create namespace default-cpu-example Create a LimitRange and a PodHere's a manifest for an example LimitRange. The manifest specifies a default CPU request and a default CPU limit.admin/resource/cpu-defaults.yaml apiVersion: v1 kind: LimitRange metadata: name: cpu-limit-range spec: limits: \- default: cpu: 1 defaultRequest: cpu: 0.5 type: Container Create the LimitRange in the default-cpu-example namespace:kubectl apply -f https://k8s.io/examples/admin/resource/cpu-defaults.yaml --namespace=default-cpu-example Now if you create a Pod in the default-cpu-example namespace, and any container in that Pod does not specify its own values for CPU request and CPU limit, then the control plane applies default values: a CPU request of 0.5 and a default CPU limit of 1.Here's a manifest for a Pod that has one container. The container does not specify a CPU request and limit.admin/resource/cpu-defaults-pod.yaml apiVersion: v1 kind: Pod metadata: name: default-cpu-demo spec: containers: \- name: default-cpu-demo-ctr image: nginx Create the Pod.kubectl apply -f https://k8s.io/examples/admin/resource/cpu-defaults-pod.yaml --namespace=default-cpu-example View the Pod's specification:kubectl get pod default-cpu-demo --output=yaml --namespace=default-cpu-example The output shows that the Pod's only container has a CPU request of 500m cpu (which you can read as “500 millicpu”), and a CPU limit of 1 cpu. These are the default values specified by the LimitRange.containers: \- image: nginx imagePullPolicy: Always name: default-cpu-demo-ctr resources: limits: cpu: "1" requests: cpu: 500m What if you specify a container's limit, but not its request?Here's a manifest for a Pod that has one container. The container specifies a CPU limit, but not a request:admin/resource/cpu-defaults-pod-2.yaml apiVersion: v1 kind: Pod metadata: name: default-cpu-demo-2 spec: containers: \- name: default-cpu-demo-2-ctr image: nginx resources: limits: cpu: "1" Create the Pod:kubectl apply -f https://k8s.io/examples/admin/resource/cpu-defaults-pod-2.yaml --namespace=default-cpu-example View the specification of the Pod that you created:kubectl get pod default-cpu-demo-2 --output=yaml --namespace=default-cpu-example The output shows that the container's CPU request is set to match its CPU limit. Notice that the container was not assigned the default CPU request value of 0.5 cpu:resources: limits: cpu: "1" requests: cpu: "1" What if you specify a container's request, but not its limit?Here's an example manifest for a Pod that has one container. The container specifies a CPU request, but not a limit:admin/resource/cpu-defaults-pod-3.yaml apiVersion: v1 kind: Pod metadata: name: default-cpu-demo-3 spec: containers: \- name: default-cpu-demo-3-ctr image: nginx resources: requests: cpu: "0.75" Create the Pod:kubectl apply -f https://k8s.io/examples/admin/resource/cpu-defaults-pod-3.yaml --namespace=default-cpu-example View the specification of the Pod that you created:kubectl get pod default-cpu-demo-3 --output=yaml --namespace=default-cpu-example The output shows that the container's CPU request is set to the value you specified at the time you created the Pod (in other words: it matches the manifest). However, the same container's CPU limit is set to 1 cpu, which is the default CPU limit for that namespace.resources: limits: cpu: "1" requests: cpu: 750m Motivation for default CPU limits and requestsIf your namespace has a CPU resource quota configured, it is helpful to have a default value in place for CPU limit. Here are two of the restrictions that a CPU resource quota imposes on a namespace:For every Pod that runs in the namespace, each of its containers must have a CPU limit.CPU limits apply a resource reservation on the node where the Pod in question is scheduled. The total amount of CPU that is reserved for use by all Pods in the namespace must not exceed a specified limit.When you add a LimitRange:If any Pod in that namespace that includes a container does not specify its own CPU limit, the control plane applies the default CPU limit to that container, and the Pod can be allowed to run in a namespace that is restricted by a CPU ResourceQuota.Clean upDelete your namespace:kubectl delete namespace default-cpu-example What's nextFor cluster administratorsConfigure Default Memory Requests and Limits for a NamespaceConfigure Minimum and Maximum Memory Constraints for a NamespaceConfigure Minimum and Maximum CPU Constraints for a NamespaceConfigure Memory and CPU Quotas for a NamespaceConfigure a Pod Quota for a NamespaceConfigure Quotas for API ObjectsFor app developersAssign Memory Resources to Containers and PodsAssign CPU Resources to Containers and PodsAssign Pod-level CPU and memory resourcesConfigure Quality of Service for Pods4.2.5.3 - Configure Minimum and Maximum Memory Constraints for a NamespaceDefine a range of valid memory resource limits for a namespace, so that every new Pod in that namespace falls within the range you configure.This page shows how to set minimum and maximum values for memory used by containers running in a namespace. You specify minimum and maximum memory values in a LimitRange object. If a Pod does not meet the constraints imposed by the LimitRange, it cannot be created in the namespace.Before you beginYou need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesYou must have access to create namespaces in your cluster.Each node in your cluster must have at least 1 GiB of memory available for Pods.Create a namespaceCreate a namespace so that the resources you create in this exercise are isolated from the rest of your cluster.kubectl create namespace constraints-mem-example Create a LimitRange and a PodHere's an example manifest for a LimitRange:admin/resource/memory-constraints.yaml apiVersion: v1 kind: LimitRange metadata: name: mem-min-max-demo-lr spec: limits: \- max: memory: 1Gi min: memory: 500Mi type: Container Create the LimitRange:kubectl apply -f https://k8s.io/examples/admin/resource/memory-constraints.yaml --namespace=constraints-mem-example View detailed information about the LimitRange:kubectl get limitrange mem-min-max-demo-lr --namespace=constraints-mem-example --output=yaml The output shows the minimum and maximum memory constraints as expected. But notice that even though you didn't specify default values in the configuration file for the LimitRange, they were created automatically. limits: \- default: memory: 1Gi defaultRequest: memory: 1Gi max: memory: 1Gi min: memory: 500Mi type: Container Now whenever you define a Pod within the constraints-mem-example namespace, Kubernetes performs these steps:If any container in that Pod does not specify its own memory request and limit, the control plane assigns the default memory request and limit to that container.Verify that every container in that Pod requests at least 500 MiB of memory.Verify that every container in that Pod requests no more than 1024 MiB (1 GiB) of memory.Here's a manifest for a Pod that has one container. Within the Pod spec, the sole container specifies a memory request of 600 MiB and a memory limit of 800 MiB. These satisfy the minimum and maximum memory constraints imposed by the LimitRange.admin/resource/memory-constraints-pod.yaml apiVersion: v1 kind: Pod metadata: name: constraints-mem-demo spec: containers: \- name: constraints-mem-demo-ctr image: nginx resources: limits: memory: "800Mi" requests: memory: "600Mi" Create the Pod:kubectl apply -f https://k8s.io/examples/admin/resource/memory-constraints-pod.yaml --namespace=constraints-mem-example Verify that the Pod is running and that its container is healthy:kubectl get pod constraints-mem-demo --namespace=constraints-mem-example View detailed information about the Pod:kubectl get pod constraints-mem-demo --output=yaml --namespace=constraints-mem-example The output shows that the container within that Pod has a memory request of 600 MiB and a memory limit of 800 MiB. These satisfy the constraints imposed by the LimitRange for this namespace:resources: limits: memory: 800Mi requests: memory: 600Mi Delete your Pod:kubectl delete pod constraints-mem-demo --namespace=constraints-mem-example Attempt to create a Pod that exceeds the maximum memory constraintHere's a manifest for a Pod that has one container. The container specifies a memory request of 800 MiB and a memory limit of 1.5 GiB.admin/resource/memory-constraints-pod-2.yaml apiVersion: v1 kind: Pod metadata: name: constraints-mem-demo-2 spec: containers: \- name: constraints-mem-demo-2-ctr image: nginx resources: limits: memory: "1.5Gi" requests: memory: "800Mi" Attempt to create the Pod:kubectl apply -f https://k8s.io/examples/admin/resource/memory-constraints-pod-2.yaml --namespace=constraints-mem-example The output shows that the Pod does not get created, because it defines a container that requests more memory than is allowed:Error from server (Forbidden): error when creating "examples/admin/resource/memory-constraints-pod-2.yaml": pods "constraints-mem-demo-2" is forbidden: maximum memory usage per Container is 1Gi, but limit is 1536Mi. Attempt to create a Pod that does not meet the minimum memory requestHere's a manifest for a Pod that has one container. That container specifies a memory request of 100 MiB and a memory limit of 800 MiB.admin/resource/memory-constraints-pod-3.yaml apiVersion: v1 kind: Pod metadata: name: constraints-mem-demo-3 spec: containers: \- name: constraints-mem-demo-3-ctr image: nginx resources: limits: memory: "800Mi" requests: memory: "100Mi" Attempt to create the Pod:kubectl apply -f https://k8s.io/examples/admin/resource/memory-constraints-pod-3.yaml --namespace=constraints-mem-example The output shows that the Pod does not get created, because it defines a container that requests less memory than the enforced minimum:Error from server (Forbidden): error when creating "examples/admin/resource/memory-constraints-pod-3.yaml": pods "constraints-mem-demo-3" is forbidden: minimum memory usage per Container is 500Mi, but request is 100Mi. Create a Pod that does not specify any memory request or limitHere's a manifest for a Pod that has one container. The container does not specify a memory request, and it does not specify a memory limit.admin/resource/memory-constraints-pod-4.yaml apiVersion: v1 kind: Pod metadata: name: constraints-mem-demo-4 spec: containers: \- name: constraints-mem-demo-4-ctr image: nginx Create the Pod:kubectl apply -f https://k8s.io/examples/admin/resource/memory-constraints-pod-4.yaml --namespace=constraints-mem-example View detailed information about the Pod:kubectl get pod constraints-mem-demo-4 --namespace=constraints-mem-example --output=yaml The output shows that the Pod's only container has a memory request of 1 GiB and a memory limit of 1 GiB. How did that container get those values?resources: limits: memory: 1Gi requests: memory: 1Gi Because your Pod did not define any memory request and limit for that container, the cluster applied a default memory request and limit from the LimitRange.This means that the definition of that Pod shows those values. You can check it using kubectl describe:# Look for the "Requests:" section of the output kubectl describe pod constraints-mem-demo-4 --namespace=constraints-mem-example At this point, your Pod might be running or it might not be running. Recall that a prerequisite for this task is that your Nodes have at least 1 GiB of memory. If each of your Nodes has only 1 GiB of memory, then there is not enough allocatable memory on any Node to accommodate a memory request of 1 GiB. If you happen to be using Nodes with 2 GiB of memory, then you probably have enough space to accommodate the 1 GiB request.Delete your Pod:kubectl delete pod constraints-mem-demo-4 --namespace=constraints-mem-example Enforcement of minimum and maximum memory constraintsThe maximum and minimum memory constraints imposed on a namespace by a LimitRange are enforced only when a Pod is created or updated. If you change the LimitRange, it does not affect Pods that were created previously.Motivation for minimum and maximum memory constraintsAs a cluster administrator, you might want to impose restrictions on the amount of memory that Pods can use. For example:Each Node in a cluster has 2 GiB of memory. You do not want to accept any Pod that requests more than 2 GiB of memory, because no Node in the cluster can support the request.A cluster is shared by your production and development departments. You want to allow production workloads to consume up to 8 GiB of memory, but you want development workloads to be limited to 512 MiB. You create separate namespaces for production and development, and you apply memory constraints to each namespace.Clean upDelete your namespace:kubectl delete namespace constraints-mem-example What's nextFor cluster administratorsConfigure Default Memory Requests and Limits for a NamespaceConfigure Default CPU Requests and Limits for a NamespaceConfigure Minimum and Maximum CPU Constraints for a NamespaceConfigure Memory and CPU Quotas for a NamespaceConfigure a Pod Quota for a NamespaceConfigure Quotas for API ObjectsFor app developersAssign Memory Resources to Containers and PodsAssign CPU Resources to Containers and PodsAssign Pod-level CPU and memory resourcesConfigure Quality of Service for Pods4.2.5.4 - Configure Minimum and Maximum CPU Constraints for a NamespaceDefine a range of valid CPU resource limits for a namespace, so that every new Pod in that namespace falls within the range you configure.This page shows how to set minimum and maximum values for the CPU resources used by containers and Pods in a namespace. You specify minimum and maximum CPU values in a LimitRange object. If a Pod does not meet the constraints imposed by the LimitRange, it cannot be created in the namespace.Before you beginYou need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesYou must have access to create namespaces in your cluster.Each node in your cluster must have at least 1.0 CPU available for Pods. See meaning of CPU to learn what Kubernetes means by “1 CPU”.Create a namespaceCreate a namespace so that the resources you create in this exercise are isolated from the rest of your cluster.kubectl create namespace constraints-cpu-example Create a LimitRange and a PodHere's a manifest for an example LimitRange:admin/resource/cpu-constraints.yaml apiVersion: v1 kind: LimitRange metadata: name: cpu-min-max-demo-lr spec: limits: \- max: cpu: "800m" min: cpu: "200m" type: Container Create the LimitRange:kubectl apply -f https://k8s.io/examples/admin/resource/cpu-constraints.yaml --namespace=constraints-cpu-example View detailed information about the LimitRange:kubectl get limitrange cpu-min-max-demo-lr --output=yaml --namespace=constraints-cpu-example The output shows the minimum and maximum CPU constraints as expected. But notice that even though you didn't specify default values in the configuration file for the LimitRange, they were created automatically.limits: \- default: cpu: 800m defaultRequest: cpu: 800m max: cpu: 800m min: cpu: 200m type: Container Now whenever you create a Pod in the constraints-cpu-example namespace (or some other client of the Kubernetes API creates an equivalent Pod), Kubernetes performs these steps:If any container in that Pod does not specify its own CPU request and limit, the control plane assigns the default CPU request and limit to that container.Verify that every container in that Pod specifies a CPU request that is greater than or equal to 200 millicpu.Verify that every container in that Pod specifies a CPU limit that is less than or equal to 800 millicpu.Note:When creating a LimitRange object, you can specify limits on huge-pages or GPUs as well. However, when both default and defaultRequest are specified on these resources, the two values must be the same.Here's a manifest for a Pod that has one container. The container manifest specifies a CPU request of 500 millicpu and a CPU limit of 800 millicpu. These satisfy the minimum and maximum CPU constraints imposed by the LimitRange for this namespace.admin/resource/cpu-constraints-pod.yaml apiVersion: v1 kind: Pod metadata: name: constraints-cpu-demo spec: containers: \- name: constraints-cpu-demo-ctr image: nginx resources: limits: cpu: "800m" requests: cpu: "500m" Create the Pod:kubectl apply -f https://k8s.io/examples/admin/resource/cpu-constraints-pod.yaml --namespace=constraints-cpu-example Verify that the Pod is running and that its container is healthy:kubectl get pod constraints-cpu-demo --namespace=constraints-cpu-example View detailed information about the Pod:kubectl get pod constraints-cpu-demo --output=yaml --namespace=constraints-cpu-example The output shows that the Pod's only container has a CPU request of 500 millicpu and CPU limit of 800 millicpu. These satisfy the constraints imposed by the LimitRange.resources: limits: cpu: 800m requests: cpu: 500m Delete the Podkubectl delete pod constraints-cpu-demo --namespace=constraints-cpu-example Attempt to create a Pod that exceeds the maximum CPU constraintHere's a manifest for a Pod that has one container. The container specifies a CPU request of 500 millicpu and a cpu limit of 1.5 cpu.admin/resource/cpu-constraints-pod-2.yaml apiVersion: v1 kind: Pod metadata: name: constraints-cpu-demo-2 spec: containers: \- name: constraints-cpu-demo-2-ctr image: nginx resources: limits: cpu: "1.5" requests: cpu: "500m" Attempt to create the Pod:kubectl apply -f https://k8s.io/examples/admin/resource/cpu-constraints-pod-2.yaml --namespace=constraints-cpu-example The output shows that the Pod does not get created, because it defines an unacceptable container. That container is not acceptable because it specifies a CPU limit that is too large:Error from server (Forbidden): error when creating "examples/admin/resource/cpu-constraints-pod-2.yaml": pods "constraints-cpu-demo-2" is forbidden: maximum cpu usage per Container is 800m, but limit is 1500m. Attempt to create a Pod that does not meet the minimum CPU requestHere's a manifest for a Pod that has one container. The container specifies a CPU request of 100 millicpu and a CPU limit of 800 millicpu.admin/resource/cpu-constraints-pod-3.yaml apiVersion: v1 kind: Pod metadata: name: constraints-cpu-demo-3 spec: containers: \- name: constraints-cpu-demo-3-ctr image: nginx resources: limits: cpu: "800m" requests: cpu: "100m" Attempt to create the Pod:kubectl apply -f https://k8s.io/examples/admin/resource/cpu-constraints-pod-3.yaml --namespace=constraints-cpu-example The output shows that the Pod does not get created, because it defines an unacceptable container. That container is not acceptable because it specifies a CPU request that is lower than the enforced minimum:Error from server (Forbidden): error when creating "examples/admin/resource/cpu-constraints-pod-3.yaml": pods "constraints-cpu-demo-3" is forbidden: minimum cpu usage per Container is 200m, but request is 100m. Create a Pod that does not specify any CPU request or limitHere's a manifest for a Pod that has one container. The container does not specify a CPU request, nor does it specify a CPU limit.admin/resource/cpu-constraints-pod-4.yaml apiVersion: v1 kind: Pod metadata: name: constraints-cpu-demo-4 spec: containers: \- name: constraints-cpu-demo-4-ctr image: vish/stress Create the Pod:kubectl apply -f https://k8s.io/examples/admin/resource/cpu-constraints-pod-4.yaml --namespace=constraints-cpu-example View detailed information about the Pod:kubectl get pod constraints-cpu-demo-4 --namespace=constraints-cpu-example --output=yaml The output shows that the Pod's single container has a CPU request of 800 millicpu and a CPU limit of 800 millicpu. How did that container get those values?resources: limits: cpu: 800m requests: cpu: 800m Because that container did not specify its own CPU request and limit, the control plane applied the default CPU request and limit from the LimitRange for this namespace.At this point, your Pod may or may not be running. Recall that a prerequisite for this task is that your Nodes must have at least 1 CPU available for use. If each of your Nodes has only 1 CPU, then there might not be enough allocatable CPU on any Node to accommodate a request of 800 millicpu. If you happen to be using Nodes with 2 CPU, then you probably have enough CPU to accommodate the 800 millicpu request.Delete your Pod:kubectl delete pod constraints-cpu-demo-4 --namespace=constraints-cpu-example Enforcement of minimum and maximum CPU constraintsThe maximum and minimum CPU constraints imposed on a namespace by a LimitRange are enforced only when a Pod is created or updated. If you change the LimitRange, it does not affect Pods that were created previously.Motivation for minimum and maximum CPU constraintsAs a cluster administrator, you might want to impose restrictions on the CPU resources that Pods can use. For example:Each Node in a cluster has 2 CPU. You do not want to accept any Pod that requests more than 2 CPU, because no Node in the cluster can support the request.A cluster is shared by your production and development departments. You want to allow production workloads to consume up to 3 CPU, but you want development workloads to be limited to 1 CPU. You create separate namespaces for production and development, and you apply CPU constraints to each namespace.Clean upDelete your namespace:kubectl delete namespace constraints-cpu-example What's nextFor cluster administratorsConfigure Default Memory Requests and Limits for a NamespaceConfigure Default CPU Requests and Limits for a NamespaceConfigure Minimum and Maximum Memory Constraints for a NamespaceConfigure Memory and CPU Quotas for a NamespaceConfigure a Pod Quota for a NamespaceConfigure Quotas for API ObjectsFor app developersAssign Memory Resources to Containers and PodsAssign CPU Resources to Containers and PodsAssign Pod-level CPU and memory resourcesConfigure Quality of Service for Pods4.2.5.5 - Configure Memory and CPU Quotas for a NamespaceDefine overall memory and CPU resource limits for a namespace.This page shows how to set quotas for the total amount memory and CPU that can be used by all Pods running in a namespace. You specify quotas in a ResourceQuota object.Before you beginYou need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesYou must have access to create namespaces in your cluster.Each node in your cluster must have at least 1 GiB of memory.Create a namespaceCreate a namespace so that the resources you create in this exercise are isolated from the rest of your cluster.kubectl create namespace quota-mem-cpu-example Create a ResourceQuotaHere is a manifest for an example ResourceQuota:admin/resource/quota-mem-cpu.yaml apiVersion: v1 kind: ResourceQuota metadata: name: mem-cpu-demo spec: hard: requests.cpu: "1" requests.memory: 1Gi limits.cpu: "2" limits.memory: 2Gi Create the ResourceQuota:kubectl apply -f https://k8s.io/examples/admin/resource/quota-mem-cpu.yaml --namespace=quota-mem-cpu-example View detailed information about the ResourceQuota:kubectl get resourcequota mem-cpu-demo --namespace=quota-mem-cpu-example --output=yaml The ResourceQuota places these requirements on the quota-mem-cpu-example namespace:For every Pod in the namespace, each container must have a memory request, memory limit, cpu request, and cpu limit.The memory request total for all Pods in that namespace must not exceed 1 GiB.The memory limit total for all Pods in that namespace must not exceed 2 GiB.The CPU request total for all Pods in that namespace must not exceed 1 cpu.The CPU limit total for all Pods in that namespace must not exceed 2 cpu.See meaning of CPU to learn what Kubernetes means by “1 CPU”.Create a PodHere is a manifest for an example Pod:admin/resource/quota-mem-cpu-pod.yaml apiVersion: v1 kind: Pod metadata: name: quota-mem-cpu-demo spec: containers: \- name: quota-mem-cpu-demo-ctr image: nginx resources: limits: memory: "800Mi" cpu: "800m" requests: memory: "600Mi" cpu: "400m" Create the Pod:kubectl apply -f https://k8s.io/examples/admin/resource/quota-mem-cpu-pod.yaml --namespace=quota-mem-cpu-example Verify that the Pod is running and that its (only) container is healthy:kubectl get pod quota-mem-cpu-demo --namespace=quota-mem-cpu-example Once again, view detailed information about the ResourceQuota:kubectl get resourcequota mem-cpu-demo --namespace=quota-mem-cpu-example --output=yaml The output shows the quota along with how much of the quota has been used. You can see that the memory and CPU requests and limits for your Pod do not exceed the quota.status: hard: limits.cpu: "2" limits.memory: 2Gi requests.cpu: "1" requests.memory: 1Gi used: limits.cpu: 800m limits.memory: 800Mi requests.cpu: 400m requests.memory: 600Mi If you have the jq tool, you can also query (using JSONPath) for just the used values, and pretty-print that that of the output. For example:kubectl get resourcequota mem-cpu-demo --namespace=quota-mem-cpu-example -o jsonpath='{ .status.used }' | jq . Attempt to create a second PodHere is a manifest for a second Pod:admin/resource/quota-mem-cpu-pod-2.yaml apiVersion: v1 kind: Pod metadata: name: quota-mem-cpu-demo-2 spec: containers: \- name: quota-mem-cpu-demo-2-ctr image: redis resources: limits: memory: "1Gi" cpu: "800m" requests: memory: "700Mi" cpu: "400m" In the manifest, you can see that the Pod has a memory request of 700 MiB. Notice that the sum of the used memory request and this new memory request exceeds the memory request quota: 600 MiB + 700 MiB > 1 GiB.Attempt to create the Pod:kubectl apply -f https://k8s.io/examples/admin/resource/quota-mem-cpu-pod-2.yaml --namespace=quota-mem-cpu-example The second Pod does not get created. The output shows that creating the second Pod would cause the memory request total to exceed the memory request quota.Error from server (Forbidden): error when creating "examples/admin/resource/quota-mem-cpu-pod-2.yaml": pods "quota-mem-cpu-demo-2" is forbidden: exceeded quota: mem-cpu-demo, requested: requests.memory=700Mi,used: requests.memory=600Mi, limited: requests.memory=1Gi DiscussionAs you have seen in this exercise, you can use a ResourceQuota to restrict the memory request total for all Pods running in a namespace. You can also restrict the totals for memory limit, cpu request, and cpu limit.Instead of managing total resource use within a namespace, you might want to restrict individual Pods, or the containers in those Pods. To achieve that kind of limiting, use a LimitRange.Clean upDelete your namespace:kubectl delete namespace quota-mem-cpu-example What's nextFor cluster administratorsConfigure Default Memory Requests and Limits for a NamespaceConfigure Default CPU Requests and Limits for a NamespaceConfigure Minimum and Maximum Memory Constraints for a NamespaceConfigure Minimum and Maximum CPU Constraints for a NamespaceConfigure a Pod Quota for a NamespaceConfigure Quotas for API ObjectsFor app developersAssign Memory Resources to Containers and PodsAssign CPU Resources to Containers and PodsAssign Pod-level CPU and memory resourcesConfigure Quality of Service for Pods4.2.5.6 - Configure a Pod Quota for a NamespaceRestrict how many Pods you can create within a namespace.This page shows how to set a quota for the total number of Pods that can run in a Namespace. You specify quotas in a ResourceQuota object.Before you beginYou need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesYou must have access to create namespaces in your cluster.Create a namespaceCreate a namespace so that the resources you create in this exercise are isolated from the rest of your cluster.kubectl create namespace quota-pod-example Create a ResourceQuotaHere is an example manifest for a ResourceQuota:admin/resource/quota-pod.yaml apiVersion: v1 kind: ResourceQuota metadata: name: pod-demo spec: hard: pods: "2" Create the ResourceQuota:kubectl apply -f https://k8s.io/examples/admin/resource/quota-pod.yaml --namespace=quota-pod-example View detailed information about the ResourceQuota:kubectl get resourcequota pod-demo --namespace=quota-pod-example --output=yaml The output shows that the namespace has a quota of two Pods, and that currently there are no Pods; that is, none of the quota is used.spec: hard: pods: "2" status: hard: pods: "2" used: pods: "0" Here is an example manifest for a Deployment:admin/resource/quota-pod-deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: pod-quota-demo spec: selector: matchLabels: purpose: quota-demo replicas: 3 template: metadata: labels: purpose: quota-demo spec: containers: \- name: pod-quota-demo image: nginx In that manifest, replicas: 3 tells Kubernetes to attempt to create three new Pods, all running the same application.Create the Deployment:kubectl apply -f https://k8s.io/examples/admin/resource/quota-pod-deployment.yaml --namespace=quota-pod-example View detailed information about the Deployment:kubectl get deployment pod-quota-demo --namespace=quota-pod-example --output=yaml The output shows that even though the Deployment specifies three replicas, only two Pods were created because of the quota you defined earlier:spec: ... replicas: 3 ... status: availableReplicas: 2 ... lastUpdateTime: 2021-04-02T20:57:05Z message: 'unable to create pods: pods "pod-quota-demo-1650323038-" is forbidden: exceeded quota: pod-demo, requested: pods=1, used: pods=2, limited: pods=2' Choice of resourceIn this task you have defined a ResourceQuota that limited the total number of Pods, but you could also limit the total number of other kinds of object. For example, you might decide to limit how many CronJobs that can live in a single namespace.Clean upDelete your namespace:kubectl delete namespace quota-pod-example What's nextFor cluster administratorsConfigure Default Memory Requests and Limits for a NamespaceConfigure Default CPU Requests and Limits for a NamespaceConfigure Minimum and Maximum Memory Constraints for a NamespaceConfigure Minimum and Maximum CPU Constraints for a NamespaceConfigure Memory and CPU Quotas for a NamespaceConfigure Quotas for API ObjectsFor app developersAssign Memory Resources to Containers and PodsAssign CPU Resources to Containers and PodsAssign Pod-level CPU and memory resourcesConfigure Quality of Service for Pods4.2.6 - Install a Network Policy Provider4.2.6.1 - Use Antrea for NetworkPolicyThis page shows how to install and use Antrea CNI plugin on Kubernetes. For background on Project Antrea, read the Introduction to Antrea.Before you beginYou need to have a Kubernetes cluster. Follow the kubeadm getting started guide to bootstrap one.Deploying Antrea with kubeadmFollow Getting Started guide to deploy Antrea for kubeadm.What's nextOnce your cluster is running, you can follow the Declare Network Policy to try out Kubernetes NetworkPolicy.4.2.6.2 - Use Calico for NetworkPolicyThis page shows a couple of quick ways to create a Calico cluster on Kubernetes.Before you beginDecide whether you want to deploy a cloud or local cluster.Creating a Calico cluster with Google Kubernetes Engine (GKE)Prerequisite: gcloud.To launch a GKE cluster with Calico, include the --enable-network-policy flag.Syntaxgcloud container clusters create [CLUSTER_NAME] --enable-network-policy Examplegcloud container clusters create my-calico-cluster --enable-network-policy To verify the deployment, use the following command.kubectl get pods --namespace=kube-system The Calico pods begin with calico. Check to make sure each one has a status of Running.Creating a local Calico cluster with kubeadmTo get a local single-host Calico cluster in fifteen minutes using kubeadm, refer to the Calico Quickstart.What's nextOnce your cluster is running, you can follow the Declare Network Policy to try out Kubernetes NetworkPolicy.4.2.6.3 - Use Cilium for NetworkPolicyThis page shows how to use Cilium for NetworkPolicy.For background on Cilium, read the Introduction to Cilium.Before you beginYou need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesTo check the version, enter kubectl version.Deploying Cilium on Minikube for Basic TestingTo get familiar with Cilium easily you can follow the Cilium Kubernetes Getting Started Guide to perform a basic DaemonSet installation of Cilium in minikube.To start minikube, minimal version required is >= v1.5.2, run the with the following arguments:minikube version minikube version: v1.5.2 minikube start --network-plugin=cni For minikube you can install Cilium using its CLI tool. To do so, first download the latest version of the CLI with the following command:curl -LO https://github.com/cilium/cilium-cli/releases/latest/download/cilium-linux-amd64.tar.gz Then extract the downloaded file to your /usr/local/bin directory with the following command:sudo tar xzvfC cilium-linux-amd64.tar.gz /usr/local/bin rm cilium-linux-amd64.tar.gz After running the above commands, you can now install Cilium with the following command:cilium install Cilium will then automatically detect the cluster configuration and create and install the appropriate components for a successful installation. The components are:Certificate Authority (CA) in Secret cilium-ca and certificates for Hubble (Cilium's observability layer).Service accounts.Cluster roles.ConfigMap.Agent DaemonSet and an Operator Deployment.After the installation, you can view the overall status of the Cilium deployment with the cilium status command. See the expected output of the status command here.The remainder of the Getting Started Guide explains how to enforce both L3/L4 (i.e., IP address + port) security policies, as well as L7 (e.g., HTTP) security policies using an example application.Deploying Cilium for Production UseFor detailed instructions around deploying Cilium for production, see: Cilium Kubernetes Installation Guide This documentation includes detailed requirements, instructions and example production DaemonSet files.Understanding Cilium componentsDeploying a cluster with Cilium adds Pods to the kube-system namespace. To see this list of Pods run:kubectl get pods --namespace=kube-system -l k8s-app=cilium You'll see a list of Pods similar to this:NAME READY STATUS RESTARTS AGE cilium-kkdhz 1/1 Running 0 3m23s ... A cilium Pod runs on each node in your cluster and enforces network policy on the traffic to/from Pods on that node using Linux BPF.What's nextOnce your cluster is running, you can follow the Declare Network Policy to try out Kubernetes NetworkPolicy with Cilium. Have fun, and if you have questions, contact us using the Cilium Slack Channel.4.2.6.4 - Use Kube-router for NetworkPolicyThis page shows how to use Kube-router for NetworkPolicy.Before you beginYou need to have a Kubernetes cluster running. If you do not already have a cluster, you can create one by using any of the cluster installers like Kops, Bootkube, Kubeadm etc.Installing Kube-router addonThe Kube-router Addon comes with a Network Policy Controller that watches Kubernetes API server for any NetworkPolicy and pods updated and configures iptables rules and ipsets to allow or block traffic as directed by the policies. Please follow the trying Kube-router with cluster installers guide to install Kube-router addon.What's nextOnce you have installed the Kube-router addon, you can follow the Declare Network Policy to try out Kubernetes NetworkPolicy.4.2.6.5 - Romana for NetworkPolicyThis page shows how to use Romana for NetworkPolicy.Before you beginComplete steps 1, 2, and 3 of the kubeadm getting started guide.Installing Romana with kubeadmFollow the containerized installation guide for kubeadm.Applying network policiesTo apply network policies use one of the following:Romana network policies.Example of Romana network policy.The NetworkPolicy API.What's nextOnce you have installed Romana, you can follow the Declare Network Policy to try out Kubernetes NetworkPolicy.4.2.6.6 - Weave Net for NetworkPolicyThis page shows how to use Weave Net for NetworkPolicy.Before you beginYou need to have a Kubernetes cluster. Follow the kubeadm getting started guide to bootstrap one.Install the Weave Net addonFollow the Integrating Kubernetes via the Addon guide.The Weave Net addon for Kubernetes comes with a Network Policy Controller that automatically monitors Kubernetes for any NetworkPolicy annotations on all namespaces and configures iptables rules to allow or block traffic as directed by the policies.Test the installationVerify that the weave works.Enter the following command:kubectl get pods -n kube-system -o wide The output is similar to this:NAME READY STATUS RESTARTS AGE IP NODE weave-net-1t1qg 2/2 Running 0 9d 192.168.2.10 worknode3 weave-net-231d7 2/2 Running 1 7d 10.2.0.17 worknodegpu weave-net-7nmwt 2/2 Running 3 9d 192.168.2.131 masternode weave-net-pmw8w 2/2 Running 0 9d 192.168.2.216 worknode2 Each Node has a weave Pod, and all Pods are Running and 2/2 READY. (2/2 means that each Pod has weave and weave-npc.)What's nextOnce you have installed the Weave Net addon, you can follow the Declare Network Policy to try out Kubernetes NetworkPolicy. If you have any question, contact us at #weave-community on Slack or Weave User Group.4.2.7 - Access Clusters Using the Kubernetes APIThis page shows how to access clusters using the Kubernetes API.Before you beginYou need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesTo check the version, enter kubectl version.Accessing the Kubernetes APIAccessing for the first time with kubectlWhen accessing the Kubernetes API for the first time, use the Kubernetes command-line tool, kubectl.To access a cluster, you need to know the location of the cluster and have credentials to access it. Typically, this is automatically set-up when you work through a Getting started guide, or someone else set up the cluster and provided you with credentials and a location.Check the location and credentials that kubectl knows about with this command:kubectl config view Many of the examples provide an introduction to using kubectl. Complete documentation is found in the kubectl manual.Directly accessing the REST APIkubectl handles locating and authenticating to the API server. If you want to directly access the REST API with an http client like curl or wget, or a browser, there are multiple ways you can locate and authenticate against the API server:Run kubectl in proxy mode (recommended). This method is recommended, since it uses the stored API server location and verifies the identity of the API server using a self-signed certificate. No man-in-the-middle (MITM) attack is possible using this method.Alternatively, you can provide the location and credentials directly to the http client. This works with client code that is confused by proxies. To protect against man in the middle attacks, you'll need to import a root cert into your browser.Using the Go or Python client libraries provides accessing kubectl in proxy mode.Using kubectl proxyThe following command runs kubectl in a mode where it acts as a reverse proxy. It handles locating the API server and authenticating.Run it like this:kubectl proxy --port=8080 & See kubectl proxy for more details.Then you can explore the API with curl, wget, or a browser, like so:curl http://localhost:8080/api/ The output is similar to this:{ "versions": [ "v1" ], "serverAddressByClientCIDRs": [ { "clientCIDR": "0.0.0.0/0", "serverAddress": "10.0.1.149:443" } ] } Without kubectl proxyIt is possible to avoid using kubectl proxy by passing an authentication token directly to the API server, like this:Using grep/cut approach:# Check all possible clusters, as your .KUBECONFIG may have multiple contexts: kubectl config view -o jsonpath='{"Cluster name\tServer\n"}{range .clusters[*]}{.name}{"\t"}{.cluster.server}{"\n"}{end}' # Select name of cluster you want to interact with from above output: export CLUSTER_NAME="some_server_name" # Point to the API server referring the cluster name APISERVER=$(kubectl config view -o jsonpath="{.clusters[?(@.name==\"$CLUSTER_NAME\")].cluster.server}") # Create a secret to hold a token for the default service account kubectl apply -f - </dev/null; do echo "waiting for token..." >&2 sleep 1 done # Get the token value TOKEN=$(kubectl get secret default-token -o jsonpath='{.data.token}' | base64 --decode) # Explore the API with TOKEN curl -X GET $APISERVER/api --header "Authorization: Bearer $TOKEN" --insecure The output is similar to this:{ "kind": "APIVersions", "versions": [ "v1" ], "serverAddressByClientCIDRs": [ { "clientCIDR": "0.0.0.0/0", "serverAddress": "10.0.1.149:443" } ] } The above example uses the --insecure flag. This leaves it subject to MITM attacks. When kubectl accesses the cluster it uses a stored root certificate and client certificates to access the server. (These are installed in the ~/.kube directory). Since cluster certificates are typically self-signed, it may take special configuration to get your http client to use root certificate.On some clusters, the API server does not require authentication; it may serve on localhost, or be protected by a firewall. There is not a standard for this. Controlling Access to the Kubernetes API describes how you can configure this as a cluster administrator.Programmatic access to the APIKubernetes officially supports client libraries for Go, Python, Java, dotnet, JavaScript, and Haskell. There are other client libraries that are provided and maintained by their authors, not the Kubernetes team. See client libraries for accessing the API from other languages and how they authenticate.Go clientTo get the library, run the following command: go get k8s.io/client-go@kubernetes- See https://github.com/kubernetes/client-go/releases to see which versions are supported.Write an application atop of the client-go clients.Note:client-go defines its own API objects, so if needed, import API definitions from client-go rather than from the main repository. For example, import "k8s.io/client-go/kubernetes" is correct.The Go client can use the same kubeconfig file as the kubectl CLI does to locate and authenticate to the API server. See this example:package main import ( "context" "fmt" "k8s.io/apimachinery/pkg/apis/meta/v1" "k8s.io/client-go/kubernetes" "k8s.io/client-go/tools/clientcmd" ) func main() { // uses the current context in kubeconfig // path-to-kubeconfig -- for example, /root/.kube/config config, _ := clientcmd.BuildConfigFromFlags("", "") // creates the clientset clientset, _ := kubernetes.NewForConfig(config) // access the API to list pods pods, _ := clientset.CoreV1().Pods("").List(context.TODO(), v1.ListOptions{}) fmt.Printf("There are %d pods in the cluster\n", len(pods.Items)) } If the application is deployed as a Pod in the cluster, see Accessing the API from within a Pod.Python clientTo use Python client, run the following command: pip install kubernetes. See Python Client Library page for more installation options.The Python client can use the same kubeconfig file as the kubectl CLI does to locate and authenticate to the API server. See this example:from kubernetes import client, config config.load_kube_config() v1=client.CoreV1Api() print("Listing pods with their IPs:") ret = v1.list_pod_for_all_namespaces(watch=False) for i in ret.items: print("%s\t%s\t%s" % (i.status.pod_ip, i.metadata.namespace, i.metadata.name)) Java clientTo install the Java Client, run:# Clone java library git clone --recursive https://github.com/kubernetes-client/java # Installing project artifacts, POM etc: cd java mvn install See https://github.com/kubernetes-client/java/releases to see which versions are supported.The Java client can use the same kubeconfig file as the kubectl CLI does to locate and authenticate to the API server. See this example:package io.kubernetes.client.examples; import io.kubernetes.client.ApiClient; import io.kubernetes.client.ApiException; import io.kubernetes.client.Configuration; import io.kubernetes.client.apis.CoreV1Api; import io.kubernetes.client.models.V1Pod; import io.kubernetes.client.models.V1PodList; import io.kubernetes.client.util.ClientBuilder; import io.kubernetes.client.util.KubeConfig; import java.io.FileReader; import java.io.IOException; /** * A simple example of how to use the Java API from an application outside a kubernetes cluster * * 

Easiest way to run this: mvn exec:java * -Dexec.mainClass="io.kubernetes.client.examples.KubeConfigFileClientExample" * */ public class KubeConfigFileClientExample { public static void main(String[] args) throws IOException, ApiException { // file path to your KubeConfig String kubeConfigPath = "~/.kube/config"; // loading the out-of-cluster config, a kubeconfig from file-system ApiClient client = ClientBuilder.kubeconfig(KubeConfig.loadKubeConfig(new FileReader(kubeConfigPath))).build(); // set the global default api-client to the in-cluster one from above Configuration.setDefaultApiClient(client); // the CoreV1Api loads default api-client from global configuration. CoreV1Api api = new CoreV1Api(); // invokes the CoreV1Api client V1PodList list = api.listPodForAllNamespaces(null, null, null, null, null, null, null, null, null); System.out.println("Listing all pods: "); for (V1Pod item : list.getItems()) { System.out.println(item.getMetadata().getName()); } } } dotnet clientTo use dotnet client, run the following command: dotnet add package KubernetesClient --version 1.6.1. See dotnet Client Library page for more installation options. See https://github.com/kubernetes-client/csharp/releases to see which versions are supported.The dotnet client can use the same kubeconfig file as the kubectl CLI does to locate and authenticate to the API server. See this example:using System; using k8s; namespace simple { internal class PodList { private static void Main(string[] args) { var config = KubernetesClientConfiguration.BuildDefaultConfig(); IKubernetes client = new Kubernetes(config); Console.WriteLine("Starting Request!"); var list = client.ListNamespacedPod("default"); foreach (var item in list.Items) { Console.WriteLine(item.Metadata.Name); } if (list.Items.Count == 0) { Console.WriteLine("Empty!"); } } } } JavaScript clientTo install JavaScript client, run the following command: npm install @kubernetes/client-node. See https://github.com/kubernetes-client/javascript/releases to see which versions are supported.The JavaScript client can use the same kubeconfig file as the kubectl CLI does to locate and authenticate to the API server. See this example:const k8s = require('@kubernetes/client-node'); const kc = new k8s.KubeConfig(); kc.loadFromDefault(); const k8sApi = kc.makeApiClient(k8s.CoreV1Api); k8sApi.listNamespacedPod('default').then((res) => { console.log(res.body); }); Haskell clientSee https://github.com/kubernetes-client/haskell/releases to see which versions are supported.The Haskell client can use the same kubeconfig file as the kubectl CLI does to locate and authenticate to the API server. See this example:exampleWithKubeConfig :: IO () exampleWithKubeConfig = do oidcCache <\- atomically $ newTVar $ Map.fromList [] (mgr, kcfg) <\- mkKubeClientConfig oidcCache $ KubeConfigFile "/path/to/kubeconfig" dispatchMime mgr kcfg (CoreV1.listPodForAllNamespaces (Accept MimeJSON)) >>= print What's nextAccessing the Kubernetes API from a Pod4.2.8 - Advertise Extended Resources for a NodeThis page shows how to specify extended resources for a Node. Extended resources allow cluster administrators to advertise node-level resources that would otherwise be unknown to Kubernetes.Before you beginYou need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesTo check the version, enter kubectl version.Get the names of your Nodeskubectl get nodes Choose one of your Nodes to use for this exercise.Advertise a new extended resource on one of your NodesTo advertise a new extended resource on a Node, send an HTTP PATCH request to the Kubernetes API server. For example, suppose one of your Nodes has four dongles attached. Here's an example of a PATCH request that advertises four dongle resources for your Node.PATCH /api/v1/nodes//status HTTP/1.1 Accept: application/json Content-Type: application/json-patch+json Host: k8s-master:8080 [ { "op": "add", "path": "/status/capacity/example.com~1dongle", "value": "4" } ] Note that Kubernetes does not need to know what a dongle is or what a dongle is for. The preceding PATCH request tells Kubernetes that your Node has four things that you call dongles.Start a proxy, so that you can easily send requests to the Kubernetes API server:kubectl proxy In another command window, send the HTTP PATCH request. Replace  with the name of your Node:curl --header "Content-Type: application/json-patch+json" \ \--request PATCH \ \--data '[{"op": "add", "path": "/status/capacity/example.com~1dongle", "value": "4"}]' \ http://localhost:8001/api/v1/nodes//status Note:In the preceding request, ~1 is the encoding for the character / in the patch path. The operation path value in JSON-Patch is interpreted as a JSON-Pointer. For more details, see IETF RFC 6901, section 3.The output shows that the Node has a capacity of 4 dongles:"capacity": { "cpu": "2", "memory": "2049008Ki", "example.com/dongle": "4", Describe your Node:kubectl describe node  Once again, the output shows the dongle resource:Capacity: cpu: 2 memory: 2049008Ki example.com/dongle: 4 Now, application developers can create Pods that request a certain number of dongles. See Assign Extended Resources to a Container.DiscussionExtended resources are similar to memory and CPU resources. For example, just as a Node has a certain amount of memory and CPU to be shared by all components running on the Node, it can have a certain number of dongles to be shared by all components running on the Node. And just as application developers can create Pods that request a certain amount of memory and CPU, they can create Pods that request a certain number of dongles.Extended resources are opaque to Kubernetes; Kubernetes does not know anything about what they are. Kubernetes knows only that a Node has a certain number of them. Extended resources must be advertised in integer amounts. For example, a Node can advertise four dongles, but not 4.5 dongles.Storage exampleSuppose a Node has 800 GiB of a special kind of disk storage. You could create a name for the special storage, say example.com/special-storage. Then you could advertise it in chunks of a certain size, say 100 GiB. In that case, your Node would advertise that it has eight resources of type example.com/special-storage.Capacity: ... example.com/special-storage: 8 If you want to allow arbitrary requests for special storage, you could advertise special storage in chunks of size 1 byte. In that case, you would advertise 800Gi resources of type example.com/special-storage.Capacity: ... example.com/special-storage: 800Gi Then a Container could request any number of bytes of special storage, up to 800Gi.Clean upHere is a PATCH request that removes the dongle advertisement from a Node.PATCH /api/v1/nodes//status HTTP/1.1 Accept: application/json Content-Type: application/json-patch+json Host: k8s-master:8080 [ { "op": "remove", "path": "/status/capacity/example.com~1dongle", } ] Start a proxy, so that you can easily send requests to the Kubernetes API server:kubectl proxy In another command window, send the HTTP PATCH request. Replace  with the name of your Node:curl --header "Content-Type: application/json-patch+json" \ \--request PATCH \ \--data '[{"op": "remove", "path": "/status/capacity/example.com~1dongle"}]' \ http://localhost:8001/api/v1/nodes//status Verify that the dongle advertisement has been removed:kubectl describe node  | grep dongle (you should not see any output)What's nextFor application developersAssign Extended Resources to a ContainerFor cluster administratorsConfigure Minimum and Maximum Memory Constraints for a NamespaceConfigure Minimum and Maximum CPU Constraints for a Namespace4.2.9 - Autoscale the DNS Service in a ClusterThis page shows how to enable and configure autoscaling of the DNS service in your Kubernetes cluster.Before you beginYou need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesTo check the version, enter kubectl version.This guide assumes your nodes use the AMD64 or Intel 64 CPU architecture.Make sure Kubernetes DNS is enabled.Determine whether DNS horizontal autoscaling is already enabledList the Deployments in your cluster in the kube-system namespace:kubectl get deployment --namespace=kube-system The output is similar to this:NAME READY UP-TO-DATE AVAILABLE AGE ... kube-dns-autoscaler 1/1 1 1 ... ... If you see "kube-dns-autoscaler" in the output, DNS horizontal autoscaling is already enabled, and you can skip to Tuning autoscaling parameters.Get the name of your DNS DeploymentList the DNS deployments in your cluster in the kube-system namespace:kubectl get deployment -l k8s-app=kube-dns --namespace=kube-system The output is similar to this:NAME READY UP-TO-DATE AVAILABLE AGE ... coredns 2/2 2 2 ... ... If you don't see a Deployment for DNS services, you can also look for it by name:kubectl get deployment --namespace=kube-system and look for a deployment named coredns or kube-dns.Your scale target isDeployment/ where  is the name of your DNS Deployment. For example, if the name of your Deployment for DNS is coredns, your scale target is Deployment/coredns.Note:CoreDNS is the default DNS service for Kubernetes. CoreDNS sets the label k8s-app=kube-dns so that it can work in clusters that originally used kube-dns.Enable DNS horizontal autoscalingIn this section, you create a new Deployment. The Pods in the Deployment run a container based on the cluster-proportional-autoscaler-amd64 image.Create a file named dns-horizontal-autoscaler.yaml with this content:admin/dns/dns-horizontal-autoscaler.yaml kind: ServiceAccount apiVersion: v1 metadata: name: kube-dns-autoscaler namespace: kube-system \--- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: system:kube-dns-autoscaler rules: \- apiGroups: [""] resources: ["nodes"] verbs: ["list", "watch"] \- apiGroups: [""] resources: ["replicationcontrollers/scale"] verbs: ["get", "update"] \- apiGroups: ["apps"] resources: ["deployments/scale", "replicasets/scale"] verbs: ["get", "update"] # Remove the configmaps rule once below issue is fixed: # kubernetes-incubator/cluster-proportional-autoscaler#16 \- apiGroups: [""] resources: ["configmaps"] verbs: ["get", "create"] \--- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: system:kube-dns-autoscaler subjects: \- kind: ServiceAccount name: kube-dns-autoscaler namespace: kube-system roleRef: kind: ClusterRole name: system:kube-dns-autoscaler apiGroup: rbac.authorization.k8s.io \--- apiVersion: apps/v1 kind: Deployment metadata: name: kube-dns-autoscaler namespace: kube-system labels: k8s-app: kube-dns-autoscaler kubernetes.io/cluster-service: "true" spec: selector: matchLabels: k8s-app: kube-dns-autoscaler template: metadata: labels: k8s-app: kube-dns-autoscaler spec: priorityClassName: system-cluster-critical securityContext: seccompProfile: type: RuntimeDefault supplementalGroups: [ 65534 ] fsGroup: 65534 nodeSelector: kubernetes.io/os: linux containers: \- name: autoscaler image: registry.k8s.io/cpa/cluster-proportional-autoscaler:1.8.4 resources: requests: cpu: "20m" memory: "10Mi" command: \- /cluster-proportional-autoscaler \- --namespace=kube-system \- --configmap=kube-dns-autoscaler # Should keep target in sync with cluster/addons/dns/kube-dns.yaml.base \- --target= # When cluster is using large nodes(with more cores), "coresPerReplica" should dominate. # If using small nodes, "nodesPerReplica" should dominate. \- --default-params={"linear":{"coresPerReplica":256,"nodesPerReplica":16,"preventSinglePointFailure":true,"includeUnschedulableNodes":true}} \- --logtostderr=true \- --v=2 tolerations: \- key: "CriticalAddonsOnly" operator: "Exists" serviceAccountName: kube-dns-autoscaler In the file, replace  with your scale target.Go to the directory that contains your configuration file, and enter this command to create the Deployment:kubectl apply -f dns-horizontal-autoscaler.yaml The output of a successful command is:deployment.apps/kube-dns-autoscaler created DNS horizontal autoscaling is now enabled.Tune DNS autoscaling parametersVerify that the kube-dns-autoscaler ConfigMap exists:kubectl get configmap --namespace=kube-system The output is similar to this:NAME DATA AGE ... kube-dns-autoscaler 1 ... ... Modify the data in the ConfigMap:kubectl edit configmap kube-dns-autoscaler --namespace=kube-system Look for this line:linear: '{"coresPerReplica":256,"min":1,"nodesPerReplica":16}' Modify the fields according to your needs. The "min" field indicates the minimal number of DNS backends. The actual number of backends is calculated using this equation:replicas = max( ceil( cores × 1/coresPerReplica ) , ceil( nodes × 1/nodesPerReplica ) ) Note that the values of both coresPerReplica and nodesPerReplica are floats.The idea is that when a cluster is using nodes that have many cores, coresPerReplica dominates. When a cluster is using nodes that have fewer cores, nodesPerReplica dominates.There are other supported scaling patterns. For details, see cluster-proportional-autoscaler.Disable DNS horizontal autoscalingThere are a few options for tuning DNS horizontal autoscaling. Which option to use depends on different conditions.Option 1: Scale down the kube-dns-autoscaler deployment to 0 replicasThis option works for all situations. Enter this command:kubectl scale deployment --replicas=0 kube-dns-autoscaler --namespace=kube-system The output is:deployment.apps/kube-dns-autoscaler scaled Verify that the replica count is zero:kubectl get rs --namespace=kube-system The output displays 0 in the DESIRED and CURRENT columns:NAME DESIRED CURRENT READY AGE ... kube-dns-autoscaler-6b59789fc8 0 0 0 ... ... Option 2: Delete the kube-dns-autoscaler deploymentThis option works if kube-dns-autoscaler is under your own control, which means no one will re-create it:kubectl delete deployment kube-dns-autoscaler --namespace=kube-system The output is:deployment.apps "kube-dns-autoscaler" deleted Option 3: Delete the kube-dns-autoscaler manifest file from the master nodeThis option works if kube-dns-autoscaler is under control of the (deprecated) Addon Manager, and you have write access to the master node.Sign in to the master node and delete the corresponding manifest file. The common path for this kube-dns-autoscaler is:/etc/kubernetes/addons/dns-horizontal-autoscaler/dns-horizontal-autoscaler.yaml After the manifest file is deleted, the Addon Manager will delete the kube-dns-autoscaler Deployment.Understanding how DNS horizontal autoscaling worksThe cluster-proportional-autoscaler application is deployed separately from the DNS service.An autoscaler Pod runs a client that polls the Kubernetes API server for the number of nodes and cores in the cluster.A desired replica count is calculated and applied to the DNS backends based on the current schedulable nodes and cores and the given scaling parameters.The scaling parameters and data points are provided via a ConfigMap to the autoscaler, and it refreshes its parameters table every poll interval to be up to date with the latest desired scaling parameters.Changes to the scaling parameters are allowed without rebuilding or restarting the autoscaler Pod.The autoscaler provides a controller interface to support two control patterns: linear and ladder.What's nextRead about Guaranteed Scheduling For Critical Add-On Pods.Learn more about the implementation of cluster-proportional-autoscaler.4.2.10 - Change the Access Mode of a PersistentVolume to ReadWriteOncePodThis page shows how to change the access mode on an existing PersistentVolume to use ReadWriteOncePod.Before you beginYou need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesYour Kubernetes server must be at or later than version v1.22.To check the version, enter kubectl version.Note:The ReadWriteOncePod access mode graduated to stable in the Kubernetes v1.29 release. If you are running a version of Kubernetes older than v1.29, you might need to enable a feature gate. Check the documentation for your version of Kubernetes.Note:The ReadWriteOncePod access mode is only supported for CSI volumes. To use this volume access mode you will need to update the following CSI sidecars to these versions or greater:csi-provisioner:v3.0.0+csi-attacher:v3.3.0+csi-resizer:v1.3.0+Why should I use ReadWriteOncePod?Prior to Kubernetes v1.22, the ReadWriteOnce access mode was commonly used to restrict PersistentVolume access for workloads that required single-writer access to storage. However, this access mode had a limitation: it restricted volume access to a single node, allowing multiple pods on the same node to read from and write to the same volume simultaneously. This could pose a risk for applications that demand strict single-writer access for data safety.If ensuring single-writer access is critical for your workloads, consider migrating your volumes to ReadWriteOncePod.Migrating existing PersistentVolumesIf you have existing PersistentVolumes, they can be migrated to use ReadWriteOncePod. Only migrations from ReadWriteOnce to ReadWriteOncePod are supported.In this example, there is already a ReadWriteOnce "cat-pictures-pvc" PersistentVolumeClaim that is bound to a "cat-pictures-pv" PersistentVolume, and a "cat-pictures-writer" Deployment that uses this PersistentVolumeClaim.Note:If your storage plugin supports Dynamic provisioning, the "cat-picutres-pv" will be created for you, but its name may differ. To get your PersistentVolume's name run:kubectl get pvc cat-pictures-pvc -o jsonpath='{.spec.volumeName}' And you can view the PVC before you make changes. Either view the manifest locally, or run kubectl get pvc  -o yaml. The output is similar to:# cat-pictures-pvc.yaml kind: PersistentVolumeClaim apiVersion: v1 metadata: name: cat-pictures-pvc spec: accessModes: \- ReadWriteOnce resources: requests: storage: 1Gi Here's an example Deployment that relies on that PersistentVolumeClaim:# cat-pictures-writer-deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: cat-pictures-writer spec: replicas: 3 selector: matchLabels: app: cat-pictures-writer template: metadata: labels: app: cat-pictures-writer spec: containers: \- name: nginx image: nginx:1.14.2 ports: \- containerPort: 80 volumeMounts: \- name: cat-pictures mountPath: /mnt volumes: \- name: cat-pictures persistentVolumeClaim: claimName: cat-pictures-pvc readOnly: false As a first step, you need to edit your PersistentVolume's spec.persistentVolumeReclaimPolicy and set it to Retain. This ensures your PersistentVolume will not be deleted when you delete the corresponding PersistentVolumeClaim:kubectl patch pv cat-pictures-pv -p '{"spec":{"persistentVolumeReclaimPolicy":"Retain"}}' Next you need to stop any workloads that are using the PersistentVolumeClaim bound to the PersistentVolume you want to migrate, and then delete the PersistentVolumeClaim. Avoid making any other changes to the PersistentVolumeClaim, such as volume resizes, until after the migration is complete.Once that is done, you need to clear your PersistentVolume's spec.claimRef.uid to ensure PersistentVolumeClaims can bind to it upon recreation:kubectl scale --replicas=0 deployment cat-pictures-writer kubectl delete pvc cat-pictures-pvc kubectl patch pv cat-pictures-pv -p '{"spec":{"claimRef":{"uid":""}}}' After that, replace the PersistentVolume's list of valid access modes to be (only) ReadWriteOncePod:kubectl patch pv cat-pictures-pv -p '{"spec":{"accessModes":["ReadWriteOncePod"]}}' Note:The ReadWriteOncePod access mode cannot be combined with other access modes. Make sure ReadWriteOncePod is the only access mode on the PersistentVolume when updating, otherwise the request will fail.Next you need to modify your PersistentVolumeClaim to set ReadWriteOncePod as the only access mode. You should also set the PersistentVolumeClaim's spec.volumeName to the name of your PersistentVolume to ensure it binds to this specific PersistentVolume.Once this is done, you can recreate your PersistentVolumeClaim and start up your workloads:# IMPORTANT: Make sure to edit your PVC in cat-pictures-pvc.yaml before applying. You need to: # - Set ReadWriteOncePod as the only access mode # - Set spec.volumeName to "cat-pictures-pv" kubectl apply -f cat-pictures-pvc.yaml kubectl apply -f cat-pictures-writer-deployment.yaml Lastly you may edit your PersistentVolume's spec.persistentVolumeReclaimPolicy and set to it back to Delete if you previously changed it.kubectl patch pv cat-pictures-pv -p '{"spec":{"persistentVolumeReclaimPolicy":"Delete"}}' What's nextLearn more about PersistentVolumes.Learn more about PersistentVolumeClaims.Learn more about Configuring a Pod to Use a PersistentVolume for Storage4.2.11 - Change the default StorageClassThis page shows how to change the default Storage Class that is used to provision volumes for PersistentVolumeClaims that have no special requirements.Before you beginYou need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesTo check the version, enter kubectl version.Why change the default storage class?Depending on the installation method, your Kubernetes cluster may be deployed with an existing StorageClass that is marked as default. This default StorageClass is then used to dynamically provision storage for PersistentVolumeClaims that do not require any specific storage class. See PersistentVolumeClaim documentation for details.The pre-installed default StorageClass may not fit well with your expected workload; for example, it might provision storage that is too expensive. If this is the case, you can either change the default StorageClass or disable it completely to avoid dynamic provisioning of storage.Deleting the default StorageClass may not work, as it may be re-created automatically by the addon manager running in your cluster. Please consult the docs for your installation for details about addon manager and how to disable individual addons.Changing the default StorageClassList the StorageClasses in your cluster:kubectl get storageclass The output is similar to this:NAME PROVISIONER AGE standard (default) kubernetes.io/gce-pd 1d gold kubernetes.io/gce-pd 1d The default StorageClass is marked by (default).Mark the default StorageClass as non-default:The default StorageClass has an annotation storageclass.kubernetes.io/is-default-class set to true. Any other value or absence of the annotation is interpreted as false.To mark a StorageClass as non-default, you need to change its value to false:kubectl patch storageclass standard -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"false"}}}' where standard is the name of your chosen StorageClass.Mark a StorageClass as default:Similar to the previous step, you need to add/set the annotation storageclass.kubernetes.io/is-default-class=true.kubectl patch storageclass gold -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}' Please note you can have multiple StorageClass marked as default. If more than one StorageClass is marked as default, a PersistentVolumeClaim without an explicitly defined storageClassName will be created using the most recently created default StorageClass. When a PersistentVolumeClaim is created with a specified volumeName, it remains in a pending state if the static volume's storageClassName does not match the StorageClass on the PersistentVolumeClaim.Verify that your chosen StorageClass is default:kubectl get storageclass The output is similar to this:NAME PROVISIONER AGE standard kubernetes.io/gce-pd 1d gold (default) kubernetes.io/gce-pd 1d What's nextLearn more about PersistentVolumes.4.2.12 - Switching from Polling to CRI Event-based Updates to Container StatusFEATURE STATE: Kubernetes v1.26 [alpha] (enabled by default: false)This page shows how to migrate nodes to use event based updates for container status. The event-based implementation reduces node resource consumption by the kubelet, compared to the legacy approach that relies on polling. You may know this feature as evented Pod lifecycle event generator (PLEG). That's the name used internally within the Kubernetes project for a key implementation detail.The polling based approach is referred to as generic PLEG.Before you beginYou need to run a version of Kubernetes that provides this feature. Kubernetes v1.27 includes beta support for event-based container status updates. The feature is beta but is disabled by default because it requires support from the container runtime.Your Kubernetes server must be at or later than version 1.26.To check the version, enter kubectl version.If you are running a different version of Kubernetes, check the documentation for that release.The container runtime in use must support container lifecycle events. The kubelet automatically switches back to the legacy generic PLEG mechanism if the container runtime does not announce support for container lifecycle events, even if you have this feature gate enabled.Why switch to Evented PLEG?The Generic PLEG incurs non-negligible overhead due to frequent polling of container statuses.This overhead is exacerbated by Kubelet's parallelized polling of container states, thus limiting its scalability and causing poor performance and reliability problems.The goal of Evented PLEG is to reduce unnecessary work during inactivity by replacing periodic polling.Switching to Evented PLEGStart the Kubelet with the feature gate EventedPLEG enabled. You can manage the kubelet feature gates editing the kubelet config file and restarting the kubelet service. You need to do this on each node where you are using this feature.Make sure the node is drained before proceeding.Start the container runtime with the container event generation enabled.ContainerdCRI-OVersion 1.7+Version 1.26+Check if the CRI-O is already configured to emit CRI events by verifying the configuration,crio config | grep enable_pod_events If it is enabled, the output should be similar to the following:enable_pod_events = true To enable it, start the CRI-O daemon with the flag --enable-pod-events=true or use a dropin config with the following lines:[crio.runtime] enable_pod_events: true Your Kubernetes server must be at or later than version 1.26.To check the version, enter kubectl version.Verify that the kubelet is using event-based container stage change monitoring. To check, look for the term EventedPLEG in the kubelet logs.The output should be similar to this:I0314 11:10:13.909915 1105457 feature_gate.go:249] feature gates: &{map[EventedPLEG:true]} If you have set --v to 4 and above, you might see more entries that indicate that the kubelet is using event-based container state monitoring.I0314 11:12:42.009542 1110177 evented.go:238] "Evented PLEG: Generated pod status from the received event" podUID=3b2c6172-b112-447a-ba96-94e7022912dc I0314 11:12:44.623326 1110177 evented.go:238] "Evented PLEG: Generated pod status from the received event" podUID=b3fba5ea-a8c5-4b76-8f43-481e17e8ec40 I0314 11:12:44.714564 1110177 evented.go:238] "Evented PLEG: Generated pod status from the received event" podUID=b3fba5ea-a8c5-4b76-8f43-481e17e8ec40 What's nextLearn more about the design in the Kubernetes Enhancement Proposal (KEP): Kubelet Evented PLEG for Better Performance.4.2.13 - Change the Reclaim Policy of a PersistentVolumeThis page shows how to change the reclaim policy of a Kubernetes PersistentVolume.Before you beginYou need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesTo check the version, enter kubectl version.Why change reclaim policy of a PersistentVolumePersistentVolumes can have various reclaim policies, including "Retain", "Recycle", and "Delete". For dynamically provisioned PersistentVolumes, the default reclaim policy is "Delete". This means that a dynamically provisioned volume is automatically deleted when a user deletes the corresponding PersistentVolumeClaim. This automatic behavior might be inappropriate if the volume contains precious data. In that case, it is more appropriate to use the "Retain" policy. With the "Retain" policy, if a user deletes a PersistentVolumeClaim, the corresponding PersistentVolume will not be deleted. Instead, it is moved to the Released phase, where all of its data can be manually recovered.Changing the reclaim policy of a PersistentVolumeList the PersistentVolumes in your cluster:kubectl get pv The output is similar to this:NAME CAPACITY ACCESSMODES RECLAIMPOLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-b6efd8da-b7b5-11e6-9d58-0ed433a7dd94 4Gi RWO Delete Bound default/claim1 manual 10s pvc-b95650f8-b7b5-11e6-9d58-0ed433a7dd94 4Gi RWO Delete Bound default/claim2 manual 6s pvc-bb3ca71d-b7b5-11e6-9d58-0ed433a7dd94 4Gi RWO Delete Bound default/claim3 manual 3s This list also includes the name of the claims that are bound to each volume for easier identification of dynamically provisioned volumes.Choose one of your PersistentVolumes and change its reclaim policy:kubectl patch pv  -p '{"spec":{"persistentVolumeReclaimPolicy":"Retain"}}' where  is the name of your chosen PersistentVolume.Note:On Windows, you must double quote any JSONPath template that contains spaces (not single quote as shown above for bash). This in turn means that you must use a single quote or escaped double quote around any literals in the template. For example:kubectl patch pv  -p "{\"spec\":{\"persistentVolumeReclaimPolicy\":\"Retain\"}}" Verify that your chosen PersistentVolume has the right policy:kubectl get pv The output is similar to this:NAME CAPACITY ACCESSMODES RECLAIMPOLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-b6efd8da-b7b5-11e6-9d58-0ed433a7dd94 4Gi RWO Delete Bound default/claim1 manual 40s pvc-b95650f8-b7b5-11e6-9d58-0ed433a7dd94 4Gi RWO Delete Bound default/claim2 manual 36s pvc-bb3ca71d-b7b5-11e6-9d58-0ed433a7dd94 4Gi RWO Retain Bound default/claim3 manual 33s In the preceding output, you can see that the volume bound to claim default/claim3 has reclaim policy Retain. It will not be automatically deleted when a user deletes claim default/claim3.What's nextLearn more about PersistentVolumes.Learn more about PersistentVolumeClaims.ReferencesPersistentVolumePay attention to the .spec.persistentVolumeReclaimPolicy field of PersistentVolume.PersistentVolumeClaim4.2.14 - Cloud Controller Manager AdministrationFEATURE STATE: Kubernetes v1.11 [beta]Since cloud providers develop and release at a different pace compared to the Kubernetes project, abstracting the provider-specific code to the cloud-controller-manager binary allows cloud vendors to evolve independently from the core Kubernetes code.The cloud-controller-manager can be linked to any cloud provider that satisfies cloudprovider.Interface. For backwards compatibility, the cloud-controller-manager provided in the core Kubernetes project uses the same cloud libraries as kube-controller-manager. Cloud providers already supported in Kubernetes core are expected to use the in-tree cloud-controller-manager to transition out of Kubernetes core.AdministrationRequirementsEvery cloud has their own set of requirements for running their own cloud provider integration, it should not be too different from the requirements when running kube-controller-manager. As a general rule of thumb you'll need:cloud authentication/authorization: your cloud may require a token or IAM rules to allow access to their APIskubernetes authentication/authorization: cloud-controller-manager may need RBAC rules set to speak to the kubernetes apiserverhigh availability: like kube-controller-manager, you may want a high available setup for cloud controller manager using leader election (on by default).Running cloud-controller-managerSuccessfully running cloud-controller-manager requires some changes to your cluster configuration.kubelet, kube-apiserver, and kube-controller-manager must be set according to the user's usage of external CCM. If the user has an external CCM (not the internal cloud controller loops in the Kubernetes Controller Manager), then --cloud-provider=external must be specified. Otherwise, it should not be specified.Keep in mind that setting up your cluster to use cloud controller manager will change your cluster behaviour in a few ways:Components that specify --cloud-provider=external will add a taint node.cloudprovider.kubernetes.io/uninitialized with an effect NoSchedule during initialization. This marks the node as needing a second initialization from an external controller before it can be scheduled work. Note that in the event that cloud controller manager is not available, new nodes in the cluster will be left unschedulable. The taint is important since the scheduler may require cloud specific information about nodes such as their region or type (high cpu, gpu, high memory, spot instance, etc).cloud information about nodes in the cluster will no longer be retrieved using local metadata, but instead all API calls to retrieve node information will go through cloud controller manager. This may mean you can restrict access to your cloud API on the kubelets for better security. For larger clusters you may want to consider if cloud controller manager will hit rate limits since it is now responsible for almost all API calls to your cloud from within the cluster.The cloud controller manager can implement:Node controller - responsible for updating kubernetes nodes using cloud APIs and deleting kubernetes nodes that were deleted on your cloud.Service controller - responsible for loadbalancers on your cloud against services of type LoadBalancer.Route controller - responsible for setting up network routes on your cloudany other features you would like to implement if you are running an out-of-tree provider.ExamplesIf you are using a cloud that is currently supported in Kubernetes core and would like to adopt cloud controller manager, see the cloud controller manager in kubernetes core.For cloud controller managers not in Kubernetes core, you can find the respective projects in repositories maintained by cloud vendors or by SIGs.For providers already in Kubernetes core, you can run the in-tree cloud controller manager as a DaemonSet in your cluster, use the following as a guideline:admin/cloud/ccm-example.yaml # This is an example of how to set up cloud-controller-manager as a Daemonset in your cluster. # It assumes that your masters can run pods and has the role node-role.kubernetes.io/master # Note that this Daemonset will not work straight out of the box for your cloud, this is # meant to be a guideline. \--- apiVersion: v1 kind: ServiceAccount metadata: name: cloud-controller-manager namespace: kube-system \--- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: system:cloud-controller-manager roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: \- kind: ServiceAccount name: cloud-controller-manager namespace: kube-system \--- apiVersion: apps/v1 kind: DaemonSet metadata: labels: k8s-app: cloud-controller-manager name: cloud-controller-manager namespace: kube-system spec: selector: matchLabels: k8s-app: cloud-controller-manager template: metadata: labels: k8s-app: cloud-controller-manager spec: serviceAccountName: cloud-controller-manager containers: \- name: cloud-controller-manager # for in-tree providers we use registry.k8s.io/cloud-controller-manager # this can be replaced with any other image for out-of-tree providers image: registry.k8s.io/cloud-controller-manager:v1.8.0 command: \- /usr/local/bin/cloud-controller-manager \- --cloud-provider=[YOUR_CLOUD_PROVIDER] # Add your own cloud provider here! \- --leader-elect=true \- --use-service-account-credentials # these flags will vary for every cloud provider \- --allocate-node-cidrs=true \- --configure-cloud-routes=true \- --cluster-cidr=172.17.0.0/16 tolerations: # this is required so CCM can bootstrap itself \- key: node.cloudprovider.kubernetes.io/uninitialized value: "true" effect: NoSchedule # these tolerations are to have the daemonset runnable on control plane nodes # remove them if your control plane nodes should not run pods \- key: node-role.kubernetes.io/control-plane operator: Exists effect: NoSchedule \- key: node-role.kubernetes.io/master operator: Exists effect: NoSchedule # this is to restrict CCM to only run on master nodes # the node selector may vary depending on your cluster setup nodeSelector: node-role.kubernetes.io/master: "" LimitationsRunning cloud controller manager comes with a few possible limitations. Although these limitations are being addressed in upcoming releases, it's important that you are aware of these limitations for production workloads.Support for VolumesCloud controller manager does not implement any of the volume controllers found in kube-controller-manager as the volume integrations also require coordination with kubelets. As we evolve CSI (container storage interface) and add stronger support for flex volume plugins, necessary support will be added to cloud controller manager so that clouds can fully integrate with volumes. Learn more about out-of-tree CSI volume plugins here.ScalabilityThe cloud-controller-manager queries your cloud provider's APIs to retrieve information for all nodes. For very large clusters, consider possible bottlenecks such as resource requirements and API rate limiting.Chicken and EggThe goal of the cloud controller manager project is to decouple development of cloud features from the core Kubernetes project. Unfortunately, many aspects of the Kubernetes project has assumptions that cloud provider features are tightly integrated into the project. As a result, adopting this new architecture can create several situations where a request is being made for information from a cloud provider, but the cloud controller manager may not be able to return that information without the original request being complete.A good example of this is the TLS bootstrapping feature in the Kubelet. TLS bootstrapping assumes that the Kubelet has the ability to ask the cloud provider (or a local metadata service) for all its address types (private, public, etc) but cloud controller manager cannot set a node's address types without being initialized in the first place which requires that the kubelet has TLS certificates to communicate with the apiserver.As this initiative evolves, changes will be made to address these issues in upcoming releases.What's nextTo build and develop your own cloud controller manager, read Developing Cloud Controller Manager.4.2.15 - Configure a kubelet image credential providerFEATURE STATE: Kubernetes v1.26 [stable]Starting from Kubernetes v1.20, the kubelet can dynamically retrieve credentials for a container image registry using exec plugins. The kubelet and the exec plugin communicate through stdio (stdin, stdout, and stderr) using Kubernetes versioned APIs. These plugins allow the kubelet to request credentials for a container registry dynamically as opposed to storing static credentials on disk. For example, the plugin may talk to a local metadata server to retrieve short-lived credentials for an image that is being pulled by the kubelet.You may be interested in using this capability if any of the below are true:API calls to a cloud provider service are required to retrieve authentication information for a registry.Credentials have short expiration times and requesting new credentials frequently is required.Storing registry credentials on disk or in imagePullSecrets is not acceptable.This guide demonstrates how to configure the kubelet's image credential provider plugin mechanism.Service Account Token for Image PullsFEATURE STATE: Kubernetes v1.33 [alpha] (enabled by default: false)Starting from Kubernetes v1.33, the kubelet can be configured to send a service account token bound to the pod for which the image pull is being performed to the credential provider plugin.This allows the plugin to exchange the token for credentials to access the image registry.To enable this feature, the KubeletServiceAccountTokenForCredentialProviders feature gate must be enabled on the kubelet, and the tokenAttributes field must be set in the CredentialProviderConfig file for the plugin.The tokenAttributes field contains information about the service account token that will be passed to the plugin, including the intended audience for the token and whether the plugin requires the pod to have a service account.Using service account token credentials can enable the following use-cases:Avoid needing a kubelet/node-based identity to pull images from a registry.Allow workloads to pull images based on their own runtime identity without long-lived/persisted secrets.Before you beginYou need a Kubernetes cluster with nodes that support kubelet credential provider plugins. This support is available in Kubernetes 1.33; Kubernetes v1.24 and v1.25 included this as a beta feature, enabled by default.If you are configuring a credential provider plugin that requires the service account token, you need a Kubernetes cluster with nodes running Kubernetes v1.33 or later and the KubeletServiceAccountTokenForCredentialProviders feature gate enabled on the kubelet.A working implementation of a credential provider exec plugin. You can build your own plugin or use one provided by cloud providers.Your Kubernetes server must be at or later than version v1.26.To check the version, enter kubectl version.Installing Plugins on NodesA credential provider plugin is an executable binary that will be run by the kubelet. Ensure that the plugin binary exists on every node in your cluster and stored in a known directory. The directory will be required later when configuring kubelet flags.Configuring the KubeletIn order to use this feature, the kubelet expects two flags to be set:--image-credential-provider-config - the path to the credential provider plugin config file.--image-credential-provider-bin-dir - the path to the directory where credential provider plugin binaries are located.Configure a kubelet credential providerThe configuration file passed into --image-credential-provider-config is read by the kubelet to determine which exec plugins should be invoked for which container images. Here's an example configuration file you may end up using if you are using the ECR-based plugin:apiVersion: kubelet.config.k8s.io/v1 kind: CredentialProviderConfig # providers is a list of credential provider helper plugins that will be enabled by the kubelet. # Multiple providers may match against a single image, in which case credentials # from all providers will be returned to the kubelet. If multiple providers are called # for a single image, the results are combined. If providers return overlapping # auth keys, the value from the provider earlier in this list is used. providers: # name is the required name of the credential provider. It must match the name of the # provider executable as seen by the kubelet. The executable must be in the kubelet's # bin directory (set by the --image-credential-provider-bin-dir flag). \- name: ecr-credential-provider # matchImages is a required list of strings used to match against images in order to # determine if this provider should be invoked. If one of the strings matches the # requested image from the kubelet, the plugin will be invoked and given a chance # to provide credentials. Images are expected to contain the registry domain # and URL path. # # Each entry in matchImages is a pattern which can optionally contain a port and a path. # Globs can be used in the domain, but not in the port or the path. Globs are supported # as subdomains like '*.k8s.io' or 'k8s.*.io', and top-level-domains such as 'k8s.*'. # Matching partial subdomains like 'app*.k8s.io' is also supported. Each glob can only match # a single subdomain segment, so `*.io` does **not** match `*.k8s.io`. # # A match exists between an image and a matchImage when all of the below are true: # - Both contain the same number of domain parts and each part matches. # - The URL path of an matchImages must be a prefix of the target image URL path. # - If the matchImages contains a port, then the port must match in the image as well. # # Example values of matchImages: # - 123456789.dkr.ecr.us-east-1.amazonaws.com # - *.azurecr.io # - gcr.io # - *.*.registry.io # - registry.io:8080/path matchImages: \- "*.dkr.ecr.*.amazonaws.com" \- "*.dkr.ecr.*.amazonaws.com.cn" \- "*.dkr.ecr-fips.*.amazonaws.com" \- "*.dkr.ecr.us-iso-east-1.c2s.ic.gov" \- "*.dkr.ecr.us-isob-east-1.sc2s.sgov.gov" # defaultCacheDuration is the default duration the plugin will cache credentials in-memory # if a cache duration is not provided in the plugin response. This field is required. defaultCacheDuration: "12h" # Required input version of the exec CredentialProviderRequest. The returned CredentialProviderResponse # MUST use the same encoding version as the input. Current supported values are: # - credentialprovider.kubelet.k8s.io/v1 apiVersion: credentialprovider.kubelet.k8s.io/v1 # Arguments to pass to the command when executing it. # +optional # args: # - --example-argument # Env defines additional environment variables to expose to the process. These # are unioned with the host's environment, as well as variables client-go uses # to pass argument to the plugin. # +optional env: \- name: AWS_PROFILE value: example_profile # tokenAttributes is the configuration for the service account token that will be passed to the plugin. # The credential provider opts in to using service account tokens for image pull by setting this field. # if this field is set without the `KubeletServiceAccountTokenForCredentialProviders` feature gate enabled, # kubelet will fail to start with invalid configuration error. # +optional tokenAttributes: # serviceAccountTokenAudience is the intended audience for the projected service account token. # +required serviceAccountTokenAudience: "" # requireServiceAccount indicates whether the plugin requires the pod to have a service account. # If set to true, kubelet will only invoke the plugin if the pod has a service account. # If set to false, kubelet will invoke the plugin even if the pod does not have a service account # and will not include a token in the CredentialProviderRequest. This is useful for plugins # that are used to pull images for pods without service accounts (e.g., static pods). # +required requireServiceAccount: true # requiredServiceAccountAnnotationKeys is the list of annotation keys that the plugin is interested in # and that are required to be present in the service account. # The keys defined in this list will be extracted from the corresponding service account and passed # to the plugin as part of the CredentialProviderRequest. If any of the keys defined in this list # are not present in the service account, kubelet will not invoke the plugin and will return an error. # This field is optional and may be empty. Plugins may use this field to extract additional information # required to fetch credentials or allow workloads to opt in to using service account tokens for image pull. # If non-empty, requireServiceAccount must be set to true. # The keys defined in this list must be unique and not overlap with the keys defined in the # optionalServiceAccountAnnotationKeys list. # +optional requiredServiceAccountAnnotationKeys: \- "example.com/required-annotation-key-1" \- "example.com/required-annotation-key-2" # optionalServiceAccountAnnotationKeys is the list of annotation keys that the plugin is interested in # and that are optional to be present in the service account. # The keys defined in this list will be extracted from the corresponding service account and passed # to the plugin as part of the CredentialProviderRequest. The plugin is responsible for validating the # existence of annotations and their values. This field is optional and may be empty. # Plugins may use this field to extract additional information required to fetch credentials. # The keys defined in this list must be unique and not overlap with the keys defined in the # requiredServiceAccountAnnotationKeys list. # +optional optionalServiceAccountAnnotationKeys: \- "example.com/optional-annotation-key-1" \- "example.com/optional-annotation-key-2" The providers field is a list of enabled plugins used by the kubelet. Each entry has a few required fields:name: the name of the plugin which MUST match the name of the executable binary that exists in the directory passed into --image-credential-provider-bin-dir.matchImages: a list of strings used to match against images in order to determine if this provider should be invoked. More on this below.defaultCacheDuration: the default duration the kubelet will cache credentials in-memory if a cache duration was not specified by the plugin.apiVersion: the API version that the kubelet and the exec plugin will use when communicating.Each credential provider can also be given optional args and environment variables as well. Consult the plugin implementors to determine what set of arguments and environment variables are required for a given plugin.If you are using the KubeletServiceAccountTokenForCredentialProviders feature gate and configuring the plugin to use the service account token by setting the tokenAttributes field, the following fields are required:serviceAccountTokenAudience: the intended audience for the projected service account token. This cannot be the empty string.requireServiceAccount: whether the plugin requires the pod to have a service account.If set to true, kubelet will only invoke the plugin if the pod has a service account.If set to false, kubelet will invoke the plugin even if the pod does not have a service account and will not include a token in the CredentialProviderRequest.This is useful for plugins that are used to pull images for pods without service accounts (e.g., static pods).Configure image matchingThe matchImages field for each credential provider is used by the kubelet to determine whether a plugin should be invoked for a given image that a Pod is using. Each entry in matchImages is an image pattern which can optionally contain a port and a path. Globs can be used in the domain, but not in the port or the path. Globs are supported as subdomains like *.k8s.io or k8s.*.io, and top-level domains such as k8s.*. Matching partial subdomains like app*.k8s.io is also supported. Each glob can only match a single subdomain segment, so *.io does NOT match *.k8s.io.A match exists between an image name and a matchImage entry when all of the below are true:Both contain the same number of domain parts and each part matches.The URL path of match image must be a prefix of the target image URL path.If the matchImages contains a port, then the port must match in the image as well.Some example values of matchImages patterns are:123456789.dkr.ecr.us-east-1.amazonaws.com*.azurecr.iogcr.io*.*.registry.iofoo.registry.io:8080/pathWhat's nextRead the details about CredentialProviderConfig in the kubelet configuration API (v1) reference.Read the kubelet credential provider API reference (v1).4.2.16 - Configure Quotas for API ObjectsThis page shows how to configure quotas for API objects, including PersistentVolumeClaims and Services. A quota restricts the number of objects, of a particular type, that can be created in a namespace. You specify quotas in a ResourceQuota object.Before you beginYou need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesTo check the version, enter kubectl version.Create a namespaceCreate a namespace so that the resources you create in this exercise are isolated from the rest of your cluster.kubectl create namespace quota-object-example Create a ResourceQuotaHere is the configuration file for a ResourceQuota object:admin/resource/quota-objects.yaml apiVersion: v1 kind: ResourceQuota metadata: name: object-quota-demo spec: hard: persistentvolumeclaims: "1" services.loadbalancers: "2" services.nodeports: "0" Create the ResourceQuota:kubectl apply -f https://k8s.io/examples/admin/resource/quota-objects.yaml --namespace=quota-object-example View detailed information about the ResourceQuota:kubectl get resourcequota object-quota-demo --namespace=quota-object-example --output=yaml The output shows that in the quota-object-example namespace, there can be at most one PersistentVolumeClaim, at most two Services of type LoadBalancer, and no Services of type NodePort.status: hard: persistentvolumeclaims: "1" services.loadbalancers: "2" services.nodeports: "0" used: persistentvolumeclaims: "0" services.loadbalancers: "0" services.nodeports: "0" Create a PersistentVolumeClaimHere is the configuration file for a PersistentVolumeClaim object:admin/resource/quota-objects-pvc.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: pvc-quota-demo spec: storageClassName: manual accessModes: \- ReadWriteOnce resources: requests: storage: 3Gi Create the PersistentVolumeClaim:kubectl apply -f https://k8s.io/examples/admin/resource/quota-objects-pvc.yaml --namespace=quota-object-example Verify that the PersistentVolumeClaim was created:kubectl get persistentvolumeclaims --namespace=quota-object-example The output shows that the PersistentVolumeClaim exists and has status Pending:NAME STATUS pvc-quota-demo Pending Attempt to create a second PersistentVolumeClaimHere is the configuration file for a second PersistentVolumeClaim:admin/resource/quota-objects-pvc-2.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: pvc-quota-demo-2 spec: storageClassName: manual accessModes: \- ReadWriteOnce resources: requests: storage: 4Gi Attempt to create the second PersistentVolumeClaim:kubectl apply -f https://k8s.io/examples/admin/resource/quota-objects-pvc-2.yaml --namespace=quota-object-example The output shows that the second PersistentVolumeClaim was not created, because it would have exceeded the quota for the namespace.persistentvolumeclaims "pvc-quota-demo-2" is forbidden: exceeded quota: object-quota-demo, requested: persistentvolumeclaims=1, used: persistentvolumeclaims=1, limited: persistentvolumeclaims=1 NotesThese are the strings used to identify API resources that can be constrained by quotas:StringAPI Object"pods"Pod"services"Service"replicationcontrollers"ReplicationController"resourcequotas"ResourceQuota"secrets"Secret"configmaps"ConfigMap"persistentvolumeclaims"PersistentVolumeClaim"services.nodeports"Service of type NodePort"services.loadbalancers"Service of type LoadBalancerClean upDelete your namespace:kubectl delete namespace quota-object-example What's nextFor cluster administratorsConfigure Default Memory Requests and Limits for a NamespaceConfigure Default CPU Requests and Limits for a NamespaceConfigure Minimum and Maximum Memory Constraints for a NamespaceConfigure Minimum and Maximum CPU Constraints for a NamespaceConfigure Memory and CPU Quotas for a NamespaceConfigure a Pod Quota for a NamespaceFor app developersAssign Memory Resources to Containers and PodsAssign CPU Resources to Containers and PodsAssign Pod-level CPU and memory resourcesConfigure Quality of Service for Pods4.2.17 - Control CPU Management Policies on the NodeFEATURE STATE: Kubernetes v1.26 [stable]Kubernetes keeps many aspects of how pods execute on nodes abstracted from the user. This is by design. However, some workloads require stronger guarantees in terms of latency and/or performance in order to operate acceptably. The kubelet provides methods to enable more complex workload placement policies while keeping the abstraction free from explicit placement directives.For detailed information on resource management, please refer to the Resource Management for Pods and Containers documentation.For detailed information on how the kubelet implements resource management, please refer to the Node ResourceManagers documentation.Before you beginYou need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesYour Kubernetes server must be at or later than version v1.26.To check the version, enter kubectl version.If you are running an older version of Kubernetes, please look at the documentation for the version you are actually running.Configuring CPU management policiesBy default, the kubelet uses CFS quota to enforce pod CPU limits. When the node runs many CPU-bound pods, the workload can move to different CPU cores depending on whether the pod is throttled and which CPU cores are available at scheduling time. Many workloads are not sensitive to this migration and thus work fine without any intervention.However, in workloads where CPU cache affinity and scheduling latency significantly affect workload performance, the kubelet allows alternative CPU management policies to determine some placement preferences on the node.Windows SupportFEATURE STATE: Kubernetes v1.32 [alpha] (enabled by default: false)CPU Manager support can be enabled on Windows by using the WindowsCPUAndMemoryAffinity feature gate and it requires support in the container runtime. Once the feature gate is enabled, follow the steps below to configure the CPU manager policy.ConfigurationThe CPU Manager policy is set with the --cpu-manager-policy kubelet flag or the cpuManagerPolicy field in KubeletConfiguration. There are two supported policies:none: the default policy.static: allows pods with certain resource characteristics to be granted increased CPU affinity and exclusivity on the node.The CPU manager periodically writes resource updates through the CRI in order to reconcile in-memory CPU assignments with cgroupfs. The reconcile frequency is set through a new Kubelet configuration value \--cpu-manager-reconcile-period. If not specified, it defaults to the same duration as --node-status-update-frequency.The behavior of the static policy can be fine-tuned using the --cpu-manager-policy-options flag. The flag takes a comma-separated list of key=value policy options. If you disable the CPUManagerPolicyOptions feature gate then you cannot fine-tune CPU manager policies. In that case, the CPU manager operates only using its default settings.In addition to the top-level CPUManagerPolicyOptions feature gate, the policy options are split into two groups: alpha quality (hidden by default) and beta quality (visible by default). The groups are guarded respectively by the CPUManagerPolicyAlphaOptions and CPUManagerPolicyBetaOptions feature gates. Diverging from the Kubernetes standard, these feature gates guard groups of options, because it would have been too cumbersome to add a feature gate for each individual option.Changing the CPU Manager PolicySince the CPU manager policy can only be applied when kubelet spawns new pods, simply changing from "none" to "static" won't apply to existing pods. So in order to properly change the CPU manager policy on a node, perform the following steps:Drain the node.Stop kubelet.Remove the old CPU manager state file. The path to this file is /var/lib/kubelet/cpu_manager_state by default. This clears the state maintained by the CPUManager so that the cpu-sets set up by the new policy won’t conflict with it.Edit the kubelet configuration to change the CPU manager policy to the desired value.Start kubelet.Repeat this process for every node that needs its CPU manager policy changed. Skipping this process will result in kubelet crashlooping with the following error:could not restore state from checkpoint: configured policy "static" differs from state checkpoint policy "none", please drain this node and delete the CPU manager checkpoint file "/var/lib/kubelet/cpu_manager_state" before restarting Kubelet Note:if the set of online CPUs changes on the node, the node must be drained and CPU manager manually reset by deleting the state file cpu_manager_state in the kubelet root directory.none policy configurationThis policy has no extra configuration items.static policy configurationThis policy manages a shared pool of CPUs that initially contains all CPUs in the node. The amount of exclusively allocatable CPUs is equal to the total number of CPUs in the node minus any CPU reservations by the kubelet --kube-reserved or \--system-reserved options. From 1.17, the CPU reservation list can be specified explicitly by kubelet --reserved-cpus option. The explicit CPU list specified by \--reserved-cpus takes precedence over the CPU reservation specified by \--kube-reserved and --system-reserved. CPUs reserved by these options are taken, in integer quantity, from the initial shared pool in ascending order by physical core ID. This shared pool is the set of CPUs on which any containers in BestEffort and Burstable pods run. Containers in Guaranteed pods with fractional CPU requests also run on CPUs in the shared pool. Only containers that are both part of a Guaranteed pod and have integer CPU requests are assigned exclusive CPUs.Note:The kubelet requires a CPU reservation greater than zero be made using either --kube-reserved and/or --system-reserved or --reserved-cpus when the static policy is enabled. This is because zero CPU reservation would allow the shared pool to become empty.Static policy optionsYou can toggle groups of options on and off based upon their maturity level using the following feature gates:CPUManagerPolicyBetaOptions default enabled. Disable to hide beta-level options.CPUManagerPolicyAlphaOptions default disabled. Enable to show alpha-level options. You will still have to enable each option using the CPUManagerPolicyOptions kubelet option.The following policy options exist for the static CPUManager policy:full-pcpus-only (GA, visible by default) (1.33 or higher)distribute-cpus-across-numa (beta, visible by default) (1.33 or higher)align-by-socket (alpha, hidden by default) (1.25 or higher)distribute-cpus-across-cores (alpha, hidden by default) (1.31 or higher)strict-cpu-reservation (beta, visible by default) (1.32 or higher)prefer-align-cpus-by-uncorecache (alpha, hidden by default) (1.32 or higher)The full-pcpus-only option can be enabled by adding full-pcpus-only=true to the CPUManager policy options. Likewise, the distribute-cpus-across-numa option can be enabled by adding distribute-cpus-across-numa=true to the CPUManager policy options. When both are set, they are "additive" in the sense that CPUs will be distributed across NUMA nodes in chunks of full-pcpus rather than individual cores. The align-by-socket policy option can be enabled by adding align-by-socket=true to the CPUManager policy options. It is also additive to the full-pcpus-only and distribute-cpus-across-numa policy options.The distribute-cpus-across-cores option can be enabled by adding distribute-cpus-across-cores=true to the CPUManager policy options. It cannot be used with full-pcpus-only or distribute-cpus-across-numa policy options together at this moment.The strict-cpu-reservation option can be enabled by adding strict-cpu-reservation=true to the CPUManager policy options followed by removing the /var/lib/kubelet/cpu_manager_state file and restart kubelet.The prefer-align-cpus-by-uncorecache option can be enabled by adding the prefer-align-cpus-by-uncorecache to the CPUManager policy options. If incompatible options are used, the kubelet will fail to start with the error explained in the logs.For mode detail about the behavior of the individual options you can configure, please refer to the Node ResourceManagers documentation.4.2.18 - Control Topology Management Policies on a nodeFEATURE STATE: Kubernetes v1.27 [stable]An increasing number of systems leverage a combination of CPUs and hardware accelerators to support latency-critical execution and high-throughput parallel computation. These include workloads in fields such as telecommunications, scientific computing, machine learning, financial services and data analytics. Such hybrid systems comprise a high performance environment.In order to extract the best performance, optimizations related to CPU isolation, memory and device locality are required. However, in Kubernetes, these optimizations are handled by a disjoint set of components.Topology Manager is a kubelet component that aims to coordinate the set of components that are responsible for these optimizations.Before you beginYou need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesYour Kubernetes server must be at or later than version v1.18.To check the version, enter kubectl version.How topology manager worksPrior to the introduction of Topology Manager, the CPU and Device Manager in Kubernetes make resource allocation decisions independently of each other. This can result in undesirable allocations on multiple-socketed systems, and performance/latency sensitive applications will suffer due to these undesirable allocations. Undesirable in this case meaning, for example, CPUs and devices being allocated from different NUMA Nodes, thus incurring additional latency.The Topology Manager is a kubelet component, which acts as a source of truth so that other kubelet components can make topology aligned resource allocation choices.The Topology Manager provides an interface for components, called Hint Providers, to send and receive topology information. The Topology Manager has a set of node level policies which are explained below.The Topology Manager receives topology information from the Hint Providers as a bitmask denoting NUMA Nodes available and a preferred allocation indication. The Topology Manager policies perform a set of operations on the hints provided and converge on the hint determined by the policy to give the optimal result. If an undesirable hint is stored, the preferred field for the hint will be set to false. In the current policies preferred is the narrowest preferred mask. The selected hint is stored as part of the Topology Manager. Depending on the policy configured, the pod can be accepted or rejected from the node based on the selected hint. The hint is then stored in the Topology Manager for use by the Hint Providers when making the resource allocation decisions.Windows SupportFEATURE STATE: Kubernetes v1.32 [alpha] (enabled by default: false)The Topology Manager support can be enabled on Windows by using the WindowsCPUAndMemoryAffinity feature gate and it requires support in the container runtime.Topology manager scopes and policiesThe Topology Manager currently:aligns Pods of all QoS classes.aligns the requested resources that Hint Provider provides topology hints for.If these conditions are met, the Topology Manager will align the requested resources.In order to customize how this alignment is carried out, the Topology Manager provides two distinct options: scope and policy.The scope defines the granularity at which you would like resource alignment to be performed, for example, at the pod or container level. And the policy defines the actual policy used to carry out the alignment, for example, best-effort, restricted, and single-numa-node. Details on the various scopes and policies available today can be found below.Note:To align CPU resources with other requested resources in a Pod spec, the CPU Manager should be enabled and proper CPU Manager policy should be configured on a Node. See Control CPU Management Policies on the Node.Note:To align memory (and hugepages) resources with other requested resources in a Pod spec, the Memory Manager should be enabled and proper Memory Manager policy should be configured on a Node. Refer to Memory Manager documentation.Topology manager scopesThe Topology Manager can deal with the alignment of resources in a couple of distinct scopes:container (default)podEither option can be selected at a time of the kubelet startup, by setting the topologyManagerScope in the kubelet configuration file.container scopeThe container scope is used by default. You can also explicitly set the topologyManagerScope to container in the kubelet configuration file.Within this scope, the Topology Manager performs a number of sequential resource alignments, i.e., for each container (in a pod) a separate alignment is computed. In other words, there is no notion of grouping the containers to a specific set of NUMA nodes, for this particular scope. In effect, the Topology Manager performs an arbitrary alignment of individual containers to NUMA nodes.The notion of grouping the containers was endorsed and implemented on purpose in the following scope, for example the pod scope.pod scopeTo select the pod scope, set topologyManagerScope in the kubelet configuration file to pod.This scope allows for grouping all containers in a pod to a common set of NUMA nodes. That is, the Topology Manager treats a pod as a whole and attempts to allocate the entire pod (all containers) to either a single NUMA node or a common set of NUMA nodes. The following examples illustrate the alignments produced by the Topology Manager on different occasions:all containers can be and are allocated to a single NUMA node;all containers can be and are allocated to a shared set of NUMA nodes.The total amount of particular resource demanded for the entire pod is calculated according to effective requests/limits formula, and thus, this total value is equal to the maximum of:the sum of all app container requests,the maximum of init container requests,for a resource.Using the pod scope in tandem with single-numa-node Topology Manager policy is specifically valuable for workloads that are latency sensitive or for high-throughput applications that perform IPC. By combining both options, you are able to place all containers in a pod onto a single NUMA node; hence, the inter-NUMA communication overhead can be eliminated for that pod.In the case of single-numa-node policy, a pod is accepted only if a suitable set of NUMA nodes is present among possible allocations. Reconsider the example above:a set containing only a single NUMA node - it leads to pod being admitted,whereas a set containing more NUMA nodes - it results in pod rejection (because instead of one NUMA node, two or more NUMA nodes are required to satisfy the allocation).To recap, the Topology Manager first computes a set of NUMA nodes and then tests it against the Topology Manager policy, which either leads to the rejection or admission of the pod.Topology manager policiesThe Topology Manager supports four allocation policies. You can set a policy via a kubelet flag, \--topology-manager-policy. There are four supported policies:none (default)best-effortrestrictedsingle-numa-nodeNote:If the Topology Manager is configured with the pod scope, the container, which is considered by the policy, is reflecting requirements of the entire pod, and thus each container from the pod will result with the same topology alignment decision.none policyThis is the default policy and does not perform any topology alignment.best-effort policyFor each container in a Pod, the kubelet, with best-effort topology management policy, calls each Hint Provider to discover their resource availability. Using this information, the Topology Manager stores the preferred NUMA Node affinity for that container. If the affinity is not preferred, the Topology Manager will store this and admit the pod to the node anyway.The Hint Providers can then use this information when making the resource allocation decision.restricted policyFor each container in a Pod, the kubelet, with restricted topology management policy, calls each Hint Provider to discover their resource availability. Using this information, the Topology Manager stores the preferred NUMA Node affinity for that container. If the affinity is not preferred, the Topology Manager will reject this pod from the node. This will result in a pod entering a Terminated state with a pod admission failure.Once the pod is in a Terminated state, the Kubernetes scheduler will not attempt to reschedule the pod. It is recommended to use a ReplicaSet or Deployment to trigger a redeployment of the pod. An external control loop could be also implemented to trigger a redeployment of pods that have the Topology Affinity error.If the pod is admitted, the Hint Providers can then use this information when making the resource allocation decision.single-numa-node policyFor each container in a Pod, the kubelet, with single-numa-node topology management policy, calls each Hint Provider to discover their resource availability. Using this information, the Topology Manager determines if a single NUMA Node affinity is possible. If it is, Topology Manager will store this and the Hint Providers can then use this information when making the resource allocation decision. If, however, this is not possible then the Topology Manager will reject the pod from the node. This will result in a pod in a Terminated state with a pod admission failure.Once the pod is in a Terminated state, the Kubernetes scheduler will not attempt to reschedule the pod. It is recommended to use a Deployment with replicas to trigger a redeployment of the Pod. An external control loop could be also implemented to trigger a redeployment of pods that have the Topology Affinity error.Topology manager policy optionsSupport for the Topology Manager policy options requires TopologyManagerPolicyOptions feature gate to be enabled (it is enabled by default).You can toggle groups of options on and off based upon their maturity level using the following feature gates:TopologyManagerPolicyBetaOptions default enabled. Enable to show beta-level options.TopologyManagerPolicyAlphaOptions default disabled. Enable to show alpha-level options.You will still have to enable each option using the TopologyManagerPolicyOptions kubelet option.prefer-closest-numa-nodesThe prefer-closest-numa-nodes option is GA since Kubernetes 1.32. In Kubernetes 1.33 this policy option is visible by default provided that the TopologyManagerPolicyOptions feature gate is enabled.The Topology Manager is not aware by default of NUMA distances, and does not take them into account when making Pod admission decisions. This limitation surfaces in multi-socket, as well as single-socket multi NUMA systems, and can cause significant performance degradation in latency-critical execution and high-throughput applications if the Topology Manager decides to align resources on non-adjacent NUMA nodes.If you specify the prefer-closest-numa-nodes policy option, the best-effort and restricted policies favor sets of NUMA nodes with shorter distance between them when making admission decisions.You can enable this option by adding prefer-closest-numa-nodes=true to the Topology Manager policy options.By default (without this option), the Topology Manager aligns resources on either a single NUMA node or, in the case where more than one NUMA node is required, using the minimum number of NUMA nodes.max-allowable-numa-nodes (beta)The max-allowable-numa-nodes option is beta since Kubernetes 1.31. In Kubernetes 1.33, this policy option is visible by default provided that the TopologyManagerPolicyOptions and TopologyManagerPolicyBetaOptions feature gates are enabled.The time to admit a pod is tied to the number of NUMA nodes on the physical machine. By default, Kubernetes does not run a kubelet with the Topology Manager enabled, on any (Kubernetes) node where more than 8 NUMA nodes are detected.Note:If you select the max-allowable-numa-nodes policy option, nodes with more than 8 NUMA nodes can be allowed to run with the Topology Manager enabled. The Kubernetes project only has limited data on the impact of using the Topology Manager on (Kubernetes) nodes with more than 8 NUMA nodes. Because of that lack of data, using this policy option with Kubernetes 1.33 is not recommended and is at your own risk.You can enable this option by adding max-allowable-numa-nodes=true to the Topology Manager policy options.Setting a value of max-allowable-numa-nodes does not (in and of itself) affect the latency of pod admission, but binding a Pod to a (Kubernetes) node with many NUMA does have an impact. Future, potential improvements to Kubernetes may improve Pod admission performance and the high latency that happens as the number of NUMA nodes increases.Pod interactions with topology manager policiesConsider the containers in the following Pod manifest:spec: containers: \- name: nginx image: nginx This pod runs in the BestEffort QoS class because no resource requests or limits are specified.spec: containers: \- name: nginx image: nginx resources: limits: memory: "200Mi" requests: memory: "100Mi" This pod runs in the Burstable QoS class because requests are less than limits.If the selected policy is anything other than none, the Topology Manager would consider these Pod specifications. The Topology Manager would consult the Hint Providers to get topology hints. In the case of the static, the CPU Manager policy would return default topology hint, because these Pods do not explicitly request CPU resources.spec: containers: \- name: nginx image: nginx resources: limits: memory: "200Mi" cpu: "2" example.com/device: "1" requests: memory: "200Mi" cpu: "2" example.com/device: "1" This pod with integer CPU request runs in the Guaranteed QoS class because requests are equal to limits.spec: containers: \- name: nginx image: nginx resources: limits: memory: "200Mi" cpu: "300m" example.com/device: "1" requests: memory: "200Mi" cpu: "300m" example.com/device: "1" This pod with sharing CPU request runs in the Guaranteed QoS class because requests are equal to limits.spec: containers: \- name: nginx image: nginx resources: limits: example.com/deviceA: "1" example.com/deviceB: "1" requests: example.com/deviceA: "1" example.com/deviceB: "1" This pod runs in the BestEffort QoS class because there are no CPU and memory requests.The Topology Manager would consider the above pods. The Topology Manager would consult the Hint Providers, which are CPU and Device Manager to get topology hints for the pods.In the case of the Guaranteed pod with integer CPU request, the static CPU Manager policy would return topology hints relating to the exclusive CPU and the Device Manager would send back hints for the requested device.In the case of the Guaranteed pod with sharing CPU request, the static CPU Manager policy would return default topology hint as there is no exclusive CPU request and the Device Manager would send back hints for the requested device.In the above two cases of the Guaranteed pod, the none CPU Manager policy would return default topology hint.In the case of the BestEffort pod, the static CPU Manager policy would send back the default topology hint as there is no CPU request and the Device Manager would send back the hints for each of the requested devices.Using this information the Topology Manager calculates the optimal hint for the pod and stores this information, which will be used by the Hint Providers when they are making their resource assignments.Known limitationsThe maximum number of NUMA nodes that Topology Manager allows is 8. With more than 8 NUMA nodes, there will be a state explosion when trying to enumerate the possible NUMA affinities and generating their hints. See max-allowable-numa-nodes (beta) for more options.The scheduler is not topology-aware, so it is possible to be scheduled on a node and then fail on the node due to the Topology Manager.4.2.19 - Customizing DNS ServiceThis page explains how to configure your DNS Pod(s) and customize the DNS resolution process in your cluster.Before you beginYou need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesYour cluster must be running the CoreDNS add-on.Your Kubernetes server must be at or later than version v1.12.To check the version, enter kubectl version.IntroductionDNS is a built-in Kubernetes service launched automatically using the addon manager cluster add-on.Note:The CoreDNS Service is named kube-dns in the metadata.name field.The intent is to ensure greater interoperability with workloads that relied on the legacy kube-dns Service name to resolve addresses internal to the cluster. Using a Service named kube-dns abstracts away the implementation detail of which DNS provider is running behind that common name.If you are running CoreDNS as a Deployment, it will typically be exposed as a Kubernetes Service with a static IP address. The kubelet passes DNS resolver information to each container with the \--cluster-dns= flag.DNS names also need domains. You configure the local domain in the kubelet with the flag --cluster-domain=.The DNS server supports forward lookups (A and AAAA records), port lookups (SRV records), reverse IP address lookups (PTR records), and more. For more information, see DNS for Services and Pods.If a Pod's dnsPolicy is set to default, it inherits the name resolution configuration from the node that the Pod runs on. The Pod's DNS resolution should behave the same as the node. But see Known issues.If you don't want this, or if you want a different DNS config for pods, you can use the kubelet's --resolv-conf flag. Set this flag to "" to prevent Pods from inheriting DNS. Set it to a valid file path to specify a file other than /etc/resolv.conf for DNS inheritance.CoreDNSCoreDNS is a general-purpose authoritative DNS server that can serve as cluster DNS, complying with the DNS specifications.CoreDNS ConfigMap optionsCoreDNS is a DNS server that is modular and pluggable, with plugins adding new functionalities. The CoreDNS server can be configured by maintaining a Corefile, which is the CoreDNS configuration file. As a cluster administrator, you can modify the ConfigMap for the CoreDNS Corefile to change how DNS service discovery behaves for that cluster.In Kubernetes, CoreDNS is installed with the following default Corefile configuration:apiVersion: v1 kind: ConfigMap metadata: name: coredns namespace: kube-system data: Corefile: | .:53 { errors health { lameduck 5s } ready kubernetes cluster.local in-addr.arpa ip6.arpa { pods insecure fallthrough in-addr.arpa ip6.arpa ttl 30 } prometheus :9153 forward . /etc/resolv.conf cache 30 loop reload loadbalance } The Corefile configuration includes the following plugins of CoreDNS:errors: Errors are logged to stdout.health: Health of CoreDNS is reported to http://localhost:8080/health. In this extended syntax lameduck will make the process unhealthy then wait for 5 seconds before the process is shut down.ready: An HTTP endpoint on port 8181 will return 200 OK, when all plugins that are able to signal readiness have done so.kubernetes: CoreDNS will reply to DNS queries based on IP of the Services and Pods. You can find more details about this plugin on the CoreDNS website.ttl allows you to set a custom TTL for responses. The default is 5 seconds. The minimum TTL allowed is 0 seconds, and the maximum is capped at 3600 seconds. Setting TTL to 0 will prevent records from being cached.The pods insecure option is provided for backward compatibility with kube-dns.You can use the pods verified option, which returns an A record only if there exists a pod in the same namespace with a matching IP.The pods disabled option can be used if you don't use pod records.prometheus: Metrics of CoreDNS are available at http://localhost:9153/metrics in the Prometheus format (also known as OpenMetrics).forward: Any queries that are not within the Kubernetes cluster domain are forwarded to predefined resolvers (/etc/resolv.conf).cache: This enables a frontend cache.loop: Detects simple forwarding loops and halts the CoreDNS process if a loop is found.reload: Allows automatic reload of a changed Corefile. After you edit the ConfigMap configuration, allow two minutes for your changes to take effect.loadbalance: This is a round-robin DNS loadbalancer that randomizes the order of A, AAAA, and MX records in the answer.You can modify the default CoreDNS behavior by modifying the ConfigMap.Configuration of Stub-domain and upstream nameserver using CoreDNSCoreDNS has the ability to configure stub-domains and upstream nameservers using the forward plugin.ExampleIf a cluster operator has a Consul domain server located at "10.150.0.1", and all Consul names have the suffix ".consul.local". To configure it in CoreDNS, the cluster administrator creates the following stanza in the CoreDNS ConfigMap.consul.local:53 { errors cache 30 forward . 10.150.0.1 } To explicitly force all non-cluster DNS lookups to go through a specific nameserver at 172.16.0.1, point the forward to the nameserver instead of /etc/resolv.confforward . 172.16.0.1 The final ConfigMap along with the default Corefile configuration looks like:apiVersion: v1 kind: ConfigMap metadata: name: coredns namespace: kube-system data: Corefile: | .:53 { errors health kubernetes cluster.local in-addr.arpa ip6.arpa { pods insecure fallthrough in-addr.arpa ip6.arpa } prometheus :9153 forward . 172.16.0.1 cache 30 loop reload loadbalance } consul.local:53 { errors cache 30 forward . 10.150.0.1 } Note:CoreDNS does not support FQDNs for stub-domains and nameservers (eg: "ns.foo.com"). During translation, all FQDN nameservers will be omitted from the CoreDNS config.What's nextRead Debugging DNS Resolution4.2.20 - Debugging DNS ResolutionThis page provides hints on diagnosing DNS problems.Before you beginYou need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesYour cluster must be configured to use the CoreDNS addon or its precursor, kube-dns.Your Kubernetes server must be at or later than version v1.6.To check the version, enter kubectl version.Create a simple Pod to use as a test environmentadmin/dns/dnsutils.yaml apiVersion: v1 kind: Pod metadata: name: dnsutils namespace: default spec: containers: \- name: dnsutils image: registry.k8s.io/e2e-test-images/agnhost:2.39 imagePullPolicy: IfNotPresent restartPolicy: Always Note:This example creates a pod in the default namespace. DNS name resolution for services depends on the namespace of the pod. For more information, review DNS for Services and Pods.Use that manifest to create a Pod:kubectl apply -f https://k8s.io/examples/admin/dns/dnsutils.yaml pod/dnsutils created …and verify its status:kubectl get pods dnsutils NAME READY STATUS RESTARTS AGE dnsutils 1/1 Running 0  Once that Pod is running, you can exec nslookup in that environment. If you see something like the following, DNS is working correctly.kubectl exec -i -t dnsutils -- nslookup kubernetes.default Server: 10.0.0.10 Address 1: 10.0.0.10 Name: kubernetes.default Address 1: 10.0.0.1 If the nslookup command fails, check the following:Check the local DNS configuration firstTake a look inside the resolv.conf file. (See Customizing DNS Service and Known issues below for more information)kubectl exec -ti dnsutils -- cat /etc/resolv.conf Verify that the search path and name server are set up like the following (note that search path may vary for different cloud providers):search default.svc.cluster.local svc.cluster.local cluster.local google.internal c.gce_project_id.internal nameserver 10.0.0.10 options ndots:5 Errors such as the following indicate a problem with the CoreDNS (or kube-dns) add-on or with associated Services:kubectl exec -i -t dnsutils -- nslookup kubernetes.default Server: 10.0.0.10 Address 1: 10.0.0.10 nslookup: can't resolve 'kubernetes.default' orkubectl exec -i -t dnsutils -- nslookup kubernetes.default Server: 10.0.0.10 Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local nslookup: can't resolve 'kubernetes.default' Check if the DNS pod is runningUse the kubectl get pods command to verify that the DNS pod is running.kubectl get pods --namespace=kube-system -l k8s-app=kube-dns NAME READY STATUS RESTARTS AGE ... coredns-7b96bf9f76-5hsxb 1/1 Running 0 1h coredns-7b96bf9f76-mvmmt 1/1 Running 0 1h ... Note:The value for label k8s-app is kube-dns for both CoreDNS and kube-dns deployments.If you see that no CoreDNS Pod is running or that the Pod has failed/completed, the DNS add-on may not be deployed by default in your current environment and you will have to deploy it manually.Check for errors in the DNS podUse the kubectl logs command to see logs for the DNS containers.For CoreDNS:kubectl logs --namespace=kube-system -l k8s-app=kube-dns Here is an example of a healthy CoreDNS log:.:53 2018/08/15 14:37:17 [INFO] CoreDNS-1.2.2 2018/08/15 14:37:17 [INFO] linux/amd64, go1.10.3, 2e322f6 CoreDNS-1.2.2 linux/amd64, go1.10.3, 2e322f6 2018/08/15 14:37:17 [INFO] plugin/reload: Running configuration MD5 = 24e6c59e83ce706f07bcc82c31b1ea1c See if there are any suspicious or unexpected messages in the logs.Is DNS service up?Verify that the DNS service is up by using the kubectl get service command.kubectl get svc --namespace=kube-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE ... kube-dns ClusterIP 10.0.0.10  53/UDP,53/TCP 1h ... Note:The service name is kube-dns for both CoreDNS and kube-dns deployments.If you have created the Service or in the case it should be created by default but it does not appear, see debugging Services for more information.Are DNS endpoints exposed?You can verify that DNS endpoints are exposed by using the kubectl get endpointslice command.kubectl get endpointslices -l k8s.io/service-name=kube-dns --namespace=kube-system NAME ADDRESSTYPE PORTS ENDPOINTS AGE kube-dns-zxoja IPv4 53 10.180.3.17,10.180.3.17 1h If you do not see the endpoints, see the endpoints section in the debugging Services documentation.For additional Kubernetes DNS examples, see the cluster-dns examples in the Kubernetes GitHub repository.Are DNS queries being received/processed?You can verify if queries are being received by CoreDNS by adding the log plugin to the CoreDNS configuration (aka Corefile). The CoreDNS Corefile is held in a ConfigMap named coredns. To edit it, use the command:kubectl -n kube-system edit configmap coredns Then add log in the Corefile section per the example below:apiVersion: v1 kind: ConfigMap metadata: name: coredns namespace: kube-system data: Corefile: | .:53 { log errors health kubernetes cluster.local in-addr.arpa ip6.arpa { pods insecure upstream fallthrough in-addr.arpa ip6.arpa } prometheus :9153 forward . /etc/resolv.conf cache 30 loop reload loadbalance } After saving the changes, it may take up to minute or two for Kubernetes to propagate these changes to the CoreDNS pods.Next, make some queries and view the logs per the sections above in this document. If CoreDNS pods are receiving the queries, you should see them in the logs.Here is an example of a query in the log:.:53 2018/08/15 14:37:15 [INFO] CoreDNS-1.2.0 2018/08/15 14:37:15 [INFO] linux/amd64, go1.10.3, 2e322f6 CoreDNS-1.2.0 linux/amd64, go1.10.3, 2e322f6 2018/09/07 15:29:04 [INFO] plugin/reload: Running configuration MD5 = 162475cdf272d8aa601e6fe67a6ad42f 2018/09/07 15:29:04 [INFO] Reloading complete 172.17.0.18:41675 - [07/Sep/2018:15:29:11 +0000] 59925 "A IN kubernetes.default.svc.cluster.local. udp 54 false 512" NOERROR qr,aa,rd,ra 106 0.000066649s Does CoreDNS have sufficient permissions?CoreDNS must be able to list service and endpointslice related resources to properly resolve service names.Sample error message:2022-03-18T07:12:15.699431183Z [INFO] 10.96.144.227:52299 - 3686 "A IN serverproxy.contoso.net.cluster.local. udp 52 false 512" SERVFAIL qr,aa,rd 145 0.000091221s First, get the current ClusterRole of system:coredns:kubectl describe clusterrole system:coredns -n kube-system Expected output:PolicyRule: Resources Non-Resource URLs Resource Names Verbs \--------- ----------------- -------------- ----- endpoints [] [] [list watch] namespaces [] [] [list watch] pods [] [] [list watch] services [] [] [list watch] endpointslices.discovery.k8s.io [] [] [list watch] If any permissions are missing, edit the ClusterRole to add them:kubectl edit clusterrole system:coredns -n kube-system Example insertion of EndpointSlices permissions:... \- apiGroups: \- discovery.k8s.io resources: \- endpointslices verbs: \- list \- watch ... Are you in the right namespace for the service?DNS queries that don't specify a namespace are limited to the pod's namespace.If the namespace of the pod and service differ, the DNS query must include the namespace of the service.This query is limited to the pod's namespace:kubectl exec -i -t dnsutils -- nslookup  This query specifies the namespace:kubectl exec -i -t dnsutils -- nslookup . To learn more about name resolution, see DNS for Services and Pods.Known issuesSome Linux distributions (e.g. Ubuntu) use a local DNS resolver by default (systemd-resolved). Systemd-resolved moves and replaces /etc/resolv.conf with a stub file that can cause a fatal forwarding loop when resolving names in upstream servers. This can be fixed manually by using kubelet's --resolv-conf flag to point to the correct resolv.conf (With systemd-resolved, this is /run/systemd/resolve/resolv.conf). kubeadm automatically detects systemd-resolved, and adjusts the kubelet flags accordingly.Kubernetes installs do not configure the nodes' resolv.conf files to use the cluster DNS by default, because that process is inherently distribution-specific. This should probably be implemented eventually.Linux's libc (a.k.a. glibc) has a limit for the DNS nameserver records to 3 by default and Kubernetes needs to consume 1 nameserver record. This means that if a local installation already uses 3 nameservers, some of those entries will be lost. To work around this limit, the node can run dnsmasq, which will provide more nameserver entries. You can also use kubelet's --resolv-conf flag.If you are using Alpine version 3.17 or earlier as your base image, DNS may not work properly due to a design issue with Alpine. Until musl version 1.24 didn't include TCP fallback to the DNS stub resolver meaning any DNS call above 512 bytes would fail. Please upgrade your images to Alpine version 3.18 or above.What's nextSee Autoscaling the DNS Service in a Cluster.Read DNS for Services and Pods4.2.21 - Declare Network PolicyThis document helps you get started using the Kubernetes NetworkPolicy API to declare network policies that govern how pods communicate with each other.Note: This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the content guide before submitting a change. More information.Before you beginYou need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesYour Kubernetes server must be at or later than version v1.8.To check the version, enter kubectl version.Make sure you've configured a network provider with network policy support. There are a number of network providers that support NetworkPolicy, including:AntreaCalicoCiliumKube-routerRomanaWeave NetCreate an nginx deployment and expose it via a serviceTo see how Kubernetes network policy works, start off by creating an nginx Deployment.kubectl create deployment nginx --image=nginx deployment.apps/nginx created Expose the Deployment through a Service called nginx.kubectl expose deployment nginx --port=80 service/nginx exposed The above commands create a Deployment with an nginx Pod and expose the Deployment through a Service named nginx. The nginx Pod and Deployment are found in the default namespace.kubectl get svc,pod NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes 10.100.0.1  443/TCP 46m service/nginx 10.100.0.16  80/TCP 33s NAME READY STATUS RESTARTS AGE pod/nginx-701339712-e0qfq 1/1 Running 0 35s Test the service by accessing it from another PodYou should be able to access the new nginx service from other Pods. To access the nginx Service from another Pod in the default namespace, start a busybox container:kubectl run busybox --rm -ti --image=busybox -- /bin/sh In your shell, run the following command:wget --spider --timeout=1 nginx Connecting to nginx (10.100.0.16:80) remote file exists Limit access to the nginx serviceTo limit the access to the nginx service so that only Pods with the label access: true can query it, create a NetworkPolicy object as follows:service/networking/nginx-policy.yaml apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: access-nginx spec: podSelector: matchLabels: app: nginx ingress: \- from: \- podSelector: matchLabels: access: "true" The name of a NetworkPolicy object must be a valid DNS subdomain name.Note:NetworkPolicy includes a podSelector which selects the grouping of Pods to which the policy applies. You can see this policy selects Pods with the label app=nginx. The label was automatically added to the Pod in the nginx Deployment. An empty podSelector selects all pods in the namespace.Assign the policy to the serviceUse kubectl to create a NetworkPolicy from the above nginx-policy.yaml file:kubectl apply -f https://k8s.io/examples/service/networking/nginx-policy.yaml networkpolicy.networking.k8s.io/access-nginx created Test access to the service when access label is not definedWhen you attempt to access the nginx Service from a Pod without the correct labels, the request times out:kubectl run busybox --rm -ti --image=busybox -- /bin/sh In your shell, run the command:wget --spider --timeout=1 nginx Connecting to nginx (10.100.0.16:80) wget: download timed out Define access label and test againYou can create a Pod with the correct labels to see that the request is allowed:kubectl run busybox --rm -ti --labels="access=true" --image=busybox -- /bin/sh In your shell, run the command:wget --spider --timeout=1 nginx Connecting to nginx (10.100.0.16:80) remote file exists 4.2.22 - Developing Cloud Controller ManagerFEATURE STATE: Kubernetes v1.11 [beta]The cloud-controller-manager is a Kubernetes control plane component that embeds cloud-specific control logic. The cloud controller manager lets you link your cluster into your cloud provider's API, and separates out the components that interact with that cloud platform from components that only interact with your cluster.By decoupling the interoperability logic between Kubernetes and the underlying cloud infrastructure, the cloud-controller-manager component enables cloud providers to release features at a different pace compared to the main Kubernetes project.BackgroundSince cloud providers develop and release at a different pace compared to the Kubernetes project, abstracting the provider-specific code to the cloud-controller-manager binary allows cloud vendors to evolve independently from the core Kubernetes code.The Kubernetes project provides skeleton cloud-controller-manager code with Go interfaces to allow you (or your cloud provider) to plug in your own implementations. This means that a cloud provider can implement a cloud-controller-manager by importing packages from Kubernetes core; each cloudprovider will register their own code by calling cloudprovider.RegisterCloudProvider to update a global variable of available cloud providers.DevelopingOut of treeTo build an out-of-tree cloud-controller-manager for your cloud:Create a go package with an implementation that satisfies cloudprovider.Interface.Use main.go in cloud-controller-manager from Kubernetes core as a template for your main.go. As mentioned above, the only difference should be the cloud package that will be imported.Import your cloud package in main.go, ensure your package has an init block to run cloudprovider.RegisterCloudProvider.Many cloud providers publish their controller manager code as open source. If you are creating a new cloud-controller-manager from scratch, you could take an existing out-of-tree cloud controller manager as your starting point.In treeFor in-tree cloud providers, you can run the in-tree cloud controller manager as a DaemonSet in your cluster. See Cloud Controller Manager Administration for more details.4.2.23 - Enable Or Disable A Kubernetes APIThis page shows how to enable or disable an API version from your cluster's control plane.Specific API versions can be turned on or off by passing --runtime-config=api/ as a command line argument to the API server. The values for this argument are a comma-separated list of API versions. Later values override earlier values.The runtime-config command line argument also supports 2 special keys:api/all, representing all known APIsapi/legacy, representing only legacy APIs. Legacy APIs are any APIs that have been explicitly deprecated.For example, to turn off all API versions except v1, pass --runtime-config=api/all=false,api/v1=true to the kube-apiserver.What's nextRead the full documentation for the kube-apiserver component.4.2.24 - Encrypting Confidential Data at RestAll of the APIs in Kubernetes that let you write persistent API resource data support at-rest encryption. For example, you can enable at-rest encryption for Secrets. This at-rest encryption is additional to any system-level encryption for the etcd cluster or for the filesystem(s) on hosts where you are running the kube-apiserver.This page shows how to enable and configure encryption of API data at rest.Note:This task covers encryption for resource data stored using the Kubernetes API. For example, you can encrypt Secret objects, including the key-value data they contain.If you want to encrypt data in filesystems that are mounted into containers, you instead need to either:use a storage integration that provides encrypted volumesencrypt the data within your own applicationBefore you beginYou need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesThis task assumes that you are running the Kubernetes API server as a static pod on each control plane node.Your cluster's control plane must use etcd v3.x (major version 3, any minor version).To encrypt a custom resource, your cluster must be running Kubernetes v1.26 or newer.To use a wildcard to match resources, your cluster must be running Kubernetes v1.27 or newer.To check the version, enter kubectl version.Determine whether encryption at rest is already enabledBy default, the API server stores plain-text representations of resources into etcd, with no at-rest encryption.The kube-apiserver process accepts an argument --encryption-provider-config that specifies a path to a configuration file. The contents of that file, if you specify one, control how Kubernetes API data is encrypted in etcd. If you are running the kube-apiserver without the --encryption-provider-config command line argument, you do not have encryption at rest enabled. If you are running the kube-apiserver with the --encryption-provider-config command line argument, and the file that it references specifies the identity provider as the first encryption provider in the list, then you do not have at-rest encryption enabled (the default identity provider does not provide any confidentiality protection.)If you are running the kube-apiserver with the --encryption-provider-config command line argument, and the file that it references specifies a provider other than identity as the first encryption provider in the list, then you already have at-rest encryption enabled. However, that check does not tell you whether a previous migration to encrypted storage has succeeded. If you are not sure, see ensure all relevant data are encrypted.Understanding the encryption at rest configuration--- # # CAUTION: this is an example configuration. # Do not use this for your own cluster! # apiVersion: apiserver.config.k8s.io/v1 kind: EncryptionConfiguration resources: \- resources: \- secrets \- configmaps \- pandas.awesome.bears.example # a custom resource API providers: # This configuration does not provide data confidentiality. The first # configured provider is specifying the "identity" mechanism, which # stores resources as plain text. # \- identity: {} # plain text, in other words NO encryption \- aesgcm: keys: \- name: key1 secret: c2VjcmV0IGlzIHNlY3VyZQ== \- name: key2 secret: dGhpcyBpcyBwYXNzd29yZA== \- aescbc: keys: \- name: key1 secret: c2VjcmV0IGlzIHNlY3VyZQ== \- name: key2 secret: dGhpcyBpcyBwYXNzd29yZA== \- secretbox: keys: \- name: key1 secret: YWJjZGVmZ2hpamtsbW5vcHFyc3R1dnd4eXoxMjM0NTY= \- resources: \- events providers: \- identity: {} # do not encrypt Events even though *.* is specified below \- resources: \- '*.apps' # wildcard match requires Kubernetes 1.27 or later providers: \- aescbc: keys: \- name: key2 secret: c2VjcmV0IGlzIHNlY3VyZSwgb3IgaXMgaXQ/Cg== \- resources: \- '*.*' # wildcard match requires Kubernetes 1.27 or later providers: \- aescbc: keys: \- name: key3 secret: c2VjcmV0IGlzIHNlY3VyZSwgSSB0aGluaw==Each resources array item is a separate config and contains a complete configuration. The resources.resources field is an array of Kubernetes resource names (resource or resource.group) that should be encrypted like Secrets, ConfigMaps, or other resources.If custom resources are added to EncryptionConfiguration and the cluster version is 1.26 or newer, any newly created custom resources mentioned in the EncryptionConfiguration will be encrypted. Any custom resources that existed in etcd prior to that version and configuration will be unencrypted until they are next written to storage. This is the same behavior as built-in resources. See the Ensure all secrets are encrypted section.The providers array is an ordered list of the possible encryption providers to use for the APIs that you listed. Each provider supports multiple keys - the keys are tried in order for decryption, and if the provider is the first provider, the first key is used for encryption.Only one provider type may be specified per entry (identity or aescbc may be provided, but not both in the same item). The first provider in the list is used to encrypt resources written into the storage. When reading resources from storage, each provider that matches the stored data attempts in order to decrypt the data. If no provider can read the stored data due to a mismatch in format or secret key, an error is returned which prevents clients from accessing that resource.EncryptionConfiguration supports the use of wildcards to specify the resources that should be encrypted. Use '*.' to encrypt all resources within a group (for eg '*.apps' in above example) or '*.*' to encrypt all resources. '*.' can be used to encrypt all resource in the core group. '*.*' will encrypt all resources, even custom resources that are added after API server start.Note:Use of wildcards that overlap within the same resource list or across multiple entries are not allowed since part of the configuration would be ineffective. The resources list's processing order and precedence are determined by the order it's listed in the configuration.If you have a wildcard covering resources and want to opt out of at-rest encryption for a particular kind of resource, you achieve that by adding a separate resources array item with the name of the resource that you want to exempt, followed by a providers array item where you specify the identity provider. You add this item to the list so that it appears earlier than the configuration where you do specify encryption (a provider that is not identity).For example, if '*.*' is enabled and you want to opt out of encryption for Events and ConfigMaps, add a new earlier item to the resources, followed by the providers array item with identity as the provider. The more specific entry must come before the wildcard entry.The new item would look similar to: ... \- resources: \- configmaps. # specifically from the core API group, # because of trailing "." \- events providers: \- identity: {} # and then other entries in resources Ensure that the exemption is listed before the wildcard '*.*' item in the resources array to give it precedence.For more detailed information about the EncryptionConfiguration struct, please refer to the encryption configuration API.Caution:If any resource is not readable via the encryption configuration (because keys were changed), and you cannot restore a working configuration, your only recourse is to delete that entry from the underlying etcd directly.Any calls to the Kubernetes API that attempt to read that resource will fail until it is deleted or a valid decryption key is provided.Available providersBefore you configure encryption-at-rest for data in your cluster's Kubernetes API, you need to select which provider(s) you will use.The following table describes each available provider.Providers for Kubernetes encryption at restNameEncryptionStrengthSpeedKey lengthidentityNoneN/AN/AN/AResources written as-is without encryption. When set as the first provider, the resource will be decrypted as new values are written. Existing encrypted resources are not automatically overwritten with the plaintext data. The identity provider is the default if you do not specify otherwise.aescbcAES-CBC with PKCS#7 paddingWeakFast16, 24, or 32-byteNot recommended due to CBC's vulnerability to padding oracle attacks. Key material accessible from control plane host.aesgcmAES-GCM with random nonceMust be rotated every 200,000 writesFastest16, 24, or 32-byteNot recommended for use except when an automated key rotation scheme is implemented. Key material accessible from control plane host.kms v1 (deprecated since Kubernetes v1.28)Uses envelope encryption scheme with DEK per resource.StrongestSlow (compared to kms version 2)32-bytesData is encrypted by data encryption keys (DEKs) using AES-GCM; DEKs are encrypted by key encryption keys (KEKs) according to configuration in Key Management Service (KMS). Simple key rotation, with a new DEK generated for each encryption, and KEK rotation controlled by the user.Read how to configure the KMS V1 provider.kms v2Uses envelope encryption scheme with DEK per API server.StrongestFast32-bytesData is encrypted by data encryption keys (DEKs) using AES-GCM; DEKs are encrypted by key encryption keys (KEKs) according to configuration in Key Management Service (KMS). Kubernetes generates a new DEK per encryption from a secret seed. The seed is rotated whenever the KEK is rotated.A good choice if using a third party tool for key management. Available as stable from Kubernetes v1.29.Read how to configure the KMS V2 provider.secretboxXSalsa20 and Poly1305StrongFaster32-byteUses relatively new encryption technologies that may not be considered acceptable in environments that require high levels of review. Key material accessible from control plane host.The identity provider is the default if you do not specify otherwise. The identity provider does not encrypt stored data and provides no additional confidentiality protection.Key storageLocal key storageEncrypting secret data with a locally managed key protects against an etcd compromise, but it fails to protect against a host compromise. Since the encryption keys are stored on the host in the EncryptionConfiguration YAML file, a skilled attacker can access that file and extract the encryption keys.Managed (KMS) key storageThe KMS provider uses envelope encryption: Kubernetes encrypts resources using a data key, and then encrypts that data key using the managed encryption service. Kubernetes generates a unique data key for each resource. The API server stores an encrypted version of the data key in etcd alongside the ciphertext; when reading the resource, the API server calls the managed encryption service and provides both the ciphertext and the (encrypted) data key. Within the managed encryption service, the provider use a key encryption key to decipher the data key, deciphers the data key, and finally recovers the plain text. Communication between the control plane and the KMS requires in-transit protection, such as TLS.Using envelope encryption creates dependence on the key encryption key, which is not stored in Kubernetes. In the KMS case, an attacker who intends to get unauthorised access to the plaintext values would need to compromise etcd and the third-party KMS provider.Protection for encryption keysYou should take appropriate measures to protect the confidential information that allows decryption, whether that is a local encryption key, or an authentication token that allows the API server to call KMS.Even when you rely on a provider to manage the use and lifecycle of the main encryption key (or keys), you are still responsible for making sure that access controls and other security measures for the managed encryption service are appropriate for your security needs.Encrypt your dataGenerate the encryption keyThe following steps assume that you are not using KMS, and therefore the steps also assume that you need to generate an encryption key. If you already have an encryption key, skip to Write an encryption configuration file.Caution:Storing the raw encryption key in the EncryptionConfig only moderately improves your security posture, compared to no encryption.For additional secrecy, consider using the kms provider as this relies on keys held outside your Kubernetes cluster. Implementations of kms can work with hardware security modules or with encryption services managed by your cloud provider.To learn about setting up encryption at rest using KMS, see Using a KMS provider for data encryption. The KMS provider plugin that you use may also come with additional specific documentation.Start by generating a new encryption key, and then encode it using base64:LinuxmacOSWindowsGenerate a 32-byte random key and base64 encode it. You can use this command:head -c 32 /dev/urandom | base64 You can use /dev/hwrng instead of /dev/urandom if you want to use your PC's built-in hardware entropy source. Not all Linux devices provide a hardware random generator.Generate a 32-byte random key and base64 encode it. You can use this command:head -c 32 /dev/urandom | base64 Generate a 32-byte random key and base64 encode it. You can use this command:# Do not run this in a session where you have set a random number # generator seed. [Convert]::ToBase64String((1..32|%{[byte](Get-Random -Max 256)})) Note:Keep the encryption key confidential, including while you generate it and ideally even after you are no longer actively using it.Replicate the encryption keyUsing a secure mechanism for file transfer, make a copy of that encryption key available to every other control plane host.At a minimum, use encryption in transit - for example, secure shell (SSH). For more security, use asymmetric encryption between hosts, or change the approach you are using so that you're relying on KMS encryption.Write an encryption configuration fileCaution:The encryption configuration file may contain keys that can decrypt content in etcd. If the configuration file contains any key material, you must properly restrict permissions on all your control plane hosts so only the user who runs the kube-apiserver can read this configuration.Create a new encryption configuration file. The contents should be similar to:--- apiVersion: apiserver.config.k8s.io/v1 kind: EncryptionConfiguration resources: \- resources: \- secrets \- configmaps \- pandas.awesome.bears.example providers: \- aescbc: keys: \- name: key1 # See the following text for more details about the secret value secret:  \- identity: {} # this fallback allows reading unencrypted secrets; # for example, during initial migration To create a new encryption key (that does not use KMS), see Generate the encryption key.Use the new encryption configuration fileYou will need to mount the new encryption config file to the kube-apiserver static pod. Here is an example on how to do that:Save the new encryption config file to /etc/kubernetes/enc/enc.yaml on the control-plane node.Edit the manifest for the kube-apiserver static pod: /etc/kubernetes/manifests/kube-apiserver.yaml so that it is similar to:--- # # This is a fragment of a manifest for a static Pod. # Check whether this is correct for your cluster and for your API server. # apiVersion: v1 kind: Pod metadata: annotations: kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 10.20.30.40:443 creationTimestamp: null labels: app.kubernetes.io/component: kube-apiserver tier: control-plane name: kube-apiserver namespace: kube-system spec: containers: \- command: \- kube-apiserver ... \- --encryption-provider-config=/etc/kubernetes/enc/enc.yaml # add this line volumeMounts: ... \- name: enc # add this line mountPath: /etc/kubernetes/enc # add this line readOnly: true # add this line ... volumes: ... \- name: enc # add this line hostPath: # add this line path: /etc/kubernetes/enc # add this line type: DirectoryOrCreate # add this line ... Restart your API server.Caution:Your config file contains keys that can decrypt the contents in etcd, so you must properly restrict permissions on your control-plane nodes so only the user who runs the kube-apiserver can read it.You now have encryption in place for one control plane host. A typical Kubernetes cluster has multiple control plane hosts, so there is more to do.Reconfigure other control plane hostsIf you have multiple API servers in your cluster, you should deploy the changes in turn to each API server.Caution:For cluster configurations with two or more control plane nodes, the encryption configuration should be identical across each control plane node.If there is a difference in the encryption provider configuration between control plane nodes, this difference may mean that the kube-apiserver can't decrypt data.When you are planning to update the encryption configuration of your cluster, plan this so that the API servers in your control plane can always decrypt the stored data (even part way through rolling out the change).Make sure that you use the same encryption configuration on each control plane host.Verify that newly written data is encryptedData is encrypted when written to etcd. After restarting your kube-apiserver, any newly created or updated Secret (or other resource kinds configured in EncryptionConfiguration) should be encrypted when stored.To check this, you can use the etcdctl command line program to retrieve the contents of your secret data.This example shows how to check this for encrypting the Secret API.Create a new Secret called secret1 in the default namespace:kubectl create secret generic secret1 -n default --from-literal=mykey=mydata Using the etcdctl command line tool, read that Secret out of etcd:ETCDCTL_API=3 etcdctl get /registry/secrets/default/secret1 [...] | hexdump -C where [...] must be the additional arguments for connecting to the etcd server.For example:ETCDCTL_API=3 etcdctl \ \--cacert=/etc/kubernetes/pki/etcd/ca.crt \ \--cert=/etc/kubernetes/pki/etcd/server.crt \ \--key=/etc/kubernetes/pki/etcd/server.key \ get /registry/secrets/default/secret1 | hexdump -C The output is similar to this (abbreviated):00000000 2f 72 65 67 69 73 74 72 79 2f 73 65 63 72 65 74 |/registry/secret| 00000010 73 2f 64 65 66 61 75 6c 74 2f 73 65 63 72 65 74 |s/default/secret| 00000020 31 0a 6b 38 73 3a 65 6e 63 3a 61 65 73 63 62 63 |1.k8s:enc:aescbc| 00000030 3a 76 31 3a 6b 65 79 31 3a c7 6c e7 d3 09 bc 06 |:v1:key1:.l.....| 00000040 25 51 91 e4 e0 6c e5 b1 4d 7a 8b 3d b9 c2 7c 6e |%Q...l..Mz.=..|n| 00000050 b4 79 df 05 28 ae 0d 8e 5f 35 13 2c c0 18 99 3e |.y..(..._5.,...>| [...] 00000110 23 3a 0d fc 28 ca 48 2d 6b 2d 46 cc 72 0b 70 4c |#:..(.H-k-F.r.pL| 00000120 a5 fc 35 43 12 4e 60 ef bf 6f fe cf df 0b ad 1f |..5C.N`..o......| 00000130 82 c4 88 53 02 da 3e 66 ff 0a |...S..>f..| 0000013a Verify the stored Secret is prefixed with k8s:enc:aescbc:v1: which indicates the aescbc provider has encrypted the resulting data. Confirm that the key name shown in etcd matches the key name specified in the EncryptionConfiguration mentioned above. In this example, you can see that the encryption key named key1 is used in etcd and in EncryptionConfiguration.Verify the Secret is correctly decrypted when retrieved via the API:kubectl get secret secret1 -n default -o yaml The output should contain mykey: bXlkYXRh, with contents of mydata encoded using base64; read decoding a Secret to learn how to completely decode the Secret.Ensure all relevant data are encryptedIt's often not enough to make sure that new objects get encrypted: you also want that encryption to apply to the objects that are already stored.For this example, you have configured your cluster so that Secrets are encrypted on write. Performing a replace operation for each Secret will encrypt that content at rest, where the objects are unchanged.You can make this change across all Secrets in your cluster:# Run this as an administrator that can read and write all Secrets kubectl get secrets --all-namespaces -o json | kubectl replace -f - The command above reads all Secrets and then updates them with the same data, in order to apply server side encryption.Note:If an error occurs due to a conflicting write, retry the command. It is safe to run that command more than once.For larger clusters, you may wish to subdivide the Secrets by namespace, or script an update.Prevent plain text retrievalIf you want to make sure that the only access to a particular API kind is done using encryption, you can remove the API server's ability to read that API's backing data as plaintext.Warning:Making this change prevents the API server from retrieving resources that are marked as encrypted at rest, but are actually stored in the clear.When you have configured encryption at rest for an API (for example: the API kind Secret, representing secrets resources in the core API group), you must ensure that all those resources in this cluster really are encrypted at rest. Check this before you carry on with the next steps.Once all Secrets in your cluster are encrypted, you can remove the identity part of the encryption configuration. For example:--- apiVersion: apiserver.config.k8s.io/v1 kind: EncryptionConfiguration resources: \- resources: \- secrets providers: \- aescbc: keys: \- name: key1 secret:  \- identity: {} # REMOVE THIS LINE…and then restart each API server in turn. This change prevents the API server from accessing a plain-text Secret, even by accident.Rotate a decryption keyChanging an encryption key for Kubernetes without incurring downtime requires a multi-step operation, especially in the presence of a highly-available deployment where multiple kube-apiserver processes are running.Generate a new key and add it as the second key entry for the current provider on all control plane nodes.Restart all kube-apiserver processes, to ensure each server can decrypt any data that are encrypted with the new key.Make a secure backup of the new encryption key. If you lose all copies of this key you would need to delete all the resources were encrypted under the lost key, and workloads may not operate as expected during the time that at-rest encryption is broken.Make the new key the first entry in the keys array so that it is used for encryption-at-rest for new writesRestart all kube-apiserver processes to ensure each control plane host now encrypts using the new keyAs a privileged user, run kubectl get secrets --all-namespaces -o json | kubectl replace -f - to encrypt all existing Secrets with the new keyAfter you have updated all existing Secrets to use the new key and have made a secure backup of the new key, remove the old decryption key from the configuration.Decrypt all dataThis example shows how to stop encrypting the Secret API at rest. If you are encrypting other API kinds, adjust the steps to match.To disable encryption at rest, place the identity provider as the first entry in your encryption configuration file:--- apiVersion: apiserver.config.k8s.io/v1 kind: EncryptionConfiguration resources: \- resources: \- secrets # list any other resources here that you previously were # encrypting at rest providers: \- identity: {} # add this line \- aescbc: keys: \- name: key1 secret:  # keep this in place # make sure it comes after "identity" Then run the following command to force decryption of all Secrets:kubectl get secrets --all-namespaces -o json | kubectl replace -f - Once you have replaced all existing encrypted resources with backing data that don't use encryption, you can remove the encryption settings from the kube-apiserver.Configure automatic reloadingYou can configure automatic reloading of encryption provider configuration. That setting determines whether the API server should load the file you specify for --encryption-provider-config only once at startup, or automatically whenever you change that file. Enabling this option allows you to change the keys for encryption at rest without restarting the API server.To allow automatic reloading, configure the API server to run with: \--encryption-provider-config-automatic-reload=true. When enabled, file changes are polled every minute to observe the modifications. The apiserver_encryption_config_controller_automatic_reload_last_timestamp_seconds metric identifies when the new config becomes effective. This allows encryption keys to be rotated without restarting the API server.What's nextRead about decrypting data that are already stored at restLearn more about the EncryptionConfiguration configuration API (v1).4.2.25 - Decrypt Confidential Data that is Already Encrypted at RestAll of the APIs in Kubernetes that let you write persistent API resource data support at-rest encryption. For example, you can enable at-rest encryption for Secrets. This at-rest encryption is additional to any system-level encryption for the etcd cluster or for the filesystem(s) on hosts where you are running the kube-apiserver.This page shows how to switch from encryption of API data at rest, so that API data are stored unencrypted. You might want to do this to improve performance; usually, though, if it was a good idea to encrypt some data, it's also a good idea to leave them encrypted.Note:This task covers encryption for resource data stored using the Kubernetes API. For example, you can encrypt Secret objects, including the key-value data they contain.If you wanted to manage encryption for data in filesystems that are mounted into containers, you instead need to either:use a storage integration that provides encrypted volumesencrypt the data within your own applicationBefore you beginYou need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesThis task assumes that you are running the Kubernetes API server as a static pod on each control plane node.Your cluster's control plane must use etcd v3.x (major version 3, any minor version).To encrypt a custom resource, your cluster must be running Kubernetes v1.26 or newer.You should have some API data that are already encrypted.To check the version, enter kubectl version.Determine whether encryption at rest is already enabledBy default, the API server uses an identity provider that stores plain-text representations of resources. The default identity provider does not provide any confidentiality protection.The kube-apiserver process accepts an argument --encryption-provider-config that specifies a path to a configuration file. The contents of that file, if you specify one, control how Kubernetes API data is encrypted in etcd. If it is not specified, you do not have encryption at rest enabled.The format of that configuration file is YAML, representing a configuration API kind named EncryptionConfiguration. You can see an example configuration in Encryption at rest configuration.If --encryption-provider-config is set, check which resources (such as secrets) are configured for encryption, and what provider is used. Make sure that the preferred provider for that resource type is not identity; you only set identity (no encryption) as default when you want to disable encryption at rest. Verify that the first-listed provider for a resource is something other than identity, which means that any new information written to resources of that type will be encrypted as configured. If you do see identity as the first-listed provider for any resource, this means that those resources are being written out to etcd without encryption.Decrypt all dataThis example shows how to stop encrypting the Secret API at rest. If you are encrypting other API kinds, adjust the steps to match.Locate the encryption configuration fileFirst, find the API server configuration files. On each control plane node, static Pod manifest for the kube-apiserver specifies a command line argument, --encryption-provider-config. You are likely to find that this file is mounted into the static Pod using a hostPath volume mount. Once you locate the volume you can find the file on the node filesystem and inspect it.Configure the API server to decrypt objectsTo disable encryption at rest, place the identity provider as the first entry in your encryption configuration file.For example, if your existing EncryptionConfiguration file reads:--- apiVersion: apiserver.config.k8s.io/v1 kind: EncryptionConfiguration resources: \- resources: \- secrets providers: \- aescbc: keys: # Do not use this (invalid) example key for encryption \- name: example secret: 2KfZgdiq2K0g2YrYpyDYs9mF2LPZhQ== then change it to:--- apiVersion: apiserver.config.k8s.io/v1 kind: EncryptionConfiguration resources: \- resources: \- secrets providers: \- identity: {} # add this line \- aescbc: keys: \- name: example secret: 2KfZgdiq2K0g2YrYpyDYs9mF2LPZhQ== and restart the kube-apiserver Pod on this node.Reconfigure other control plane hostsIf you have multiple API servers in your cluster, you should deploy the changes in turn to each API server.Make sure that you use the same encryption configuration on each control plane host.Force decryptionThen run the following command to force decryption of all Secrets:# If you are decrypting a different kind of object, change "secrets" to match. kubectl get secrets --all-namespaces -o json | kubectl replace -f - Once you have replaced all existing encrypted resources with backing data that don't use encryption, you can remove the encryption settings from the kube-apiserver.The command line options to remove are:--encryption-provider-config--encryption-provider-config-automatic-reloadRestart the kube-apiserver Pod again to apply the new configuration.Reconfigure other control plane hostsIf you have multiple API servers in your cluster, you should again deploy the changes in turn to each API server.Make sure that you use the same encryption configuration on each control plane host.What's nextLearn more about the EncryptionConfiguration configuration API (v1).4.2.26 - Guaranteed Scheduling For Critical Add-On PodsKubernetes core components such as the API server, scheduler, and controller-manager run on a control plane node. However, add-ons must run on a regular cluster node. Some of these add-ons are critical to a fully functional cluster, such as metrics-server, DNS, and UI. A cluster may stop working properly if a critical add-on is evicted (either manually or as a side effect of another operation like upgrade) and becomes pending (for example when the cluster is highly utilized and either there are other pending pods that schedule into the space vacated by the evicted critical add-on pod or the amount of resources available on the node changed for some other reason).Note that marking a pod as critical is not meant to prevent evictions entirely; it only prevents the pod from becoming permanently unavailable. A static pod marked as critical can't be evicted. However, non-static pods marked as critical are always rescheduled.Marking pod as criticalTo mark a Pod as critical, set priorityClassName for that Pod to system-cluster-critical or system-node-critical. system-node-critical is the highest available priority, even higher than system-cluster-critical.4.2.27 - IP Masquerade Agent User GuideThis page shows how to configure and enable the ip-masq-agent.Before you beginYou need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesTo check the version, enter kubectl version.IP Masquerade Agent User GuideThe ip-masq-agent configures iptables rules to hide a pod's IP address behind the cluster node's IP address. This is typically done when sending traffic to destinations outside the cluster's pod CIDR range.Key TermsNAT (Network Address Translation): Is a method of remapping one IP address to another by modifying either the source and/or destination address information in the IP header. Typically performed by a device doing IP routing.Masquerading: A form of NAT that is typically used to perform a many to one address translation, where multiple source IP addresses are masked behind a single address, which is typically the device doing the IP routing. In Kubernetes this is the Node's IP address.CIDR (Classless Inter-Domain Routing): Based on the variable-length subnet masking, allows specifying arbitrary-length prefixes. CIDR introduced a new method of representation for IP addresses, now commonly known as CIDR notation, in which an address or routing prefix is written with a suffix indicating the number of bits of the prefix, such as 192.168.2.0/24.Link Local: A link-local address is a network address that is valid only for communications within the network segment or the broadcast domain that the host is connected to. Link-local addresses for IPv4 are defined in the address block 169.254.0.0/16 in CIDR notation.The ip-masq-agent configures iptables rules to handle masquerading node/pod IP addresses when sending traffic to destinations outside the cluster node's IP and the Cluster IP range. This essentially hides pod IP addresses behind the cluster node's IP address. In some environments, traffic to "external" addresses must come from a known machine address. For example, in Google Cloud, any traffic to the internet must come from a VM's IP. When containers are used, as in Google Kubernetes Engine, the Pod IP will be rejected for egress. To avoid this, we must hide the Pod IP behind the VM's own IP address - generally known as "masquerade". By default, the agent is configured to treat the three private IP ranges specified by RFC 1918 as non-masquerade CIDR. These ranges are 10.0.0.0/8, 172.16.0.0/12, and 192.168.0.0/16. The agent will also treat link-local (169.254.0.0/16) as a non-masquerade CIDR by default. The agent is configured to reload its configuration from the location /etc/config/ip-masq-agent every 60 seconds, which is also configurable.The agent configuration file must be written in YAML or JSON syntax, and may contain three optional keys:nonMasqueradeCIDRs: A list of strings in CIDR notation that specify the non-masquerade ranges.masqLinkLocal: A Boolean (true/false) which indicates whether to masquerade traffic to the link local prefix 169.254.0.0/16. False by default.resyncInterval: A time interval at which the agent attempts to reload config from disk. For example: '30s', where 's' means seconds, 'ms' means milliseconds.Traffic to 10.0.0.0/8, 172.16.0.0/12 and 192.168.0.0/16 ranges will NOT be masqueraded. Any other traffic (assumed to be internet) will be masqueraded. An example of a local destination from a pod could be its Node's IP address as well as another node's address or one of the IP addresses in Cluster's IP range. Any other traffic will be masqueraded by default. The below entries show the default set of rules that are applied by the ip-masq-agent:iptables -t nat -L IP-MASQ-AGENT target prot opt source destination RETURN all -- anywhere 169.254.0.0/16 /* ip-masq-agent: cluster-local traffic should not be subject to MASQUERADE */ ADDRTYPE match dst-type !LOCAL RETURN all -- anywhere 10.0.0.0/8 /* ip-masq-agent: cluster-local traffic should not be subject to MASQUERADE */ ADDRTYPE match dst-type !LOCAL RETURN all -- anywhere 172.16.0.0/12 /* ip-masq-agent: cluster-local traffic should not be subject to MASQUERADE */ ADDRTYPE match dst-type !LOCAL RETURN all -- anywhere 192.168.0.0/16 /* ip-masq-agent: cluster-local traffic should not be subject to MASQUERADE */ ADDRTYPE match dst-type !LOCAL MASQUERADE all -- anywhere anywhere /* ip-masq-agent: outbound traffic should be subject to MASQUERADE (this match must come after cluster-local CIDR matches) */ ADDRTYPE match dst-type !LOCAL By default, in GCE/Google Kubernetes Engine, if network policy is enabled or you are using a cluster CIDR not in the 10.0.0.0/8 range, the ip-masq-agent will run in your cluster. If you are running in another environment, you can add the ip-masq-agent DaemonSet to your cluster.Create an ip-masq-agentTo create an ip-masq-agent, run the following kubectl command:kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/ip-masq-agent/master/ip-masq-agent.yaml You must also apply the appropriate node label to any nodes in your cluster that you want the agent to run on.kubectl label nodes my-node node.kubernetes.io/masq-agent-ds-ready=true More information can be found in the ip-masq-agent documentation here.In most cases, the default set of rules should be sufficient; however, if this is not the case for your cluster, you can create and apply a ConfigMap to customize the IP ranges that are affected. For example, to allow only 10.0.0.0/8 to be considered by the ip-masq-agent, you can create the following ConfigMap in a file called "config".Note:It is important that the file is called config since, by default, that will be used as the key for lookup by the ip-masq-agent:nonMasqueradeCIDRs: \- 10.0.0.0/8 resyncInterval: 60s Run the following command to add the configmap to your cluster:kubectl create configmap ip-masq-agent --from-file=config --namespace=kube-system This will update a file located at /etc/config/ip-masq-agent which is periodically checked every resyncInterval and applied to the cluster node. After the resync interval has expired, you should see the iptables rules reflect your changes:iptables -t nat -L IP-MASQ-AGENT Chain IP-MASQ-AGENT (1 references) target prot opt source destination RETURN all -- anywhere 169.254.0.0/16 /* ip-masq-agent: cluster-local traffic should not be subject to MASQUERADE */ ADDRTYPE match dst-type !LOCAL RETURN all -- anywhere 10.0.0.0/8 /* ip-masq-agent: cluster-local MASQUERADE all -- anywhere anywhere /* ip-masq-agent: outbound traffic should be subject to MASQUERADE (this match must come after cluster-local CIDR matches) */ ADDRTYPE match dst-type !LOCAL By default, the link local range (169.254.0.0/16) is also handled by the ip-masq agent, which sets up the appropriate iptables rules. To have the ip-masq-agent ignore link local, you can set masqLinkLocal to true in the ConfigMap.nonMasqueradeCIDRs: \- 10.0.0.0/8 resyncInterval: 60s masqLinkLocal: true 4.2.28 - Limit Storage ConsumptionThis example demonstrates how to limit the amount of storage consumed in a namespace.The following resources are used in the demonstration: ResourceQuota, LimitRange, and PersistentVolumeClaim.Before you beginYou need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesTo check the version, enter kubectl version.Scenario: Limiting Storage ConsumptionThe cluster-admin is operating a cluster on behalf of a user population and the admin wants to control how much storage a single namespace can consume in order to control cost.The admin would like to limit:The number of persistent volume claims in a namespaceThe amount of storage each claim can requestThe amount of cumulative storage the namespace can haveLimitRange to limit requests for storageAdding a LimitRange to a namespace enforces storage request sizes to a minimum and maximum. Storage is requested via PersistentVolumeClaim. The admission controller that enforces limit ranges will reject any PVC that is above or below the values set by the admin.In this example, a PVC requesting 10Gi of storage would be rejected because it exceeds the 2Gi max.apiVersion: v1 kind: LimitRange metadata: name: storagelimits spec: limits: \- type: PersistentVolumeClaim max: storage: 2Gi min: storage: 1Gi Minimum storage requests are used when the underlying storage provider requires certain minimums. For example, AWS EBS volumes have a 1Gi minimum requirement.ResourceQuota to limit PVC count and cumulative storage capacityAdmins can limit the number of PVCs in a namespace as well as the cumulative capacity of those PVCs. New PVCs that exceed either maximum value will be rejected.In this example, a 6th PVC in the namespace would be rejected because it exceeds the maximum count of 5. Alternatively, a 5Gi maximum quota when combined with the 2Gi max limit above, cannot have 3 PVCs where each has 2Gi. That would be 6Gi requested for a namespace capped at 5Gi.apiVersion: v1 kind: ResourceQuota metadata: name: storagequota spec: hard: persistentvolumeclaims: "5" requests.storage: "5Gi" SummaryA limit range can put a ceiling on how much storage is requested while a resource quota can effectively cap the storage consumed by a namespace through claim counts and cumulative storage capacity. The allows a cluster-admin to plan their cluster's storage budget without risk of any one project going over their allotment.4.2.29 - Migrate Replicated Control Plane To Use Cloud Controller ManagerThe cloud-controller-manager is a Kubernetes control plane component that embeds cloud-specific control logic. The cloud controller manager lets you link your cluster into your cloud provider's API, and separates out the components that interact with that cloud platform from components that only interact with your cluster.By decoupling the interoperability logic between Kubernetes and the underlying cloud infrastructure, the cloud-controller-manager component enables cloud providers to release features at a different pace compared to the main Kubernetes project.BackgroundAs part of the cloud provider extraction effort, all cloud specific controllers must be moved out of the kube-controller-manager. All existing clusters that run cloud controllers in the kube-controller-manager must migrate to instead run the controllers in a cloud provider specific cloud-controller-manager.Leader Migration provides a mechanism in which HA clusters can safely migrate "cloud specific" controllers between the kube-controller-manager and the cloud-controller-manager via a shared resource lock between the two components while upgrading the replicated control plane. For a single-node control plane, or if unavailability of controller managers can be tolerated during the upgrade, Leader Migration is not needed and this guide can be ignored.Leader Migration can be enabled by setting --enable-leader-migration on kube-controller-manager or cloud-controller-manager. Leader Migration only applies during the upgrade and can be safely disabled or left enabled after the upgrade is complete.This guide walks you through the manual process of upgrading the control plane from kube-controller-manager with built-in cloud provider to running both kube-controller-manager and cloud-controller-manager. If you use a tool to deploy and manage the cluster, please refer to the documentation of the tool and the cloud provider for specific instructions of the migration.Before you beginIt is assumed that the control plane is running Kubernetes version N and to be upgraded to version N + 1. Although it is possible to migrate within the same version, ideally the migration should be performed as part of an upgrade so that changes of configuration can be aligned to each release. The exact versions of N and N + 1 depend on each cloud provider. For example, if a cloud provider builds a cloud-controller-manager to work with Kubernetes 1.24, then N can be 1.23 and N + 1 can be 1.24.The control plane nodes should run kube-controller-manager with Leader Election enabled, which is the default. As of version N, an in-tree cloud provider must be set with --cloud-provider flag and cloud-controller-manager should not yet be deployed.The out-of-tree cloud provider must have built a cloud-controller-manager with Leader Migration implementation. If the cloud provider imports k8s.io/cloud-provider and k8s.io/controller-manager of version v0.21.0 or later, Leader Migration will be available. However, for version before v0.22.0, Leader Migration is alpha and requires feature gate ControllerManagerLeaderMigration to be enabled in cloud-controller-manager.This guide assumes that kubelet of each control plane node starts kube-controller-manager and cloud-controller-manager as static pods defined by their manifests. If the components run in a different setting, please adjust the steps accordingly.For authorization, this guide assumes that the cluster uses RBAC. If another authorization mode grants permissions to kube-controller-manager and cloud-controller-manager components, please grant the needed access in a way that matches the mode.Grant access to Migration LeaseThe default permissions of the controller manager allow only accesses to their main Lease. In order for the migration to work, accesses to another Lease are required.You can grant kube-controller-manager full access to the leases API by modifying the system::leader-locking-kube-controller-manager role. This task guide assumes that the name of the migration lease is cloud-provider-extraction-migration.kubectl patch -n kube-system role 'system::leader-locking-kube-controller-manager' -p '{"rules": [ {"apiGroups":[ "coordination.k8s.io"], "resources": ["leases"], "resourceNames": ["cloud-provider-extraction-migration"], "verbs": ["create", "list", "get", "update"] } ]}' --type=merge` Do the same to the system::leader-locking-cloud-controller-manager role.kubectl patch -n kube-system role 'system::leader-locking-cloud-controller-manager' -p '{"rules": [ {"apiGroups":[ "coordination.k8s.io"], "resources": ["leases"], "resourceNames": ["cloud-provider-extraction-migration"], "verbs": ["create", "list", "get", "update"] } ]}' --type=merge` Initial Leader Migration configurationLeader Migration optionally takes a configuration file representing the state of controller-to-manager assignment. At this moment, with in-tree cloud provider, kube-controller-manager runs route, service, and cloud-node-lifecycle. The following example configuration shows the assignment.Leader Migration can be enabled without a configuration. Please see Default Configuration for details.kind: LeaderMigrationConfiguration apiVersion: controllermanager.config.k8s.io/v1 leaderName: cloud-provider-extraction-migration resourceLock: leases controllerLeaders: \- name: route component: kube-controller-manager \- name: service component: kube-controller-manager \- name: cloud-node-lifecycle component: kube-controller-manager Alternatively, because the controllers can run under either controller managers, setting component to * for both sides makes the configuration file consistent between both parties of the migration.# wildcard version kind: LeaderMigrationConfiguration apiVersion: controllermanager.config.k8s.io/v1 leaderName: cloud-provider-extraction-migration resourceLock: leases controllerLeaders: \- name: route component: * \- name: service component: * \- name: cloud-node-lifecycle component: * On each control plane node, save the content to /etc/leadermigration.conf, and update the manifest of kube-controller-manager so that the file is mounted inside the container at the same location. Also, update the same manifest to add the following arguments:--enable-leader-migration to enable Leader Migration on the controller manager--leader-migration-config=/etc/leadermigration.conf to set configuration fileRestart kube-controller-manager on each node. At this moment, kube-controller-manager has leader migration enabled and is ready for the migration.Deploy Cloud Controller ManagerIn version N + 1, the desired state of controller-to-manager assignment can be represented by a new configuration file, shown as follows. Please note component field of each controllerLeaders changing from kube-controller-manager to cloud-controller-manager. Alternatively, use the wildcard version mentioned above, which has the same effect.kind: LeaderMigrationConfiguration apiVersion: controllermanager.config.k8s.io/v1 leaderName: cloud-provider-extraction-migration resourceLock: leases controllerLeaders: \- name: route component: cloud-controller-manager \- name: service component: cloud-controller-manager \- name: cloud-node-lifecycle component: cloud-controller-manager When creating control plane nodes of version N + 1, the content should be deployed to /etc/leadermigration.conf. The manifest of cloud-controller-manager should be updated to mount the configuration file in the same manner as kube-controller-manager of version N. Similarly, add --enable-leader-migration and --leader-migration-config=/etc/leadermigration.conf to the arguments of cloud-controller-manager.Create a new control plane node of version N + 1 with the updated cloud-controller-manager manifest, and with the --cloud-provider flag set to external for kube-controller-manager. kube-controller-manager of version N + 1 MUST NOT have Leader Migration enabled because, with an external cloud provider, it does not run the migrated controllers anymore, and thus it is not involved in the migration.Please refer to Cloud Controller Manager Administration for more detail on how to deploy cloud-controller-manager.Upgrade Control PlaneThe control plane now contains nodes of both version N and N + 1. The nodes of version N run kube-controller-manager only, and these of version N + 1 run both kube-controller-manager and cloud-controller-manager. The migrated controllers, as specified in the configuration, are running under either kube-controller-manager of version N or cloud-controller-manager of version N + 1 depending on which controller manager holds the migration lease. No controller will ever be running under both controller managers at any time.In a rolling manner, create a new control plane node of version N + 1 and bring down one of version N until the control plane contains only nodes of version N + 1. If a rollback from version N + 1 to N is required, add nodes of version N with Leader Migration enabled for kube-controller-manager back to the control plane, replacing one of version N + 1 each time until there are only nodes of version N.(Optional) Disable Leader MigrationNow that the control plane has been upgraded to run both kube-controller-manager and cloud-controller-manager of version N + 1, Leader Migration has finished its job and can be safely disabled to save one Lease resource. It is safe to re-enable Leader Migration for the rollback in the future.In a rolling manager, update manifest of cloud-controller-manager to unset both \--enable-leader-migration and --leader-migration-config= flag, also remove the mount of /etc/leadermigration.conf, and finally remove /etc/leadermigration.conf. To re-enable Leader Migration, recreate the configuration file and add its mount and the flags that enable Leader Migration back to cloud-controller-manager.Default ConfigurationStarting Kubernetes 1.22, Leader Migration provides a default configuration suitable for the default controller-to-manager assignment. The default configuration can be enabled by setting --enable-leader-migration but without --leader-migration-config=.For kube-controller-manager and cloud-controller-manager, if there are no flags that enable any in-tree cloud provider or change ownership of controllers, the default configuration can be used to avoid manual creation of the configuration file.Special case: migrating the Node IPAM controllerIf your cloud provider provides an implementation of Node IPAM controller, you should switch to the implementation in cloud-controller-manager. Disable Node IPAM controller in kube-controller-manager of version N + 1 by adding \--controllers=*,-nodeipam to its flags. Then add nodeipam to the list of migrated controllers.# wildcard version, with nodeipam kind: LeaderMigrationConfiguration apiVersion: controllermanager.config.k8s.io/v1 leaderName: cloud-provider-extraction-migration resourceLock: leases controllerLeaders: \- name: route component: * \- name: service component: * \- name: cloud-node-lifecycle component: * \- name: nodeipam \- component: * What's nextRead the Controller Manager Leader Migration enhancement proposal.4.2.30 - Operating etcd clusters for Kubernetesetcd is a consistent and highly-available key value store used as Kubernetes' backing store for all cluster data.If your Kubernetes cluster uses etcd as its backing store, make sure you have a back up plan for the data.You can find in-depth information about etcd in the official documentation.Before you beginBefore you follow steps in this page to deploy, manage, back up or restore etcd, you need to understand the typical expectations for operating an etcd cluster. Refer to the etcd documentation for more context.Key details include:The minimum recommended etcd versions to run in production are 3.4.22+ and 3.5.6+.etcd is a leader-based distributed system. Ensure that the leader periodically send heartbeats on time to all followers to keep the cluster stable.You should run etcd as a cluster with an odd number of members.Aim to ensure that no resource starvation occurs.Performance and stability of the cluster is sensitive to network and disk I/O. Any resource starvation can lead to heartbeat timeout, causing instability of the cluster. An unstable etcd indicates that no leader is elected. Under such circumstances, a cluster cannot make any changes to its current state, which implies no new pods can be scheduled.Resource requirements for etcdOperating etcd with limited resources is suitable only for testing purposes. For deploying in production, advanced hardware configuration is required. Before deploying etcd in production, see resource requirement reference.Keeping etcd clusters stable is critical to the stability of Kubernetes clusters. Therefore, run etcd clusters on dedicated machines or isolated environments for guaranteed resource requirements.ToolsDepending on which specific outcome you're working on, you will need the etcdctl tool or the etcdutl tool (you may need both).Understanding etcdctl and etcdutletcdctl and etcdutl are command-line tools used to interact with etcd clusters, but they serve different purposes:etcdctl: This is the primary command-line client for interacting with etcd over a network. It is used for day-to-day operations such as managing keys and values, administering the cluster, checking health, and more.etcdutl: This is an administration utility designed to operate directly on etcd data files, including migrating data between etcd versions, defragmenting the database, restoring snapshots, and validating data consistency. For network operations, etcdctl should be used.For more information on etcdutl, you can refer to the etcd recovery documentation.Starting etcd clustersThis section covers starting a single-node and multi-node etcd cluster.This guide assumes that etcd is already installed.Single-node etcd clusterUse a single-node etcd cluster only for testing purposes.Run the following:etcd --listen-client-urls=http://$PRIVATE_IP:2379 \ \--advertise-client-urls=http://$PRIVATE_IP:2379 Start the Kubernetes API server with the flag \--etcd-servers=$PRIVATE_IP:2379.Make sure PRIVATE_IP is set to your etcd client IP.Multi-node etcd clusterFor durability and high availability, run etcd as a multi-node cluster in production and back it up periodically. A five-member cluster is recommended in production. For more information, see FAQ documentation.As you're using Kubernetes, you have the option to run etcd as a container inside one or more Pods. The kubeadm tool sets up etcd static pods by default, or you can deploy a separate cluster and instruct kubeadm to use that etcd cluster as the control plane's backing store.You configure an etcd cluster either by static member information or by dynamic discovery. For more information on clustering, see etcd clustering documentation.For an example, consider a five-member etcd cluster running with the following client URLs: http://$IP1:2379, http://$IP2:2379, http://$IP3:2379, http://$IP4:2379, and http://$IP5:2379. To start a Kubernetes API server:Run the following:etcd --listen-client-urls=http://$IP1:2379,http://$IP2:2379,http://$IP3:2379,http://$IP4:2379,http://$IP5:2379 --advertise-client-urls=http://$IP1:2379,http://$IP2:2379,http://$IP3:2379,http://$IP4:2379,http://$IP5:2379 Start the Kubernetes API servers with the flag \--etcd-servers=$IP1:2379,$IP2:2379,$IP3:2379,$IP4:2379,$IP5:2379.Make sure the IP variables are set to your client IP addresses.Multi-node etcd cluster with load balancerTo run a load balancing etcd cluster:Set up an etcd cluster.Configure a load balancer in front of the etcd cluster. For example, let the address of the load balancer be $LB.Start Kubernetes API Servers with the flag --etcd-servers=$LB:2379.Securing etcd clustersAccess to etcd is equivalent to root permission in the cluster so ideally only the API server should have access to it. Considering the sensitivity of the data, it is recommended to grant permission to only those nodes that require access to etcd clusters.To secure etcd, either set up firewall rules or use the security features provided by etcd. etcd security features depend on x509 Public Key Infrastructure (PKI). To begin, establish secure communication channels by generating a key and certificate pair. For example, use key pairs peer.key and peer.cert for securing communication between etcd members, and client.key and client.cert for securing communication between etcd and its clients. See the example scripts provided by the etcd project to generate key pairs and CA files for client authentication.Securing communicationTo configure etcd with secure peer communication, specify flags \--peer-key-file=peer.key and --peer-cert-file=peer.cert, and use HTTPS as the URL schema.Similarly, to configure etcd with secure client communication, specify flags \--key=k8sclient.key and --cert=k8sclient.cert, and use HTTPS as the URL schema. Here is an example on a client command that uses secure communication:ETCDCTL_API=3 etcdctl --endpoints 10.2.0.9:2379 \ \--cert=/etc/kubernetes/pki/etcd/server.crt \ \--key=/etc/kubernetes/pki/etcd/server.key \ \--cacert=/etc/kubernetes/pki/etcd/ca.crt \ member list Limiting access of etcd clustersAfter configuring secure communication, restrict the access of the etcd cluster to only the Kubernetes API servers using TLS authentication.For example, consider key pairs k8sclient.key and k8sclient.cert that are trusted by the CA etcd.ca. When etcd is configured with --client-cert-auth along with TLS, it verifies the certificates from clients by using system CAs or the CA passed in by --trusted-ca-file flag. Specifying flags \--client-cert-auth=true and --trusted-ca-file=etcd.ca will restrict the access to clients with the certificate k8sclient.cert.Once etcd is configured correctly, only clients with valid certificates can access it. To give Kubernetes API servers the access, configure them with the flags --etcd-certfile=k8sclient.cert, --etcd-keyfile=k8sclient.key and \--etcd-cafile=ca.cert.Note:etcd authentication is not planned for Kubernetes.Replacing a failed etcd memberetcd cluster achieves high availability by tolerating minor member failures. However, to improve the overall health of the cluster, replace failed members immediately. When multiple members fail, replace them one by one. Replacing a failed member involves two steps: removing the failed member and adding a new member.Though etcd keeps unique member IDs internally, it is recommended to use a unique name for each member to avoid human errors. For example, consider a three-member etcd cluster. Let the URLs be, member1=http://10.0.0.1, member2=http://10.0.0.2, and member3=http://10.0.0.3. When member1 fails, replace it with member4=http://10.0.0.4.Get the member ID of the failed member1:etcdctl --endpoints=http://10.0.0.2,http://10.0.0.3 member list The following message is displayed:8211f1d0f64f3269, started, member1, http://10.0.0.1:2380, http://10.0.0.1:2379 91bc3c398fb3c146, started, member2, http://10.0.0.2:2380, http://10.0.0.2:2379 fd422379fda50e48, started, member3, http://10.0.0.3:2380, http://10.0.0.3:2379 Do either of the following:If each Kubernetes API server is configured to communicate with all etcd members, remove the failed member from the --etcd-servers flag, then restart each Kubernetes API server.If each Kubernetes API server communicates with a single etcd member, then stop the Kubernetes API server that communicates with the failed etcd.Stop the etcd server on the broken node. It is possible that other clients besides the Kubernetes API server are causing traffic to etcd and it is desirable to stop all traffic to prevent writes to the data directory.Remove the failed member:etcdctl member remove 8211f1d0f64f3269 The following message is displayed:Removed member 8211f1d0f64f3269 from cluster Add the new member:etcdctl member add member4 --peer-urls=http://10.0.0.4:2380 The following message is displayed:Member 2be1eb8f84b7f63e added to cluster ef37ad9dc622a7c4 Start the newly added member on a machine with the IP 10.0.0.4:export ETCD_NAME="member4" export ETCD_INITIAL_CLUSTER="member2=http://10.0.0.2:2380,member3=http://10.0.0.3:2380,member4=http://10.0.0.4:2380" export ETCD_INITIAL_CLUSTER_STATE=existing etcd [flags] Do either of the following:If each Kubernetes API server is configured to communicate with all etcd members, add the newly added member to the --etcd-servers flag, then restart each Kubernetes API server.If each Kubernetes API server communicates with a single etcd member, start the Kubernetes API server that was stopped in step 2. Then configure Kubernetes API server clients to again route requests to the Kubernetes API server that was stopped. This can often be done by configuring a load balancer.For more information on cluster reconfiguration, see etcd reconfiguration documentation.Backing up an etcd clusterAll Kubernetes objects are stored in etcd. Periodically backing up the etcd cluster data is important to recover Kubernetes clusters under disaster scenarios, such as losing all control plane nodes. The snapshot file contains all the Kubernetes state and critical information. In order to keep the sensitive Kubernetes data safe, encrypt the snapshot files.Backing up an etcd cluster can be accomplished in two ways: etcd built-in snapshot and volume snapshot.Built-in snapshotetcd supports built-in snapshot. A snapshot may either be created from a live member with the etcdctl snapshot save command or by copying the member/snap/db file from an etcd data directory that is not currently used by an etcd process. Creating the snapshot will not affect the performance of the member.Below is an example for creating a snapshot of the keyspace served by $ENDPOINT to the file snapshot.db:ETCDCTL_API=3 etcdctl --endpoints $ENDPOINT snapshot save snapshot.db Verify the snapshot:Use etcdutlUse etcdctl (Deprecated)The below example depicts the usage of the etcdutl tool for verifying a snapshot:etcdutl --write-out=table snapshot status snapshot.db This should generate an output resembling the example provided below:+----------+----------+------------+------------+ | HASH | REVISION | TOTAL KEYS | TOTAL SIZE | +----------+----------+------------+------------+ | fe01cf57 | 10 | 7 | 2.1 MB | +----------+----------+------------+------------+ Note:The usage of etcdctl snapshot status has been deprecated since etcd v3.5.x and is slated for removal from etcd v3.6. It is recommended to utilize etcdutl instead.The below example depicts the usage of the etcdctl tool for verifying a snapshot:export ETCDCTL_API=3 etcdctl --write-out=table snapshot status snapshot.db This should generate an output resembling the example provided below:Deprecated: Use `etcdutl snapshot status` instead. +----------+----------+------------+------------+ | HASH | REVISION | TOTAL KEYS | TOTAL SIZE | +----------+----------+------------+------------+ | fe01cf57 | 10 | 7 | 2.1 MB | +----------+----------+------------+------------+ Volume snapshotIf etcd is running on a storage volume that supports backup, such as Amazon Elastic Block Store, back up etcd data by creating a snapshot of the storage volume.Snapshot using etcdctl optionsWe can also create the snapshot using various options given by etcdctl. For example:ETCDCTL_API=3 etcdctl -h will list various options available from etcdctl. For example, you can create a snapshot by specifying the endpoint, certificates and key as shown below:ETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379 \ \--cacert= \--cert= \--key= \ snapshot save  where trusted-ca-file, cert-file and key-file can be obtained from the description of the etcd Pod.Scaling out etcd clustersScaling out etcd clusters increases availability by trading off performance. Scaling does not increase cluster performance nor capability. A general rule is not to scale out or in etcd clusters. Do not configure any auto scaling groups for etcd clusters. It is strongly recommended to always run a static five-member etcd cluster for production Kubernetes clusters at any officially supported scale.A reasonable scaling is to upgrade a three-member cluster to a five-member one, when more reliability is desired. See etcd reconfiguration documentation for information on how to add members into an existing cluster.Restoring an etcd clusterCaution:If any API servers are running in your cluster, you should not attempt to restore instances of etcd. Instead, follow these steps to restore etcd:stop all API server instancesrestore state in all etcd instancesrestart all API server instancesThe Kubernetes project also recommends restarting Kubernetes components (kube-scheduler, kube-controller-manager, kubelet) to ensure that they don't rely on some stale data. In practice the restore takes a bit of time. During the restoration, critical components will lose leader lock and restart themselves.etcd supports restoring from snapshots that are taken from an etcd process of the major.minor version. Restoring a version from a different patch version of etcd is also supported. A restore operation is employed to recover the data of a failed cluster.Before starting the restore operation, a snapshot file must be present. It can either be a snapshot file from a previous backup operation, or from a remaining data directory.Use etcdutlUse etcdctl (Deprecated)When restoring the cluster using etcdutl, use the --data-dir option to specify to which folder the cluster should be restored:etcdutl --data-dir  snapshot restore snapshot.db where  is a directory that will be created during the restore process.Note:The usage of etcdctl for restoring has been deprecated since etcd v3.5.x and is slated for removal from etcd v3.6. It is recommended to utilize etcdutl instead.The below example depicts the usage of the etcdctl tool for the restore operation:export ETCDCTL_API=3 etcdctl --data-dir  snapshot restore snapshot.db If  is the same folder as before, delete it and stop the etcd process before restoring the cluster. Otherwise, change etcd configuration and restart the etcd process after restoration to have it use the new data directory: first change /etc/kubernetes/manifests/etcd.yaml's volumes.hostPath.path for name: etcd-data to , then execute kubectl -n kube-system delete pod  or systemctl restart kubelet.service (or both).For more information and examples on restoring a cluster from a snapshot file, see etcd disaster recovery documentation.If the access URLs of the restored cluster are changed from the previous cluster, the Kubernetes API server must be reconfigured accordingly. In this case, restart Kubernetes API servers with the flag \--etcd-servers=$NEW_ETCD_CLUSTER instead of the flag \--etcd-servers=$OLD_ETCD_CLUSTER. Replace $NEW_ETCD_CLUSTER and $OLD_ETCD_CLUSTER with the respective IP addresses. If a load balancer is used in front of an etcd cluster, you might need to update the load balancer instead.If the majority of etcd members have permanently failed, the etcd cluster is considered failed. In this scenario, Kubernetes cannot make any changes to its current state. Although the scheduled pods might continue to run, no new pods can be scheduled. In such cases, recover the etcd cluster and potentially reconfigure Kubernetes API servers to fix the issue.Upgrading etcd clustersCaution:Before you start an upgrade, back up your etcd cluster first.For details on etcd upgrade, refer to the etcd upgrades documentation.Maintaining etcd clustersFor more details on etcd maintenance, please refer to the etcd maintenance documentation.Cluster defragmentation🛇 This item links to a third party project or product that is not part of Kubernetes itself. More informationDefragmentation is an expensive operation, so it should be executed as infrequently as possible. On the other hand, it's also necessary to make sure any etcd member will not exceed the storage quota. The Kubernetes project recommends that when you perform defragmentation, you use a tool such as etcd-defrag.You can also run the defragmentation tool as a Kubernetes CronJob, to make sure that defragmentation happens regularly. See etcd-defrag-cronjob.yaml for details.4.2.31 - Reserve Compute Resources for System DaemonsKubernetes nodes can be scheduled to Capacity. Pods can consume all the available capacity on a node by default. This is an issue because nodes typically run quite a few system daemons that power the OS and Kubernetes itself. Unless resources are set aside for these system daemons, pods and system daemons compete for resources and lead to resource starvation issues on the node.The kubelet exposes a feature named 'Node Allocatable' that helps to reserve compute resources for system daemons. Kubernetes recommends cluster administrators to configure 'Node Allocatable' based on their workload density on each node.Before you beginYou need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesYou can configure below kubelet configuration settings using the kubelet configuration file.Node Allocatable'Allocatable' on a Kubernetes node is defined as the amount of compute resources that are available for pods. The scheduler does not over-subscribe 'Allocatable'. 'CPU', 'memory' and 'ephemeral-storage' are supported as of now.Node Allocatable is exposed as part of v1.Node object in the API and as part of kubectl describe node in the CLI.Resources can be reserved for two categories of system daemons in the kubelet.Enabling QoS and Pod level cgroupsTo properly enforce node allocatable constraints on the node, you must enable the new cgroup hierarchy via the cgroupsPerQOS setting. This setting is enabled by default. When enabled, the kubelet will parent all end-user pods under a cgroup hierarchy managed by the kubelet.Configuring a cgroup driverThe kubelet supports manipulation of the cgroup hierarchy on the host using a cgroup driver. The driver is configured via the cgroupDriver setting.The supported values are the following:cgroupfs is the default driver that performs direct manipulation of the cgroup filesystem on the host in order to manage cgroup sandboxes.systemd is an alternative driver that manages cgroup sandboxes using transient slices for resources that are supported by that init system.Depending on the configuration of the associated container runtime, operators may have to choose a particular cgroup driver to ensure proper system behavior. For example, if operators use the systemd cgroup driver provided by the containerd runtime, the kubelet must be configured to use the systemd cgroup driver.Kube ReservedKubeletConfiguration Setting: kubeReserved: {}. Example value {cpu: 100m, memory: 100Mi, ephemeral-storage: 1Gi, pid=1000}KubeletConfiguration Setting: kubeReservedCgroup: ""kubeReserved is meant to capture resource reservation for kubernetes system daemons like the kubelet, container runtime, etc. It is not meant to reserve resources for system daemons that are run as pods. kubeReserved is typically a function of pod density on the nodes.In addition to cpu, memory, and ephemeral-storage, pid may be specified to reserve the specified number of process IDs for kubernetes system daemons.To optionally enforce kubeReserved on kubernetes system daemons, specify the parent control group for kube daemons as the value for kubeReservedCgroup setting, and add kube-reserved to enforceNodeAllocatable.It is recommended that the kubernetes system daemons are placed under a top level control group (runtime.slice on systemd machines for example). Each system daemon should ideally run within its own child control group. Refer to the design proposal for more details on recommended control group hierarchy.Note that Kubelet does not create kubeReservedCgroup if it doesn't exist. The kubelet will fail to start if an invalid cgroup is specified. With systemd cgroup driver, you should follow a specific pattern for the name of the cgroup you define: the name should be the value you set for kubeReservedCgroup, with .slice appended.System ReservedKubeletConfiguration Setting: systemReserved: {}. Example value {cpu: 100m, memory: 100Mi, ephemeral-storage: 1Gi, pid=1000}KubeletConfiguration Setting: systemReservedCgroup: ""systemReserved is meant to capture resource reservation for OS system daemons like sshd, udev, etc. systemReserved should reserve memory for the kernel too since kernel memory is not accounted to pods in Kubernetes at this time. Reserving resources for user login sessions is also recommended (user.slice in systemd world).In addition to cpu, memory, and ephemeral-storage, pid may be specified to reserve the specified number of process IDs for OS system daemons.To optionally enforce systemReserved on system daemons, specify the parent control group for OS system daemons as the value for systemReservedCgroup setting, and add system-reserved to enforceNodeAllocatable.It is recommended that the OS system daemons are placed under a top level control group (system.slice on systemd machines for example).Note that kubelet does not create systemReservedCgroup if it doesn't exist. kubelet will fail if an invalid cgroup is specified. With systemd cgroup driver, you should follow a specific pattern for the name of the cgroup you define: the name should be the value you set for systemReservedCgroup, with .slice appended.Explicitly Reserved CPU ListFEATURE STATE: Kubernetes v1.17 [stable]KubeletConfiguration Setting: reservedSystemCPUs:. Example value 0-3reservedSystemCPUs is meant to define an explicit CPU set for OS system daemons and kubernetes system daemons. reservedSystemCPUs is for systems that do not intend to define separate top level cgroups for OS system daemons and kubernetes system daemons with regard to cpuset resource. If the Kubelet does not have kubeReservedCgroup and systemReservedCgroup, the explicit cpuset provided by reservedSystemCPUs will take precedence over the CPUs defined by kubeReservedCgroup and systemReservedCgroup options.This option is specifically designed for Telco/NFV use cases where uncontrolled interrupts/timers may impact the workload performance. you can use this option to define the explicit cpuset for the system/kubernetes daemons as well as the interrupts/timers, so the rest CPUs on the system can be used exclusively for workloads, with less impact from uncontrolled interrupts/timers. To move the system daemon, kubernetes daemons and interrupts/timers to the explicit cpuset defined by this option, other mechanism outside Kubernetes should be used. For example: in Centos, you can do this using the tuned toolset.Eviction ThresholdsKubeletConfiguration Setting: evictionHard: {memory.available: "100Mi", nodefs.available: "10%", nodefs.inodesFree: "5%", imagefs.available: "15%"}. Example value: {memory.available: "<500Mi"}Memory pressure at the node level leads to System OOMs which affects the entire node and all pods running on it. Nodes can go offline temporarily until memory has been reclaimed. To avoid (or reduce the probability of) system OOMs kubelet provides out of resource management. Evictions are supported for memory and ephemeral-storage only. By reserving some memory via evictionHard setting, the kubelet attempts to evict pods whenever memory availability on the node drops below the reserved value. Hypothetically, if system daemons did not exist on a node, pods cannot use more than capacity - eviction-hard. For this reason, resources reserved for evictions are not available for pods.Enforcing Node AllocatableKubeletConfiguration setting: enforceNodeAllocatable: [pods]. Example value: [pods,system-reserved,kube-reserved]The scheduler treats 'Allocatable' as the available capacity for pods.kubelet enforce 'Allocatable' across pods by default. Enforcement is performed by evicting pods whenever the overall usage across all pods exceeds 'Allocatable'. More details on eviction policy can be found on the node pressure eviction page. This enforcement is controlled by specifying pods value to the KubeletConfiguration setting enforceNodeAllocatable.Optionally, kubelet can be made to enforce kubeReserved and systemReserved by specifying kube-reserved & system-reserved values in the same setting. Note that to enforce kubeReserved or systemReserved, kubeReservedCgroup or systemReservedCgroup needs to be specified respectively.General GuidelinesSystem daemons are expected to be treated similar to Guaranteed pods. System daemons can burst within their bounding control groups and this behavior needs to be managed as part of kubernetes deployments. For example, kubelet should have its own control group and share kubeReserved resources with the container runtime. However, Kubelet cannot burst and use up all available Node resources if kubeReserved is enforced.Be extra careful while enforcing systemReserved reservation since it can lead to critical system services being CPU starved, OOM killed, or unable to fork on the node. The recommendation is to enforce systemReserved only if a user has profiled their nodes exhaustively to come up with precise estimates and is confident in their ability to recover if any process in that group is oom-killed.To begin with enforce 'Allocatable' on pods.Once adequate monitoring and alerting is in place to track kube system daemons, attempt to enforce kubeReserved based on usage heuristics.If absolutely necessary, enforce systemReserved over time.The resource requirements of kube system daemons may grow over time as more and more features are added. Over time, kubernetes project will attempt to bring down utilization of node system daemons, but that is not a priority as of now. So expect a drop in Allocatable capacity in future releases.Example ScenarioHere is an example to illustrate Node Allocatable computation:Node has 32Gi of memory, 16 CPUs and 100Gi of StoragekubeReserved is set to {cpu: 1000m, memory: 2Gi, ephemeral-storage: 1Gi}systemReserved is set to {cpu: 500m, memory: 1Gi, ephemeral-storage: 1Gi}evictionHard is set to {memory.available: "<500Mi", nodefs.available: "<10%"}Under this scenario, 'Allocatable' will be 14.5 CPUs, 28.5Gi of memory and 88Gi of local storage. Scheduler ensures that the total memory requests across all pods on this node does not exceed 28.5Gi and storage doesn't exceed 88Gi. Kubelet evicts pods whenever the overall memory usage across pods exceeds 28.5Gi, or if overall disk usage exceeds 88Gi. If all processes on the node consume as much CPU as they can, pods together cannot consume more than 14.5 CPUs.If kubeReserved and/or systemReserved is not enforced and system daemons exceed their reservation, kubelet evicts pods whenever the overall node memory usage is higher than 31.5Gi or storage is greater than 90Gi.4.2.32 - Running Kubernetes Node Components as a Non-root UserFEATURE STATE: Kubernetes v1.22 [alpha]This document describes how to run Kubernetes Node components such as kubelet, CRI, OCI, and CNI without root privileges, by using a user namespace.This technique is also known as rootless mode.Note:This document describes how to run Kubernetes Node components (and hence pods) as a non-root user.If you are just looking for how to run a pod as a non-root user, see SecurityContext.Before you beginYour Kubernetes server must be at or later than version 1.22.To check the version, enter kubectl version.Enable Cgroup v2Enable systemd with user sessionConfigure several sysctl values, depending on host Linux distributionEnsure that your unprivileged user is listed in /etc/subuid and /etc/subgidEnable the KubeletInUserNamespace feature gateRunning Kubernetes inside Rootless Docker/Podmankindkind supports running Kubernetes inside Rootless Docker or Rootless Podman.See Running kind with Rootless Docker.minikubeminikube also supports running Kubernetes inside Rootless Docker or Rootless Podman.See the Minikube documentation:Rootless DockerRootless PodmanRunning Kubernetes inside Unprivileged ContainersNote: This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the content guide before submitting a change. More information.sysboxSysbox is an open-source container runtime (similar to "runc") that supports running system-level workloads such as Docker and Kubernetes inside unprivileged containers isolated with the Linux user namespace.See Sysbox Quick Start Guide: Kubernetes-in-Docker for more info.Sysbox supports running Kubernetes inside unprivileged containers without requiring Cgroup v2 and without the KubeletInUserNamespace feature gate. It does this by exposing specially crafted /proc and /sys filesystems inside the container plus several other advanced OS virtualization techniques.Running Rootless Kubernetes directly on a hostNote: This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the content guide before submitting a change. More information.K3sK3s experimentally supports rootless mode.See Running K3s with Rootless mode for the usage.UsernetesUsernetes is a reference distribution of Kubernetes that can be installed under $HOME directory without the root privilege.Usernetes supports both containerd and CRI-O as CRI runtimes. Usernetes supports multi-node clusters using Flannel (VXLAN).See the Usernetes repo for the usage.Manually deploy a node that runs the kubelet in a user namespaceThis section provides hints for running Kubernetes in a user namespace manually.Note:This section is intended to be read by developers of Kubernetes distributions, not by end users.Creating a user namespaceThe first step is to create a user namespace.If you are trying to run Kubernetes in a user-namespaced container such as Rootless Docker/Podman or LXC/LXD, you are all set, and you can go to the next subsection.Otherwise you have to create a user namespace by yourself, by calling unshare(2) with CLONE_NEWUSER.A user namespace can be also unshared by using command line tools such as:unshare(1)RootlessKitbecome-rootAfter unsharing the user namespace, you will also have to unshare other namespaces such as mount namespace.You do not need to call chroot() nor pivot_root() after unsharing the mount namespace, however, you have to mount writable filesystems on several directories in the namespace.At least, the following directories need to be writable in the namespace (not outside the namespace):/etc/run/var/logs/var/lib/kubelet/var/lib/cni/var/lib/containerd (for containerd)/var/lib/containers (for CRI-O)Creating a delegated cgroup treeIn addition to the user namespace, you also need to have a writable cgroup tree with cgroup v2.Note:Kubernetes support for running Node components in user namespaces requires cgroup v2. Cgroup v1 is not supported.If you are trying to run Kubernetes in Rootless Docker/Podman or LXC/LXD on a systemd-based host, you are all set.Otherwise you have to create a systemd unit with Delegate=yes property to delegate a cgroup tree with writable permission.On your node, systemd must already be configured to allow delegation; for more details, see cgroup v2 in the Rootless Containers documentation.Configuring networkNote: This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the content guide before submitting a change. More information.The network namespace of the Node components has to have a non-loopback interface, which can be for example configured with slirp4netns, VPNKit, or lxc-user-nic(1).The network namespaces of the Pods can be configured with regular CNI plugins. For multi-node networking, Flannel (VXLAN, 8472/UDP) is known to work.Ports such as the kubelet port (10250/TCP) and NodePort service ports have to be exposed from the Node network namespace to the host with an external port forwarder, such as RootlessKit, slirp4netns, or socat(1).You can use the port forwarder from K3s. See Running K3s in Rootless Mode for more details. The implementation can be found in the pkg/rootlessports package of k3s.Configuring CRIThe kubelet relies on a container runtime. You should deploy a container runtime such as containerd or CRI-O and ensure that it is running within the user namespace before the kubelet starts.containerdCRI-ORunning CRI plugin of containerd in a user namespace is supported since containerd 1.4.Running containerd within a user namespace requires the following configurations.version = 2 [plugins."io.containerd.grpc.v1.cri"] # Disable AppArmor disable_apparmor = true # Ignore an error during setting oom_score_adj restrict_oom_score_adj = true # Disable hugetlb cgroup v2 controller (because systemd does not support delegating hugetlb controller) disable_hugetlb_controller = true [plugins."io.containerd.grpc.v1.cri".containerd] # Using non-fuse overlayfs is also possible for kernel >= 5.11, but requires SELinux to be disabled snapshotter = "fuse-overlayfs" [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc.options] # We use cgroupfs that is delegated by systemd, so we do not use SystemdCgroup driver # (unless you run another systemd in the namespace) SystemdCgroup = false The default path of the configuration file is /etc/containerd/config.toml. The path can be specified with containerd -c /path/to/containerd/config.toml.Running CRI-O in a user namespace is supported since CRI-O 1.22.CRI-O requires an environment variable _CRIO_ROOTLESS=1 to be set.The following configurations are also recommended:[crio] storage_driver = "overlay" # Using non-fuse overlayfs is also possible for kernel >= 5.11, but requires SELinux to be disabled storage_option = ["overlay.mount_program=/usr/local/bin/fuse-overlayfs"] [crio.runtime] # We use cgroupfs that is delegated by systemd, so we do not use "systemd" driver # (unless you run another systemd in the namespace) cgroup_manager = "cgroupfs" The default path of the configuration file is /etc/crio/crio.conf. The path can be specified with crio --config /path/to/crio/crio.conf.Configuring kubeletRunning kubelet in a user namespace requires the following configuration:apiVersion: kubelet.config.k8s.io/v1beta1 kind: KubeletConfiguration featureGates: KubeletInUserNamespace: true # We use cgroupfs that is delegated by systemd, so we do not use "systemd" driver # (unless you run another systemd in the namespace) cgroupDriver: "cgroupfs" When the KubeletInUserNamespace feature gate is enabled, the kubelet ignores errors that may happen during setting the following sysctl values on the node.vm.overcommit_memoryvm.panic_on_oomkernel.panickernel.panic_on_oopskernel.keys.root_maxkeyskernel.keys.root_maxbytes.Within a user namespace, the kubelet also ignores any error raised from trying to open /dev/kmsg. This feature gate also allows kube-proxy to ignore an error during setting RLIMIT_NOFILE.The KubeletInUserNamespace feature gate was introduced in Kubernetes v1.22 with "alpha" status.Running kubelet in a user namespace without using this feature gate is also possible by mounting a specially crafted proc filesystem (as done by Sysbox), but not officially supported.Configuring kube-proxyRunning kube-proxy in a user namespace requires the following configuration:apiVersion: kubeproxy.config.k8s.io/v1alpha1 kind: KubeProxyConfiguration mode: "iptables" # or "userspace" conntrack: # Skip setting sysctl value "net.netfilter.nf_conntrack_max" maxPerCore: 0 # Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established" tcpEstablishedTimeout: 0s # Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close" tcpCloseWaitTimeout: 0s CaveatsMost of "non-local" volume drivers such as nfs and iscsi do not work. Local volumes like local, hostPath, emptyDir, configMap, secret, and downwardAPI are known to work.Some CNI plugins may not work. Flannel (VXLAN) is known to work.For more on this, see the Caveats and Future work page on the rootlesscontaine.rs website.See Alsorootlesscontaine.rsRootless Containers 2020 (KubeCon NA 2020)Running kind with Rootless DockerUsernetesRunning K3s with rootless modeKEP-2033: Kubelet-in-UserNS (aka Rootless mode)4.2.33 - Safely Drain a NodeThis page shows how to safely drain a node, optionally respecting the PodDisruptionBudget you have defined.Before you beginThis task assumes that you have met the following prerequisites:You do not require your applications to be highly available during the node drain, orYou have read about the PodDisruptionBudget concept, and have configured PodDisruptionBudgets for applications that need them.(Optional) Configure a disruption budgetTo ensure that your workloads remain available during maintenance, you can configure a PodDisruptionBudget.If availability is important for any applications that run or could run on the node(s) that you are draining, configure a PodDisruptionBudgets first and then continue following this guide.It is recommended to set AlwaysAllow Unhealthy Pod Eviction Policy to your PodDisruptionBudgets to support eviction of misbehaving applications during a node drain. The default behavior is to wait for the application pods to become healthy before the drain can proceed.Use kubectl drain to remove a node from serviceYou can use kubectl drain to safely evict all of your pods from a node before you perform maintenance on the node (e.g. kernel upgrade, hardware maintenance, etc.). Safe evictions allow the pod's containers to gracefully terminate and will respect the PodDisruptionBudgets you have specified.Note:By default kubectl drain ignores certain system pods on the node that cannot be killed; see the kubectl drain documentation for more details.When kubectl drain returns successfully, that indicates that all of the pods (except the ones excluded as described in the previous paragraph) have been safely evicted (respecting the desired graceful termination period, and respecting the PodDisruptionBudget you have defined). It is then safe to bring down the node by powering down its physical machine or, if running on a cloud platform, deleting its virtual machine.Note:If any new Pods tolerate the node.kubernetes.io/unschedulable taint, then those Pods might be scheduled to the node you have drained. Avoid tolerating that taint other than for DaemonSets.If you or another API user directly set the nodeName field for a Pod (bypassing the scheduler), then the Pod is bound to the specified node and will run there, even though you have drained that node and marked it unschedulable.First, identify the name of the node you wish to drain. You can list all of the nodes in your cluster withkubectl get nodes Next, tell Kubernetes to drain the node:kubectl drain --ignore-daemonsets  If there are pods managed by a DaemonSet, you will need to specify \--ignore-daemonsets with kubectl to successfully drain the node. The kubectl drain subcommand on its own does not actually drain a node of its DaemonSet pods: the DaemonSet controller (part of the control plane) immediately replaces missing Pods with new equivalent Pods. The DaemonSet controller also creates Pods that ignore unschedulable taints, which allows the new Pods to launch onto a node that you are draining.Once it returns (without giving an error), you can power down the node (or equivalently, if on a cloud platform, delete the virtual machine backing the node). If you leave the node in the cluster during the maintenance operation, you need to runkubectl uncordon  afterwards to tell Kubernetes that it can resume scheduling new pods onto the node.Draining multiple nodes in parallelThe kubectl drain command should only be issued to a single node at a time. However, you can run multiple kubectl drain commands for different nodes in parallel, in different terminals or in the background. Multiple drain commands running concurrently will still respect the PodDisruptionBudget you specify.For example, if you have a StatefulSet with three replicas and have set a PodDisruptionBudget for that set specifying minAvailable: 2, kubectl drain only evicts a pod from the StatefulSet if all three replicas pods are healthy; if then you issue multiple drain commands in parallel, Kubernetes respects the PodDisruptionBudget and ensures that only 1 (calculated as replicas - minAvailable) Pod is unavailable at any given time. Any drains that would cause the number of healthy replicas to fall below the specified budget are blocked.The Eviction APIIf you prefer not to use kubectl drain (such as to avoid calling to an external command, or to get finer control over the pod eviction process), you can also programmatically cause evictions using the eviction API.For more information, see API-initiated eviction.What's nextFollow steps to protect your application by configuring a Pod Disruption Budget.4.2.34 - Securing a ClusterThis document covers topics related to protecting a cluster from accidental or malicious access and provides recommendations on overall security.Before you beginYou need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesTo check the version, enter kubectl version.Controlling access to the Kubernetes APIAs Kubernetes is entirely API-driven, controlling and limiting who can access the cluster and what actions they are allowed to perform is the first line of defense.Use Transport Layer Security (TLS) for all API trafficKubernetes expects that all API communication in the cluster is encrypted by default with TLS, and the majority of installation methods will allow the necessary certificates to be created and distributed to the cluster components. Note that some components and installation methods may enable local ports over HTTP and administrators should familiarize themselves with the settings of each component to identify potentially unsecured traffic.API AuthenticationChoose an authentication mechanism for the API servers to use that matches the common access patterns when you install a cluster. For instance, small, single-user clusters may wish to use a simple certificate or static Bearer token approach. Larger clusters may wish to integrate an existing OIDC or LDAP server that allow users to be subdivided into groups.All API clients must be authenticated, even those that are part of the infrastructure like nodes, proxies, the scheduler, and volume plugins. These clients are typically service accounts or use x509 client certificates, and they are created automatically at cluster startup or are setup as part of the cluster installation.Consult the authentication reference document for more information.API AuthorizationOnce authenticated, every API call is also expected to pass an authorization check. Kubernetes ships an integrated Role-Based Access Control (RBAC) component that matches an incoming user or group to a set of permissions bundled into roles. These permissions combine verbs (get, create, delete) with resources (pods, services, nodes) and can be namespace-scoped or cluster-scoped. A set of out-of-the-box roles are provided that offer reasonable default separation of responsibility depending on what actions a client might want to perform. It is recommended that you use the Node and RBAC authorizers together, in combination with the NodeRestriction admission plugin.As with authentication, simple and broad roles may be appropriate for smaller clusters, but as more users interact with the cluster, it may become necessary to separate teams into separate namespaces with more limited roles.With authorization, it is important to understand how updates on one object may cause actions in other places. For instance, a user may not be able to create pods directly, but allowing them to create a deployment, which creates pods on their behalf, will let them create those pods indirectly. Likewise, deleting a node from the API will result in the pods scheduled to that node being terminated and recreated on other nodes. The out-of-the box roles represent a balance between flexibility and common use cases, but more limited roles should be carefully reviewed to prevent accidental escalation. You can make roles specific to your use case if the out-of-box ones don't meet your needs.Consult the authorization reference section for more information.Controlling access to the KubeletKubelets expose HTTPS endpoints which grant powerful control over the node and containers. By default Kubelets allow unauthenticated access to this API.Production clusters should enable Kubelet authentication and authorization.Consult the Kubelet authentication/authorization reference for more information.Controlling the capabilities of a workload or user at runtimeAuthorization in Kubernetes is intentionally high level, focused on coarse actions on resources. More powerful controls exist as policies to limit by use case how those objects act on the cluster, themselves, and other resources.Limiting resource usage on a clusterResource quota limits the number or capacity of resources granted to a namespace. This is most often used to limit the amount of CPU, memory, or persistent disk a namespace can allocate, but can also control how many pods, services, or volumes exist in each namespace.Limit ranges restrict the maximum or minimum size of some of the resources above, to prevent users from requesting unreasonably high or low values for commonly reserved resources like memory, or to provide default limits when none are specified.Controlling what privileges containers run withA pod definition contains a security context that allows it to request access to run as a specific Linux user on a node (like root), access to run privileged or access the host network, and other controls that would otherwise allow it to run unfettered on a hosting node.You can configure Pod security admission to enforce use of a particular Pod Security Standard in a namespace, or to detect breaches.Generally, most application workloads need limited access to host resources so they can successfully run as a root process (uid 0) without access to host information. However, considering the privileges associated with the root user, you should write application containers to run as a non-root user. Similarly, administrators who wish to prevent client applications from escaping their containers should apply the Baseline or Restricted Pod Security Standard.Preventing containers from loading unwanted kernel modulesThe Linux kernel automatically loads kernel modules from disk if needed in certain circumstances, such as when a piece of hardware is attached or a filesystem is mounted. Of particular relevance to Kubernetes, even unprivileged processes can cause certain network-protocol-related kernel modules to be loaded, just by creating a socket of the appropriate type. This may allow an attacker to exploit a security hole in a kernel module that the administrator assumed was not in use.To prevent specific modules from being automatically loaded, you can uninstall them from the node, or add rules to block them. On most Linux distributions, you can do that by creating a file such as /etc/modprobe.d/kubernetes-blacklist.conf with contents like:# DCCP is unlikely to be needed, has had multiple serious # vulnerabilities, and is not well-maintained. blacklist dccp # SCTP is not used in most Kubernetes clusters, and has also had # vulnerabilities in the past. blacklist sctp To block module loading more generically, you can use a Linux Security Module (such as SELinux) to completely deny the module_request permission to containers, preventing the kernel from loading modules for containers under any circumstances. (Pods would still be able to use modules that had been loaded manually, or modules that were loaded by the kernel on behalf of some more-privileged process.)Restricting network accessThe network policies for a namespace allows application authors to restrict which pods in other namespaces may access pods and ports within their namespaces. Many of the supported Kubernetes networking providers now respect network policy.Quota and limit ranges can also be used to control whether users may request node ports or load-balanced services, which on many clusters can control whether those users applications are visible outside of the cluster.Additional protections may be available that control network rules on a per-plugin or per- environment basis, such as per-node firewalls, physically separating cluster nodes to prevent cross talk, or advanced networking policy.Restricting cloud metadata API accessCloud platforms (AWS, Azure, GCE, etc.) often expose metadata services locally to instances. By default these APIs are accessible by pods running on an instance and can contain cloud credentials for that node, or provisioning data such as kubelet credentials. These credentials can be used to escalate within the cluster or to other cloud services under the same account.When running Kubernetes on a cloud platform, limit permissions given to instance credentials, use network policies to restrict pod access to the metadata API, and avoid using provisioning data to deliver secrets.Controlling which nodes pods may accessBy default, there are no restrictions on which nodes may run a pod. Kubernetes offers a rich set of policies for controlling placement of pods onto nodes and the taint-based pod placement and eviction that are available to end users. For many clusters use of these policies to separate workloads can be a convention that authors adopt or enforce via tooling.As an administrator, a beta admission plugin PodNodeSelector can be used to force pods within a namespace to default or require a specific node selector, and if end users cannot alter namespaces, this can strongly limit the placement of all of the pods in a specific workload.Protecting cluster components from compromiseThis section describes some common patterns for protecting clusters from compromise.Restrict access to etcdWrite access to the etcd backend for the API is equivalent to gaining root on the entire cluster, and read access can be used to escalate fairly quickly. Administrators should always use strong credentials from the API servers to their etcd server, such as mutual auth via TLS client certificates, and it is often recommended to isolate the etcd servers behind a firewall that only the API servers may access.Caution:Allowing other components within the cluster to access the master etcd instance with read or write access to the full keyspace is equivalent to granting cluster-admin access. Using separate etcd instances for non-master components or using etcd ACLs to restrict read and write access to a subset of the keyspace is strongly recommended.Enable audit loggingThe audit logger is a beta feature that records actions taken by the API for later analysis in the event of a compromise. It is recommended to enable audit logging and archive the audit file on a secure server.Restrict access to alpha or beta featuresAlpha and beta Kubernetes features are in active development and may have limitations or bugs that result in security vulnerabilities. Always assess the value an alpha or beta feature may provide against the possible risk to your security posture. When in doubt, disable features you do not use.Rotate infrastructure credentials frequentlyThe shorter the lifetime of a secret or credential the harder it is for an attacker to make use of that credential. Set short lifetimes on certificates and automate their rotation. Use an authentication provider that can control how long issued tokens are available and use short lifetimes where possible. If you use service-account tokens in external integrations, plan to rotate those tokens frequently. For example, once the bootstrap phase is complete, a bootstrap token used for setting up nodes should be revoked or its authorization removed.Review third party integrations before enabling themMany third party integrations to Kubernetes may alter the security profile of your cluster. When enabling an integration, always review the permissions that an extension requests before granting it access. For example, many security integrations may request access to view all secrets on your cluster which is effectively making that component a cluster admin. When in doubt, restrict the integration to functioning in a single namespace if possible.Components that create pods may also be unexpectedly powerful if they can do so inside namespaces like the kube-system namespace, because those pods can gain access to service account secrets or run with elevated permissions if those service accounts are granted access to permissive PodSecurityPolicies.If you use Pod Security admission and allow any component to create Pods within a namespace that permits privileged Pods, those Pods may be able to escape their containers and use this widened access to elevate their privileges.You should not allow untrusted components to create Pods in any system namespace (those with names that start with kube-) nor in any namespace where that access grant allows the possibility of privilege escalation.Encrypt secrets at restIn general, the etcd database will contain any information accessible via the Kubernetes API and may grant an attacker significant visibility into the state of your cluster. Always encrypt your backups using a well reviewed backup and encryption solution, and consider using full disk encryption where possible.Kubernetes supports optional encryption at rest for information in the Kubernetes API. This lets you ensure that when Kubernetes stores data for objects (for example, Secret or ConfigMap objects), the API server writes an encrypted representation of the object. That encryption means that even someone who has access to etcd backup data is unable to view the content of those objects. In Kubernetes 1.33 you can also encrypt custom resources; encryption-at-rest for extension APIs defined in CustomResourceDefinitions was added to Kubernetes as part of the v1.26 release.Receiving alerts for security updates and reporting vulnerabilitiesJoin the kubernetes-announce group for emails about security announcements. See the security reporting page for more on how to report vulnerabilities.What's nextSecurity Checklist for additional information on Kubernetes security guidance.Seccomp Node Reference4.2.35 - Set Kubelet Parameters Via A Configuration FileBefore you beginSome steps in this page use the jq tool. If you don't have jq, you can install it via your operating system's software sources, or fetch it from https://jqlang.github.io/jq/.Some steps also involve installing curl, which can be installed via your operating system's software sources.A subset of the kubelet's configuration parameters may be set via an on-disk config file, as a substitute for command-line flags.Providing parameters via a config file is the recommended approach because it simplifies node deployment and configuration management.Create the config fileThe subset of the kubelet's configuration that can be configured via a file is defined by the KubeletConfiguration struct.The configuration file must be a JSON or YAML representation of the parameters in this struct. Make sure the kubelet has read permissions on the file.Here is an example of what this file might look like:apiVersion: kubelet.config.k8s.io/v1beta1 kind: KubeletConfiguration address: "192.168.0.8" port: 20250 serializeImagePulls: false evictionHard: memory.available: "100Mi" nodefs.available: "10%" nodefs.inodesFree: "5%" imagefs.available: "15%" imagefs.inodesFree: "5%" In this example, the kubelet is configured with the following settings:address: The kubelet will serve on IP address 192.168.0.8.port: The kubelet will serve on port 20250.serializeImagePulls: Image pulls will be done in parallel.evictionHard: The kubelet will evict Pods under one of the following conditions:When the node's available memory drops below 100MiB.When the node's main filesystem's available space is less than 10%.When the image filesystem's available space is less than 15%.When more than 95% of the node's main filesystem's inodes are in use.Note:In the example, by changing the default value of only one parameter for evictionHard, the default values of other parameters will not be inherited and will be set to zero. In order to provide custom values, you should provide all the threshold values respectively. Alternatively, you can set the MergeDefaultEvictionSettings to true in the kubelet configuration file, if any parameter is changed then the other parameters will inherit their default values instead of 0.The imagefs is an optional filesystem that container runtimes use to store container images and container writable layers.Start a kubelet process configured via the config fileNote:If you use kubeadm to initialize your cluster, use the kubelet-config while creating your cluster with kubeadm init. See configuring kubelet using kubeadm for details.Start the kubelet with the --config flag set to the path of the kubelet's config file. The kubelet will then load its config from this file.Note that command line flags which target the same value as a config file will override that value. This helps ensure backwards compatibility with the command-line API.Note that relative file paths in the kubelet config file are resolved relative to the location of the kubelet config file, whereas relative paths in command line flags are resolved relative to the kubelet's current working directory.Note that some default values differ between command-line flags and the kubelet config file. If --config is provided and the values are not specified via the command line, the defaults for the KubeletConfiguration version apply. In the above example, this version is kubelet.config.k8s.io/v1beta1.Drop-in directory for kubelet configuration filesFEATURE STATE: Kubernetes v1.30 [beta]You can specify a drop-in configuration directory for the kubelet. By default, the kubelet does not look for drop-in configuration files anywhere - you must specify a path. For example: --config-dir=/etc/kubernetes/kubelet.conf.dFor Kubernetes v1.28 to v1.29, you can only specify --config-dir if you also set the environment variable KUBELET_CONFIG_DROPIN_DIR_ALPHA for the kubelet process (the value of that variable does not matter).Note:The suffix of a valid kubelet drop-in configuration file must be .conf. For instance: 99-kubelet-address.confThe kubelet processes files in its config drop-in directory by sorting the entire file name alphanumerically. For instance, 00-kubelet.conf is processed first, and then overridden with a file named 01-kubelet.conf.These files may contain partial configurations but should not be invalid and must include type metadata, specifically apiVersion and kind. Validation is only performed on the final resulting configuration structure stored internally in the kubelet. This offers flexibility in managing and merging kubelet configurations from different sources while preventing undesirable configurations. However, it is important to note that behavior varies based on the data type of the configuration fields.Different data types in the kubelet configuration structure merge differently. See the reference document for more information.Kubelet configuration merging orderOn startup, the kubelet merges configuration from:Feature gates specified over the command line (lowest precedence).The kubelet configuration.Drop-in configuration files, according to sort order.Command line arguments excluding feature gates (highest precedence).Note:The config drop-in dir mechanism for the kubelet is similar but different from how the kubeadm tool allows you to patch configuration. The kubeadm tool uses a specific patching strategy for its configuration, whereas the only patch strategy for kubelet configuration drop-in files is replace. The kubelet determines the order of merges based on sorting the suffixes alphanumerically, and replaces every field present in a higher priority file.Viewing the kubelet configurationSince the configuration could now be spread over multiple files with this feature, if someone wants to inspect the final actuated configuration, they can follow these steps to inspect the kubelet configuration:Start a proxy server using kubectl proxy in your terminal.kubectl proxy Which gives output like:Starting to serve on 127.0.0.1:8001 Open another terminal window and use curl to fetch the kubelet configuration. Replace  with the actual name of your node:curl -X GET http://127.0.0.1:8001/api/v1/nodes//proxy/configz | jq . { "kubeletconfig": { "enableServer": true, "staticPodPath": "/var/run/kubernetes/static-pods", "syncFrequency": "1m0s", "fileCheckFrequency": "20s", "httpCheckFrequency": "20s", "address": "192.168.1.16", "port": 10250, "readOnlyPort": 10255, "tlsCertFile": "/var/lib/kubelet/pki/kubelet.crt", "tlsPrivateKeyFile": "/var/lib/kubelet/pki/kubelet.key", "rotateCertificates": true, "authentication": { "x509": { "clientCAFile": "/var/run/kubernetes/client-ca.crt" }, "webhook": { "enabled": true, "cacheTTL": "2m0s" }, "anonymous": { "enabled": true } }, "authorization": { "mode": "AlwaysAllow", "webhook": { "cacheAuthorizedTTL": "5m0s", "cacheUnauthorizedTTL": "30s" } }, "registryPullQPS": 5, "registryBurst": 10, "eventRecordQPS": 50, "eventBurst": 100, "enableDebuggingHandlers": true, "healthzPort": 10248, "healthzBindAddress": "127.0.0.1", "oomScoreAdj": -999, "clusterDomain": "cluster.local", "clusterDNS": [ "10.0.0.10" ], "streamingConnectionIdleTimeout": "4h0m0s", "nodeStatusUpdateFrequency": "10s", "nodeStatusReportFrequency": "5m0s", "nodeLeaseDurationSeconds": 40, "imageMinimumGCAge": "2m0s", "imageMaximumGCAge": "0s", "imageGCHighThresholdPercent": 85, "imageGCLowThresholdPercent": 80, "volumeStatsAggPeriod": "1m0s", "cgroupsPerQOS": true, "cgroupDriver": "systemd", "cpuManagerPolicy": "none", "cpuManagerReconcilePeriod": "10s", "memoryManagerPolicy": "None", "topologyManagerPolicy": "none", "topologyManagerScope": "container", "runtimeRequestTimeout": "2m0s", "hairpinMode": "promiscuous-bridge", "maxPods": 110, "podPidsLimit": -1, "resolvConf": "/run/systemd/resolve/resolv.conf", "cpuCFSQuota": true, "cpuCFSQuotaPeriod": "100ms", "nodeStatusMaxImages": 50, "maxOpenFiles": 1000000, "contentType": "application/vnd.kubernetes.protobuf", "kubeAPIQPS": 50, "kubeAPIBurst": 100, "serializeImagePulls": true, "evictionHard": { "imagefs.available": "15%", "memory.available": "100Mi", "nodefs.available": "10%", "nodefs.inodesFree": "5%", "imagefs.inodesFree": "5%" }, "evictionPressureTransitionPeriod": "1m0s", "mergeDefaultEvictionSettings": false, "enableControllerAttachDetach": true, "makeIPTablesUtilChains": true, "iptablesMasqueradeBit": 14, "iptablesDropBit": 15, "featureGates": { "AllAlpha": false }, "failSwapOn": false, "memorySwap": {}, "containerLogMaxSize": "10Mi", "containerLogMaxFiles": 5, "configMapAndSecretChangeDetectionStrategy": "Watch", "enforceNodeAllocatable": [ "pods" ], "volumePluginDir": "/usr/libexec/kubernetes/kubelet-plugins/volume/exec/", "logging": { "format": "text", "flushFrequency": "5s", "verbosity": 3, "options": { "json": { "infoBufferSize": "0" } } }, "enableSystemLogHandler": true, "enableSystemLogQuery": false, "shutdownGracePeriod": "0s", "shutdownGracePeriodCriticalPods": "0s", "enableProfilingHandler": true, "enableDebugFlagsHandler": true, "seccompDefault": false, "memoryThrottlingFactor": 0.9, "registerNode": true, "localStorageCapacityIsolation": true, "containerRuntimeEndpoint": "unix:///var/run/crio/crio.sock" } } What's nextLearn more about kubelet configuration by checking the KubeletConfiguration reference.Learn more about kubelet configuration merging in the reference document.4.2.36 - Share a Cluster with NamespacesThis page shows how to view, work in, and delete namespaces. The page also shows how to use Kubernetes namespaces to subdivide your cluster.Before you beginHave an existing Kubernetes cluster.You have a basic understanding of Kubernetes Pods, Services, and Deployments.Viewing namespacesList the current namespaces in a cluster using:kubectl get namespaces NAME STATUS AGE default Active 11d kube-node-lease Active 11d kube-public Active 11d kube-system Active 11d Kubernetes starts with four initial namespaces:default The default namespace for objects with no other namespacekube-node-lease This namespace holds Lease objects associated with each node. Node leases allow the kubelet to send heartbeats so that the control plane can detect node failure.kube-public This namespace is created automatically and is readable by all users (including those not authenticated). This namespace is mostly reserved for cluster usage, in case that some resources should be visible and readable publicly throughout the whole cluster. The public aspect of this namespace is only a convention, not a requirement.kube-system The namespace for objects created by the Kubernetes systemYou can also get the summary of a specific namespace using:kubectl get namespaces  Or you can get detailed information with:kubectl describe namespaces  Name: default Labels:  Annotations:  Status: Active No resource quota. Resource Limits Type Resource Min Max Default \---- -------- --- --- --- Container cpu - - 100m Note that these details show both resource quota (if present) as well as resource limit ranges.Resource quota tracks aggregate usage of resources in the Namespace and allows cluster operators to define Hard resource usage limits that a Namespace may consume.A limit range defines min/max constraints on the amount of resources a single entity can consume in a Namespace.See Admission control: Limit RangeA namespace can be in one of two phases:Active the namespace is in useTerminating the namespace is being deleted, and can not be used for new objectsFor more details, see Namespace in the API reference.Creating a new namespaceNote:Avoid creating namespace with prefix kube-, since it is reserved for Kubernetes system namespaces.Create a new YAML file called my-namespace.yaml with the contents:apiVersion: v1 kind: Namespace metadata: name:  Then run:kubectl create -f ./my-namespace.yaml Alternatively, you can create namespace using below command:kubectl create namespace  The name of your namespace must be a valid DNS label.There's an optional field finalizers, which allows observables to purge resources whenever the namespace is deleted. Keep in mind that if you specify a nonexistent finalizer, the namespace will be created but will get stuck in the Terminating state if the user tries to delete it.More information on finalizers can be found in the namespace design doc.Deleting a namespaceDelete a namespace withkubectl delete namespaces  Warning:This deletes everything under the namespace!This delete is asynchronous, so for a time you will see the namespace in the Terminating state.Subdividing your cluster using Kubernetes namespacesBy default, a Kubernetes cluster will instantiate a default namespace when provisioning the cluster to hold the default set of Pods, Services, and Deployments used by the cluster.Assuming you have a fresh cluster, you can introspect the available namespaces by doing the following:kubectl get namespaces NAME STATUS AGE default Active 13m Create new namespacesFor this exercise, we will create two additional Kubernetes namespaces to hold our content.In a scenario where an organization is using a shared Kubernetes cluster for development and production use cases:The development team would like to maintain a space in the cluster where they can get a view on the list of Pods, Services, and Deployments they use to build and run their application. In this space, Kubernetes resources come and go, and the restrictions on who can or cannot modify resources are relaxed to enable agile development.The operations team would like to maintain a space in the cluster where they can enforce strict procedures on who can or cannot manipulate the set of Pods, Services, and Deployments that run the production site.One pattern this organization could follow is to partition the Kubernetes cluster into two namespaces: development and production. Let's create two new namespaces to hold our work.Create the development namespace using kubectl:kubectl create -f https://k8s.io/examples/admin/namespace-dev.json And then let's create the production namespace using kubectl:kubectl create -f https://k8s.io/examples/admin/namespace-prod.json To be sure things are right, list all of the namespaces in our cluster.kubectl get namespaces --show-labels NAME STATUS AGE LABELS default Active 32m  development Active 29s name=development production Active 23s name=production Create pods in each namespaceA Kubernetes namespace provides the scope for Pods, Services, and Deployments in the cluster. Users interacting with one namespace do not see the content in another namespace. To demonstrate this, let's spin up a simple Deployment and Pods in the development namespace.kubectl create deployment snowflake \ \--image=registry.k8s.io/serve_hostname \ -n=development --replicas=2 We have created a deployment whose replica size is 2 that is running the pod called snowflake with a basic container that serves the hostname.kubectl get deployment -n=development NAME READY UP-TO-DATE AVAILABLE AGE snowflake 2/2 2 2 2m kubectl get pods -l app=snowflake -n=development NAME READY STATUS RESTARTS AGE snowflake-3968820950-9dgr8 1/1 Running 0 2m snowflake-3968820950-vgc4n 1/1 Running 0 2m And this is great, developers are able to do what they want, and they do not have to worry about affecting content in the production namespace.Let's switch to the production namespace and show how resources in one namespace are hidden from the other. The production namespace should be empty, and the following commands should return nothing.kubectl get deployment -n=production kubectl get pods -n=production Production likes to run cattle, so let's create some cattle pods.kubectl create deployment cattle --image=registry.k8s.io/serve_hostname -n=production kubectl scale deployment cattle --replicas=5 -n=production kubectl get deployment -n=production NAME READY UP-TO-DATE AVAILABLE AGE cattle 5/5 5 5 10s kubectl get pods -l app=cattle -n=production NAME READY STATUS RESTARTS AGE cattle-2263376956-41xy6 1/1 Running 0 34s cattle-2263376956-kw466 1/1 Running 0 34s cattle-2263376956-n4v97 1/1 Running 0 34s cattle-2263376956-p5p3i 1/1 Running 0 34s cattle-2263376956-sxpth 1/1 Running 0 34s At this point, it should be clear that the resources users create in one namespace are hidden from the other namespace.As the policy support in Kubernetes evolves, we will extend this scenario to show how you can provide different authorization rules for each namespace.Understanding the motivation for using namespacesA single cluster should be able to satisfy the needs of multiple users or groups of users (henceforth in this document a user community).Kubernetes namespaces help different projects, teams, or customers to share a Kubernetes cluster.It does this by providing the following:A scope for names.A mechanism to attach authorization and policy to a subsection of the cluster.Use of multiple namespaces is optional.Each user community wants to be able to work in isolation from other communities. Each user community has its own:resources (pods, services, replication controllers, etc.)policies (who can or cannot perform actions in their community)constraints (this community is allowed this much quota, etc.)A cluster operator may create a Namespace for each unique user community.The Namespace provides a unique scope for:named resources (to avoid basic naming collisions)delegated management authority to trusted usersability to limit community resource consumptionUse cases include:As a cluster operator, I want to support multiple user communities on a single cluster.As a cluster operator, I want to delegate authority to partitions of the cluster to trusted users in those communities.As a cluster operator, I want to limit the amount of resources each community can consume in order to limit the impact to other communities using the cluster.As a cluster user, I want to interact with resources that are pertinent to my user community in isolation of what other user communities are doing on the cluster.Understanding namespaces and DNSWhen you create a Service, it creates a corresponding DNS entry. This entry is of the form ..svc.cluster.local, which means that if a container uses  it will resolve to the service which is local to a namespace. This is useful for using the same configuration across multiple namespaces such as Development, Staging and Production. If you want to reach across namespaces, you need to use the fully qualified domain name (FQDN).What's nextLearn more about setting the namespace preference.Learn more about setting the namespace for a requestSee namespaces design.4.2.37 - Upgrade A ClusterThis page provides an overview of the steps you should follow to upgrade a Kubernetes cluster.The Kubernetes project recommends upgrading to the latest patch releases promptly, and to ensure that you are running a supported minor release of Kubernetes. Following this recommendation helps you to to stay secure.The way that you upgrade a cluster depends on how you initially deployed it and on any subsequent changes.At a high level, the steps you perform are:Upgrade the control planeUpgrade the nodes in your clusterUpgrade clients such as kubectlAdjust manifests and other resources based on the API changes that accompany the new Kubernetes versionBefore you beginYou must have an existing cluster. This page is about upgrading from Kubernetes 1.32 to Kubernetes 1.33. If your cluster is not currently running Kubernetes 1.32 then please check the documentation for the version of Kubernetes that you plan to upgrade to.Upgrade approacheskubeadmIf your cluster was deployed using the kubeadm tool, refer to Upgrading kubeadm clusters for detailed information on how to upgrade the cluster.Once you have upgraded the cluster, remember to install the latest version of kubectl.Manual deploymentsCaution:These steps do not account for third-party extensions such as network and storage plugins.You should manually update the control plane following this sequence:etcd (all instances)kube-apiserver (all control plane hosts)kube-controller-managerkube-schedulercloud controller manager, if you use oneAt this point you should install the latest version of kubectl.For each node in your cluster, drain that node and then either replace it with a new node that uses the 1.33 kubelet, or upgrade the kubelet on that node and bring the node back into service.Caution:Draining nodes before upgrading kubelet ensures that pods are re-admitted and containers are re-created, which may be necessary to resolve some security issues or other important bugs.Other deploymentsRefer to the documentation for your cluster deployment tool to learn the recommended set up steps for maintenance.Post-upgrade tasksSwitch your cluster's storage API versionThe objects that are serialized into etcd for a cluster's internal representation of the Kubernetes resources active in the cluster are written using a particular version of the API.When the supported API changes, these objects may need to be rewritten in the newer API. Failure to do this will eventually result in resources that are no longer decodable or usable by the Kubernetes API server.For each affected object, fetch it using the latest supported API and then write it back also using the latest supported API.Update manifestsUpgrading to a new Kubernetes version can provide new APIs.You can use kubectl convert command to convert manifests between different API versions. For example:kubectl convert -f pod.yaml --output-version v1 The kubectl tool replaces the contents of pod.yaml with a manifest that sets kind to Pod (unchanged), but with a revised apiVersion.Device PluginsIf your cluster is running device plugins and the node needs to be upgraded to a Kubernetes release with a newer device plugin API version, device plugins must be upgraded to support both version before the node is upgraded in order to guarantee that device allocations continue to complete successfully during the upgrade.Refer to API compatibility and Kubelet Device Manager API Versions for more details.4.2.38 - Use Cascading Deletion in a ClusterThis page shows you how to specify the type of cascading deletion to use in your cluster during garbage collection.Before you beginYou need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesYou also need to create a sample Deployment to experiment with the different types of cascading deletion. You will need to recreate the Deployment for each type.Check owner references on your podsCheck that the ownerReferences field is present on your pods:kubectl get pods -l app=nginx --output=yaml The output has an ownerReferences field similar to this:apiVersion: v1 ... ownerReferences: \- apiVersion: apps/v1 blockOwnerDeletion: true controller: true kind: ReplicaSet name: nginx-deployment-6b474476c4 uid: 4fdcd81c-bd5d-41f7-97af-3a3b759af9a7 ... Use foreground cascading deletionBy default, Kubernetes uses background cascading deletion to delete dependents of an object. You can switch to foreground cascading deletion using either kubectl or the Kubernetes API, depending on the Kubernetes version your cluster runs.To check the version, enter kubectl version.You can delete objects using foreground cascading deletion using kubectl or the Kubernetes API.Using kubectlRun the following command:kubectl delete deployment nginx-deployment --cascade=foreground Using the Kubernetes APIStart a local proxy session:kubectl proxy --port=8080 Use curl to trigger deletion:curl -X DELETE localhost:8080/apis/apps/v1/namespaces/default/deployments/nginx-deployment \ -d '{"kind":"DeleteOptions","apiVersion":"v1","propagationPolicy":"Foreground"}' \ -H "Content-Type: application/json" The output contains a foregroundDeletion finalizer like this:"kind": "Deployment", "apiVersion": "apps/v1", "metadata": { "name": "nginx-deployment", "namespace": "default", "uid": "d1ce1b02-cae8-4288-8a53-30e84d8fa505", "resourceVersion": "1363097", "creationTimestamp": "2021-07-08T20:24:37Z", "deletionTimestamp": "2021-07-08T20:27:39Z", "finalizers": [ "foregroundDeletion" ] ... Use background cascading deletionCreate a sample Deployment.Use either kubectl or the Kubernetes API to delete the Deployment, depending on the Kubernetes version your cluster runs.To check the version, enter kubectl version.You can delete objects using background cascading deletion using kubectl or the Kubernetes API.Kubernetes uses background cascading deletion by default, and does so even if you run the following commands without the --cascade flag or the propagationPolicy argument.Using kubectlRun the following command:kubectl delete deployment nginx-deployment --cascade=background Using the Kubernetes APIStart a local proxy session:kubectl proxy --port=8080 Use curl to trigger deletion:curl -X DELETE localhost:8080/apis/apps/v1/namespaces/default/deployments/nginx-deployment \ -d '{"kind":"DeleteOptions","apiVersion":"v1","propagationPolicy":"Background"}' \ -H "Content-Type: application/json" The output is similar to this:"kind": "Status", "apiVersion": "v1", ... "status": "Success", "details": { "name": "nginx-deployment", "group": "apps", "kind": "deployments", "uid": "cc9eefb9-2d49-4445-b1c1-d261c9396456" } Delete owner objects and orphan dependentsBy default, when you tell Kubernetes to delete an object, the controller also deletes dependent objects. You can make Kubernetes orphan these dependents using kubectl or the Kubernetes API, depending on the Kubernetes version your cluster runs.To check the version, enter kubectl version.Using kubectlRun the following command:kubectl delete deployment nginx-deployment --cascade=orphan Using the Kubernetes APIStart a local proxy session:kubectl proxy --port=8080 Use curl to trigger deletion:curl -X DELETE localhost:8080/apis/apps/v1/namespaces/default/deployments/nginx-deployment \ -d '{"kind":"DeleteOptions","apiVersion":"v1","propagationPolicy":"Orphan"}' \ -H "Content-Type: application/json" The output contains orphan in the finalizers field, similar to this:"kind": "Deployment", "apiVersion": "apps/v1", "namespace": "default", "uid": "6f577034-42a0-479d-be21-78018c466f1f", "creationTimestamp": "2021-07-09T16:46:37Z", "deletionTimestamp": "2021-07-09T16:47:08Z", "deletionGracePeriodSeconds": 0, "finalizers": [ "orphan" ], ... You can check that the Pods managed by the Deployment are still running:kubectl get pods -l app=nginx What's nextLearn about owners and dependents in Kubernetes.Learn about Kubernetes finalizers.Learn about garbage collection.4.2.39 - Using a KMS provider for data encryptionThis page shows how to configure a Key Management Service (KMS) provider and plugin to enable secret data encryption. In Kubernetes 1.33 there are two versions of KMS at-rest encryption. You should use KMS v2 if feasible because KMS v1 is deprecated (since Kubernetes v1.28) and disabled by default (since Kubernetes v1.29). KMS v2 offers significantly better performance characteristics than KMS v1.Caution:This documentation is for the generally available implementation of KMS v2 (and for the deprecated version 1 implementation). If you are using any control plane components older than Kubernetes v1.29, please check the equivalent page in the documentation for the version of Kubernetes that your cluster is running. Earlier releases of Kubernetes had different behavior that may be relevant for information security.Before you beginYou need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesThe version of Kubernetes that you need depends on which KMS API version you have selected. Kubernetes recommends using KMS v2.If you selected KMS API v1 to support clusters prior to version v1.27 or if you have a legacy KMS plugin that only supports KMS v1, any supported Kubernetes version will work. This API is deprecated as of Kubernetes v1.28. Kubernetes does not recommend the use of this API.To check the version, enter kubectl version.KMS v1FEATURE STATE: Kubernetes v1.28 [deprecated]Kubernetes version 1.10.0 or later is requiredFor version 1.29 and later, the v1 implementation of KMS is disabled by default. To enable the feature, set --feature-gates=KMSv1=true to configure a KMS v1 provider.Your cluster must use etcd v3 or laterKMS v2FEATURE STATE: Kubernetes v1.29 [stable]Your cluster must use etcd v3 or laterKMS encryption and per-object encryption keysThe KMS encryption provider uses an envelope encryption scheme to encrypt data in etcd. The data is encrypted using a data encryption key (DEK). The DEKs are encrypted with a key encryption key (KEK) that is stored and managed in a remote KMS.If you use the (deprecated) v1 implementation of KMS, a new DEK is generated for each encryption.With KMS v2, a new DEK is generated per encryption: the API server uses a key derivation function to generate single use data encryption keys from a secret seed combined with some random data. The seed is rotated whenever the KEK is rotated (see the Understanding key_id and Key Rotation section below for more details).The KMS provider uses gRPC to communicate with a specific KMS plugin over a UNIX domain socket. The KMS plugin, which is implemented as a gRPC server and deployed on the same host(s) as the Kubernetes control plane, is responsible for all communication with the remote KMS.Configuring the KMS providerTo configure a KMS provider on the API server, include a provider of type kms in the providers array in the encryption configuration file and set the following properties:KMS v1apiVersion: API Version for KMS provider. Leave this value empty or set it to v1.name: Display name of the KMS plugin. Cannot be changed once set.endpoint: Listen address of the gRPC server (KMS plugin). The endpoint is a UNIX domain socket.cachesize: Number of data encryption keys (DEKs) to be cached in the clear. When cached, DEKs can be used without another call to the KMS; whereas DEKs that are not cached require a call to the KMS to unwrap.timeout: How long should kube-apiserver wait for kms-plugin to respond before returning an error (default is 3 seconds).KMS v2apiVersion: API Version for KMS provider. Set this to v2.name: Display name of the KMS plugin. Cannot be changed once set.endpoint: Listen address of the gRPC server (KMS plugin). The endpoint is a UNIX domain socket.timeout: How long should kube-apiserver wait for kms-plugin to respond before returning an error (default is 3 seconds).KMS v2 does not support the cachesize property. All data encryption keys (DEKs) will be cached in the clear once the server has unwrapped them via a call to the KMS. Once cached, DEKs can be used to perform decryption indefinitely without making a call to the KMS.See Understanding the encryption at rest configuration.Implementing a KMS pluginTo implement a KMS plugin, you can develop a new plugin gRPC server or enable a KMS plugin already provided by your cloud provider. You then integrate the plugin with the remote KMS and deploy it on the Kubernetes control plane.Enabling the KMS supported by your cloud providerRefer to your cloud provider for instructions on enabling the cloud provider-specific KMS plugin.Developing a KMS plugin gRPC serverYou can develop a KMS plugin gRPC server using a stub file available for Go. For other languages, you use a proto file to create a stub file that you can use to develop the gRPC server code.KMS v1Using Go: Use the functions and data structures in the stub file: api.pb.go to develop the gRPC server codeUsing languages other than Go: Use the protoc compiler with the proto file: api.proto to generate a stub file for the specific languageKMS v2Using Go: A high level library is provided to make the process easier. Low level implementations can use the functions and data structures in the stub file: api.pb.go to develop the gRPC server codeUsing languages other than Go: Use the protoc compiler with the proto file: api.proto to generate a stub file for the specific languageThen use the functions and data structures in the stub file to develop the server code.NotesKMS v1kms plugin version: v1beta1In response to procedure call Version, a compatible KMS plugin should return v1beta1 as VersionResponse.version.message version: v1beta1All messages from KMS provider have the version field set to v1beta1.protocol: UNIX domain socket (unix)The plugin is implemented as a gRPC server that listens at UNIX domain socket. The plugin deployment should create a file on the file system to run the gRPC unix domain socket connection. The API server (gRPC client) is configured with the KMS provider (gRPC server) unix domain socket endpoint in order to communicate with it. An abstract Linux socket may be used by starting the endpoint with /@, i.e. unix:///@foo. Care must be taken when using this type of socket as they do not have concept of ACL (unlike traditional file based sockets). However, they are subject to Linux networking namespace, so will only be accessible to containers within the same pod unless host networking is used.KMS v2KMS plugin version: v2In response to the Status remote procedure call, a compatible KMS plugin should return its KMS compatibility version as StatusResponse.version. That status response should also include "ok" as StatusResponse.healthz and a key_id (remote KMS KEK ID) as StatusResponse.key_id. The Kubernetes project recommends you make your plugin compatible with the stable v2 KMS API. Kubernetes 1.33 also supports the v2beta1 API for KMS; future Kubernetes releases are likely to continue supporting that beta version.The API server polls the Status procedure call approximately every minute when everything is healthy, and every 10 seconds when the plugin is not healthy. Plugins must take care to optimize this call as it will be under constant load.EncryptionThe EncryptRequest procedure call provides the plaintext and a UID for logging purposes. The response must include the ciphertext, the key_id for the KEK used, and, optionally, any metadata that the KMS plugin needs to aid in future DecryptRequest calls (via the annotations field). The plugin must guarantee that any distinct plaintext results in a distinct response (ciphertext, key_id, annotations).If the plugin returns a non-empty annotations map, all map keys must be fully qualified domain names such as example.com. An example use case of annotation is {"kms.example.io/remote-kms-auditid":""}The API server does not perform the EncryptRequest procedure call at a high rate. Plugin implementations should still aim to keep each request's latency at under 100 milliseconds.DecryptionThe DecryptRequest procedure call provides the (ciphertext, key_id, annotations) from EncryptRequest and a UID for logging purposes. As expected, it is the inverse of the EncryptRequest call. Plugins must verify that the key_id is one that they understand - they must not attempt to decrypt data unless they are sure that it was encrypted by them at an earlier time.The API server may perform thousands of DecryptRequest procedure calls on startup to fill its watch cache. Thus plugin implementations must perform these calls as quickly as possible, and should aim to keep each request's latency at under 10 milliseconds.Understanding key_id and Key RotationThe key_id is the public, non-secret name of the remote KMS KEK that is currently in use. It may be logged during regular operation of the API server, and thus must not contain any private data. Plugin implementations are encouraged to use a hash to avoid leaking any data. The KMS v2 metrics take care to hash this value before exposing it via the /metrics endpoint.The API server considers the key_id returned from the Status procedure call to be authoritative. Thus, a change to this value signals to the API server that the remote KEK has changed, and data encrypted with the old KEK should be marked stale when a no-op write is performed (as described below). If an EncryptRequest procedure call returns a key_id that is different from Status, the response is thrown away and the plugin is considered unhealthy. Thus implementations must guarantee that the key_id returned from Status will be the same as the one returned by EncryptRequest. Furthermore, plugins must ensure that the key_id is stable and does not flip-flop between values (i.e. during a remote KEK rotation).Plugins must not re-use key_ids, even in situations where a previously used remote KEK has been reinstated. For example, if a plugin was using key_id=A, switched to key_id=B, and then went back to key_id=A - instead of reporting key_id=A the plugin should report some derivative value such as key_id=A_001 or use a new value such as key_id=C.Since the API server polls Status about every minute, key_id rotation is not immediate. Furthermore, the API server will coast on the last valid state for about three minutes. Thus if a user wants to take a passive approach to storage migration (i.e. by waiting), they must schedule a migration to occur at 3 + N + M minutes after the remote KEK has been rotated (N is how long it takes the plugin to observe the key_id change and M is the desired buffer to allow config changes to be processed - a minimum M of five minutes is recommend). Note that no API server restart is required to perform KEK rotation.Caution:Because you don't control the number of writes performed with the DEK, the Kubernetes project recommends rotating the KEK at least every 90 days.protocol: UNIX domain socket (unix)The plugin is implemented as a gRPC server that listens at UNIX domain socket. The plugin deployment should create a file on the file system to run the gRPC unix domain socket connection. The API server (gRPC client) is configured with the KMS provider (gRPC server) unix domain socket endpoint in order to communicate with it. An abstract Linux socket may be used by starting the endpoint with /@, i.e. unix:///@foo. Care must be taken when using this type of socket as they do not have concept of ACL (unlike traditional file based sockets). However, they are subject to Linux networking namespace, so will only be accessible to containers within the same pod unless host networking is used.Integrating a KMS plugin with the remote KMSThe KMS plugin can communicate with the remote KMS using any protocol supported by the KMS. All configuration data, including authentication credentials the KMS plugin uses to communicate with the remote KMS, are stored and managed by the KMS plugin independently. The KMS plugin can encode the ciphertext with additional metadata that may be required before sending it to the KMS for decryption (KMS v2 makes this process easier by providing a dedicated annotations field).Deploying the KMS pluginEnsure that the KMS plugin runs on the same host(s) as the Kubernetes API server(s).Encrypting your data with the KMS providerTo encrypt the data:Create a new EncryptionConfiguration file using the appropriate properties for the kms provider to encrypt resources like Secrets and ConfigMaps. If you want to encrypt an extension API that is defined in a CustomResourceDefinition, your cluster must be running Kubernetes v1.26 or newer.Set the --encryption-provider-config flag on the kube-apiserver to point to the location of the configuration file.--encryption-provider-config-automatic-reload boolean argument determines if the file set by --encryption-provider-config should be automatically reloaded if the disk contents change.Restart your API server.KMS v1apiVersion: apiserver.config.k8s.io/v1 kind: EncryptionConfiguration resources: \- resources: \- secrets \- configmaps \- pandas.awesome.bears.example providers: \- kms: name: myKmsPluginFoo endpoint: unix:///tmp/socketfile-foo.sock cachesize: 100 timeout: 3s \- kms: name: myKmsPluginBar endpoint: unix:///tmp/socketfile-bar.sock cachesize: 100 timeout: 3s KMS v2apiVersion: apiserver.config.k8s.io/v1 kind: EncryptionConfiguration resources: \- resources: \- secrets \- configmaps \- pandas.awesome.bears.example providers: \- kms: apiVersion: v2 name: myKmsPluginFoo endpoint: unix:///tmp/socketfile-foo.sock timeout: 3s \- kms: apiVersion: v2 name: myKmsPluginBar endpoint: unix:///tmp/socketfile-bar.sock timeout: 3s Setting --encryption-provider-config-automatic-reload to true collapses all health checks to a single health check endpoint. Individual health checks are only available when KMS v1 providers are in use and the encryption config is not auto-reloaded.The following table summarizes the health check endpoints for each KMS version:KMS configurationsWithout Automatic ReloadWith Automatic ReloadKMS v1 onlyIndividual HealthchecksSingle HealthcheckKMS v2 onlySingle HealthcheckSingle HealthcheckBoth KMS v1 and v2Individual HealthchecksSingle HealthcheckNo KMSNoneSingle HealthcheckSingle Healthcheck means that the only health check endpoint is /healthz/kms-providers.Individual Healthchecks means that each KMS plugin has an associated health check endpoint based on its location in the encryption config: /healthz/kms-provider-0, /healthz/kms-provider-1 etc.These healthcheck endpoint paths are hard coded and generated/controlled by the server. The indices for individual healthchecks corresponds to the order in which the KMS encryption config is processed.Until the steps defined in Ensuring all secrets are encrypted are performed, the providers list should end with the identity: {} provider to allow unencrypted data to be read. Once all resources are encrypted, the identity provider should be removed to prevent the API server from honoring unencrypted data.For details about the EncryptionConfiguration format, please check the API server encryption API reference.Verifying that the data is encryptedWhen encryption at rest is correctly configured, resources are encrypted on write. After restarting your kube-apiserver, any newly created or updated Secret or other resource types configured in EncryptionConfiguration should be encrypted when stored. To verify, you can use the etcdctl command line program to retrieve the contents of your secret data.Create a new secret called secret1 in the default namespace:kubectl create secret generic secret1 -n default --from-literal=mykey=mydata Using the etcdctl command line, read that secret out of etcd:ETCDCTL_API=3 etcdctl get /kubernetes.io/secrets/default/secret1 [...] | hexdump -C where [...] contains the additional arguments for connecting to the etcd server.Verify the stored secret is prefixed with k8s:enc:kms:v1: for KMS v1 or prefixed with k8s:enc:kms:v2: for KMS v2, which indicates that the kms provider has encrypted the resulting data.Verify that the secret is correctly decrypted when retrieved via the API:kubectl describe secret secret1 -n default The Secret should contain mykey: mydataEnsuring all secrets are encryptedWhen encryption at rest is correctly configured, resources are encrypted on write. Thus we can perform an in-place no-op update to ensure that data is encrypted.The following command reads all secrets and then updates them to apply server side encryption. If an error occurs due to a conflicting write, retry the command. For larger clusters, you may wish to subdivide the secrets by namespace or script an update.kubectl get secrets --all-namespaces -o json | kubectl replace -f - Switching from a local encryption provider to the KMS providerTo switch from a local encryption provider to the kms provider and re-encrypt all of the secrets:Add the kms provider as the first entry in the configuration file as shown in the following example.apiVersion: apiserver.config.k8s.io/v1 kind: EncryptionConfiguration resources: \- resources: \- secrets providers: \- kms: apiVersion: v2 name : myKmsPlugin endpoint: unix:///tmp/socketfile.sock \- aescbc: keys: \- name: key1 secret:  Restart all kube-apiserver processes.Run the following command to force all secrets to be re-encrypted using the kms provider.kubectl get secrets --all-namespaces -o json | kubectl replace -f - What's nextIf you no longer want to use encryption for data persisted in the Kubernetes API, read decrypt data that are already stored at rest.4.2.40 - Using CoreDNS for Service DiscoveryThis page describes the CoreDNS upgrade process and how to install CoreDNS instead of kube-dns.Before you beginYou need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesYour Kubernetes server must be at or later than version v1.9.To check the version, enter kubectl version.About CoreDNSCoreDNS is a flexible, extensible DNS server that can serve as the Kubernetes cluster DNS. Like Kubernetes, the CoreDNS project is hosted by the CNCF.You can use CoreDNS instead of kube-dns in your cluster by replacing kube-dns in an existing deployment, or by using tools like kubeadm that will deploy and upgrade the cluster for you.Installing CoreDNSFor manual deployment or replacement of kube-dns, see the documentation at the CoreDNS website.Migrating to CoreDNSUpgrading an existing cluster with kubeadmIn Kubernetes version 1.21, kubeadm removed its support for kube-dns as a DNS application. For kubeadm v1.33, the only supported cluster DNS application is CoreDNS.You can move to CoreDNS when you use kubeadm to upgrade a cluster that is using kube-dns. In this case, kubeadm generates the CoreDNS configuration ("Corefile") based upon the kube-dns ConfigMap, preserving configurations for stub domains, and upstream name server.Upgrading CoreDNSYou can check the version of CoreDNS that kubeadm installs for each version of Kubernetes in the page CoreDNS version in Kubernetes.CoreDNS can be upgraded manually in case you want to only upgrade CoreDNS or use your own custom image. There is a helpful guideline and walkthrough available to ensure a smooth upgrade. Make sure the existing CoreDNS configuration ("Corefile") is retained when upgrading your cluster.If you are upgrading your cluster using the kubeadm tool, kubeadm can take care of retaining the existing CoreDNS configuration automatically.Tuning CoreDNSWhen resource utilisation is a concern, it may be useful to tune the configuration of CoreDNS. For more details, check out the documentation on scaling CoreDNS.What's nextYou can configure CoreDNS to support many more use cases than kube-dns does by modifying the CoreDNS configuration ("Corefile"). For more information, see the documentation for the kubernetes CoreDNS plugin, or read the Custom DNS Entries for Kubernetes. in the CoreDNS blog.4.2.41 - Using NodeLocal DNSCache in Kubernetes ClustersFEATURE STATE: Kubernetes v1.18 [stable]This page provides an overview of NodeLocal DNSCache feature in Kubernetes.Before you beginYou need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesTo check the version, enter kubectl version.IntroductionNodeLocal DNSCache improves Cluster DNS performance by running a DNS caching agent on cluster nodes as a DaemonSet. In today's architecture, Pods in 'ClusterFirst' DNS mode reach out to a kube-dns serviceIP for DNS queries. This is translated to a kube-dns/CoreDNS endpoint via iptables rules added by kube-proxy. With this new architecture, Pods will reach out to the DNS caching agent running on the same node, thereby avoiding iptables DNAT rules and connection tracking. The local caching agent will query kube-dns service for cache misses of cluster hostnames ("cluster.local" suffix by default).MotivationWith the current DNS architecture, it is possible that Pods with the highest DNS QPS have to reach out to a different node, if there is no local kube-dns/CoreDNS instance. Having a local cache will help improve the latency in such scenarios.Skipping iptables DNAT and connection tracking will help reduce conntrack races and avoid UDP DNS entries filling up conntrack table.Connections from the local caching agent to kube-dns service can be upgraded to TCP. TCP conntrack entries will be removed on connection close in contrast with UDP entries that have to timeout (default nf_conntrack_udp_timeout is 30 seconds)Upgrading DNS queries from UDP to TCP would reduce tail latency attributed to dropped UDP packets and DNS timeouts usually up to 30s (3 retries + 10s timeout). Since the nodelocal cache listens for UDP DNS queries, applications don't need to be changed.Metrics & visibility into DNS requests at a node level.Negative caching can be re-enabled, thereby reducing the number of queries for the kube-dns service.Architecture DiagramThis is the path followed by DNS Queries after NodeLocal DNSCache is enabled:Nodelocal DNSCache flowThis image shows how NodeLocal DNSCache handles DNS queries.ConfigurationNote:The local listen IP address for NodeLocal DNSCache can be any address that can be guaranteed to not collide with any existing IP in your cluster. It's recommended to use an address with a local scope, for example, from the 'link-local' range '169.254.0.0/16' for IPv4 or from the 'Unique Local Address' range in IPv6 'fd00::/8'.This feature can be enabled using the following steps:Prepare a manifest similar to the sample nodelocaldns.yaml and save it as nodelocaldns.yaml.If using IPv6, the CoreDNS configuration file needs to enclose all the IPv6 addresses into square brackets if used in 'IP:Port' format. If you are using the sample manifest from the previous point, this will require you to modify the configuration line L70 like this: "health [__PILLAR__LOCAL__DNS__]:8080"Substitute the variables in the manifest with the right values:kubedns=`kubectl get svc kube-dns -n kube-system -o jsonpath={.spec.clusterIP}` domain= localdns= is "cluster.local" by default.  is the local listen IP address chosen for NodeLocal DNSCache.If kube-proxy is running in IPTABLES mode:sed -i "s/__PILLAR__LOCAL__DNS__/$localdns/g; s/__PILLAR__DNS__DOMAIN__/$domain/g; s/__PILLAR__DNS__SERVER__/$kubedns/g" nodelocaldns.yaml __PILLAR__CLUSTER__DNS__ and __PILLAR__UPSTREAM__SERVERS__ will be populated by the node-local-dns pods. In this mode, the node-local-dns pods listen on both the kube-dns service IP as well as , so pods can look up DNS records using either IP address.If kube-proxy is running in IPVS mode:sed -i "s/__PILLAR__LOCAL__DNS__/$localdns/g; s/__PILLAR__DNS__DOMAIN__/$domain/g; s/,__PILLAR__DNS__SERVER__//g; s/__PILLAR__CLUSTER__DNS__/$kubedns/g" nodelocaldns.yaml In this mode, the node-local-dns pods listen only on . The node-local-dns interface cannot bind the kube-dns cluster IP since the interface used for IPVS loadbalancing already uses this address. __PILLAR__UPSTREAM__SERVERS__ will be populated by the node-local-dns pods.Run kubectl create -f nodelocaldns.yamlIf using kube-proxy in IPVS mode, --cluster-dns flag to kubelet needs to be modified to use  that NodeLocal DNSCache is listening on. Otherwise, there is no need to modify the value of the --cluster-dns flag, since NodeLocal DNSCache listens on both the kube-dns service IP as well as .Once enabled, the node-local-dns Pods will run in the kube-system namespace on each of the cluster nodes. This Pod runs CoreDNS in cache mode, so all CoreDNS metrics exposed by the different plugins will be available on a per-node basis.You can disable this feature by removing the DaemonSet, using kubectl delete -f . You should also revert any changes you made to the kubelet configuration.StubDomains and Upstream server ConfigurationStubDomains and upstream servers specified in the kube-dns ConfigMap in the kube-system namespace are automatically picked up by node-local-dns pods. The ConfigMap contents need to follow the format shown in the example. The node-local-dns ConfigMap can also be modified directly with the stubDomain configuration in the Corefile format. Some cloud providers might not allow modifying node-local-dns ConfigMap directly. In those cases, the kube-dns ConfigMap can be updated.Setting memory limitsThe node-local-dns Pods use memory for storing cache entries and processing queries. Since they do not watch Kubernetes objects, the cluster size or the number of Services / EndpointSlices do not directly affect memory usage. Memory usage is influenced by the DNS query pattern. From CoreDNS docs,The default cache size is 10000 entries, which uses about 30 MB when completely filled.This would be the memory usage for each server block (if the cache gets completely filled). Memory usage can be reduced by specifying smaller cache sizes.The number of concurrent queries is linked to the memory demand, because each extra goroutine used for handling a query requires an amount of memory. You can set an upper limit using the max_concurrent option in the forward plugin.If a node-local-dns Pod attempts to use more memory than is available (because of total system resources, or because of a configured resource limit), the operating system may shut down that pod's container. If this happens, the container that is terminated (“OOMKilled”) does not clean up the custom packet filtering rules that it previously added during startup. The node-local-dns container should get restarted (since managed as part of a DaemonSet), but this will lead to a brief DNS downtime each time that the container fails: the packet filtering rules direct DNS queries to a local Pod that is unhealthy.You can determine a suitable memory limit by running node-local-dns pods without a limit and measuring the peak usage. You can also set up and use a VerticalPodAutoscaler in recommender mode, and then check its recommendations.4.2.42 - Using sysctls in a Kubernetes ClusterFEATURE STATE: Kubernetes v1.21 [stable]This document describes how to configure and use kernel parameters within a Kubernetes cluster using the sysctl interface.Note:Starting from Kubernetes version 1.23, the kubelet supports the use of either / or . as separators for sysctl names. Starting from Kubernetes version 1.25, setting Sysctls for a Pod supports setting sysctls with slashes. For example, you can represent the same sysctl name as kernel.shm_rmid_forced using a period as the separator, or as kernel/shm_rmid_forced using a slash as a separator. For more sysctl parameter conversion method details, please refer to the page sysctl.d(5) from the Linux man-pages project.Before you beginNote:sysctl is a Linux-specific command-line tool used to configure various kernel parameters and it is not available on non-Linux operating systems.You need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesFor some steps, you also need to be able to reconfigure the command line options for the kubelets running on your cluster.Listing all Sysctl ParametersIn Linux, the sysctl interface allows an administrator to modify kernel parameters at runtime. Parameters are available via the /proc/sys/ virtual process file system. The parameters cover various subsystems such as:kernel (common prefix: kernel.)networking (common prefix: net.)virtual memory (common prefix: vm.)MDADM (common prefix: dev.)More subsystems are described in Kernel docs.To get a list of all parameters, you can runsudo sysctl -a Safe and Unsafe SysctlsKubernetes classes sysctls as either safe or unsafe. In addition to proper namespacing, a safe sysctl must be properly isolated between pods on the same node. This means that setting a safe sysctl for one podmust not have any influence on any other pod on the nodemust not allow to harm the node's healthmust not allow to gain CPU or memory resources outside of the resource limits of a pod.By far, most of the namespaced sysctls are not necessarily considered safe. The following sysctls are supported in the safe set:kernel.shm_rmid_forced;net.ipv4.ip_local_port_range;net.ipv4.tcp_syncookies;net.ipv4.ping_group_range (since Kubernetes 1.18);net.ipv4.ip_unprivileged_port_start (since Kubernetes 1.22);net.ipv4.ip_local_reserved_ports (since Kubernetes 1.27, needs kernel 3.16+);net.ipv4.tcp_keepalive_time (since Kubernetes 1.29, needs kernel 4.5+);net.ipv4.tcp_fin_timeout (since Kubernetes 1.29, needs kernel 4.6+);net.ipv4.tcp_keepalive_intvl (since Kubernetes 1.29, needs kernel 4.5+);net.ipv4.tcp_keepalive_probes (since Kubernetes 1.29, needs kernel 4.5+).net.ipv4.tcp_rmem (since Kubernetes 1.32, needs kernel 4.15+).net.ipv4.tcp_wmem (since Kubernetes 1.32, needs kernel 4.15+).Note:There are some exceptions to the set of safe sysctls:The net.* sysctls are not allowed with host networking enabled.The net.ipv4.tcp_syncookies sysctl is not namespaced on Linux kernel version 4.5 or lower.This list will be extended in future Kubernetes versions when the kubelet supports better isolation mechanisms.Enabling Unsafe SysctlsAll safe sysctls are enabled by default.All unsafe sysctls are disabled by default and must be allowed manually by the cluster admin on a per-node basis. Pods with disabled unsafe sysctls will be scheduled, but will fail to launch.With the warning above in mind, the cluster admin can allow certain unsafe sysctls for very special situations such as high-performance or real-time application tuning. Unsafe sysctls are enabled on a node-by-node basis with a flag of the kubelet; for example:kubelet --allowed-unsafe-sysctls \ 'kernel.msg*,net.core.somaxconn' ... For Minikube, this can be done via the extra-config flag:minikube start --extra-config="kubelet.allowed-unsafe-sysctls=kernel.msg*,net.core.somaxconn"... Only namespaced sysctls can be enabled this way.Setting Sysctls for a PodA number of sysctls are namespaced in today's Linux kernels. This means that they can be set independently for each pod on a node. Only namespaced sysctls are configurable via the pod securityContext within Kubernetes.The following sysctls are known to be namespaced. This list could change in future versions of the Linux kernel.kernel.shm*,kernel.msg*,kernel.sem,fs.mqueue.*,Those net.* that can be set in container networking namespace. However, there are exceptions (e.g., net.netfilter.nf_conntrack_max and net.netfilter.nf_conntrack_expect_max can be set in container networking namespace but are unnamespaced before Linux 5.12.2).Sysctls with no namespace are called node-level sysctls. If you need to set them, you must manually configure them on each node's operating system, or by using a DaemonSet with privileged containers.Use the pod securityContext to configure namespaced sysctls. The securityContext applies to all containers in the same pod.This example uses the pod securityContext to set a safe sysctl kernel.shm_rmid_forced and two unsafe sysctls net.core.somaxconn and kernel.msgmax. There is no distinction between safe and unsafe sysctls in the specification.Warning:Only modify sysctl parameters after you understand their effects, to avoid destabilizing your operating system.apiVersion: v1 kind: Pod metadata: name: sysctl-example spec: securityContext: sysctls: \- name: kernel.shm_rmid_forced value: "0" \- name: net.core.somaxconn value: "1024" \- name: kernel.msgmax value: "65536" ... Warning:Due to their nature of being unsafe, the use of unsafe sysctls is at-your-own-risk and can lead to severe problems like wrong behavior of containers, resource shortage or complete breakage of a node.It is good practice to consider nodes with special sysctl settings as tainted within a cluster, and only schedule pods onto them which need those sysctl settings. It is suggested to use the Kubernetes taints and toleration feature to implement this.A pod with the unsafe sysctls will fail to launch on any node which has not enabled those two unsafe sysctls explicitly. As with node-level sysctls it is recommended to use taints and toleration feature or taints on nodes to schedule those pods onto the right nodes.4.2.43 - Utilizing the NUMA-aware Memory ManagerFEATURE STATE: Kubernetes v1.32 [stable] (enabled by default: true)The Kubernetes Memory Manager enables the feature of guaranteed memory (and hugepages) allocation for pods in the Guaranteed QoS class.The Memory Manager employs hint generation protocol to yield the most suitable NUMA affinity for a pod. The Memory Manager feeds the central manager (Topology Manager) with these affinity hints. Based on both the hints and Topology Manager policy, the pod is rejected or admitted to the node.Moreover, the Memory Manager ensures that the memory which a pod requests is allocated from a minimum number of NUMA nodes.The Memory Manager is only pertinent to Linux based hosts.Before you beginYou need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesYour Kubernetes server must be at or later than version v1.32.To check the version, enter kubectl version.To align memory resources with other requested resources in a Pod spec:the CPU Manager should be enabled and proper CPU Manager policy should be configured on a Node. See control CPU Management Policies;the Topology Manager should be enabled and proper Topology Manager policy should be configured on a Node. See control Topology Management Policies.Starting from v1.22, the Memory Manager is enabled by default through MemoryManager feature gate.Preceding v1.22, the kubelet must be started with the following flag:--feature-gates=MemoryManager=truein order to enable the Memory Manager feature.How does the Memory Manager Operate?The Memory Manager currently offers the guaranteed memory (and hugepages) allocation for Pods in Guaranteed QoS class. To immediately put the Memory Manager into operation follow the guidelines in the section Memory Manager configuration, and subsequently, prepare and deploy a Guaranteed pod as illustrated in the section Placing a Pod in the Guaranteed QoS class.The Memory Manager is a Hint Provider, and it provides topology hints for the Topology Manager which then aligns the requested resources according to these topology hints. On Linux, it also enforces cgroups (i.e. cpuset.mems) for pods. The complete flow diagram concerning pod admission and deployment process is illustrated in Memory Manager KEP: Design Overview and below:During this process, the Memory Manager updates its internal counters stored in Node Map and Memory Maps to manage guaranteed memory allocation.The Memory Manager updates the Node Map during the startup and runtime as follows.StartupThis occurs once a node administrator employs --reserved-memory (section Reserved memory flag). In this case, the Node Map becomes updated to reflect this reservation as illustrated in Memory Manager KEP: Memory Maps at start-up (with examples).The administrator must provide --reserved-memory flag when Static policy is configured.RuntimeReference Memory Manager KEP: Memory Maps at runtime (with examples) illustrates how a successful pod deployment affects the Node Map, and it also relates to how potential Out-of-Memory (OOM) situations are handled further by Kubernetes or operating system.Important topic in the context of Memory Manager operation is the management of NUMA groups. Each time pod's memory request is in excess of single NUMA node capacity, the Memory Manager attempts to create a group that comprises several NUMA nodes and features extend memory capacity. The problem has been solved as elaborated in Memory Manager KEP: How to enable the guaranteed memory allocation over many NUMA nodes?. Also, reference Memory Manager KEP: Simulation - how the Memory Manager works? (by examples) illustrates how the management of groups occurs.Windows SupportFEATURE STATE: Kubernetes v1.32 [alpha] (enabled by default: false)Windows support can be enabled via the WindowsCPUAndMemoryAffinity feature gate and it requires support in the container runtime. Only the BestEffort Policy is supported on Windows.Memory Manager configurationOther Managers should be first pre-configured. Next, the Memory Manager feature should be enabled and be run with Static policy (section Static policy). Optionally, some amount of memory can be reserved for system or kubelet processes to increase node stability (section Reserved memory flag).PoliciesMemory Manager supports two policies. You can select a policy via a kubelet flag --memory-manager-policy:None (default)Static (Linux only)BestEffort (Windows Only)None policyThis is the default policy and does not affect the memory allocation in any way. It acts the same as if the Memory Manager is not present at all.The None policy returns default topology hint. This special hint denotes that Hint Provider (Memory Manager in this case) has no preference for NUMA affinity with any resource.Static policyIn the case of the Guaranteed pod, the Static Memory Manager policy returns topology hints relating to the set of NUMA nodes where the memory can be guaranteed, and reserves the memory through updating the internal NodeMap object.In the case of the BestEffort or Burstable pod, the Static Memory Manager policy sends back the default topology hint as there is no request for the guaranteed memory, and does not reserve the memory in the internal NodeMap object.This policy is only supported on Linux.BestEffort policyFEATURE STATE: Kubernetes v1.32 [alpha] (enabled by default: false)This policy is only supported on Windows.On Windows, NUMA node assignment works differently than Linux. There is no mechanism to ensure that Memory access only comes from a specific NUMA node. Instead the Windows scheduler will select the most optimal NUMA node based on the CPU(s) assignments. It is possible that Windows might use other NUMA nodes if deemed optimal by the Windows scheduler.The policy does track the amount of memory available and requested through the internal NodeMap. The memory manager will make a best effort at ensuring that enough memory is available on a NUMA node before making the assignment.This means that in most cases memory assignment should function as expected.Reserved memory flagThe Node Allocatable mechanism is commonly used by node administrators to reserve K8S node system resources for the kubelet or operating system processes in order to enhance the node stability. A dedicated set of flags can be used for this purpose to set the total amount of reserved memory for a node. This pre-configured value is subsequently utilized to calculate the real amount of node's "allocatable" memory available to pods.The Kubernetes scheduler incorporates "allocatable" to optimise pod scheduling process. The foregoing flags include --kube-reserved, --system-reserved and --eviction-threshold. The sum of their values will account for the total amount of reserved memory.A new --reserved-memory flag was added to Memory Manager to allow for this total reserved memory to be split (by a node administrator) and accordingly reserved across many NUMA nodes.The flag specifies a comma-separated list of memory reservations of different memory types per NUMA node. Memory reservations across multiple NUMA nodes can be specified using semicolon as separator. This parameter is only useful in the context of the Memory Manager feature. The Memory Manager will not use this reserved memory for the allocation of container workloads.For example, if you have a NUMA node "NUMA0" with 10Gi of memory available, and the --reserved-memory was specified to reserve 1Gi of memory at "NUMA0", the Memory Manager assumes that only 9Gi is available for containers.You can omit this parameter, however, you should be aware that the quantity of reserved memory from all NUMA nodes should be equal to the quantity of memory specified by the Node Allocatable feature. If at least one node allocatable parameter is non-zero, you will need to specify \--reserved-memory for at least one NUMA node. In fact, eviction-hard threshold value is equal to 100Mi by default, so if Static policy is used, --reserved-memory is obligatory.Also, avoid the following configurations:duplicates, i.e. the same NUMA node or memory type, but with a different value;setting zero limit for any of memory types;NUMA node IDs that do not exist in the machine hardware;memory type names different than memory or hugepages- (hugepages of particular  should also exist).Syntax:--reserved-memory N:memory-type1=value1,memory-type2=value2,...N (integer) - NUMA node index, e.g. 0memory-type (string) - represents memory type:memory - conventional memoryhugepages-2Mi or hugepages-1Gi - hugepagesvalue (string) - the quantity of reserved memory, e.g. 1GiExample usage:--reserved-memory 0:memory=1Gi,hugepages-1Gi=2Gior--reserved-memory 0:memory=1Gi --reserved-memory 1:memory=2Gior--reserved-memory '0:memory=1Gi;1:memory=2Gi'When you specify values for --reserved-memory flag, you must comply with the setting that you prior provided via Node Allocatable Feature flags. That is, the following rule must be obeyed for each memory type:sum(reserved-memory(i)) = kube-reserved + system-reserved + eviction-threshold,where i is an index of a NUMA node.If you do not follow the formula above, the Memory Manager will show an error on startup.In other words, the example above illustrates that for the conventional memory (type=memory), we reserve 3Gi in total, i.e.:sum(reserved-memory(i)) = reserved-memory(0) + reserved-memory(1) = 1Gi + 2Gi = 3GiAn example of kubelet command-line arguments relevant to the node Allocatable configuration:--kube-reserved=cpu=500m,memory=50Mi--system-reserved=cpu=123m,memory=333Mi--eviction-hard=memory.available<500MiNote:The default hard eviction threshold is 100MiB, and not zero. Remember to increase the quantity of memory that you reserve by setting --reserved-memory by that hard eviction threshold. Otherwise, the kubelet will not start Memory Manager and display an error.Here is an example of a correct configuration:--kube-reserved=cpu=4,memory=4Gi \--system-reserved=cpu=1,memory=1Gi \--memory-manager-policy=Static \--reserved-memory '0:memory=3Gi;1:memory=2148Mi' Prior to Kubernetes 1.32, you also need to add--feature-gates=MemoryManager=true Let us validate the configuration above:kube-reserved + system-reserved + eviction-hard(default) = reserved-memory(0) + reserved-memory(1)4GiB + 1GiB + 100MiB = 3GiB + 2148MiB5120MiB + 100MiB = 3072MiB + 2148MiB5220MiB = 5220MiB (which is correct)Placing a Pod in the Guaranteed QoS classIf the selected policy is anything other than None, the Memory Manager identifies pods that are in the Guaranteed QoS class. The Memory Manager provides specific topology hints to the Topology Manager for each Guaranteed pod. For pods in a QoS class other than Guaranteed, the Memory Manager provides default topology hints to the Topology Manager.The following excerpts from pod manifests assign a pod to the Guaranteed QoS class.Pod with integer CPU(s) runs in the Guaranteed QoS class, when requests are equal to limits:spec: containers: \- name: nginx image: nginx resources: limits: memory: "200Mi" cpu: "2" example.com/device: "1" requests: memory: "200Mi" cpu: "2" example.com/device: "1" Also, a pod sharing CPU(s) runs in the Guaranteed QoS class, when requests are equal to limits.spec: containers: \- name: nginx image: nginx resources: limits: memory: "200Mi" cpu: "300m" example.com/device: "1" requests: memory: "200Mi" cpu: "300m" example.com/device: "1" Notice that both CPU and memory requests must be specified for a Pod to lend it to Guaranteed QoS class.TroubleshootingThe following means can be used to troubleshoot the reason why a pod could not be deployed or became rejected at a node:pod status - indicates topology affinity errorssystem logs - include valuable information for debugging, e.g., about generated hintsstate file - the dump of internal state of the Memory Manager (includes Node Map and Memory Maps)starting from v1.22, the device plugin resource API can be used to retrieve information about the memory reserved for containersPod status (TopologyAffinityError)This error typically occurs in the following situations:a node has not enough resources available to satisfy the pod's requestthe pod's request is rejected due to particular Topology Manager policy constraintsThe error appears in the status of a pod:kubectl get pods NAME READY STATUS RESTARTS AGE guaranteed 0/1 TopologyAffinityError 0 113s Use kubectl describe pod  or kubectl get events to obtain detailed error message:Warning TopologyAffinityError 10m kubelet, dell8 Resources cannot be allocated with Topology locality System logsSearch system logs with respect to a particular pod.The set of hints that Memory Manager generated for the pod can be found in the logs. Also, the set of hints generated by CPU Manager should be present in the logs.Topology Manager merges these hints to calculate a single best hint. The best hint should be also present in the logs.The best hint indicates where to allocate all the resources. Topology Manager tests this hint against its current policy, and based on the verdict, it either admits the pod to the node or rejects it.Also, search the logs for occurrences associated with the Memory Manager, e.g. to find out information about cgroups and cpuset.mems updates.Examine the memory manager state on a nodeLet us first deploy a sample Guaranteed pod whose specification is as follows:apiVersion: v1 kind: Pod metadata: name: guaranteed spec: containers: \- name: guaranteed image: consumer imagePullPolicy: Never resources: limits: cpu: "2" memory: 150Gi requests: cpu: "2" memory: 150Gi command: ["sleep","infinity"] Next, let us log into the node where it was deployed and examine the state file in /var/lib/kubelet/memory_manager_state:{ "policyName":"Static", "machineState":{ "0":{ "numberOfAssignments":1, "memoryMap":{ "hugepages-1Gi":{ "total":0, "systemReserved":0, "allocatable":0, "reserved":0, "free":0 }, "memory":{ "total":134987354112, "systemReserved":3221225472, "allocatable":131766128640, "reserved":131766128640, "free":0 } }, "nodes":[ 0, 1 ] }, "1":{ "numberOfAssignments":1, "memoryMap":{ "hugepages-1Gi":{ "total":0, "systemReserved":0, "allocatable":0, "reserved":0, "free":0 }, "memory":{ "total":135286722560, "systemReserved":2252341248, "allocatable":133034381312, "reserved":29295144960, "free":103739236352 } }, "nodes":[ 0, 1 ] } }, "entries":{ "fa9bdd38-6df9-4cf9-aa67-8c4814da37a8":{ "guaranteed":[ { "numaAffinity":[ 0, 1 ], "type":"memory", "size":161061273600 } ] } }, "checksum":4142013182 } It can be deduced from the state file that the pod was pinned to both NUMA nodes, i.e.:"numaAffinity":[ 0, 1 ], Pinned term means that pod's memory consumption is constrained (through cgroups configuration) to these NUMA nodes.This automatically implies that Memory Manager instantiated a new group that comprises these two NUMA nodes, i.e. 0 and 1 indexed NUMA nodes.Notice that the management of groups is handled in a relatively complex manner, and further elaboration is provided in Memory Manager KEP in this and this sections.In order to analyse memory resources available in a group,the corresponding entries from NUMA nodes belonging to the group must be added up.For example, the total amount of free "conventional" memory in the group can be computed by adding up the free memory available at every NUMA node in the group, i.e., in the "memory" section of NUMA node 0 ("free":0) and NUMA node 1 ("free":103739236352). So, the total amount of free "conventional" memory in this group is equal to 0 + 103739236352 bytes.The line "systemReserved":3221225472 indicates that the administrator of this node reserved 3221225472 bytes (i.e. 3Gi) to serve kubelet and system processes at NUMA node 0, by using --reserved-memory flag.Device plugin resource APIThe kubelet provides a PodResourceLister gRPC service to enable discovery of resources and associated metadata. By using its List gRPC endpoint, information about reserved memory for each container can be retrieved, which is contained in protobuf ContainerMemory message. This information can be retrieved solely for pods in Guaranteed QoS class.What's nextMemory Manager KEP: Design OverviewMemory Manager KEP: Memory Maps at start-up (with examples)Memory Manager KEP: Memory Maps at runtime (with examples)Memory Manager KEP: Simulation - how the Memory Manager works? (by examples)Memory Manager KEP: The Concept of Node Map and Memory MapsMemory Manager KEP: How to enable the guaranteed memory allocation over many NUMA nodes?4.2.44 - Verify Signed Kubernetes ArtifactsFEATURE STATE: Kubernetes v1.26 [beta]Before you beginYou will need to have the following tools installed:cosign (install guide)curl (often provided by your operating system)jq (download jq)Verifying binary signaturesThe Kubernetes release process signs all binary artifacts (tarballs, SPDX files, standalone binaries) by using cosign's keyless signing. To verify a particular binary, retrieve it together with its signature and certificate:URL=https://dl.k8s.io/release/v1.33.0/bin/linux/amd64 BINARY=kubectl FILES=( "$BINARY" "$BINARY.sig" "$BINARY.cert" ) for FILE in "${FILES[@]}"; do curl -sSfL --retry 3 --retry-delay 3 "$URL/$FILE" -o "$FILE" done Then verify the blob by using cosign verify-blob:cosign verify-blob "$BINARY" \ \--signature "$BINARY".sig \ \--certificate "$BINARY".cert \ \--certificate-identity krel-staging@k8s-releng-prod.iam.gserviceaccount.com \ \--certificate-oidc-issuer https://accounts.google.com Note:Cosign 2.0 requires the --certificate-identity and --certificate-oidc-issuer options.To learn more about keyless signing, please refer to Keyless Signatures.Previous versions of Cosign required that you set COSIGN_EXPERIMENTAL=1.For additional information, please refer to the sigstore BlogVerifying image signaturesFor a complete list of images that are signed please refer to Releases.Pick one image from this list and verify its signature using the cosign verify command:cosign verify registry.k8s.io/kube-apiserver-amd64:v1.33.0 \ \--certificate-identity krel-trust@k8s-releng-prod.iam.gserviceaccount.com \ \--certificate-oidc-issuer https://accounts.google.com \ | jq . Verifying images for all control plane componentsTo verify all signed control plane images for the latest stable version (v1.33.0), please run the following commands:curl -Ls "https://sbom.k8s.io/$(curl -Ls https://dl.k8s.io/release/stable.txt)/release" \ | grep "SPDXID: SPDXRef-Package-registry.k8s.io" \ | grep -v sha256 | cut -d- -f3- | sed 's/-/\//' | sed 's/-v1/:v1/' \ | sort > images.txt input=images.txt while IFS= read -r image do cosign verify "$image" \ \--certificate-identity krel-trust@k8s-releng-prod.iam.gserviceaccount.com \ \--certificate-oidc-issuer https://accounts.google.com \ | jq . done < "$input" Once you have verified an image, you can specify the image by its digest in your Pod manifests as per this example:registry-url/image-name@sha256:45b23dee08af5e43a7fea6c4cf9c25ccf269ee113168c19722f87876677c5cb2 For more information, please refer to the Image Pull Policy section.Verifying Image Signatures with Admission ControllerFor non-control plane images (for example conformance image), signatures can also be verified at deploy time using sigstore policy-controller admission controller.Here are some helpful resources to get started with policy-controller:InstallationConfiguration OptionsVerify the Software Bill Of MaterialsYou can verify the Kubernetes Software Bill of Materials (SBOM) by using the sigstore certificate and signature, or the corresponding SHA files:# Retrieve the latest available Kubernetes release version VERSION=$(curl -Ls https://dl.k8s.io/release/stable.txt) # Verify the SHA512 sum curl -Ls "https://sbom.k8s.io/$VERSION/release" -o "$VERSION.spdx" echo "$(curl -Ls "https://sbom.k8s.io/$VERSION/release.sha512") $VERSION.spdx" | sha512sum --check # Verify the SHA256 sum echo "$(curl -Ls "https://sbom.k8s.io/$VERSION/release.sha256") $VERSION.spdx" | sha256sum --check # Retrieve sigstore signature and certificate curl -Ls "https://sbom.k8s.io/$VERSION/release.sig" -o "$VERSION.spdx.sig" curl -Ls "https://sbom.k8s.io/$VERSION/release.cert" -o "$VERSION.spdx.cert" # Verify the sigstore signature cosign verify-blob \ \--certificate "$VERSION.spdx.cert" \ \--signature "$VERSION.spdx.sig" \ \--certificate-identity krel-staging@k8s-releng-prod.iam.gserviceaccount.com \ \--certificate-oidc-issuer https://accounts.google.com \ "$VERSION.spdx" 4.3 - Configure Pods and ContainersPerform common configuration tasks for Pods and containers.4.3.1 - Assign Memory Resources to Containers and PodsThis page shows how to assign a memory request and a memory limit to a Container. A Container is guaranteed to have as much memory as it requests, but is not allowed to use more memory than its limit.Before you beginYou need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesTo check the version, enter kubectl version.Each node in your cluster must have at least 300 MiB of memory.A few of the steps on this page require you to run the metrics-server service in your cluster. If you have the metrics-server running, you can skip those steps.If you are running Minikube, run the following command to enable the metrics-server:minikube addons enable metrics-server To see whether the metrics-server is running, or another provider of the resource metrics API (metrics.k8s.io), run the following command:kubectl get apiservices If the resource metrics API is available, the output includes a reference to metrics.k8s.io.NAME v1beta1.metrics.k8s.io Create a namespaceCreate a namespace so that the resources you create in this exercise are isolated from the rest of your cluster.kubectl create namespace mem-example Specify a memory request and a memory limitTo specify a memory request for a Container, include the resources:requests field in the Container's resource manifest. To specify a memory limit, include resources:limits.In this exercise, you create a Pod that has one Container. The Container has a memory request of 100 MiB and a memory limit of 200 MiB. Here's the configuration file for the Pod:pods/resource/memory-request-limit.yaml apiVersion: v1 kind: Pod metadata: name: memory-demo namespace: mem-example spec: containers: \- name: memory-demo-ctr image: polinux/stress resources: requests: memory: "100Mi" limits: memory: "200Mi" command: ["stress"] args: ["--vm", "1", "--vm-bytes", "150M", "--vm-hang", "1"] The args section in the configuration file provides arguments for the Container when it starts. The "--vm-bytes", "150M" arguments tell the Container to attempt to allocate 150 MiB of memory.Create the Pod:kubectl apply -f https://k8s.io/examples/pods/resource/memory-request-limit.yaml --namespace=mem-example Verify that the Pod Container is running:kubectl get pod memory-demo --namespace=mem-example View detailed information about the Pod:kubectl get pod memory-demo --output=yaml --namespace=mem-example The output shows that the one Container in the Pod has a memory request of 100 MiB and a memory limit of 200 MiB.... resources: requests: memory: 100Mi limits: memory: 200Mi ... Run kubectl top to fetch the metrics for the pod:kubectl top pod memory-demo --namespace=mem-example The output shows that the Pod is using about 162,900,000 bytes of memory, which is about 150 MiB. This is greater than the Pod's 100 MiB request, but within the Pod's 200 MiB limit.NAME CPU(cores) MEMORY(bytes) memory-demo  162856960 Delete your Pod:kubectl delete pod memory-demo --namespace=mem-example Exceed a Container's memory limitA Container can exceed its memory request if the Node has memory available. But a Container is not allowed to use more than its memory limit. If a Container allocates more memory than its limit, the Container becomes a candidate for termination. If the Container continues to consume memory beyond its limit, the Container is terminated. If a terminated Container can be restarted, the kubelet restarts it, as with any other type of runtime failure.In this exercise, you create a Pod that attempts to allocate more memory than its limit. Here is the configuration file for a Pod that has one Container with a memory request of 50 MiB and a memory limit of 100 MiB:pods/resource/memory-request-limit-2.yaml apiVersion: v1 kind: Pod metadata: name: memory-demo-2 namespace: mem-example spec: containers: \- name: memory-demo-2-ctr image: polinux/stress resources: requests: memory: "50Mi" limits: memory: "100Mi" command: ["stress"] args: ["--vm", "1", "--vm-bytes", "250M", "--vm-hang", "1"] In the args section of the configuration file, you can see that the Container will attempt to allocate 250 MiB of memory, which is well above the 100 MiB limit.Create the Pod:kubectl apply -f https://k8s.io/examples/pods/resource/memory-request-limit-2.yaml --namespace=mem-example View detailed information about the Pod:kubectl get pod memory-demo-2 --namespace=mem-example At this point, the Container might be running or killed. Repeat the preceding command until the Container is killed:NAME READY STATUS RESTARTS AGE memory-demo-2 0/1 OOMKilled 1 24s Get a more detailed view of the Container status:kubectl get pod memory-demo-2 --output=yaml --namespace=mem-example The output shows that the Container was killed because it is out of memory (OOM):lastState: terminated: containerID: 65183c1877aaec2e8427bc95609cc52677a454b56fcb24340dbd22917c23b10f exitCode: 137 finishedAt: 2017-06-20T20:52:19Z reason: OOMKilled startedAt: null The Container in this exercise can be restarted, so the kubelet restarts it. Repeat this command several times to see that the Container is repeatedly killed and restarted:kubectl get pod memory-demo-2 --namespace=mem-example The output shows that the Container is killed, restarted, killed again, restarted again, and so on:kubectl get pod memory-demo-2 --namespace=mem-example NAME READY STATUS RESTARTS AGE memory-demo-2 0/1 OOMKilled 1 37s kubectl get pod memory-demo-2 --namespace=mem-example NAME READY STATUS RESTARTS AGE memory-demo-2 1/1 Running 2 40s View detailed information about the Pod history:kubectl describe pod memory-demo-2 --namespace=mem-example The output shows that the Container starts and fails repeatedly:... Normal Created Created container with id 66a3a20aa7980e61be4922780bf9d24d1a1d8b7395c09861225b0eba1b1f8511 ... Warning BackOff Back-off restarting failed container View detailed information about your cluster's Nodes:kubectl describe nodes The output includes a record of the Container being killed because of an out-of-memory condition:Warning OOMKilling Memory cgroup out of memory: Kill process 4481 (stress) score 1994 or sacrifice child Delete your Pod:kubectl delete pod memory-demo-2 --namespace=mem-example Specify a memory request that is too big for your NodesMemory requests and limits are associated with Containers, but it is useful to think of a Pod as having a memory request and limit. The memory request for the Pod is the sum of the memory requests for all the Containers in the Pod. Likewise, the memory limit for the Pod is the sum of the limits of all the Containers in the Pod.Pod scheduling is based on requests. A Pod is scheduled to run on a Node only if the Node has enough available memory to satisfy the Pod's memory request.In this exercise, you create a Pod that has a memory request so big that it exceeds the capacity of any Node in your cluster. Here is the configuration file for a Pod that has one Container with a request for 1000 GiB of memory, which likely exceeds the capacity of any Node in your cluster.pods/resource/memory-request-limit-3.yaml apiVersion: v1 kind: Pod metadata: name: memory-demo-3 namespace: mem-example spec: containers: \- name: memory-demo-3-ctr image: polinux/stress resources: requests: memory: "1000Gi" limits: memory: "1000Gi" command: ["stress"] args: ["--vm", "1", "--vm-bytes", "150M", "--vm-hang", "1"] Create the Pod:kubectl apply -f https://k8s.io/examples/pods/resource/memory-request-limit-3.yaml --namespace=mem-example View the Pod status:kubectl get pod memory-demo-3 --namespace=mem-example The output shows that the Pod status is PENDING. That is, the Pod is not scheduled to run on any Node, and it will remain in the PENDING state indefinitely:kubectl get pod memory-demo-3 --namespace=mem-example NAME READY STATUS RESTARTS AGE memory-demo-3 0/1 Pending 0 25s View detailed information about the Pod, including events:kubectl describe pod memory-demo-3 --namespace=mem-example The output shows that the Container cannot be scheduled because of insufficient memory on the Nodes:Events: ... Reason Message \------ ------- ... FailedScheduling No nodes are available that match all of the following predicates:: Insufficient memory (3). Memory unitsThe memory resource is measured in bytes. You can express memory as a plain integer or a fixed-point integer with one of these suffixes: E, P, T, G, M, K, Ei, Pi, Ti, Gi, Mi, Ki. For example, the following represent approximately the same value:128974848, 129e6, 129M, 123Mi Delete your Pod:kubectl delete pod memory-demo-3 --namespace=mem-example If you do not specify a memory limitIf you do not specify a memory limit for a Container, one of the following situations applies:The Container has no upper bound on the amount of memory it uses. The Container could use all of the memory available on the Node where it is running which in turn could invoke the OOM Killer. Further, in case of an OOM Kill, a container with no resource limits will have a greater chance of being killed.The Container is running in a namespace that has a default memory limit, and the Container is automatically assigned the default limit. Cluster administrators can use a LimitRange to specify a default value for the memory limit.Motivation for memory requests and limitsBy configuring memory requests and limits for the Containers that run in your cluster, you can make efficient use of the memory resources available on your cluster's Nodes. By keeping a Pod's memory request low, you give the Pod a good chance of being scheduled. By having a memory limit that is greater than the memory request, you accomplish two things:The Pod can have bursts of activity where it makes use of memory that happens to be available.The amount of memory a Pod can use during a burst is limited to some reasonable amount.Clean upDelete your namespace. This deletes all the Pods that you created for this task:kubectl delete namespace mem-example What's nextFor app developersAssign CPU Resources to Containers and PodsAssign Pod-level CPU and memory resourcesConfigure Quality of Service for PodsResize CPU and Memory Resources assigned to ContainersFor cluster administratorsConfigure Default Memory Requests and Limits for a NamespaceConfigure Default CPU Requests and Limits for a NamespaceConfigure Minimum and Maximum Memory Constraints for a NamespaceConfigure Minimum and Maximum CPU Constraints for a NamespaceConfigure Memory and CPU Quotas for a NamespaceConfigure a Pod Quota for a NamespaceConfigure Quotas for API ObjectsResize CPU and Memory Resources assigned to Containers4.3.2 - Assign CPU Resources to Containers and PodsThis page shows how to assign a CPU request and a CPU limit to a container. Containers cannot use more CPU than the configured limit. Provided the system has CPU time free, a container is guaranteed to be allocated as much CPU as it requests.Before you beginYou need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesTo check the version, enter kubectl version.Your cluster must have at least 1 CPU available for use to run the task examples.A few of the steps on this page require you to run the metrics-server service in your cluster. If you have the metrics-server running, you can skip those steps.If you are running Minikube, run the following command to enable metrics-server:minikube addons enable metrics-server To see whether metrics-server (or another provider of the resource metrics API, metrics.k8s.io) is running, type the following command:kubectl get apiservices If the resource metrics API is available, the output will include a reference to metrics.k8s.io.NAME v1beta1.metrics.k8s.io Create a namespaceCreate a Namespace so that the resources you create in this exercise are isolated from the rest of your cluster.kubectl create namespace cpu-example Specify a CPU request and a CPU limitTo specify a CPU request for a container, include the resources:requests field in the Container resource manifest. To specify a CPU limit, include resources:limits.In this exercise, you create a Pod that has one container. The container has a request of 0.5 CPU and a limit of 1 CPU. Here is the configuration file for the Pod:pods/resource/cpu-request-limit.yaml apiVersion: v1 kind: Pod metadata: name: cpu-demo namespace: cpu-example spec: containers: \- name: cpu-demo-ctr image: vish/stress resources: limits: cpu: "1" requests: cpu: "0.5" args: \- -cpus \- "2" The args section of the configuration file provides arguments for the container when it starts. The -cpus "2" argument tells the Container to attempt to use 2 CPUs.Create the Pod:kubectl apply -f https://k8s.io/examples/pods/resource/cpu-request-limit.yaml --namespace=cpu-example Verify that the Pod is running:kubectl get pod cpu-demo --namespace=cpu-example View detailed information about the Pod:kubectl get pod cpu-demo --output=yaml --namespace=cpu-example The output shows that the one container in the Pod has a CPU request of 500 milliCPU and a CPU limit of 1 CPU.resources: limits: cpu: "1" requests: cpu: 500m Use kubectl top to fetch the metrics for the Pod:kubectl top pod cpu-demo --namespace=cpu-example This example output shows that the Pod is using 974 milliCPU, which is slightly less than the limit of 1 CPU specified in the Pod configuration.NAME CPU(cores) MEMORY(bytes) cpu-demo 974m  Recall that by setting -cpu "2", you configured the Container to attempt to use 2 CPUs, but the Container is only being allowed to use about 1 CPU. The container's CPU use is being throttled, because the container is attempting to use more CPU resources than its limit.Note:Another possible explanation for the CPU use being below 1.0 is that the Node might not have enough CPU resources available. Recall that the prerequisites for this exercise require your cluster to have at least 1 CPU available for use. If your Container runs on a Node that has only 1 CPU, the Container cannot use more than 1 CPU regardless of the CPU limit specified for the Container.CPU unitsThe CPU resource is measured in CPU units. One CPU, in Kubernetes, is equivalent to:1 AWS vCPU1 GCP Core1 Azure vCore1 Hyperthread on a bare-metal Intel processor with HyperthreadingFractional values are allowed. A Container that requests 0.5 CPU is guaranteed half as much CPU as a Container that requests 1 CPU. You can use the suffix m to mean milli. For example 100m CPU, 100 milliCPU, and 0.1 CPU are all the same. Precision finer than 1m is not allowed.CPU is always requested as an absolute quantity, never as a relative quantity; 0.1 is the same amount of CPU on a single-core, dual-core, or 48-core machine.Delete your Pod:kubectl delete pod cpu-demo --namespace=cpu-example Specify a CPU request that is too big for your NodesCPU requests and limits are associated with Containers, but it is useful to think of a Pod as having a CPU request and limit. The CPU request for a Pod is the sum of the CPU requests for all the Containers in the Pod. Likewise, the CPU limit for a Pod is the sum of the CPU limits for all the Containers in the Pod.Pod scheduling is based on requests. A Pod is scheduled to run on a Node only if the Node has enough CPU resources available to satisfy the Pod CPU request.In this exercise, you create a Pod that has a CPU request so big that it exceeds the capacity of any Node in your cluster. Here is the configuration file for a Pod that has one Container. The Container requests 100 CPU, which is likely to exceed the capacity of any Node in your cluster.pods/resource/cpu-request-limit-2.yaml apiVersion: v1 kind: Pod metadata: name: cpu-demo-2 namespace: cpu-example spec: containers: \- name: cpu-demo-ctr-2 image: vish/stress resources: limits: cpu: "100" requests: cpu: "100" args: \- -cpus \- "2" Create the Pod:kubectl apply -f https://k8s.io/examples/pods/resource/cpu-request-limit-2.yaml --namespace=cpu-example View the Pod status:kubectl get pod cpu-demo-2 --namespace=cpu-example The output shows that the Pod status is Pending. That is, the Pod has not been scheduled to run on any Node, and it will remain in the Pending state indefinitely:NAME READY STATUS RESTARTS AGE cpu-demo-2 0/1 Pending 0 7m View detailed information about the Pod, including events:kubectl describe pod cpu-demo-2 --namespace=cpu-example The output shows that the Container cannot be scheduled because of insufficient CPU resources on the Nodes:Events: Reason Message \------ ------- FailedScheduling No nodes are available that match all of the following predicates:: Insufficient cpu (3). Delete your Pod:kubectl delete pod cpu-demo-2 --namespace=cpu-example If you do not specify a CPU limitIf you do not specify a CPU limit for a Container, then one of these situations applies:The Container has no upper bound on the CPU resources it can use. The Container could use all of the CPU resources available on the Node where it is running.The Container is running in a namespace that has a default CPU limit, and the Container is automatically assigned the default limit. Cluster administrators can use a LimitRange to specify a default value for the CPU limit.If you specify a CPU limit but do not specify a CPU requestIf you specify a CPU limit for a Container but do not specify a CPU request, Kubernetes automatically assigns a CPU request that matches the limit. Similarly, if a Container specifies its own memory limit, but does not specify a memory request, Kubernetes automatically assigns a memory request that matches the limit.Motivation for CPU requests and limitsBy configuring the CPU requests and limits of the Containers that run in your cluster, you can make efficient use of the CPU resources available on your cluster Nodes. By keeping a Pod CPU request low, you give the Pod a good chance of being scheduled. By having a CPU limit that is greater than the CPU request, you accomplish two things:The Pod can have bursts of activity where it makes use of CPU resources that happen to be available.The amount of CPU resources a Pod can use during a burst is limited to some reasonable amount.Clean upDelete your namespace:kubectl delete namespace cpu-example What's nextFor app developersAssign Memory Resources to Containers and PodsAssign Pod-level CPU and memory resourcesConfigure Quality of Service for PodsResize CPU and Memory Resources assigned to ContainersFor cluster administratorsConfigure Default Memory Requests and Limits for a NamespaceConfigure Default CPU Requests and Limits for a NamespaceConfigure Minimum and Maximum Memory Constraints for a NamespaceConfigure Minimum and Maximum CPU Constraints for a NamespaceConfigure Memory and CPU Quotas for a NamespaceConfigure a Pod Quota for a NamespaceConfigure Quotas for API ObjectsResize CPU and Memory Resources assigned to Containers4.3.3 - Assign Pod-level CPU and memory resourcesFEATURE STATE: Kubernetes v1.32 [alpha] (enabled by default: false)This page shows how to specify CPU and memory resources for a Pod at pod-level in addition to container-level resource specifications. A Kubernetes node allocates resources to a pod based on the pod's resource requests. These requests can be defined at the pod level or individually for containers within the pod. When both are present, the pod-level requests take precedence.Similarly, a pod's resource usage is restricted by limits, which can also be set at the pod-level or individually for containers within the pod. Again, pod-level limits are prioritized when both are present. This allows for flexible resource management, enabling you to control resource allocation at both the pod and container levels.In order to specify the resources at pod-level, it is required to enable PodLevelResources feature gate.For Pod Level Resources:Priority: When both pod-level and container-level resources are specified, pod-level resources take precedence.QoS: Pod-level resources take precedence in influencing the QoS class of the pod.OOM Score: The OOM score adjustment calculation considers both pod-level and container-level resources.Compatibility: Pod-level resources are designed to be compatible with existing features.Before you beginYou need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesYour Kubernetes server must be at or later than version 1.32.To check the version, enter kubectl version.The PodLevelResources feature gate must be enabled for your control plane and for all nodes in your cluster.Create a namespaceCreate a namespace so that the resources you create in this exercise are isolated from the rest of your cluster.kubectl create namespace pod-resources-example Create a pod with memory requests and limits at pod-levelTo specify memory requests for a Pod at pod-level, include the resources.requests.memory field in the Pod spec manifest. To specify a memory limit, include resources.limits.memory.In this exercise, you create a Pod that has one Container. The Pod has a memory request of 100 MiB and a memory limit of 200 MiB. Here's the configuration file for the Pod:pods/resource/pod-level-memory-request-limit.yaml apiVersion: v1 kind: Pod metadata: name: memory-demo namespace: pod-resources-example spec: resources: requests: memory: "100Mi" limits: memory: "200Mi" containers: \- name: memory-demo-ctr image: nginx command: ["stress"] args: ["--vm", "1", "--vm-bytes", "150M", "--vm-hang", "1"] The args section in the manifest provides arguments for the container when it starts. The "--vm-bytes", "150M" arguments tell the Container to attempt to allocate 150 MiB of memory.Create the Pod:kubectl apply -f https://k8s.io/examples/pods/resource/pod-level-memory-request-limit.yaml --namespace=pod-resources-example Verify that the Pod is running:kubectl get pod memory-demo --namespace=pod-resources-example View detailed information about the Pod:kubectl get pod memory-demo --output=yaml --namespace=pod-resources-example The output shows that the Pod has a memory request of 100 MiB and a memory limit of 200 MiB.... spec: containers: ... resources: requests: memory: 100Mi limits: memory: 200Mi ... Run kubectl top to fetch the metrics for the pod:kubectl top pod memory-demo --namespace=pod-resources-example The output shows that the Pod is using about 162,900,000 bytes of memory, which is about 150 MiB. This is greater than the Pod's 100 MiB request, but within the Pod's 200 MiB limit.NAME CPU(cores) MEMORY(bytes) memory-demo  162856960 Create a pod with CPU requests and limits at pod-levelTo specify a CPU request for a Pod, include the resources.requests.cpu field in the Pod spec manifest. To specify a CPU limit, include resources.limits.cpu.In this exercise, you create a Pod that has one container. The Pod has a request of 0.5 CPU and a limit of 1 CPU. Here is the configuration file for the Pod:pods/resource/pod-level-cpu-request-limit.yaml apiVersion: v1 kind: Pod metadata: name: cpu-demo namespace: pod-resources-example spec: resources: limits: cpu: "1" requests: cpu: "0.5" containers: \- name: cpu-demo-ctr image: vish/stress args: \- -cpus \- "2" The args section of the configuration file provides arguments for the container when it starts. The -cpus "2" argument tells the Container to attempt to use 2 CPUs.Create the Pod:kubectl apply -f https://k8s.io/examples/pods/resource/pod-level-cpu-request-limit.yaml --namespace=pod-resources-example Verify that the Pod is running:kubectl get pod cpu-demo --namespace=pod-resources-example View detailed information about the Pod:kubectl get pod cpu-demo --output=yaml --namespace=pod-resources-example The output shows that the Pod has a CPU request of 500 milliCPU and a CPU limit of 1 CPU.spec: containers: ... resources: limits: cpu: "1" requests: cpu: 500m Use kubectl top to fetch the metrics for the Pod:kubectl top pod cpu-demo --namespace=pod-resources-example This example output shows that the Pod is using 974 milliCPU, which is slightly less than the limit of 1 CPU specified in the Pod configuration.NAME CPU(cores) MEMORY(bytes) cpu-demo 974m  Recall that by setting -cpu "2", you configured the Container to attempt to use 2 CPUs, but the Container is only being allowed to use about 1 CPU. The container's CPU use is being throttled, because the container is attempting to use more CPU resources than the Pod CPU limit.Create a pod with resource requests and limits at both pod-level and container-levelTo assign CPU and memory resources to a Pod, you can specify them at both the pod level and the container level. Include the resources field in the Pod spec to define resources for the entire Pod. Additionally, include the resources field within container's specification in the Pod's manifest to set container-specific resource requirements.In this exercise, you'll create a Pod with two containers to explore the interaction of pod-level and container-level resource specifications. The Pod itself will have defined CPU requests and limits, while only one of the containers will have its own explicit resource requests and limits. The other container will inherit the resource constraints from the pod-level settings. Here's the configuration file for the Pod:pods/resource/pod-level-resources.yaml apiVersion: v1 kind: Pod metadata: name: pod-resources-demo namespace: pod-resources-example spec: resources: limits: cpu: "1" memory: "200Mi" requests: cpu: "1" memory: "100Mi" containers: \- name: pod-resources-demo-ctr-1 image: nginx resources: limits: cpu: "0.5" memory: "100Mi" requests: cpu: "0.5" memory: "50Mi" \- name: pod-resources-demo-ctr-2 image: fedora command: \- sleep \- inf Create the Pod:kubectl apply -f https://k8s.io/examples/pods/resource/pod-level-resources.yaml --namespace=pod-resources-example Verify that the Pod Container is running:kubectl get pod-resources-demo --namespace=pod-resources-example View detailed information about the Pod:kubectl get pod memory-demo --output=yaml --namespace=pod-resources-example The output shows that one container in the Pod has a memory request of 50 MiB and a CPU request of 0.5 cores, with a memory limit of 100 MiB and a CPU limit of 0.5 cores. The Pod itself has a memory request of 100 MiB and a CPU request of 1 core, and a memory limit of 200 MiB and a CPU limit of 1 core.... containers: name: pod-resources-demo-ctr-1 resources: requests: cpu: 500m memory: 50Mi limits: cpu: 500m memory: 100Mi ... name: pod-resources-demo-ctr-2 resources: {} resources: limits: cpu: 1 memory: 200Mi requests: cpu: 1 memory: 100Mi ... Since pod-level requests and limits are specified, the request guarantees for both containers in the pod will be equal 1 core or CPU and 100Mi of memory. Additionally, both containers together won't be able to use more resources than specified in the pod-level limits, ensuring they cannot exceed a combined total of 200 MiB of memory and 1 core of CPU.Clean upDelete your namespace:kubectl delete namespace pod-resources-example What's nextFor application developersAssign Memory Resources to Containers and PodsAssign CPU Resources to Containers and PodsFor cluster administratorsConfigure Default Memory Requests and Limits for a NamespaceConfigure Default CPU Requests and Limits for a NamespaceConfigure Minimum and Maximum Memory Constraints for a NamespaceConfigure Minimum and Maximum CPU Constraints for a NamespaceConfigure Memory and CPU Quotas for a Namespace4.3.4 - Configure GMSA for Windows Pods and containersFEATURE STATE: Kubernetes v1.18 [stable]This page shows how to configure Group Managed Service Accounts (GMSA) for Pods and containers that will run on Windows nodes. Group Managed Service Accounts are a specific type of Active Directory account that provides automatic password management, simplified service principal name (SPN) management, and the ability to delegate the management to other administrators across multiple servers.In Kubernetes, GMSA credential specs are configured at a Kubernetes cluster-wide scope as Custom Resources. Windows Pods, as well as individual containers within a Pod, can be configured to use a GMSA for domain based functions (e.g. Kerberos authentication) when interacting with other Windows services.Before you beginYou need to have a Kubernetes cluster and the kubectl command-line tool must be configured to communicate with your cluster. The cluster is expected to have Windows worker nodes. This section covers a set of initial steps required once for each cluster:Install the GMSACredentialSpec CRDA CustomResourceDefinition(CRD) for GMSA credential spec resources needs to be configured on the cluster to define the custom resource type GMSACredentialSpec. Download the GMSA CRD YAML and save it as gmsa-crd.yaml. Next, install the CRD with kubectl apply -f gmsa-crd.yamlInstall webhooks to validate GMSA usersTwo webhooks need to be configured on the Kubernetes cluster to populate and validate GMSA credential spec references at the Pod or container level:A mutating webhook that expands references to GMSAs (by name from a Pod specification) into the full credential spec in JSON form within the Pod spec.A validating webhook ensures all references to GMSAs are authorized to be used by the Pod service account.Installing the above webhooks and associated objects require the steps below:Create a certificate key pair (that will be used to allow the webhook container to communicate to the cluster)Install a secret with the certificate from above.Create a deployment for the core webhook logic.Create the validating and mutating webhook configurations referring to the deployment.A script can be used to deploy and configure the GMSA webhooks and associated objects mentioned above. The script can be run with a --dry-run=server option to allow you to review the changes that would be made to your cluster.The YAML template used by the script may also be used to deploy the webhooks and associated objects manually (with appropriate substitutions for the parameters)Configure GMSAs and Windows nodes in Active DirectoryBefore Pods in Kubernetes can be configured to use GMSAs, the desired GMSAs need to be provisioned in Active Directory as described in the Windows GMSA documentation. Windows worker nodes (that are part of the Kubernetes cluster) need to be configured in Active Directory to access the secret credentials associated with the desired GMSA as described in the Windows GMSA documentation.Create GMSA credential spec resourcesWith the GMSACredentialSpec CRD installed (as described earlier), custom resources containing GMSA credential specs can be configured. The GMSA credential spec does not contain secret or sensitive data. It is information that a container runtime can use to describe the desired GMSA of a container to Windows. GMSA credential specs can be generated in YAML format with a utility PowerShell script.Following are the steps for generating a GMSA credential spec YAML manually in JSON format and then converting it:Import the CredentialSpec module: ipmo CredentialSpec.psm1Create a credential spec in JSON format using New-CredentialSpec. To create a GMSA credential spec named WebApp1, invoke New-CredentialSpec -Name WebApp1 -AccountName WebApp1 -Domain $(Get-ADDomain -Current LocalComputer)Use Get-CredentialSpec to show the path of the JSON file.Convert the credspec file from JSON to YAML format and apply the necessary header fields apiVersion, kind, metadata and credspec to make it a GMSACredentialSpec custom resource that can be configured in Kubernetes.The following YAML configuration describes a GMSA credential spec named gmsa-WebApp1:apiVersion: windows.k8s.io/v1 kind: GMSACredentialSpec metadata: name: gmsa-WebApp1 # This is an arbitrary name but it will be used as a reference credspec: ActiveDirectoryConfig: GroupManagedServiceAccounts: \- Name: WebApp1 # Username of the GMSA account Scope: CONTOSO # NETBIOS Domain Name \- Name: WebApp1 # Username of the GMSA account Scope: contoso.com # DNS Domain Name CmsPlugins: \- ActiveDirectory DomainJoinConfig: DnsName: contoso.com # DNS Domain Name DnsTreeName: contoso.com # DNS Domain Name Root Guid: 244818ae-87ac-4fcd-92ec-e79e5252348a # GUID of the Domain MachineAccountName: WebApp1 # Username of the GMSA account NetBiosName: CONTOSO # NETBIOS Domain Name Sid: S-1-5-21-2126449477-2524075714-3094792973 # SID of the Domain The above credential spec resource may be saved as gmsa-Webapp1-credspec.yaml and applied to the cluster using: kubectl apply -f gmsa-Webapp1-credspec.ymlConfigure cluster role to enable RBAC on specific GMSA credential specsA cluster role needs to be defined for each GMSA credential spec resource. This authorizes the use verb on a specific GMSA resource by a subject which is typically a service account. The following example shows a cluster role that authorizes usage of the gmsa-WebApp1 credential spec from above. Save the file as gmsa-webapp1-role.yaml and apply using kubectl apply -f gmsa-webapp1-role.yaml# Create the Role to read the credspec apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: webapp1-role rules: \- apiGroups: ["windows.k8s.io"] resources: ["gmsacredentialspecs"] verbs: ["use"] resourceNames: ["gmsa-WebApp1"] Assign role to service accounts to use specific GMSA credspecsA service account (that Pods will be configured with) needs to be bound to the cluster role create above. This authorizes the service account to use the desired GMSA credential spec resource. The following shows the default service account being bound to a cluster role webapp1-role to use gmsa-WebApp1 credential spec resource created above.apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: allow-default-svc-account-read-on-gmsa-WebApp1 namespace: default subjects: \- kind: ServiceAccount name: default namespace: default roleRef: kind: ClusterRole name: webapp1-role apiGroup: rbac.authorization.k8s.io Configure GMSA credential spec reference in Pod specThe Pod spec field securityContext.windowsOptions.gmsaCredentialSpecName is used to specify references to desired GMSA credential spec custom resources in Pod specs. This configures all containers in the Pod spec to use the specified GMSA. A sample Pod spec with the annotation populated to refer to gmsa-WebApp1:apiVersion: apps/v1 kind: Deployment metadata: labels: run: with-creds name: with-creds namespace: default spec: replicas: 1 selector: matchLabels: run: with-creds template: metadata: labels: run: with-creds spec: securityContext: windowsOptions: gmsaCredentialSpecName: gmsa-webapp1 containers: \- image: mcr.microsoft.com/windows/servercore/iis:windowsservercore-ltsc2019 imagePullPolicy: Always name: iis nodeSelector: kubernetes.io/os: windows Individual containers in a Pod spec can also specify the desired GMSA credspec using a per-container securityContext.windowsOptions.gmsaCredentialSpecName field. For example:apiVersion: apps/v1 kind: Deployment metadata: labels: run: with-creds name: with-creds namespace: default spec: replicas: 1 selector: matchLabels: run: with-creds template: metadata: labels: run: with-creds spec: containers: \- image: mcr.microsoft.com/windows/servercore/iis:windowsservercore-ltsc2019 imagePullPolicy: Always name: iis securityContext: windowsOptions: gmsaCredentialSpecName: gmsa-Webapp1 nodeSelector: kubernetes.io/os: windows As Pod specs with GMSA fields populated (as described above) are applied in a cluster, the following sequence of events take place:The mutating webhook resolves and expands all references to GMSA credential spec resources to the contents of the GMSA credential spec.The validating webhook ensures the service account associated with the Pod is authorized for the use verb on the specified GMSA credential spec.The container runtime configures each Windows container with the specified GMSA credential spec so that the container can assume the identity of the GMSA in Active Directory and access services in the domain using that identity.Authenticating to network shares using hostname or FQDNIf you are experiencing issues connecting to SMB shares from Pods using hostname or FQDN, but are able to access the shares via their IPv4 address then make sure the following registry key is set on the Windows nodes.reg add "HKLM\SYSTEM\CurrentControlSet\Services\hns\State" /v EnableCompartmentNamespace /t REG_DWORD /d 1 Running Pods will then need to be recreated to pick up the behavior changes. More information on how this registry key is used can be found hereTroubleshootingIf you are having difficulties getting GMSA to work in your environment, there are a few troubleshooting steps you can take.First, make sure the credspec has been passed to the Pod. To do this you will need to exec into one of your Pods and check the output of the nltest.exe /parentdomain command.In the example below the Pod did not get the credspec correctly:kubectl exec -it iis-auth-7776966999-n5nzr powershell.exe nltest.exe /parentdomain results in the following error:Getting parent domain failed: Status = 1722 0x6ba RPC_S_SERVER_UNAVAILABLE If your Pod did get the credspec correctly, then next check communication with the domain. First, from inside of your Pod, quickly do an nslookup to find the root of your domain.This will tell us 3 things:The Pod can reach the DCThe DC can reach the PodDNS is working correctly.If the DNS and communication test passes, next you will need to check if the Pod has established secure channel communication with the domain. To do this, again, exec into your Pod and run the nltest.exe /query command.nltest.exe /query Results in the following output:I_NetLogonControl failed: Status = 1722 0x6ba RPC_S_SERVER_UNAVAILABLE This tells us that for some reason, the Pod was unable to logon to the domain using the account specified in the credspec. You can try to repair the secure channel by running the following:nltest /sc_reset:domain.example If the command is successful you will see and output similar to this:Flags: 30 HAS_IP HAS_TIMESERV Trusted DC Name \\\dc10.domain.example Trusted DC Connection Status Status = 0 0x0 NERR_Success The command completed successfully If the above corrects the error, you can automate the step by adding the following lifecycle hook to your Pod spec. If it did not correct the error, you will need to examine your credspec again and confirm that it is correct and complete. image: registry.domain.example/iis-auth:1809v1 lifecycle: postStart: exec: command: ["powershell.exe","-command","do { Restart-Service -Name netlogon } while ( $($Result = (nltest.exe /query); if ($Result -like '*0x0 NERR_Success*') {return $true} else {return $false}) -eq $false)"] imagePullPolicy: IfNotPresent If you add the lifecycle section show above to your Pod spec, the Pod will execute the commands listed to restart the netlogon service until the nltest.exe /query command exits without error.4.3.5 - Resize CPU and Memory Resources assigned to ContainersFEATURE STATE: Kubernetes v1.33 [beta] (enabled by default: true)This page explains how to change the CPU and memory resource requests and limits assigned to a container without recreating the Pod.Traditionally, changing a Pod's resource requirements necessitated deleting the existing Pod and creating a replacement, often managed by a workload controller. In-place Pod Resize allows changing the CPU/memory allocation of container(s) within a running Pod while potentially avoiding application disruption.Key Concepts:Desired Resources: A container's spec.containers[*].resources represent the desired resources for the container, and are mutable for CPU and memory.Actual Resources: The status.containerStatuses[*].resources field reflects the resources currently configured for a running container. For containers that haven't started or were restarted, it reflects the resources allocated upon their next start.Triggering a Resize: You can request a resize by updating the desired requests and limits in the Pod's specification. This is typically done using kubectl patch, kubectl apply, or kubectl edit targeting the Pod's resize subresource. When the desired resources don't match the allocated resources, the Kubelet will attempt to resize the container.Allocated Resources (Advanced): The status.containerStatuses[*].allocatedResources field tracks resource values confirmed by the Kubelet, primarily used for internal scheduling logic. For most monitoring and validation purposes, focus on status.containerStatuses[*].resources.If a node has pods with a pending or incomplete resize (see Pod Resize Status below), the scheduler uses the maximum of a container's desired requests, allocated requests, and actual requests from the status when making scheduling decisions.Before you beginYou need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesYour Kubernetes server must be at or later than version 1.33.To check the version, enter kubectl version.The InPlacePodVerticalScaling feature gate must be enabled for your control plane and for all nodes in your cluster.The kubectl client version must be at least v1.32 to use the --subresource=resize flag.Pod resize statusThe Kubelet updates the Pod's status conditions to indicate the state of a resize request:type: PodResizePending: The Kubelet cannot immediately grant the request. The message field provides an explanation of why.reason: Infeasible: The requested resize is impossible on the current node (for example, requesting more resources than the node has).reason: Deferred: The requested resize is currently not possible, but might become feasible later (for example if another pod is removed). The Kubelet will retry the resize.type: PodResizeInProgress: The Kubelet has accepted the resize and allocated resources, but the changes are still being applied. This is usually brief but might take longer depending on the resource type and runtime behavior. Any errors during actuation are reported in the message field (along with reason: Error).Container resize policiesYou can control whether a container should be restarted when resizing by setting resizePolicy in the container specification. This allows fine-grained control based on resource type (CPU or memory). resizePolicy: \- resourceName: cpu restartPolicy: NotRequired \- resourceName: memory restartPolicy: RestartContainer NotRequired: (Default) Apply the resource change to the running container without restarting it.RestartContainer: Restart the container to apply the new resource values. This is often necessary for memory changes because many applications and runtimes cannot adjust their memory allocation dynamically.If resizePolicy[*].restartPolicy is not specified for a resource, it defaults to NotRequired.Note:If a Pod's overall restartPolicy is Never, then any container resizePolicy must be NotRequired for all resources. You cannot configure a resize policy that would require a restart in such Pods.Example Scenario:Consider a container configured with restartPolicy: NotRequired for CPU and restartPolicy: RestartContainer for memory.If only CPU resources are changed, the container is resized in-place.If only memory resources are changed, the container is restarted.If both CPU and memory resources are changed simultaneously, the container is restarted (due to the memory policy).LimitationsFor Kubernetes 1.33, resizing pod resources in-place has the following limitations:Resource Types: Only CPU and memory resources can be resized.Memory Decrease: Memory limits cannot be decreased unless the resizePolicy for memory is RestartContainer. Memory requests can generally be decreased.QoS Class: The Pod's original Quality of Service (QoS) class (Guaranteed, Burstable, or BestEffort) is determined at creation and cannot be changed by a resize. The resized resource values must still adhere to the rules of the original QoS class:Guaranteed: Requests must continue to equal limits for both CPU and memory after resizing.Burstable: Requests and limits cannot become equal for both CPU and memory simultaneously (as this would change it to Guaranteed).BestEffort: Resource requirements (requests or limits) cannot be added (as this would change it to Burstable or Guaranteed).Container Types: Non-restartable init containers and ephemeral containers cannot be resized. Sidecar containers can be resized.Resource Removal: Resource requests and limits cannot be entirely removed once set; they can only be changed to different values.Operating System: Windows pods do not support in-place resize.Node Policies: Pods managed by static CPU or Memory manager policies cannot be resized in-place.Swap: Pods utilizing swap memory cannot resize memory requests unless the resizePolicy for memory is RestartContainer.These restrictions might be relaxed in future Kubernetes versions.Example 1: Resizing CPU without restartFirst, create a Pod designed for in-place CPU resize and restart-required memory resize.pods/resource/pod-resize.yaml apiVersion: v1 kind: Pod metadata: name: resize-demo spec: containers: \- name: pause image: registry.k8s.io/pause:3.8 resizePolicy: \- resourceName: cpu restartPolicy: NotRequired # Default, but explicit here \- resourceName: memory restartPolicy: RestartContainer resources: limits: memory: "200Mi" cpu: "700m" requests: memory: "200Mi" cpu: "700m" Create the pod:kubectl create -f pod-resize.yaml This pod starts in the Guaranteed QoS class. Verify its initial state:# Wait a moment for the pod to be running kubectl get pod resize-demo --output=yaml Observe the spec.containers[0].resources and status.containerStatuses[0].resources. They should match the manifest (700m CPU, 200Mi memory). Note the status.containerStatuses[0].restartCount (should be 0).Now, increase the CPU request and limit to 800m. You use kubectl patch with the --subresource resize command line argument.kubectl patch pod resize-demo --subresource resize --patch \ '{"spec":{"containers":[{"name":"pause", "resources":{"requests":{"cpu":"800m"}, "limits":{"cpu":"800m"}}}]}}' # Alternative methods: # kubectl -n qos-example edit pod resize-demo --subresource resize # kubectl -n qos-example apply -f  \--subresource resize Note:The --subresource resize command line argument requires kubectl client version v1.32.0 or later. Older versions will report an invalid subresource error.Check the pod status again after patching:kubectl get pod resize-demo --output=yaml --namespace=qos-example You should see:spec.containers[0].resources now shows cpu: 800m.status.containerStatuses[0].resources also shows cpu: 800m, indicating the resize was successful on the node.status.containerStatuses[0].restartCount remains 0, because the CPU resizePolicy was NotRequired.Example 2: Resizing memory with restartNow, resize the memory for the same pod by increasing it to 300Mi. Since the memory resizePolicy is RestartContainer, the container is expected to restart.kubectl patch pod resize-demo --subresource resize --patch \ '{"spec":{"containers":[{"name":"pause", "resources":{"requests":{"memory":"300Mi"}, "limits":{"memory":"300Mi"}}}]}}' Check the pod status shortly after patching:kubectl get pod resize-demo --output=yaml You should now observe:spec.containers[0].resources shows memory: 300Mi.status.containerStatuses[0].resources also shows memory: 300Mi.status.containerStatuses[0].restartCount has increased to 1 (or more, if restarts happened previously), indicating the container was restarted to apply the memory change.Troubleshooting: Infeasible resize requestNext, try requesting an unreasonable amount of CPU, such as 1000 full cores (written as "1000" instead of "1000m" for millicores), which likely exceeds node capacity.# Attempt to patch with an excessively large CPU request kubectl patch pod resize-demo --subresource resize --patch \ '{"spec":{"containers":[{"name":"pause", "resources":{"requests":{"cpu":"1000"}, "limits":{"cpu":"1000"}}}]}}' Query the Pod's details:kubectl get pod resize-demo --output=yaml You'll see changes indicating the problem:The spec.containers[0].resources reflects the desired state (cpu: "1000").A condition with type: PodResizePending and reason: Infeasible was added to the Pod.The condition's message will explain why (Node didn't have enough capacity: cpu, requested: 800000, capacity: ...)Crucially, status.containerStatuses[0].resources will still show the previous values (cpu: 800m, memory: 300Mi), because the infeasible resize was not applied by the Kubelet.The restartCount will not have changed due to this failed attempt.To fix this, you would need to patch the pod again with feasible resource values.Clean upDelete the pod:kubectl delete pod resize-demo What's nextFor application developersAssign Memory Resources to Containers and PodsAssign CPU Resources to Containers and PodsAssign Pod-level CPU and memory resourcesFor cluster administratorsConfigure Default Memory Requests and Limits for a NamespaceConfigure Default CPU Requests and Limits for a NamespaceConfigure Minimum and Maximum Memory Constraints for a NamespaceConfigure Minimum and Maximum CPU Constraints for a NamespaceConfigure Memory and CPU Quotas for a Namespace4.3.6 - Configure RunAsUserName for Windows pods and containersFEATURE STATE: Kubernetes v1.18 [stable]This page shows how to use the runAsUserName setting for Pods and containers that will run on Windows nodes. This is roughly equivalent of the Linux-specific runAsUser setting, allowing you to run applications in a container as a different username than the default.Before you beginYou need to have a Kubernetes cluster and the kubectl command-line tool must be configured to communicate with your cluster. The cluster is expected to have Windows worker nodes where pods with containers running Windows workloads will get scheduled.Set the Username for a PodTo specify the username with which to execute the Pod's container processes, include the securityContext field (PodSecurityContext) in the Pod specification, and within it, the windowsOptions (WindowsSecurityContextOptions) field containing the runAsUserName field.The Windows security context options that you specify for a Pod apply to all Containers and init Containers in the Pod.Here is a configuration file for a Windows Pod that has the runAsUserName field set:windows/run-as-username-pod.yaml apiVersion: v1 kind: Pod metadata: name: run-as-username-pod-demo spec: securityContext: windowsOptions: runAsUserName: "ContainerUser" containers: \- name: run-as-username-demo image: mcr.microsoft.com/windows/servercore:ltsc2019 command: ["ping", "-t", "localhost"] nodeSelector: kubernetes.io/os: windows Create the Pod:kubectl apply -f https://k8s.io/examples/windows/run-as-username-pod.yaml Verify that the Pod's Container is running:kubectl get pod run-as-username-pod-demo Get a shell to the running Container:kubectl exec -it run-as-username-pod-demo -- powershell Check that the shell is running user the correct username:echo $env:USERNAME The output should be:ContainerUser Set the Username for a ContainerTo specify the username with which to execute a Container's processes, include the securityContext field (SecurityContext) in the Container manifest, and within it, the windowsOptions (WindowsSecurityContextOptions) field containing the runAsUserName field.The Windows security context options that you specify for a Container apply only to that individual Container, and they override the settings made at the Pod level.Here is the configuration file for a Pod that has one Container, and the runAsUserName field is set at the Pod level and the Container level:windows/run-as-username-container.yaml apiVersion: v1 kind: Pod metadata: name: run-as-username-container-demo spec: securityContext: windowsOptions: runAsUserName: "ContainerUser" containers: \- name: run-as-username-demo image: mcr.microsoft.com/windows/servercore:ltsc2019 command: ["ping", "-t", "localhost"] securityContext: windowsOptions: runAsUserName: "ContainerAdministrator" nodeSelector: kubernetes.io/os: windows Create the Pod:kubectl apply -f https://k8s.io/examples/windows/run-as-username-container.yaml Verify that the Pod's Container is running:kubectl get pod run-as-username-container-demo Get a shell to the running Container:kubectl exec -it run-as-username-container-demo -- powershell Check that the shell is running user the correct username (the one set at the Container level):echo $env:USERNAME The output should be:ContainerAdministrator Windows Username limitationsIn order to use this feature, the value set in the runAsUserName field must be a valid username. It must have the following format: DOMAIN\USER, where DOMAIN\ is optional. Windows user names are case insensitive. Additionally, there are some restrictions regarding the DOMAIN and USER:The runAsUserName field cannot be empty, and it cannot contain control characters (ASCII values: 0x00-0x1F, 0x7F)The DOMAIN must be either a NetBios name, or a DNS name, each with their own restrictions:NetBios names: maximum 15 characters, cannot start with . (dot), and cannot contain the following characters: \ / : * ? " < > |DNS names: maximum 255 characters, contains only alphanumeric characters, dots, and dashes, and it cannot start or end with a . (dot) or - (dash).The USER must have at most 20 characters, it cannot contain only dots or spaces, and it cannot contain the following characters: " / \ [ ] : ; | = , + * ? < > @.Examples of acceptable values for the runAsUserName field: ContainerAdministrator, ContainerUser, NT AUTHORITY\NETWORK SERVICE, NT AUTHORITY\LOCAL SERVICE.For more information about these limtations, check here and here.What's nextGuide for scheduling Windows containers in KubernetesManaging Workload Identity with Group Managed Service Accounts (GMSA)Configure GMSA for Windows pods and containers4.3.7 - Create a Windows HostProcess PodFEATURE STATE: Kubernetes v1.26 [stable]Windows HostProcess containers enable you to run containerized workloads on a Windows host. These containers operate as normal processes but have access to the host network namespace, storage, and devices when given the appropriate user privileges. HostProcess containers can be used to deploy network plugins, storage configurations, device plugins, kube-proxy, and other components to Windows nodes without the need for dedicated proxies or the direct installation of host services.Administrative tasks such as installation of security patches, event log collection, and more can be performed without requiring cluster operators to log onto each Windows node. HostProcess containers can run as any user that is available on the host or is in the domain of the host machine, allowing administrators to restrict resource access through user permissions. While neither filesystem or process isolation are supported, a new volume is created on the host upon starting the container to give it a clean and consolidated workspace. HostProcess containers can also be built on top of existing Windows base images and do not inherit the same compatibility requirements as Windows server containers, meaning that the version of the base images does not need to match that of the host. It is, however, recommended that you use the same base image version as your Windows Server container workloads to ensure you do not have any unused images taking up space on the node. HostProcess containers also support volume mounts within the container volume.When should I use a Windows HostProcess container?When you need to perform tasks which require the networking namespace of the host. HostProcess containers have access to the host's network interfaces and IP addresses.You need access to resources on the host such as the filesystem, event logs, etc.Installation of specific device drivers or Windows services.Consolidation of administrative tasks and security policies. This reduces the degree of privileges needed by Windows nodes.Before you beginThis task guide is specific to Kubernetes v1.33. If you are not running Kubernetes v1.33, check the documentation for that version of Kubernetes.In Kubernetes 1.33, the HostProcess container feature is enabled by default. The kubelet will communicate with containerd directly by passing the hostprocess flag via CRI. You can use the latest version of containerd (v1.6+) to run HostProcess containers. How to install containerd.LimitationsThese limitations are relevant for Kubernetes v1.33:HostProcess containers require containerd 1.6 or higher container runtime and containerd 1.7 is recommended.HostProcess pods can only contain HostProcess containers. This is a current limitation of the Windows OS; non-privileged Windows containers cannot share a vNIC with the host IP namespace.HostProcess containers run as a process on the host and do not have any degree of isolation other than resource constraints imposed on the HostProcess user account. Neither filesystem or Hyper-V isolation are supported for HostProcess containers.Volume mounts are supported and are mounted under the container volume. See Volume MountsA limited set of host user accounts are available for HostProcess containers by default. See Choosing a User Account.Resource limits (disk, memory, cpu count) are supported in the same fashion as processes on the host.Both Named pipe mounts and Unix domain sockets are not supported and should instead be accessed via their path on the host (e.g. \\\\.\pipe\\*)HostProcess Pod configuration requirementsEnabling a Windows HostProcess pod requires setting the right configurations in the pod security configuration. Of the policies defined in the Pod Security Standards HostProcess pods are disallowed by the baseline and restricted policies. It is therefore recommended that HostProcess pods run in alignment with the privileged profile.When running under the privileged policy, here are the configurations which need to be set to enable the creation of a HostProcess pod:Privileged policy specificationControlPolicysecurityContext.windowsOptions.hostProcessWindows pods offer the ability to run HostProcess containers which enables privileged access to the Windows node.Allowed ValuestruehostNetworkPods container HostProcess containers must use the host's network namespace.Allowed ValuestruesecurityContext.windowsOptions.runAsUserNameSpecification of which user the HostProcess container should run as is required for the pod spec.Allowed ValuesNT AUTHORITY\SYSTEMNT AUTHORITY\Local serviceNT AUTHORITY\NetworkServiceLocal usergroup names (see below)runAsNonRootBecause HostProcess containers have privileged access to the host, the runAsNonRoot field cannot be set to true.Allowed ValuesUndefined/NilfalseExample manifest (excerpt)spec: securityContext: windowsOptions: hostProcess: true runAsUserName: "NT AUTHORITY\\\Local service" hostNetwork: true containers: \- name: test image: image1:latest command: \- ping \- -t \- 127.0.0.1 nodeSelector: "kubernetes.io/os": windows Volume mountsHostProcess containers support the ability to mount volumes within the container volume space. Volume mount behavior differs depending on the version of containerd runtime used by on the node.Containerd v1.6Applications running inside the container can access volume mounts directly via relative or absolute paths. An environment variable $CONTAINER_SANDBOX_MOUNT_POINT is set upon container creation and provides the absolute host path to the container volume. Relative paths are based upon the .spec.containers.volumeMounts.mountPath configuration.To access service account tokens (for example) the following path structures are supported within the container:.\var\run\secrets\kubernetes.io\serviceaccount\$CONTAINER_SANDBOX_MOUNT_POINT\var\run\secrets\kubernetes.io\serviceaccount\Containerd v1.7 (and greater)Applications running inside the container can access volume mounts directly via the volumeMount's specified mountPath (just like Linux and non-HostProcess Windows containers).For backwards compatibility volumes can also be accessed via using the same relative paths configured by containerd v1.6.As an example, to access service account tokens within the container you would use one of the following paths:c:\var\run\secrets\kubernetes.io\serviceaccount/var/run/secrets/kubernetes.io/serviceaccount/$CONTAINER_SANDBOX_MOUNT_POINT\var\run\secrets\kubernetes.io\serviceaccount\Resource limitsResource limits (disk, memory, cpu count) are applied to the job and are job wide. For example, with a limit of 10MB set, the memory allocated for any HostProcess job object will be capped at 10MB. This is the same behavior as other Windows container types. These limits would be specified the same way they are currently for whatever orchestrator or runtime is being used. The only difference is in the disk resource usage calculation used for resource tracking due to the difference in how HostProcess containers are bootstrapped.Choosing a user accountSystem accountsBy default, HostProcess containers support the ability to run as one of three supported Windows service accounts:LocalSystemLocalServiceNetworkServiceYou should select an appropriate Windows service account for each HostProcess container, aiming to limit the degree of privileges so as to avoid accidental (or even malicious) damage to the host. The LocalSystem service account has the highest level of privilege of the three and should be used only if absolutely necessary. Where possible, use the LocalService service account as it is the least privileged of the three options.Local accountsIf configured, HostProcess containers can also run as local user accounts which allows for node operators to give fine-grained access to workloads.To run HostProcess containers as a local user; A local usergroup must first be created on the node and the name of that local usergroup must be specified in the runAsUserName field in the deployment. Prior to initializing the HostProcess container, a new ephemeral local user account to be created and joined to the specified usergroup, from which the container is run. This provides a number a benefits including eliminating the need to manage passwords for local user accounts. An initial HostProcess container running as a service account can be used to prepare the user groups for later HostProcess containers.Note:Running HostProcess containers as local user accounts requires containerd v1.7+Example:Create a local user group on the node (this can be done in another HostProcess container).net localgroup hpc-localgroup /add Grant access to desired resources on the node to the local usergroup. This can be done with tools like icacls.Set runAsUserName to the name of the local usergroup for the pod or individual containers.securityContext: windowsOptions: hostProcess: true runAsUserName: hpc-localgroup Schedule the pod!Base Image for HostProcess ContainersHostProcess containers can be built from any of the existing Windows Container base images.Additionally a new base mage has been created just for HostProcess containers! For more information please check out the windows-host-process-containers-base-image github project.Troubleshooting HostProcess containersHostProcess containers fail to start with failed to create user process token: failed to logon user: Access is denied.: unknownEnsure containerd is running as LocalSystem or LocalService service accounts. User accounts (even Administrator accounts) do not have permissions to create logon tokens for any of the supported user accounts.4.3.8 - Configure Quality of Service for PodsThis page shows how to configure Pods so that they will be assigned particular Quality of Service (QoS) classes. Kubernetes uses QoS classes to make decisions about evicting Pods when Node resources are exceeded.When Kubernetes creates a Pod it assigns one of these QoS classes to the Pod:GuaranteedBurstableBestEffortBefore you beginYou need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesYou also need to be able to create and delete namespaces.Create a namespaceCreate a namespace so that the resources you create in this exercise are isolated from the rest of your cluster.kubectl create namespace qos-example Create a Pod that gets assigned a QoS class of GuaranteedFor a Pod to be given a QoS class of Guaranteed:Every Container in the Pod must have a memory limit and a memory request.For every Container in the Pod, the memory limit must equal the memory request.Every Container in the Pod must have a CPU limit and a CPU request.For every Container in the Pod, the CPU limit must equal the CPU request.These restrictions apply to init containers and app containers equally. Ephemeral containers cannot define resources so these restrictions do not apply.Here is a manifest for a Pod that has one Container. The Container has a memory limit and a memory request, both equal to 200 MiB. The Container has a CPU limit and a CPU request, both equal to 700 milliCPU:pods/qos/qos-pod.yaml apiVersion: v1 kind: Pod metadata: name: qos-demo namespace: qos-example spec: containers: \- name: qos-demo-ctr image: nginx resources: limits: memory: "200Mi" cpu: "700m" requests: memory: "200Mi" cpu: "700m" Create the Pod:kubectl apply -f https://k8s.io/examples/pods/qos/qos-pod.yaml --namespace=qos-example View detailed information about the Pod:kubectl get pod qos-demo --namespace=qos-example --output=yaml The output shows that Kubernetes gave the Pod a QoS class of Guaranteed. The output also verifies that the Pod Container has a memory request that matches its memory limit, and it has a CPU request that matches its CPU limit.spec: containers: ... resources: limits: cpu: 700m memory: 200Mi requests: cpu: 700m memory: 200Mi ... status: qosClass: Guaranteed Note:If a Container specifies its own memory limit, but does not specify a memory request, Kubernetes automatically assigns a memory request that matches the limit. Similarly, if a Container specifies its own CPU limit, but does not specify a CPU request, Kubernetes automatically assigns a CPU request that matches the limit.Clean upDelete your Pod:kubectl delete pod qos-demo --namespace=qos-example Create a Pod that gets assigned a QoS class of BurstableA Pod is given a QoS class of Burstable if:The Pod does not meet the criteria for QoS class Guaranteed.At least one Container in the Pod has a memory or CPU request or limit.Here is a manifest for a Pod that has one Container. The Container has a memory limit of 200 MiB and a memory request of 100 MiB.pods/qos/qos-pod-2.yaml apiVersion: v1 kind: Pod metadata: name: qos-demo-2 namespace: qos-example spec: containers: \- name: qos-demo-2-ctr image: nginx resources: limits: memory: "200Mi" requests: memory: "100Mi" Create the Pod:kubectl apply -f https://k8s.io/examples/pods/qos/qos-pod-2.yaml --namespace=qos-example View detailed information about the Pod:kubectl get pod qos-demo-2 --namespace=qos-example --output=yaml The output shows that Kubernetes gave the Pod a QoS class of Burstable:spec: containers: \- image: nginx imagePullPolicy: Always name: qos-demo-2-ctr resources: limits: memory: 200Mi requests: memory: 100Mi ... status: qosClass: Burstable Clean upDelete your Pod:kubectl delete pod qos-demo-2 --namespace=qos-example Create a Pod that gets assigned a QoS class of BestEffortFor a Pod to be given a QoS class of BestEffort, the Containers in the Pod must not have any memory or CPU limits or requests.Here is a manifest for a Pod that has one Container. The Container has no memory or CPU limits or requests:pods/qos/qos-pod-3.yaml apiVersion: v1 kind: Pod metadata: name: qos-demo-3 namespace: qos-example spec: containers: \- name: qos-demo-3-ctr image: nginx Create the Pod:kubectl apply -f https://k8s.io/examples/pods/qos/qos-pod-3.yaml --namespace=qos-example View detailed information about the Pod:kubectl get pod qos-demo-3 --namespace=qos-example --output=yaml The output shows that Kubernetes gave the Pod a QoS class of BestEffort:spec: containers: ... resources: {} ... status: qosClass: BestEffort Clean upDelete your Pod:kubectl delete pod qos-demo-3 --namespace=qos-example Create a Pod that has two ContainersHere is a manifest for a Pod that has two Containers. One container specifies a memory request of 200 MiB. The other Container does not specify any requests or limits.pods/qos/qos-pod-4.yaml apiVersion: v1 kind: Pod metadata: name: qos-demo-4 namespace: qos-example spec: containers: \- name: qos-demo-4-ctr-1 image: nginx resources: requests: memory: "200Mi" \- name: qos-demo-4-ctr-2 image: redis Notice that this Pod meets the criteria for QoS class Burstable. That is, it does not meet the criteria for QoS class Guaranteed, and one of its Containers has a memory request.Create the Pod:kubectl apply -f https://k8s.io/examples/pods/qos/qos-pod-4.yaml --namespace=qos-example View detailed information about the Pod:kubectl get pod qos-demo-4 --namespace=qos-example --output=yaml The output shows that Kubernetes gave the Pod a QoS class of Burstable:spec: containers: ... name: qos-demo-4-ctr-1 resources: requests: memory: 200Mi ... name: qos-demo-4-ctr-2 resources: {} ... status: qosClass: Burstable Retrieve the QoS class for a PodRather than see all the fields, you can view just the field you need:kubectl --namespace=qos-example get pod qos-demo-4 -o jsonpath='{ .status.qosClass}{"\n"}' Burstable Clean upDelete your namespace:kubectl delete namespace qos-example What's nextFor app developersAssign Memory Resources to Containers and PodsAssign CPU Resources to Containers and PodsFor cluster administratorsConfigure Default Memory Requests and Limits for a NamespaceConfigure Default CPU Requests and Limits for a NamespaceConfigure Minimum and Maximum Memory Constraints for a NamespaceConfigure Minimum and Maximum CPU Constraints for a NamespaceConfigure Memory and CPU Quotas for a NamespaceConfigure a Pod Quota for a NamespaceConfigure Quotas for API ObjectsControl Topology Management policies on a node4.3.9 - Assign Extended Resources to a ContainerFEATURE STATE: Kubernetes v1.33 [stable]This page shows how to assign extended resources to a Container.Before you beginYou need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesTo check the version, enter kubectl version.Before you do this exercise, do the exercise in Advertise Extended Resources for a Node. That will configure one of your Nodes to advertise a dongle resource.Assign an extended resource to a PodTo request an extended resource, include the resources:requests field in your Container manifest. Extended resources are fully qualified with any domain outside of *.kubernetes.io/. Valid extended resource names have the form example.com/foo where example.com is replaced with your organization's domain and foo is a descriptive resource name.Here is the configuration file for a Pod that has one Container:pods/resource/extended-resource-pod.yaml apiVersion: v1 kind: Pod metadata: name: extended-resource-demo spec: containers: \- name: extended-resource-demo-ctr image: nginx resources: requests: example.com/dongle: 3 limits: example.com/dongle: 3 In the configuration file, you can see that the Container requests 3 dongles.Create a Pod:kubectl apply -f https://k8s.io/examples/pods/resource/extended-resource-pod.yaml Verify that the Pod is running:kubectl get pod extended-resource-demo Describe the Pod:kubectl describe pod extended-resource-demo The output shows dongle requests:Limits: example.com/dongle: 3 Requests: example.com/dongle: 3 Attempt to create a second PodHere is the configuration file for a Pod that has one Container. The Container requests two dongles.pods/resource/extended-resource-pod-2.yaml apiVersion: v1 kind: Pod metadata: name: extended-resource-demo-2 spec: containers: \- name: extended-resource-demo-2-ctr image: nginx resources: requests: example.com/dongle: 2 limits: example.com/dongle: 2 Kubernetes will not be able to satisfy the request for two dongles, because the first Pod used three of the four available dongles.Attempt to create a Pod:kubectl apply -f https://k8s.io/examples/pods/resource/extended-resource-pod-2.yaml Describe the Podkubectl describe pod extended-resource-demo-2 The output shows that the Pod cannot be scheduled, because there is no Node that has 2 dongles available:Conditions: Type Status PodScheduled False ... Events: ... ... Warning FailedScheduling pod (extended-resource-demo-2) failed to fit in any node fit failure summary on nodes : Insufficient example.com/dongle (1) View the Pod status:kubectl get pod extended-resource-demo-2 The output shows that the Pod was created, but not scheduled to run on a Node. It has a status of Pending:NAME READY STATUS RESTARTS AGE extended-resource-demo-2 0/1 Pending 0 6m Clean upDelete the Pods that you created for this exercise:kubectl delete pod extended-resource-demo kubectl delete pod extended-resource-demo-2 What's nextFor application developersAssign Memory Resources to Containers and PodsAssign CPU Resources to Containers and PodsFor cluster administratorsAdvertise Extended Resources for a Node4.3.10 - Configure a Pod to Use a Volume for StorageThis page shows how to configure a Pod to use a Volume for storage.A Container's file system lives only as long as the Container does. So when a Container terminates and restarts, filesystem changes are lost. For more consistent storage that is independent of the Container, you can use a Volume. This is especially important for stateful applications, such as key-value stores (such as Redis) and databases.Before you beginYou need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesTo check the version, enter kubectl version.Configure a volume for a PodIn this exercise, you create a Pod that runs one Container. This Pod has a Volume of type emptyDir that lasts for the life of the Pod, even if the Container terminates and restarts. Here is the configuration file for the Pod:pods/storage/redis.yaml apiVersion: v1 kind: Pod metadata: name: redis spec: containers: \- name: redis image: redis volumeMounts: \- name: redis-storage mountPath: /data/redis volumes: \- name: redis-storage emptyDir: {} Create the Pod:kubectl apply -f https://k8s.io/examples/pods/storage/redis.yaml Verify that the Pod's Container is running, and then watch for changes to the Pod:kubectl get pod redis --watch The output looks like this:NAME READY STATUS RESTARTS AGE redis 1/1 Running 0 13s In another terminal, get a shell to the running Container:kubectl exec -it redis -- /bin/bash In your shell, go to /data/redis, and then create a file:root@redis:/data# cd /data/redis/ root@redis:/data/redis# echo Hello > test-file In your shell, list the running processes:root@redis:/data/redis# apt-get update root@redis:/data/redis# apt-get install procps root@redis:/data/redis# ps aux The output is similar to this:USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND redis 1 0.1 0.1 33308 3828 ? Ssl 00:46 0:00 redis-server *:6379 root 12 0.0 0.0 20228 3020 ? Ss 00:47 0:00 /bin/bash root 15 0.0 0.0 17500 2072 ? R+ 00:48 0:00 ps aux In your shell, kill the Redis process:root@redis:/data/redis# kill  where  is the Redis process ID (PID).In your original terminal, watch for changes to the Redis Pod. Eventually, you will see something like this:NAME READY STATUS RESTARTS AGE redis 1/1 Running 0 13s redis 0/1 Completed 0 6m redis 1/1 Running 1 6m At this point, the Container has terminated and restarted. This is because the Redis Pod has a restartPolicy of Always.Get a shell into the restarted Container:kubectl exec -it redis -- /bin/bash In your shell, go to /data/redis, and verify that test-file is still there.root@redis:/data/redis# cd /data/redis/ root@redis:/data/redis# ls test-file Delete the Pod that you created for this exercise:kubectl delete pod redis What's nextSee Volume.See Pod.In addition to the local disk storage provided by emptyDir, Kubernetes supports many different network-attached storage solutions, including PD on GCE and EBS on EC2, which are preferred for critical data and will handle details such as mounting and unmounting the devices on the nodes. See Volumes for more details.4.3.11 - Configure a Pod to Use a PersistentVolume for StorageThis page shows you how to configure a Pod to use a PersistentVolumeClaim for storage. Here is a summary of the process:You, as cluster administrator, create a PersistentVolume backed by physical storage. You do not associate the volume with any Pod.You, now taking the role of a developer / cluster user, create a PersistentVolumeClaim that is automatically bound to a suitable PersistentVolume.You create a Pod that uses the above PersistentVolumeClaim for storage.Before you beginYou need to have a Kubernetes cluster that has only one Node, and the kubectl command-line tool must be configured to communicate with your cluster. If you do not already have a single-node cluster, you can create one by using Minikube.Familiarize yourself with the material in Persistent Volumes.Create an index.html file on your NodeOpen a shell to the single Node in your cluster. How you open a shell depends on how you set up your cluster. For example, if you are using Minikube, you can open a shell to your Node by entering minikube ssh.In your shell on that Node, create a /mnt/data directory:# This assumes that your Node uses "sudo" to run commands # as the superuser sudo mkdir /mnt/data In the /mnt/data directory, create an index.html file:# This again assumes that your Node uses "sudo" to run commands # as the superuser sudo sh -c "echo 'Hello from Kubernetes storage' > /mnt/data/index.html" Note:If your Node uses a tool for superuser access other than sudo, you can usually make this work if you replace sudo with the name of the other tool.Test that the index.html file exists:cat /mnt/data/index.html The output should be:Hello from Kubernetes storage You can now close the shell to your Node.Create a PersistentVolumeIn this exercise, you create a hostPath PersistentVolume. Kubernetes supports hostPath for development and testing on a single-node cluster. A hostPath PersistentVolume uses a file or directory on the Node to emulate network-attached storage.In a production cluster, you would not use hostPath. Instead a cluster administrator would provision a network resource like a Google Compute Engine persistent disk, an NFS share, or an Amazon Elastic Block Store volume. Cluster administrators can also use StorageClasses to set up dynamic provisioning.Here is the configuration file for the hostPath PersistentVolume:pods/storage/pv-volume.yaml apiVersion: v1 kind: PersistentVolume metadata: name: task-pv-volume labels: type: local spec: storageClassName: manual capacity: storage: 10Gi accessModes: \- ReadWriteOnce hostPath: path: "/mnt/data" The configuration file specifies that the volume is at /mnt/data on the cluster's Node. The configuration also specifies a size of 10 gibibytes and an access mode of ReadWriteOnce, which means the volume can be mounted as read-write by a single Node. It defines the StorageClass name manual for the PersistentVolume, which will be used to bind PersistentVolumeClaim requests to this PersistentVolume.Note:This example uses the ReadWriteOnce access mode, for simplicity. For production use, the Kubernetes project recommends using the ReadWriteOncePod access mode instead.Create the PersistentVolume:kubectl apply -f https://k8s.io/examples/pods/storage/pv-volume.yaml View information about the PersistentVolume:kubectl get pv task-pv-volume The output shows that the PersistentVolume has a STATUS of Available. This means it has not yet been bound to a PersistentVolumeClaim.NAME CAPACITY ACCESSMODES RECLAIMPOLICY STATUS CLAIM STORAGECLASS REASON AGE task-pv-volume 10Gi RWO Retain Available manual 4s Create a PersistentVolumeClaimThe next step is to create a PersistentVolumeClaim. Pods use PersistentVolumeClaims to request physical storage. In this exercise, you create a PersistentVolumeClaim that requests a volume of at least three gibibytes that can provide read-write access for at most one Node at a time.Here is the configuration file for the PersistentVolumeClaim:pods/storage/pv-claim.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: task-pv-claim spec: storageClassName: manual accessModes: \- ReadWriteOnce resources: requests: storage: 3Gi Create the PersistentVolumeClaim:kubectl apply -f https://k8s.io/examples/pods/storage/pv-claim.yaml After you create the PersistentVolumeClaim, the Kubernetes control plane looks for a PersistentVolume that satisfies the claim's requirements. If the control plane finds a suitable PersistentVolume with the same StorageClass, it binds the claim to the volume.Look again at the PersistentVolume:kubectl get pv task-pv-volume Now the output shows a STATUS of Bound.NAME CAPACITY ACCESSMODES RECLAIMPOLICY STATUS CLAIM STORAGECLASS REASON AGE task-pv-volume 10Gi RWO Retain Bound default/task-pv-claim manual 2m Look at the PersistentVolumeClaim:kubectl get pvc task-pv-claim The output shows that the PersistentVolumeClaim is bound to your PersistentVolume, task-pv-volume.NAME STATUS VOLUME CAPACITY ACCESSMODES STORAGECLASS AGE task-pv-claim Bound task-pv-volume 10Gi RWO manual 30s Create a PodThe next step is to create a Pod that uses your PersistentVolumeClaim as a volume.Here is the configuration file for the Pod:pods/storage/pv-pod.yaml apiVersion: v1 kind: Pod metadata: name: task-pv-pod spec: volumes: \- name: task-pv-storage persistentVolumeClaim: claimName: task-pv-claim containers: \- name: task-pv-container image: nginx ports: \- containerPort: 80 name: "http-server" volumeMounts: \- mountPath: "/usr/share/nginx/html" name: task-pv-storage Notice that the Pod's configuration file specifies a PersistentVolumeClaim, but it does not specify a PersistentVolume. From the Pod's point of view, the claim is a volume.Create the Pod:kubectl apply -f https://k8s.io/examples/pods/storage/pv-pod.yaml Verify that the container in the Pod is running:kubectl get pod task-pv-pod Get a shell to the container running in your Pod:kubectl exec -it task-pv-pod -- /bin/bash In your shell, verify that nginx is serving the index.html file from the hostPath volume:# Be sure to run these 3 commands inside the root shell that comes from # running "kubectl exec" in the previous step apt update apt install curl curl http://localhost/ The output shows the text that you wrote to the index.html file on the hostPath volume:Hello from Kubernetes storage If you see that message, you have successfully configured a Pod to use storage from a PersistentVolumeClaim.Clean upDelete the Pod:kubectl delete pod task-pv-pod Mounting the same PersistentVolume in two placesYou have understood how to create a PersistentVolume & PersistentVolumeClaim, and how to mount the volume to a single location in a container. Let's explore how you can mount the same PersistentVolume at two different locations in a container. Below is an example:pods/storage/pv-duplicate.yaml apiVersion: v1 kind: Pod metadata: name: test spec: containers: \- name: test image: nginx volumeMounts: # a mount for site-data \- name: config mountPath: /usr/share/nginx/html subPath: html # another mount for nginx config \- name: config mountPath: /etc/nginx/nginx.conf subPath: nginx.conf volumes: \- name: config persistentVolumeClaim: claimName: task-pv-storage Here:subPath: This field allows specific files or directories from the mounted PersistentVolume to be exposed at different locations within the container. In this example:subPath: html mounts the html directory.subPath: nginx.conf mounts a specific file, nginx.conf.Since the first subPath is html, an html directory has to be created within /mnt/data/ on the node.The second subPath nginx.conf means that a file within the /mnt/data/ directory will be used. No other directory needs to be created.Two volume mounts will be made on your nginx container:/usr/share/nginx/html for the static website/etc/nginx/nginx.conf for the default configMove the index.html file on your Node to a new folderThe index.html file mentioned here refers to the one created in the "Create an index.html file on your Node" section.Open a shell to the single Node in your cluster. How you open a shell depends on how you set up your cluster. For example, if you are using Minikube, you can open a shell to your Node by entering minikube ssh.Create a /mnt/data/html directory:# This assumes that your Node uses "sudo" to run commands # as the superuser sudo mkdir /mnt/data/html Move index.html into the directory:# Move index.html from its current location to the html sub-directory sudo mv /mnt/data/index.html html Create a new nginx.conf filepods/storage/nginx.conf user nginx; worker_processes auto; error_log /var/log/nginx/error.log notice; pid /var/run/nginx.pid; events { worker_connections 1024; } http { include /etc/nginx/mime.types; default_type application/octet-stream; log_format main '$remote_addr - $remote_user [$time_local] "$request" ' '$status $body_bytes_sent "$http_referer" ' '"$http_user_agent" "$http_x_forwarded_for"'; access_log /var/log/nginx/access.log main; sendfile on; #tcp_nopush on; keepalive_timeout 60; #gzip on; include /etc/nginx/conf.d/*.conf; }This is a modified version of the default nginx.conf file. Here, the default keepalive_timeout has been modified to 60Create the nginx.conf file:cat < /mnt/data/nginx.conf user nginx; worker_processes auto; error_log /var/log/nginx/error.log notice; pid /var/run/nginx.pid; events { worker_connections 1024; } http { include /etc/nginx/mime.types; default_type application/octet-stream; log_format main '\$remote_addr - \$remote_user [\$time_local] "\$request" ' '\$status \$body_bytes_sent "\$http_referer" ' '"\$http_user_agent" "\$http_x_forwarded_for"'; access_log /var/log/nginx/access.log main; sendfile on; #tcp_nopush on; keepalive_timeout 60; #gzip on; include /etc/nginx/conf.d/*.conf; } EOF Create a PodHere we will create a pod that uses the existing persistentVolume and persistentVolumeClaim. However, the pod mounts only a specific file, nginx.conf, and directory, html, to the container.Create the Pod:kubectl apply -f https://k8s.io/examples/pods/storage/pv-duplicate.yaml Verify that the container in the Pod is running:kubectl get pod test Get a shell to the container running in your Pod:kubectl exec -it test -- /bin/bash In your shell, verify that nginx is serving the index.html file from the hostPath volume:# Be sure to run these 3 commands inside the root shell that comes from # running "kubectl exec" in the previous step apt update apt install curl curl http://localhost/ The output shows the text that you wrote to the index.html file on the hostPath volume:Hello from Kubernetes storage In your shell, also verify that nginx is serving the nginx.conf file from the hostPath volume:# Be sure to run these commands inside the root shell that comes from # running "kubectl exec" in the previous step cat /etc/nginx/nginx.conf | grep keepalive_timeout The output shows the modified text that you wrote to the nginx.conf file on the hostPath volume:keepalive_timeout 60; If you see these messages, you have successfully configured a Pod to use a specific file and directory in a storage from a PersistentVolumeClaim.Clean upDelete the Pod:kubectl delete pod test kubectl delete pvc task-pv-claim kubectl delete pv task-pv-volume If you don't already have a shell open to the Node in your cluster, open a new shell the same way that you did earlier.In the shell on your Node, remove the file and directory that you created:# This assumes that your Node uses "sudo" to run commands # as the superuser sudo rm /mnt/data/html/index.html sudo rm /mnt/data/nginx.conf sudo rmdir /mnt/data You can now close the shell to your Node.Access controlStorage configured with a group ID (GID) allows writing only by Pods using the same GID. Mismatched or missing GIDs cause permission denied errors. To reduce the need for coordination with users, an administrator can annotate a PersistentVolume with a GID. Then the GID is automatically added to any Pod that uses the PersistentVolume.Use the pv.beta.kubernetes.io/gid annotation as follows:apiVersion: v1 kind: PersistentVolume metadata: name: pv1 annotations: pv.beta.kubernetes.io/gid: "1234" When a Pod consumes a PersistentVolume that has a GID annotation, the annotated GID is applied to all containers in the Pod in the same way that GIDs specified in the Pod's security context are. Every GID, whether it originates from a PersistentVolume annotation or the Pod's specification, is applied to the first process run in each container.Note:When a Pod consumes a PersistentVolume, the GIDs associated with the PersistentVolume are not present on the Pod resource itself.What's nextLearn more about PersistentVolumes.Read the Persistent Storage design document.ReferencePersistentVolumePersistentVolumeSpecPersistentVolumeClaimPersistentVolumeClaimSpec4.3.12 - Configure a Pod to Use a Projected Volume for StorageThis page shows how to use a projected Volume to mount several existing volume sources into the same directory. Currently, secret, configMap, downwardAPI, and serviceAccountToken volumes can be projected.Note:serviceAccountToken is not a volume type.Before you beginYou need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesTo check the version, enter kubectl version.Configure a projected volume for a podIn this exercise, you create username and password Secrets from local files. You then create a Pod that runs one container, using a projected Volume to mount the Secrets into the same shared directory.Here is the configuration file for the Pod:pods/storage/projected.yaml apiVersion: v1 kind: Pod metadata: name: test-projected-volume spec: containers: \- name: test-projected-volume image: busybox:1.28 args: \- sleep \- "86400" volumeMounts: \- name: all-in-one mountPath: "/projected-volume" readOnly: true volumes: \- name: all-in-one projected: sources: \- secret: name: user \- secret: name: pass Create the Secrets:# Create files containing the username and password: echo -n "admin" > ./username.txt echo -n "1f2d1e2e67df" > ./password.txt # Package these files into secrets: kubectl create secret generic user --from-file=./username.txt kubectl create secret generic pass --from-file=./password.txt Create the Pod:kubectl apply -f https://k8s.io/examples/pods/storage/projected.yaml Verify that the Pod's container is running, and then watch for changes to the Pod:kubectl get --watch pod test-projected-volume The output looks like this:NAME READY STATUS RESTARTS AGE test-projected-volume 1/1 Running 0 14s In another terminal, get a shell to the running container:kubectl exec -it test-projected-volume -- /bin/sh In your shell, verify that the projected-volume directory contains your projected sources:ls /projected-volume/ Clean upDelete the Pod and the Secrets:kubectl delete pod test-projected-volume kubectl delete secret user pass What's nextLearn more about projected volumes.Read the all-in-one volume design document.4.3.13 - Configure a Security Context for a Pod or ContainerA security context defines privilege and access control settings for a Pod or Container. Security context settings include, but are not limited to:Discretionary Access Control: Permission to access an object, like a file, is based on user ID (UID) and group ID (GID).Security Enhanced Linux (SELinux): Objects are assigned security labels.Running as privileged or unprivileged.Linux Capabilities: Give a process some privileges, but not all the privileges of the root user.AppArmor: Use program profiles to restrict the capabilities of individual programs.Seccomp: Filter a process's system calls.allowPrivilegeEscalation: Controls whether a process can gain more privileges than its parent process. This bool directly controls whether the no_new_privs flag gets set on the container process. allowPrivilegeEscalation is always true when the container:is run as privileged, orhas CAP_SYS_ADMINreadOnlyRootFilesystem: Mounts the container's root filesystem as read-only.The above bullets are not a complete set of security context settings -- please see SecurityContext for a comprehensive list.Before you beginYou need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesTo check the version, enter kubectl version.Set the security context for a PodTo specify security settings for a Pod, include the securityContext field in the Pod specification. The securityContext field is a PodSecurityContext object. The security settings that you specify for a Pod apply to all Containers in the Pod. Here is a configuration file for a Pod that has a securityContext and an emptyDir volume:pods/security/security-context.yaml apiVersion: v1 kind: Pod metadata: name: security-context-demo spec: securityContext: runAsUser: 1000 runAsGroup: 3000 fsGroup: 2000 supplementalGroups: [4000] volumes: \- name: sec-ctx-vol emptyDir: {} containers: \- name: sec-ctx-demo image: busybox:1.28 command: [ "sh", "-c", "sleep 1h" ] volumeMounts: \- name: sec-ctx-vol mountPath: /data/demo securityContext: allowPrivilegeEscalation: false In the configuration file, the runAsUser field specifies that for any Containers in the Pod, all processes run with user ID 1000. The runAsGroup field specifies the primary group ID of 3000 for all processes within any containers of the Pod. If this field is omitted, the primary group ID of the containers will be root(0). Any files created will also be owned by user 1000 and group 3000 when runAsGroup is specified. Since fsGroup field is specified, all processes of the container are also part of the supplementary group ID 2000. The owner for volume /data/demo and any files created in that volume will be Group ID 2000. Additionally, when the supplementalGroups field is specified, all processes of the container are also part of the specified groups. If this field is omitted, it means empty.Create the Pod:kubectl apply -f https://k8s.io/examples/pods/security/security-context.yaml Verify that the Pod's Container is running:kubectl get pod security-context-demo Get a shell to the running Container:kubectl exec -it security-context-demo -- sh In your shell, list the running processes:ps The output shows that the processes are running as user 1000, which is the value of runAsUser:PID USER TIME COMMAND 1 1000 0:00 sleep 1h 6 1000 0:00 sh ... In your shell, navigate to /data, and list the one directory:cd /data ls -l The output shows that the /data/demo directory has group ID 2000, which is the value of fsGroup.drwxrwsrwx 2 root 2000 4096 Jun 6 20:08 demo In your shell, navigate to /data/demo, and create a file:cd demo echo hello > testfile List the file in the /data/demo directory:ls -l The output shows that testfile has group ID 2000, which is the value of fsGroup.-rw-r--r-- 1 1000 2000 6 Jun 6 20:08 testfile Run the following command:id The output is similar to this:uid=1000 gid=3000 groups=2000,3000,4000 From the output, you can see that gid is 3000 which is same as the runAsGroup field. If the runAsGroup was omitted, the gid would remain as 0 (root) and the process will be able to interact with files that are owned by the root(0) group and groups that have the required group permissions for the root (0) group. You can also see that groups contains the group IDs which are specified by fsGroup and supplementalGroups, in addition to gid.Exit your shell:exit Implicit group memberships defined in /etc/group in the container imageBy default, kubernetes merges group information from the Pod with information defined in /etc/group in the container image.pods/security/security-context-5.yaml apiVersion: v1 kind: Pod metadata: name: security-context-demo spec: securityContext: runAsUser: 1000 runAsGroup: 3000 supplementalGroups: [4000] containers: \- name: sec-ctx-demo image: registry.k8s.io/e2e-test-images/agnhost:2.45 command: [ "sh", "-c", "sleep 1h" ] securityContext: allowPrivilegeEscalation: false This Pod security context contains runAsUser, runAsGroup and supplementalGroups. However, you can see that the actual supplementary groups attached to the container process will include group IDs which come from /etc/group in the container image.Create the Pod:kubectl apply -f https://k8s.io/examples/pods/security/security-context-5.yaml Verify that the Pod's Container is running:kubectl get pod security-context-demo Get a shell to the running Container:kubectl exec -it security-context-demo -- sh Check the process identity:$ id The output is similar to this:uid=1000 gid=3000 groups=3000,4000,50000 You can see that groups includes group ID 50000. This is because the user (uid=1000), which is defined in the image, belongs to the group (gid=50000), which is defined in /etc/group inside the container image.Check the /etc/group in the container image:$ cat /etc/group You can see that uid 1000 belongs to group 50000.... user-defined-in-image:x:1000: group-defined-in-image:x:50000:user-defined-in-image Exit your shell:exit Note:Implicitly merged supplementary groups may cause security problems particularly when accessing the volumes (see kubernetes/kubernetes#112879 for details). If you want to avoid this. Please see the below section.Configure fine-grained SupplementalGroups control for a PodFEATURE STATE: Kubernetes v1.33 [beta] (enabled by default: true)This feature can be enabled by setting the SupplementalGroupsPolicy feature gate for kubelet and kube-apiserver, and setting the .spec.securityContext.supplementalGroupsPolicy field for a pod.The supplementalGroupsPolicy field defines the policy for calculating the supplementary groups for the container processes in a pod. There are two valid values for this field:Merge: The group membership defined in /etc/group for the container's primary user will be merged. This is the default policy if not specified.Strict: Only group IDs in fsGroup, supplementalGroups, or runAsGroup fields are attached as the supplementary groups of the container processes. This means no group membership from /etc/group for the container's primary user will be merged.When the feature is enabled, it also exposes the process identity attached to the first container process in .status.containerStatuses[].user.linux field. It would be useful for detecting if implicit group ID's are attached.pods/security/security-context-6.yaml apiVersion: v1 kind: Pod metadata: name: security-context-demo spec: securityContext: runAsUser: 1000 runAsGroup: 3000 supplementalGroups: [4000] supplementalGroupsPolicy: Strict containers: \- name: sec-ctx-demo image: registry.k8s.io/e2e-test-images/agnhost:2.45 command: [ "sh", "-c", "sleep 1h" ] securityContext: allowPrivilegeEscalation: false This pod manifest defines supplementalGroupsPolicy=Strict. You can see that no group memberships defined in /etc/group are merged to the supplementary groups for container processes.Create the Pod:kubectl apply -f https://k8s.io/examples/pods/security/security-context-6.yaml Verify that the Pod's Container is running:kubectl get pod security-context-demo Check the process identity:kubectl exec -it security-context-demo -- id The output is similar to this:uid=1000 gid=3000 groups=3000,4000 See the Pod's status:kubectl get pod security-context-demo -o yaml You can see that the status.containerStatuses[].user.linux field exposes the process identitiy attached to the first container process.... status: containerStatuses: \- name: sec-ctx-demo user: linux: gid: 3000 supplementalGroups: \- 3000 \- 4000 uid: 1000 ... Note:Please note that the values in the status.containerStatuses[].user.linux field is the first attached process identity to the first container process in the container. If the container has sufficient privilege to make system calls related to process identity (e.g. setuid(2), setgid(2) or setgroups(2), etc.), the container process can change its identity. Thus, the actual process identity will be dynamic.ImplementationsNote: This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the content guide before submitting a change. More information.The following container runtimes are known to support fine-grained SupplementalGroups control.CRI-level:containerd, since v2.0CRI-O, since v1.31You can see if the feature is supported in the Node status.apiVersion: v1 kind: Node ... status: features: supplementalGroupsPolicy: true Note:At this alpha release(from v1.31 to v1.32), when a pod with SupplementalGroupsPolicy=Strict are scheduled to a node that does NOT support this feature(i.e. .status.features.supplementalGroupsPolicy=false), the pod's supplemental groups policy falls back to the Merge policy silently.However, since the beta release (v1.33), to enforce the policy more strictly, such pod creation will be rejected by kubelet because the node cannot ensure the specified policy. When your pod is rejected, you will see warning events with reason=SupplementalGroupsPolicyNotSupported like below:apiVersion: v1 kind: Event ... type: Warning reason: SupplementalGroupsPolicyNotSupported message: "SupplementalGroupsPolicy=Strict is not supported in this node" involvedObject: apiVersion: v1 kind: Pod ... Configure volume permission and ownership change policy for PodsFEATURE STATE: Kubernetes v1.23 [stable]By default, Kubernetes recursively changes ownership and permissions for the contents of each volume to match the fsGroup specified in a Pod's securityContext when that volume is mounted. For large volumes, checking and changing ownership and permissions can take a lot of time, slowing Pod startup. You can use the fsGroupChangePolicy field inside a securityContext to control the way that Kubernetes checks and manages ownership and permissions for a volume.fsGroupChangePolicy - fsGroupChangePolicy defines behavior for changing ownership and permission of the volume before being exposed inside a Pod. This field only applies to volume types that support fsGroup controlled ownership and permissions. This field has two possible values:OnRootMismatch: Only change permissions and ownership if the permission and the ownership of root directory does not match with expected permissions of the volume. This could help shorten the time it takes to change ownership and permission of a volume.Always: Always change permission and ownership of the volume when volume is mounted.For example:securityContext: runAsUser: 1000 runAsGroup: 3000 fsGroup: 2000 fsGroupChangePolicy: "OnRootMismatch" Note:This field has no effect on ephemeral volume types such as secret, configMap, and emptydir.Delegating volume permission and ownership change to CSI driverFEATURE STATE: Kubernetes v1.26 [stable]If you deploy a Container Storage Interface (CSI) driver which supports the VOLUME_MOUNT_GROUP NodeServiceCapability, the process of setting file ownership and permissions based on the fsGroup specified in the securityContext will be performed by the CSI driver instead of Kubernetes. In this case, since Kubernetes doesn't perform any ownership and permission change, fsGroupChangePolicy does not take effect, and as specified by CSI, the driver is expected to mount the volume with the provided fsGroup, resulting in a volume that is readable/writable by the fsGroup.Set the security context for a ContainerTo specify security settings for a Container, include the securityContext field in the Container manifest. The securityContext field is a SecurityContext object. Security settings that you specify for a Container apply only to the individual Container, and they override settings made at the Pod level when there is overlap. Container settings do not affect the Pod's Volumes.Here is the configuration file for a Pod that has one Container. Both the Pod and the Container have a securityContext field:pods/security/security-context-2.yaml apiVersion: v1 kind: Pod metadata: name: security-context-demo-2 spec: securityContext: runAsUser: 1000 containers: \- name: sec-ctx-demo-2 image: gcr.io/google-samples/hello-app:2.0 securityContext: runAsUser: 2000 allowPrivilegeEscalation: false Create the Pod:kubectl apply -f https://k8s.io/examples/pods/security/security-context-2.yaml Verify that the Pod's Container is running:kubectl get pod security-context-demo-2 Get a shell into the running Container:kubectl exec -it security-context-demo-2 -- sh In your shell, list the running processes:ps aux The output shows that the processes are running as user 2000. This is the value of runAsUser specified for the Container. It overrides the value 1000 that is specified for the Pod.USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND 2000 1 0.0 0.0 4336 764 ? Ss 20:36 0:00 /bin/sh -c node server.js 2000 8 0.1 0.5 772124 22604 ? Sl 20:36 0:00 node server.js ... Exit your shell:exit Set capabilities for a ContainerWith Linux capabilities, you can grant certain privileges to a process without granting all the privileges of the root user. To add or remove Linux capabilities for a Container, include the capabilities field in the securityContext section of the Container manifest.First, see what happens when you don't include a capabilities field. Here is configuration file that does not add or remove any Container capabilities:pods/security/security-context-3.yaml apiVersion: v1 kind: Pod metadata: name: security-context-demo-3 spec: containers: \- name: sec-ctx-3 image: gcr.io/google-samples/hello-app:2.0 Create the Pod:kubectl apply -f https://k8s.io/examples/pods/security/security-context-3.yaml Verify that the Pod's Container is running:kubectl get pod security-context-demo-3 Get a shell into the running Container:kubectl exec -it security-context-demo-3 -- sh In your shell, list the running processes:ps aux The output shows the process IDs (PIDs) for the Container:USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND root 1 0.0 0.0 4336 796 ? Ss 18:17 0:00 /bin/sh -c node server.js root 5 0.1 0.5 772124 22700 ? Sl 18:17 0:00 node server.js In your shell, view the status for process 1:cd /proc/1 cat status The output shows the capabilities bitmap for the process:... CapPrm: 00000000a80425fb CapEff: 00000000a80425fb ... Make a note of the capabilities bitmap, and then exit your shell:exit Next, run a Container that is the same as the preceding container, except that it has additional capabilities set.Here is the configuration file for a Pod that runs one Container. The configuration adds the CAP_NET_ADMIN and CAP_SYS_TIME capabilities:pods/security/security-context-4.yaml apiVersion: v1 kind: Pod metadata: name: security-context-demo-4 spec: containers: \- name: sec-ctx-4 image: gcr.io/google-samples/hello-app:2.0 securityContext: capabilities: add: ["NET_ADMIN", "SYS_TIME"] Create the Pod:kubectl apply -f https://k8s.io/examples/pods/security/security-context-4.yaml Get a shell into the running Container:kubectl exec -it security-context-demo-4 -- sh In your shell, view the capabilities for process 1:cd /proc/1 cat status The output shows capabilities bitmap for the process:... CapPrm: 00000000aa0435fb CapEff: 00000000aa0435fb ... Compare the capabilities of the two Containers:00000000a80425fb 00000000aa0435fb In the capability bitmap of the first container, bits 12 and 25 are clear. In the second container, bits 12 and 25 are set. Bit 12 is CAP_NET_ADMIN, and bit 25 is CAP_SYS_TIME. See capability.h for definitions of the capability constants.Note:Linux capability constants have the form CAP_XXX. But when you list capabilities in your container manifest, you must omit the CAP_ portion of the constant. For example, to add CAP_SYS_TIME, include SYS_TIME in your list of capabilities.Set the Seccomp Profile for a ContainerTo set the Seccomp profile for a Container, include the seccompProfile field in the securityContext section of your Pod or Container manifest. The seccompProfile field is a SeccompProfile object consisting of type and localhostProfile. Valid options for type include RuntimeDefault, Unconfined, and Localhost. localhostProfile must only be set if type: Localhost. It indicates the path of the pre-configured profile on the node, relative to the kubelet's configured Seccomp profile location (configured with the --root-dir flag).Here is an example that sets the Seccomp profile to the node's container runtime default profile:... securityContext: seccompProfile: type: RuntimeDefault Here is an example that sets the Seccomp profile to a pre-configured file at /seccomp/my-profiles/profile-allow.json:... securityContext: seccompProfile: type: Localhost localhostProfile: my-profiles/profile-allow.json Set the AppArmor Profile for a ContainerTo set the AppArmor profile for a Container, include the appArmorProfile field in the securityContext section of your Container. The appArmorProfile field is a AppArmorProfile object consisting of type and localhostProfile. Valid options for type include RuntimeDefault(default), Unconfined, and Localhost. localhostProfile must only be set if type is Localhost. It indicates the name of the pre-configured profile on the node. The profile needs to be loaded onto all nodes suitable for the Pod, since you don't know where the pod will be scheduled. Approaches for setting up custom profiles are discussed in Setting up nodes with profiles.Note: If containers[*].securityContext.appArmorProfile.type is explicitly set to RuntimeDefault, then the Pod will not be admitted if AppArmor is not enabled on the Node. However if containers[*].securityContext.appArmorProfile.type is not specified, then the default (which is also RuntimeDefault) will only be applied if the node has AppArmor enabled. If the node has AppArmor disabled the Pod will be admitted but the Container will not be restricted by the RuntimeDefault profile.Here is an example that sets the AppArmor profile to the node's container runtime default profile:... containers: \- name: container-1 securityContext: appArmorProfile: type: RuntimeDefault Here is an example that sets the AppArmor profile to a pre-configured profile named k8s-apparmor-example-deny-write:... containers: \- name: container-1 securityContext: appArmorProfile: type: Localhost localhostProfile: k8s-apparmor-example-deny-write For more details please see, Restrict a Container's Access to Resources with AppArmor.Assign SELinux labels to a ContainerTo assign SELinux labels to a Container, include the seLinuxOptions field in the securityContext section of your Pod or Container manifest. The seLinuxOptions field is an SELinuxOptions object. Here's an example that applies an SELinux level:... securityContext: seLinuxOptions: level: "s0:c123,c456" Note:To assign SELinux labels, the SELinux security module must be loaded on the host operating system. On Windows and Linux worker nodes without SELinux support, this field and any SELinux feature gates described below have no effect.Efficient SELinux volume relabelingFEATURE STATE: Kubernetes v1.28 [beta] (enabled by default: true)Note:Kubernetes v1.27 introduced an early limited form of this behavior that was only applicable to volumes (and PersistentVolumeClaims) using the ReadWriteOncePod access mode.Kubernetes v1.33 promotes SELinuxChangePolicy and SELinuxMount feature gates as beta to widen that performance improvement to other kinds of PersistentVolumeClaims, as explained in detail below. While in beta, SELinuxMount is still disabled by default.With SELinuxMount feature gate disabled (the default in Kubernetes 1.33 and any previous release), the container runtime recursively assigns SELinux label to all files on all Pod volumes by default. To speed up this process, Kubernetes can change the SELinux label of a volume instantly by using a mount option -o context=.To benefit from this speedup, all these conditions must be met:The feature gate SELinuxMountReadWriteOncePod must be enabled.Pod must use PersistentVolumeClaim with applicable accessModes and feature gates:Either the volume has accessModes: ["ReadWriteOncePod"], and feature gate SELinuxMountReadWriteOncePod is enabled.Or the volume can use any other access modes and all feature gates SELinuxMountReadWriteOncePod, SELinuxChangePolicy and SELinuxMount must be enabled and the Pod has spec.securityContext.seLinuxChangePolicy either nil (default) or MountOption.Pod (or all its Containers that use the PersistentVolumeClaim) must have seLinuxOptions set.The corresponding PersistentVolume must be either:A volume that uses the legacy in-tree iscsi, rbd or fc volume type.Or a volume that uses a CSI driver. The CSI driver must announce that it supports mounting with -o context by setting spec.seLinuxMount: true in its CSIDriver instance.When any of these conditions is not met, SELinux relabelling happens another way: the container runtime recursively changes the SELinux label for all inodes (files and directories) in the volume. Calling out explicitly, this applies to Kubernetes ephemeral volumes like secret, configMap and projected, and all volumes whose CSIDriver instance does not explicitly announce mounting with -o context.When this speedup is used, all Pods that use the same applicable volume concurrently on the same node must have the same SELinux label. A Pod with a different SELinux label will fail to start and will be ContainerCreating until all Pods with other SELinux labels that use the volume are deleted.FEATURE STATE: Kubernetes v1.33 [beta] (enabled by default: true)For Pods that want to opt-out from relabeling using mount options, they can set spec.securityContext.seLinuxChangePolicy to Recursive. This is required when multiple pods share a single volume on the same node, but they run with different SELinux labels that allows simultaneous access to the volume. For example, a privileged pod running with label spc_t and an unprivileged pod running with the default label container_file_t. With unset spec.securityContext.seLinuxChangePolicy (or with the default value MountOption), only one of such pods is able to run on a node, the other one gets ContainerCreating with error conflicting SELinux labels of volume :  and .SELinuxWarningControllerTo make it easier to identify Pods that are affected by the change in SELinux volume relabeling, a new controller called SELinuxWarningController has been introduced in kube-controller-manager. It is disabled by default and can be enabled by either setting the --controllers=*,selinux-warning-controller command line flag, or by setting genericControllerManagerConfiguration.controllers field in KubeControllerManagerConfiguration. This controller requires SELinuxChangePolicy feature gate to be enabled.When enabled, the controller observes running Pods and when it detects that two Pods use the same volume with different SELinux labels:It emits an event to both of the Pods. kubectl describe pod  the shows SELinuxLabel "" conflicts with pod  that uses the same volume as this pod with SELinuxLabel "". If both pods land on the same node, only one of them may access the volume.Raise selinux_warning_controller_selinux_volume_conflict metric. The metric has both pod names + namespaces as labels to identify the affected pods easily.A cluster admin can use this information to identify pods affected by the planning change and proactively opt-out Pods from the optimization (i.e. set spec.securityContext.seLinuxChangePolicy: Recursive).Warning:We strongly recommend clusters that use SELinux to enable this controller and make sure that selinux_warning_controller_selinux_volume_conflict metric does not report any conflicts before enabling SELinuxMount feature gate or upgrading to a version where SELinuxMount is enabled by default.Feature gatesThe following feature gates control the behavior of SELinux volume relabeling:SELinuxMountReadWriteOncePod: enables the optimization for volumes with accessModes: ["ReadWriteOncePod"]. This is a very safe feature gate to enable, as it cannot happen that two pods can share one single volume with this access mode. This feature gate is enabled by default sine v1.28.SELinuxChangePolicy: enables spec.securityContext.seLinuxChangePolicy field in Pod and related SELinuxWarningController in kube-controller-manager. This feature can be used before enabling SELinuxMount to check Pods running on a cluster, and to pro-actively opt-out Pods from the optimization. This feature gate requires SELinuxMountReadWriteOncePod enabled. It is beta and enabled by default in 1.33.SELinuxMount enables the optimization for all eligible volumes. Since it can break existing workloads, we recommend enabling SELinuxChangePolicy feature gate + SELinuxWarningController first to check the impact of the change. This feature gate requires SELinuxMountReadWriteOncePod and SELinuxChangePolicy enabled. It is beta, but disabled by default in 1.33.Managing access to the /proc filesystemFEATURE STATE: Kubernetes v1.31 [beta] (enabled by default: false)For runtimes that follow the OCI runtime specification, containers default to running in a mode where there are multiple paths that are both masked and read-only. The result of this is the container has these paths present inside the container's mount namespace, and they can function similarly to if the container was an isolated host, but the container process cannot write to them. The list of masked and read-only paths are as follows:Masked Paths:/proc/asound/proc/acpi/proc/kcore/proc/keys/proc/latency_stats/proc/timer_list/proc/timer_stats/proc/sched_debug/proc/scsi/sys/firmware/sys/devices/virtual/powercapRead-Only Paths:/proc/bus/proc/fs/proc/irq/proc/sys/proc/sysrq-triggerFor some Pods, you might want to bypass that default masking of paths. The most common context for wanting this is if you are trying to run containers within a Kubernetes container (within a pod).The securityContext field procMount allows a user to request a container's /proc be Unmasked, or be mounted as read-write by the container process. This also applies to /sys/firmware which is not in /proc.... securityContext: procMount: Unmasked Note:Setting procMount to Unmasked requires the spec.hostUsers value in the pod spec to be false. In other words: a container that wishes to have an Unmasked /proc or unmasked /sys must also be in a user namespace. Kubernetes v1.12 to v1.29 did not enforce that requirement.DiscussionThe security context for a Pod applies to the Pod's Containers and also to the Pod's Volumes when applicable. Specifically fsGroup and seLinuxOptions are applied to Volumes as follows:fsGroup: Volumes that support ownership management are modified to be owned and writable by the GID specified in fsGroup. See the Ownership Management design document for more details.seLinuxOptions: Volumes that support SELinux labeling are relabeled to be accessible by the label specified under seLinuxOptions. Usually you only need to set the level section. This sets the Multi-Category Security (MCS) label given to all Containers in the Pod as well as the Volumes.Warning:After you specify an MCS label for a Pod, all Pods with the same label can access the Volume. If you need inter-Pod protection, you must assign a unique MCS label to each Pod.Clean upDelete the Pod:kubectl delete pod security-context-demo kubectl delete pod security-context-demo-2 kubectl delete pod security-context-demo-3 kubectl delete pod security-context-demo-4 What's nextPodSecurityContextSecurityContextCRI Plugin Config GuideSecurity Contexts design documentOwnership Management design documentPodSecurity AdmissionAllowPrivilegeEscalation design documentFor more information about security mechanisms in Linux, see Overview of Linux Kernel Security Features (Note: Some information is out of date)Read about User Namespaces for Linux pods.Masked Paths in the OCI Runtime Specification4.3.14 - Configure Service Accounts for PodsKubernetes offers two distinct ways for clients that run within your cluster, or that otherwise have a relationship to your cluster's control plane to authenticate to the API server.A service account provides an identity for processes that run in a Pod, and maps to a ServiceAccount object. When you authenticate to the API server, you identify yourself as a particular user. Kubernetes recognises the concept of a user, however, Kubernetes itself does not have a User API.This task guide is about ServiceAccounts, which do exist in the Kubernetes API. The guide shows you some ways to configure ServiceAccounts for Pods.Before you beginYou need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesUse the default service account to access the API serverWhen Pods contact the API server, Pods authenticate as a particular ServiceAccount (for example, default). There is always at least one ServiceAccount in each namespace.Every Kubernetes namespace contains at least one ServiceAccount: the default ServiceAccount for that namespace, named default. If you do not specify a ServiceAccount when you create a Pod, Kubernetes automatically assigns the ServiceAccount named default in that namespace.You can fetch the details for a Pod you have created. For example:kubectl get pods/ -o yaml In the output, you see a field spec.serviceAccountName. Kubernetes automatically sets that value if you don't specify it when you create a Pod.An application running inside a Pod can access the Kubernetes API using automatically mounted service account credentials. See accessing the Cluster to learn more.When a Pod authenticates as a ServiceAccount, its level of access depends on the authorization plugin and policy in use.The API credentials are automatically revoked when the Pod is deleted, even if finalizers are in place. In particular, the API credentials are revoked 60 seconds beyond the .metadata.deletionTimestamp set on the Pod (the deletion timestamp is typically the time that the delete request was accepted plus the Pod's termination grace period).Opt out of API credential automountingIf you don't want the kubelet to automatically mount a ServiceAccount's API credentials, you can opt out of the default behavior. You can opt out of automounting API credentials on /var/run/secrets/kubernetes.io/serviceaccount/token for a service account by setting automountServiceAccountToken: false on the ServiceAccount:For example:apiVersion: v1 kind: ServiceAccount metadata: name: build-robot automountServiceAccountToken: false ... You can also opt out of automounting API credentials for a particular Pod:apiVersion: v1 kind: Pod metadata: name: my-pod spec: serviceAccountName: build-robot automountServiceAccountToken: false ... If both the ServiceAccount and the Pod's .spec specify a value for automountServiceAccountToken, the Pod spec takes precedence.Use more than one ServiceAccountEvery namespace has at least one ServiceAccount: the default ServiceAccount resource, called default. You can list all ServiceAccount resources in your current namespace with:kubectl get serviceaccounts The output is similar to this:NAME SECRETS AGE default 1 1d You can create additional ServiceAccount objects like this:kubectl apply -f - < Annotations: kubernetes.io/service-account.name: build-robot kubernetes.io/service-account.uid: da68f9c6-9d26-11e7-b84e-002dc52800da Type: kubernetes.io/service-account-token Data ==== ca.crt: 1338 bytes namespace: 7 bytes token: ... Note:The content of token is omitted here.Take care not to display the contents of a kubernetes.io/service-account-token Secret somewhere that your terminal / computer screen could be seen by an onlooker.When you delete a ServiceAccount that has an associated Secret, the Kubernetes control plane automatically cleans up the long-lived token from that Secret.Note:If you view the ServiceAccount using:kubectl get serviceaccount build-robot -o yamlYou can't see the build-robot-secret Secret in the ServiceAccount API objects .secrets field because that field is only populated with auto-generated Secrets.Add ImagePullSecrets to a service accountFirst, create an imagePullSecret. Next, verify it has been created. For example:Create an imagePullSecret, as described in Specifying ImagePullSecrets on a Pod.kubectl create secret docker-registry myregistrykey --docker-server= \ \--docker-username=DUMMY_USERNAME --docker-password=DUMMY_DOCKER_PASSWORD \ \--docker-email=DUMMY_DOCKER_EMAIL Verify it has been created.kubectl get secrets myregistrykey The output is similar to this:NAME TYPE DATA AGE myregistrykey kubernetes.io/.dockerconfigjson 1 1d Add image pull secret to service accountNext, modify the default service account for the namespace to use this Secret as an imagePullSecret.kubectl patch serviceaccount default -p '{"imagePullSecrets": [{"name": "myregistrykey"}]}' You can achieve the same outcome by editing the object manually:kubectl edit serviceaccount/default The output of the sa.yaml file is similar to this:Your selected text editor will open with a configuration looking something like this:apiVersion: v1 kind: ServiceAccount metadata: creationTimestamp: 2021-07-07T22:02:39Z name: default namespace: default resourceVersion: "243024" uid: 052fb0f4-3d50-11e5-b066-42010af0d7b6 Using your editor, delete the line with key resourceVersion, add lines for imagePullSecrets: and save it. Leave the uid value set the same as you found it.After you made those changes, the edited ServiceAccount looks something like this:apiVersion: v1 kind: ServiceAccount metadata: creationTimestamp: 2021-07-07T22:02:39Z name: default namespace: default uid: 052fb0f4-3d50-11e5-b066-42010af0d7b6 imagePullSecrets: \- name: myregistrykey Verify that imagePullSecrets are set for new PodsNow, when a new Pod is created in the current namespace and using the default ServiceAccount, the new Pod has its spec.imagePullSecrets field set automatically:kubectl run nginx --image=/nginx --restart=Never kubectl get pod nginx -o=jsonpath='{.spec.imagePullSecrets[0].name}{"\n"}' The output is:myregistrykey ServiceAccount token volume projectionFEATURE STATE: Kubernetes v1.20 [stable]Note:To enable and use token request projection, you must specify each of the following command line arguments to kube-apiserver:--service-account-issuerdefines the Identifier of the service account token issuer. You can specify the \--service-account-issuer argument multiple times, this can be useful to enable a non-disruptive change of the issuer. When this flag is specified multiple times, the first is used to generate tokens and all are used to determine which issuers are accepted. You must be running Kubernetes v1.22 or later to be able to specify \--service-account-issuer multiple times.--service-account-key-filespecifies the path to a file containing PEM-encoded X.509 private or public keys (RSA or ECDSA), used to verify ServiceAccount tokens. The specified file can contain multiple keys, and the flag can be specified multiple times with different files. If specified multiple times, tokens signed by any of the specified keys are considered valid by the Kubernetes API server.--service-account-signing-key-filespecifies the path to a file that contains the current private key of the service account token issuer. The issuer signs issued ID tokens with this private key.--api-audiences (can be omitted)defines audiences for ServiceAccount tokens. The service account token authenticator validates that tokens used against the API are bound to at least one of these audiences. If api-audiences is specified multiple times, tokens for any of the specified audiences are considered valid by the Kubernetes API server. If you specify the --service-account-issuer command line argument but you don't set --api-audiences, the control plane defaults to a single element audience list that contains only the issuer URL.The kubelet can also project a ServiceAccount token into a Pod. You can specify desired properties of the token, such as the audience and the validity duration. These properties are not configurable on the default ServiceAccount token. The token will also become invalid against the API when either the Pod or the ServiceAccount is deleted.You can configure this behavior for the spec of a Pod using a projected volume type called ServiceAccountToken.The token from this projected volume is a JSON Web Token (JWT). The JSON payload of this token follows a well defined schema - an example payload for a pod bound token:{ "aud": [ # matches the requested audiences, or the API server's default audiences when none are explicitly requested "https://kubernetes.default.svc" ], "exp": 1731613413, "iat": 1700077413, "iss": "https://kubernetes.default.svc", # matches the first value passed to the --service-account-issuer flag "jti": "ea28ed49-2e11-4280-9ec5-bc3d1d84661a", "kubernetes.io": { "namespace": "kube-system", "node": { "name": "127.0.0.1", "uid": "58456cb0-dd00-45ed-b797-5578fdceaced" }, "pod": { "name": "coredns-69cbfb9798-jv9gn", "uid": "778a530c-b3f4-47c0-9cd5-ab018fb64f33" }, "serviceaccount": { "name": "coredns", "uid": "a087d5a0-e1dd-43ec-93ac-f13d89cd13af" }, "warnafter": 1700081020 }, "nbf": 1700077413, "sub": "system:serviceaccount:kube-system:coredns" } Launch a Pod using service account token projectionTo provide a Pod with a token with an audience of vault and a validity duration of two hours, you could define a Pod manifest that is similar to:pods/pod-projected-svc-token.yaml apiVersion: v1 kind: Pod metadata: name: nginx spec: containers: \- image: nginx name: nginx volumeMounts: \- mountPath: /var/run/secrets/tokens name: vault-token serviceAccountName: build-robot volumes: \- name: vault-token projected: sources: \- serviceAccountToken: path: vault-token expirationSeconds: 7200 audience: vault Create the Pod:kubectl create -f https://k8s.io/examples/pods/pod-projected-svc-token.yaml The kubelet will: request and store the token on behalf of the Pod; make the token available to the Pod at a configurable file path; and refresh the token as it approaches expiration. The kubelet proactively requests rotation for the token if it is older than 80% of its total time-to-live (TTL), or if the token is older than 24 hours.The application is responsible for reloading the token when it rotates. It's often good enough for the application to load the token on a schedule (for example: once every 5 minutes), without tracking the actual expiry time.Service account issuer discoveryFEATURE STATE: Kubernetes v1.21 [stable]If you have enabled token projection for ServiceAccounts in your cluster, then you can also make use of the discovery feature. Kubernetes provides a way for clients to federate as an identity provider, so that one or more external systems can act as a relying party.Note:The issuer URL must comply with the OIDC Discovery Spec. In practice, this means it must use the https scheme, and should serve an OpenID provider configuration at {service-account-issuer}/.well-known/openid-configuration.If the URL does not comply, ServiceAccount issuer discovery endpoints are not registered or accessible.When enabled, the Kubernetes API server publishes an OpenID Provider Configuration document via HTTP. The configuration document is published at /.well-known/openid-configuration. The OpenID Provider Configuration is sometimes referred to as the discovery document. The Kubernetes API server publishes the related JSON Web Key Set (JWKS), also via HTTP, at /openid/v1/jwks.Note:The responses served at /.well-known/openid-configuration and /openid/v1/jwks are designed to be OIDC compatible, but not strictly OIDC compliant. Those documents contain only the parameters necessary to perform validation of Kubernetes service account tokens.Clusters that use RBAC include a default ClusterRole called system:service-account-issuer-discovery. A default ClusterRoleBinding assigns this role to the system:serviceaccounts group, which all ServiceAccounts implicitly belong to. This allows pods running on the cluster to access the service account discovery document via their mounted service account token. Administrators may, additionally, choose to bind the role to system:authenticated or system:unauthenticated depending on their security requirements and which external systems they intend to federate with.The JWKS response contains public keys that a relying party can use to validate the Kubernetes service account tokens. Relying parties first query for the OpenID Provider Configuration, and use the jwks_uri field in the response to find the JWKS.In many cases, Kubernetes API servers are not available on the public internet, but public endpoints that serve cached responses from the API server can be made available by users or by service providers. In these cases, it is possible to override the jwks_uri in the OpenID Provider Configuration so that it points to the public endpoint, rather than the API server's address, by passing the \--service-account-jwks-uri flag to the API server. Like the issuer URL, the JWKS URI is required to use the https scheme.What's nextSee also:Read the Cluster Admin Guide to Service AccountsRead about Authorization in KubernetesRead about Secretsor learn to distribute credentials securely using Secretsbut also bear in mind that using Secrets for authenticating as a ServiceAccount is deprecated. The recommended alternative is ServiceAccount token volume projection.Read about projected volumes.For background on OIDC discovery, read the ServiceAccount signing key retrieval Kubernetes Enhancement ProposalRead the OIDC Discovery Spec4.3.15 - Pull an Image from a Private RegistryThis page shows how to create a Pod that uses a Secret to pull an image from a private container image registry or repository. There are many private registries in use. This task uses Docker Hub as an example registry.🛇 This item links to a third party project or product that is not part of Kubernetes itself. More informationBefore you beginYou need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesTo do this exercise, you need the docker command line tool, and a Docker ID for which you know the password.If you are using a different private container registry, you need the command line tool for that registry and any login information for the registry.Log in to Docker HubOn your laptop, you must authenticate with a registry in order to pull a private image.Use the docker tool to log in to Docker Hub. See the log in section of Docker ID accounts for more information.docker login When prompted, enter your Docker ID, and then the credential you want to use (access token, or the password for your Docker ID).The login process creates or updates a config.json file that holds an authorization token. Review how Kubernetes interprets this file.View the config.json file:cat ~/.docker/config.json The output contains a section similar to this:{ "auths": { "https://index.docker.io/v1/": { "auth": "c3R...zE2" } } } Note:If you use a Docker credentials store, you won't see that auth entry but a credsStore entry with the name of the store as value. In that case, you can create a secret directly. See Create a Secret by providing credentials on the command line.Create a Secret based on existing credentialsA Kubernetes cluster uses the Secret of kubernetes.io/dockerconfigjson type to authenticate with a container registry to pull a private image.If you already ran docker login, you can copy that credential into Kubernetes:kubectl create secret generic regcred \ \--from-file=.dockerconfigjson= \ \--type=kubernetes.io/dockerconfigjson If you need more control (for example, to set a namespace or a label on the new secret) then you can customise the Secret before storing it. Be sure to:set the name of the data item to .dockerconfigjsonbase64 encode the Docker configuration file and then paste that string, unbroken as the value for field data[".dockerconfigjson"]set type to kubernetes.io/dockerconfigjsonExample:apiVersion: v1 kind: Secret metadata: name: myregistrykey namespace: awesomeapps data: .dockerconfigjson: UmVhbGx5IHJlYWxseSByZWVlZWVlZWVlZWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWxsbGxsbGxsbGxsbGxsbGxsbGxsbGxsbGxsbGxsbGx5eXl5eXl5eXl5eXl5eXl5eXl5eSBsbGxsbGxsbGxsbGxsbG9vb29vb29vb29vb29vb29vb29vb29vb29vb25ubm5ubm5ubm5ubm5ubm5ubm5ubm5ubmdnZ2dnZ2dnZ2dnZ2dnZ2dnZ2cgYXV0aCBrZXlzCg== type: kubernetes.io/dockerconfigjson If you get the error message error: no objects passed to create, it may mean the base64 encoded string is invalid. If you get an error message like Secret "myregistrykey" is invalid: data[.dockerconfigjson]: invalid value ..., it means the base64 encoded string in the data was successfully decoded, but could not be parsed as a .docker/config.json file.Create a Secret by providing credentials on the command lineCreate this Secret, naming it regcred:kubectl create secret docker-registry regcred --docker-server= \--docker-username= \--docker-password= \--docker-email= where: is your Private Docker Registry FQDN. Use https://index.docker.io/v1/ for DockerHub. is your Docker username. is your Docker password. is your Docker email.You have successfully set your Docker credentials in the cluster as a Secret called regcred.Note:Typing secrets on the command line may store them in your shell history unprotected, and those secrets might also be visible to other users on your PC during the time that kubectl is running.Inspecting the Secret regcredTo understand the contents of the regcred Secret you created, start by viewing the Secret in YAML format:kubectl get secret regcred --output=yaml The output is similar to this:apiVersion: v1 kind: Secret metadata: ... name: regcred ... data: .dockerconfigjson: eyJodHRwczovL2luZGV4L ... J0QUl6RTIifX0= type: kubernetes.io/dockerconfigjson The value of the .dockerconfigjson field is a base64 representation of your Docker credentials.To understand what is in the .dockerconfigjson field, convert the secret data to a readable format:kubectl get secret regcred --output="jsonpath={.data.\\.dockerconfigjson}" | base64 --decode The output is similar to this:{"auths":{"your.private.registry.example.com":{"username":"janedoe","password":"xxxxxxxxxxx","email":"jdoe@example.com","auth":"c3R...zE2"}}} To understand what is in the auth field, convert the base64-encoded data to a readable format:echo "c3R...zE2" | base64 --decode The output, username and password concatenated with a :, is similar to this:janedoe:xxxxxxxxxxx Notice that the Secret data contains the authorization token similar to your local ~/.docker/config.json file.You have successfully set your Docker credentials as a Secret called regcred in the cluster.Create a Pod that uses your SecretHere is a manifest for an example Pod that needs access to your Docker credentials in regcred:pods/private-reg-pod.yaml apiVersion: v1 kind: Pod metadata: name: private-reg spec: containers: \- name: private-reg-container image:  imagePullSecrets: \- name: regcred Download the above file onto your computer:curl -L -o my-private-reg-pod.yaml https://k8s.io/examples/pods/private-reg-pod.yaml In file my-private-reg-pod.yaml, replace  with the path to an image in a private registry such as:your.private.registry.example.com/janedoe/jdoe-private:v1 To pull the image from the private registry, Kubernetes needs credentials. The imagePullSecrets field in the configuration file specifies that Kubernetes should get the credentials from a Secret named regcred.Create a Pod that uses your Secret, and verify that the Pod is running:kubectl apply -f my-private-reg-pod.yaml kubectl get pod private-reg Note:To use image pull secrets for a Pod (or a Deployment, or other object that has a pod template that you are using), you need to make sure that the appropriate Secret does exist in the right namespace. The namespace to use is the same namespace where you defined the Pod.Also, in case the Pod fails to start with the status ImagePullBackOff, view the Pod events:kubectl describe pod private-reg If you then see an event with the reason set to FailedToRetrieveImagePullSecret, Kubernetes can't find a Secret with name (regcred, in this example).Make sure that the Secret you have specified exists, and that its name is spelled properly.Events: ... Reason ... Message \------ ------- ... FailedToRetrieveImagePullSecret ... Unable to retrieve some image pull secrets (); attempting to pull the image may not succeed. Using images from multiple registriesA pod can have multiple containers, each container image can be from a different registry. You can use multiple imagePullSecrets with one pod, and each can contain multiple credentials.The image pull will be attempted using each credential that matches the registry. If no credentials match the registry, the image pull will be attempted without authorization or using custom runtime specific configuration.What's nextLearn more about Secretsor read the API reference for SecretLearn more about using a private registry.Learn more about adding image pull secrets to a service account.See kubectl create secret docker-registry.See the imagePullSecrets field within the container definitions of a Pod4.3.16 - Configure Liveness, Readiness and Startup ProbesThis page shows how to configure liveness, readiness and startup probes for containers.For more information about probes, see Liveness, Readiness and Startup ProbesThe kubelet uses liveness probes to know when to restart a container. For example, liveness probes could catch a deadlock, where an application is running, but unable to make progress. Restarting a container in such a state can help to make the application more available despite bugs.A common pattern for liveness probes is to use the same low-cost HTTP endpoint as for readiness probes, but with a higher failureThreshold. This ensures that the pod is observed as not-ready for some period of time before it is hard killed.The kubelet uses readiness probes to know when a container is ready to start accepting traffic. One use of this signal is to control which Pods are used as backends for Services. A Pod is considered ready when its Ready condition is true. When a Pod is not ready, it is removed from Service load balancers. A Pod's Ready condition is false when its Node's Ready condition is not true, when one of the Pod's readinessGates is false, or when at least one of its containers is not ready.The kubelet uses startup probes to know when a container application has started. If such a probe is configured, liveness and readiness probes do not start until it succeeds, making sure those probes don't interfere with the application startup. This can be used to adopt liveness checks on slow starting containers, avoiding them getting killed by the kubelet before they are up and running.Caution:Liveness probes can be a powerful way to recover from application failures, but they should be used with caution. Liveness probes must be configured carefully to ensure that they truly indicate unrecoverable application failure, for example a deadlock.Note:Incorrect implementation of liveness probes can lead to cascading failures. This results in restarting of container under high load; failed client requests as your application became less scalable; and increased workload on remaining pods due to some failed pods. Understand the difference between readiness and liveness probes and when to apply them for your app.Before you beginYou need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesDefine a liveness commandMany applications running for long periods of time eventually transition to broken states, and cannot recover except by being restarted. Kubernetes provides liveness probes to detect and remedy such situations.In this exercise, you create a Pod that runs a container based on the registry.k8s.io/busybox:1.27.2 image. Here is the configuration file for the Pod:pods/probe/exec-liveness.yaml apiVersion: v1 kind: Pod metadata: labels: test: liveness name: liveness-exec spec: containers: \- name: liveness image: registry.k8s.io/busybox:1.27.2 args: \- /bin/sh \- -c \- touch /tmp/healthy; sleep 30; rm -f /tmp/healthy; sleep 600 livenessProbe: exec: command: \- cat \- /tmp/healthy initialDelaySeconds: 5 periodSeconds: 5 In the configuration file, you can see that the Pod has a single Container. The periodSeconds field specifies that the kubelet should perform a liveness probe every 5 seconds. The initialDelaySeconds field tells the kubelet that it should wait 5 seconds before performing the first probe. To perform a probe, the kubelet executes the command cat /tmp/healthy in the target container. If the command succeeds, it returns 0, and the kubelet considers the container to be alive and healthy. If the command returns a non-zero value, the kubelet kills the container and restarts it.When the container starts, it executes this command:/bin/sh -c "touch /tmp/healthy; sleep 30; rm -f /tmp/healthy; sleep 600" For the first 30 seconds of the container's life, there is a /tmp/healthy file. So during the first 30 seconds, the command cat /tmp/healthy returns a success code. After 30 seconds, cat /tmp/healthy returns a failure code.Create the Pod:kubectl apply -f https://k8s.io/examples/pods/probe/exec-liveness.yaml Within 30 seconds, view the Pod events:kubectl describe pod liveness-exec The output indicates that no liveness probes have failed yet:Type Reason Age From Message \---- ------ ---- ---- ------- Normal Scheduled 11s default-scheduler Successfully assigned default/liveness-exec to node01 Normal Pulling 9s kubelet, node01 Pulling image "registry.k8s.io/busybox:1.27.2" Normal Pulled 7s kubelet, node01 Successfully pulled image "registry.k8s.io/busybox:1.27.2" Normal Created 7s kubelet, node01 Created container liveness Normal Started 7s kubelet, node01 Started container liveness After 35 seconds, view the Pod events again:kubectl describe pod liveness-exec At the bottom of the output, there are messages indicating that the liveness probes have failed, and the failed containers have been killed and recreated.Type Reason Age From Message \---- ------ ---- ---- ------- Normal Scheduled 57s default-scheduler Successfully assigned default/liveness-exec to node01 Normal Pulling 55s kubelet, node01 Pulling image "registry.k8s.io/busybox:1.27.2" Normal Pulled 53s kubelet, node01 Successfully pulled image "registry.k8s.io/busybox:1.27.2" Normal Created 53s kubelet, node01 Created container liveness Normal Started 53s kubelet, node01 Started container liveness Warning Unhealthy 10s (x3 over 20s) kubelet, node01 Liveness probe failed: cat: can't open '/tmp/healthy': No such file or directory Normal Killing 10s kubelet, node01 Container liveness failed liveness probe, will be restarted Wait another 30 seconds, and verify that the container has been restarted:kubectl get pod liveness-exec The output shows that RESTARTS has been incremented. Note that the RESTARTS counter increments as soon as a failed container comes back to the running state:NAME READY STATUS RESTARTS AGE liveness-exec 1/1 Running 1 1m Define a liveness HTTP requestAnother kind of liveness probe uses an HTTP GET request. Here is the configuration file for a Pod that runs a container based on the registry.k8s.io/e2e-test-images/agnhost image.pods/probe/http-liveness.yaml apiVersion: v1 kind: Pod metadata: labels: test: liveness name: liveness-http spec: containers: \- name: liveness image: registry.k8s.io/e2e-test-images/agnhost:2.40 args: \- liveness livenessProbe: httpGet: path: /healthz port: 8080 httpHeaders: \- name: Custom-Header value: Awesome initialDelaySeconds: 3 periodSeconds: 3 In the configuration file, you can see that the Pod has a single container. The periodSeconds field specifies that the kubelet should perform a liveness probe every 3 seconds. The initialDelaySeconds field tells the kubelet that it should wait 3 seconds before performing the first probe. To perform a probe, the kubelet sends an HTTP GET request to the server that is running in the container and listening on port 8080. If the handler for the server's /healthz path returns a success code, the kubelet considers the container to be alive and healthy. If the handler returns a failure code, the kubelet kills the container and restarts it.Any code greater than or equal to 200 and less than 400 indicates success. Any other code indicates failure.You can see the source code for the server in server.go.For the first 10 seconds that the container is alive, the /healthz handler returns a status of 200. After that, the handler returns a status of 500.http.HandleFunc("/healthz", func(w http.ResponseWriter, r *http.Request) { duration := time.Now().Sub(started) if duration.Seconds() > 10 { w.WriteHeader(500) w.Write([]byte(fmt.Sprintf("error: %v", duration.Seconds()))) } else { w.WriteHeader(200) w.Write([]byte("ok")) } }) The kubelet starts performing health checks 3 seconds after the container starts. So the first couple of health checks will succeed. But after 10 seconds, the health checks will fail, and the kubelet will kill and restart the container.To try the HTTP liveness check, create a Pod:kubectl apply -f https://k8s.io/examples/pods/probe/http-liveness.yaml After 10 seconds, view Pod events to verify that liveness probes have failed and the container has been restarted:kubectl describe pod liveness-http In releases after v1.13, local HTTP proxy environment variable settings do not affect the HTTP liveness probe.Define a TCP liveness probeA third type of liveness probe uses a TCP socket. With this configuration, the kubelet will attempt to open a socket to your container on the specified port. If it can establish a connection, the container is considered healthy, if it can't it is considered a failure.pods/probe/tcp-liveness-readiness.yaml apiVersion: v1 kind: Pod metadata: name: goproxy labels: app: goproxy spec: containers: \- name: goproxy image: registry.k8s.io/goproxy:0.1 ports: \- containerPort: 8080 readinessProbe: tcpSocket: port: 8080 initialDelaySeconds: 15 periodSeconds: 10 livenessProbe: tcpSocket: port: 8080 initialDelaySeconds: 15 periodSeconds: 10 As you can see, configuration for a TCP check is quite similar to an HTTP check. This example uses both readiness and liveness probes. The kubelet will run the first liveness probe 15 seconds after the container starts. This will attempt to connect to the goproxy container on port 8080. If the liveness probe fails, the container will be restarted. The kubelet will continue to run this check every 10 seconds.In addition to the liveness probe, this configuration includes a readiness probe. The kubelet will run the first readiness probe 15 seconds after the container starts. Similar to the liveness probe, this will attempt to connect to the goproxy container on port 8080. If the probe succeeds, the Pod will be marked as ready and will receive traffic from services. If the readiness probe fails, the pod will be marked unready and will not receive traffic from any services.To try the TCP liveness check, create a Pod:kubectl apply -f https://k8s.io/examples/pods/probe/tcp-liveness-readiness.yaml After 15 seconds, view Pod events to verify that liveness probes:kubectl describe pod goproxy Define a gRPC liveness probeFEATURE STATE: Kubernetes v1.27 [stable]If your application implements the gRPC Health Checking Protocol, this example shows how to configure Kubernetes to use it for application liveness checks. Similarly you can configure readiness and startup probes.Here is an example manifest:pods/probe/grpc-liveness.yaml apiVersion: v1 kind: Pod metadata: name: etcd-with-grpc spec: containers: \- name: etcd image: registry.k8s.io/etcd:3.5.1-0 command: [ "/usr/local/bin/etcd", "--data-dir", "/var/lib/etcd", "--listen-client-urls", "http://0.0.0.0:2379", "--advertise-client-urls", "http://127.0.0.1:2379", "--log-level", "debug"] ports: \- containerPort: 2379 livenessProbe: grpc: port: 2379 initialDelaySeconds: 10 To use a gRPC probe, port must be configured. If you want to distinguish probes of different types and probes for different features you can use the service field. You can set service to the value liveness and make your gRPC Health Checking endpoint respond to this request differently than when you set service set to readiness. This lets you use the same endpoint for different kinds of container health check rather than listening on two different ports. If you want to specify your own custom service name and also specify a probe type, the Kubernetes project recommends that you use a name that concatenates those. For example: myservice-liveness (using - as a separator).Note:Unlike HTTP or TCP probes, you cannot specify the health check port by name, and you cannot configure a custom hostname.Configuration problems (for example: incorrect port or service, unimplemented health checking protocol) are considered a probe failure, similar to HTTP and TCP probes.To try the gRPC liveness check, create a Pod using the command below. In the example below, the etcd pod is configured to use gRPC liveness probe.kubectl apply -f https://k8s.io/examples/pods/probe/grpc-liveness.yaml After 15 seconds, view Pod events to verify that the liveness check has not failed:kubectl describe pod etcd-with-grpc When using a gRPC probe, there are some technical details to be aware of:The probes run against the pod IP address or its hostname. Be sure to configure your gRPC endpoint to listen on the Pod's IP address.The probes do not support any authentication parameters (like -tls).There are no error codes for built-in probes. All errors are considered as probe failures.If ExecProbeTimeout feature gate is set to false, grpc-health-probe does not respect the timeoutSeconds setting (which defaults to 1s), while built-in probe would fail on timeout.Use a named portYou can use a named port for HTTP and TCP probes. gRPC probes do not support named ports.For example:ports: \- name: liveness-port containerPort: 8080 livenessProbe: httpGet: path: /healthz port: liveness-port Protect slow starting containers with startup probesSometimes, you have to deal with applications that require additional startup time on their first initialization. In such cases, it can be tricky to set up liveness probe parameters without compromising the fast response to deadlocks that motivated such a probe. The solution is to set up a startup probe with the same command, HTTP or TCP check, with a failureThreshold * periodSeconds long enough to cover the worst case startup time.So, the previous example would become:ports: \- name: liveness-port containerPort: 8080 livenessProbe: httpGet: path: /healthz port: liveness-port failureThreshold: 1 periodSeconds: 10 startupProbe: httpGet: path: /healthz port: liveness-port failureThreshold: 30 periodSeconds: 10 Thanks to the startup probe, the application will have a maximum of 5 minutes (30 * 10 = 300s) to finish its startup. Once the startup probe has succeeded once, the liveness probe takes over to provide a fast response to container deadlocks. If the startup probe never succeeds, the container is killed after 300s and subject to the pod's restartPolicy.Define readiness probesSometimes, applications are temporarily unable to serve traffic. For example, an application might need to load large data or configuration files during startup, or depend on external services after startup. In such cases, you don't want to kill the application, but you don't want to send it requests either. Kubernetes provides readiness probes to detect and mitigate these situations. A pod with containers reporting that they are not ready does not receive traffic through Kubernetes Services.Note:Readiness probes runs on the container during its whole lifecycle.Caution:The readiness and liveness probes do not depend on each other to succeed. If you want to wait before executing a readiness probe, you should use initialDelaySeconds or a startupProbe.Readiness probes are configured similarly to liveness probes. The only difference is that you use the readinessProbe field instead of the livenessProbe field.readinessProbe: exec: command: \- cat \- /tmp/healthy initialDelaySeconds: 5 periodSeconds: 5 Configuration for HTTP and TCP readiness probes also remains identical to liveness probes.Readiness and liveness probes can be used in parallel for the same container. Using both can ensure that traffic does not reach a container that is not ready for it, and that containers are restarted when they fail.Configure ProbesProbes have a number of fields that you can use to more precisely control the behavior of startup, liveness and readiness checks:initialDelaySeconds: Number of seconds after the container has started before startup, liveness or readiness probes are initiated. If a startup probe is defined, liveness and readiness probe delays do not begin until the startup probe has succeeded. If the value of periodSeconds is greater than initialDelaySeconds then the initialDelaySeconds will be ignored. Defaults to 0 seconds. Minimum value is 0.periodSeconds: How often (in seconds) to perform the probe. Default to 10 seconds. The minimum value is 1. While a container is not Ready, the ReadinessProbe may be executed at times other than the configured periodSeconds interval. This is to make the Pod ready faster.timeoutSeconds: Number of seconds after which the probe times out. Defaults to 1 second. Minimum value is 1.successThreshold: Minimum consecutive successes for the probe to be considered successful after having failed. Defaults to 1. Must be 1 for liveness and startup Probes. Minimum value is 1.failureThreshold: After a probe fails failureThreshold times in a row, Kubernetes considers that the overall check has failed: the container is not ready/healthy/live. Defaults to 3. Minimum value is 1. For the case of a startup or liveness probe, if at least failureThreshold probes have failed, Kubernetes treats the container as unhealthy and triggers a restart for that specific container. The kubelet honors the setting of terminationGracePeriodSeconds for that container. For a failed readiness probe, the kubelet continues running the container that failed checks, and also continues to run more probes; because the check failed, the kubelet sets the Ready condition on the Pod to false.terminationGracePeriodSeconds: configure a grace period for the kubelet to wait between triggering a shut down of the failed container, and then forcing the container runtime to stop that container. The default is to inherit the Pod-level value for terminationGracePeriodSeconds (30 seconds if not specified), and the minimum value is 1. See probe-level terminationGracePeriodSeconds for more detail.Caution:Incorrect implementation of readiness probes may result in an ever growing number of processes in the container, and resource starvation if this is left unchecked.HTTP probesHTTP probes have additional fields that can be set on httpGet:host: Host name to connect to, defaults to the pod IP. You probably want to set "Host" in httpHeaders instead.scheme: Scheme to use for connecting to the host (HTTP or HTTPS). Defaults to "HTTP".path: Path to access on the HTTP server. Defaults to "/".httpHeaders: Custom headers to set in the request. HTTP allows repeated headers.port: Name or number of the port to access on the container. Number must be in the range 1 to 65535.For an HTTP probe, the kubelet sends an HTTP request to the specified port and path to perform the check. The kubelet sends the probe to the Pod's IP address, unless the address is overridden by the optional host field in httpGet. If scheme field is set to HTTPS, the kubelet sends an HTTPS request skipping the certificate verification. In most scenarios, you do not want to set the host field. Here's one scenario where you would set it. Suppose the container listens on 127.0.0.1 and the Pod's hostNetwork field is true. Then host, under httpGet, should be set to 127.0.0.1. If your pod relies on virtual hosts, which is probably the more common case, you should not use host, but rather set the Host header in httpHeaders.For an HTTP probe, the kubelet sends two request headers in addition to the mandatory Host header:User-Agent: The default value is kube-probe/1.33, where 1.33 is the version of the kubelet.Accept: The default value is */*.You can override the default headers by defining httpHeaders for the probe. For example:livenessProbe: httpGet: httpHeaders: \- name: Accept value: application/json startupProbe: httpGet: httpHeaders: \- name: User-Agent value: MyUserAgent You can also remove these two headers by defining them with an empty value.livenessProbe: httpGet: httpHeaders: \- name: Accept value: "" startupProbe: httpGet: httpHeaders: \- name: User-Agent value: "" Note:When the kubelet probes a Pod using HTTP, it only follows redirects if the redirect is to the same host. If the kubelet receives 11 or more redirects during probing, the probe is considered successful and a related Event is created:Events: Type Reason Age From Message \---- ------ ---- ---- ------- Normal Scheduled 29m default-scheduler Successfully assigned default/httpbin-7b8bc9cb85-bjzwn to daocloud Normal Pulling 29m kubelet Pulling image "docker.io/kennethreitz/httpbin" Normal Pulled 24m kubelet Successfully pulled image "docker.io/kennethreitz/httpbin" in 5m12.402735213s Normal Created 24m kubelet Created container httpbin Normal Started 24m kubelet Started container httpbin Warning ProbeWarning 4m11s (x1197 over 24m) kubelet Readiness probe warning: Probe terminated redirects If the kubelet receives a redirect where the hostname is different from the request, the outcome of the probe is treated as successful and kubelet creates an event to report the redirect failure.TCP probesFor a TCP probe, the kubelet makes the probe connection at the node, not in the Pod, which means that you can not use a service name in the host parameter since the kubelet is unable to resolve it.Probe-level terminationGracePeriodSecondsFEATURE STATE: Kubernetes v1.28 [stable]In 1.25 and above, users can specify a probe-level terminationGracePeriodSeconds as part of the probe specification. When both a pod- and probe-level terminationGracePeriodSeconds are set, the kubelet will use the probe-level value.When setting the terminationGracePeriodSeconds, please note the following:The kubelet always honors the probe-level terminationGracePeriodSeconds field if it is present on a Pod.If you have existing Pods where the terminationGracePeriodSeconds field is set and you no longer wish to use per-probe termination grace periods, you must delete those existing Pods.For example:spec: terminationGracePeriodSeconds: 3600 # pod-level containers: \- name: test image: ... ports: \- name: liveness-port containerPort: 8080 livenessProbe: httpGet: path: /healthz port: liveness-port failureThreshold: 1 periodSeconds: 60 # Override pod-level terminationGracePeriodSeconds # terminationGracePeriodSeconds: 60 Probe-level terminationGracePeriodSeconds cannot be set for readiness probes. It will be rejected by the API server.What's nextLearn more about Container Probes.You can also read the API references for:Pod, and specifically:container(s)probe(s)4.3.17 - Assign Pods to NodesThis page shows how to assign a Kubernetes Pod to a particular node in a Kubernetes cluster.Before you beginYou need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesTo check the version, enter kubectl version.Add a label to a nodeList the nodes in your cluster, along with their labels:kubectl get nodes --show-labels The output is similar to this:NAME STATUS ROLES AGE VERSION LABELS worker0 Ready  1d v1.13.0 ...,kubernetes.io/hostname=worker0 worker1 Ready  1d v1.13.0 ...,kubernetes.io/hostname=worker1 worker2 Ready  1d v1.13.0 ...,kubernetes.io/hostname=worker2 Choose one of your nodes, and add a label to it:kubectl label nodes  disktype=ssd where  is the name of your chosen node.Verify that your chosen node has a disktype=ssd label:kubectl get nodes --show-labels The output is similar to this:NAME STATUS ROLES AGE VERSION LABELS worker0 Ready  1d v1.13.0 ...,disktype=ssd,kubernetes.io/hostname=worker0 worker1 Ready  1d v1.13.0 ...,kubernetes.io/hostname=worker1 worker2 Ready  1d v1.13.0 ...,kubernetes.io/hostname=worker2 In the preceding output, you can see that the worker0 node has a disktype=ssd label.Create a pod that gets scheduled to your chosen nodeThis pod configuration file describes a pod that has a node selector, disktype: ssd. This means that the pod will get scheduled on a node that has a disktype=ssd label.pods/pod-nginx.yaml apiVersion: v1 kind: Pod metadata: name: nginx labels: env: test spec: containers: \- name: nginx image: nginx imagePullPolicy: IfNotPresent nodeSelector: disktype: ssd Use the configuration file to create a pod that will get scheduled on your chosen node:kubectl apply -f https://k8s.io/examples/pods/pod-nginx.yaml Verify that the pod is running on your chosen node:kubectl get pods --output=wide The output is similar to this:NAME READY STATUS RESTARTS AGE IP NODE nginx 1/1 Running 0 13s 10.200.0.4 worker0 Create a pod that gets scheduled to specific nodeYou can also schedule a pod to one specific node via setting nodeName.pods/pod-nginx-specific-node.yaml apiVersion: v1 kind: Pod metadata: name: nginx spec: nodeName: foo-node # schedule pod to specific node containers: \- name: nginx image: nginx imagePullPolicy: IfNotPresent Use the configuration file to create a pod that will get scheduled on foo-node only.What's nextLearn more about labels and selectors.Learn more about nodes.4.3.18 - Assign Pods to Nodes using Node AffinityThis page shows how to assign a Kubernetes Pod to a particular node using Node Affinity in a Kubernetes cluster.Before you beginYou need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesYour Kubernetes server must be at or later than version v1.10.To check the version, enter kubectl version.Add a label to a nodeList the nodes in your cluster, along with their labels:kubectl get nodes --show-labels The output is similar to this:NAME STATUS ROLES AGE VERSION LABELS worker0 Ready  1d v1.13.0 ...,kubernetes.io/hostname=worker0 worker1 Ready  1d v1.13.0 ...,kubernetes.io/hostname=worker1 worker2 Ready  1d v1.13.0 ...,kubernetes.io/hostname=worker2 Choose one of your nodes, and add a label to it:kubectl label nodes  disktype=ssd where  is the name of your chosen node.Verify that your chosen node has a disktype=ssd label:kubectl get nodes --show-labels The output is similar to this:NAME STATUS ROLES AGE VERSION LABELS worker0 Ready  1d v1.13.0 ...,disktype=ssd,kubernetes.io/hostname=worker0 worker1 Ready  1d v1.13.0 ...,kubernetes.io/hostname=worker1 worker2 Ready  1d v1.13.0 ...,kubernetes.io/hostname=worker2 In the preceding output, you can see that the worker0 node has a disktype=ssd label.Schedule a Pod using required node affinityThis manifest describes a Pod that has a requiredDuringSchedulingIgnoredDuringExecution node affinity,disktype: ssd. This means that the pod will get scheduled only on a node that has a disktype=ssd label.pods/pod-nginx-required-affinity.yaml apiVersion: v1 kind: Pod metadata: name: nginx spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: \- matchExpressions: \- key: disktype operator: In values: \- ssd containers: \- name: nginx image: nginx imagePullPolicy: IfNotPresent Apply the manifest to create a Pod that is scheduled onto your chosen node:kubectl apply -f https://k8s.io/examples/pods/pod-nginx-required-affinity.yaml Verify that the pod is running on your chosen node:kubectl get pods --output=wide The output is similar to this:NAME READY STATUS RESTARTS AGE IP NODE nginx 1/1 Running 0 13s 10.200.0.4 worker0 Schedule a Pod using preferred node affinityThis manifest describes a Pod that has a preferredDuringSchedulingIgnoredDuringExecution node affinity,disktype: ssd. This means that the pod will prefer a node that has a disktype=ssd label.pods/pod-nginx-preferred-affinity.yaml apiVersion: v1 kind: Pod metadata: name: nginx spec: affinity: nodeAffinity: preferredDuringSchedulingIgnoredDuringExecution: \- weight: 1 preference: matchExpressions: \- key: disktype operator: In values: \- ssd containers: \- name: nginx image: nginx imagePullPolicy: IfNotPresent Apply the manifest to create a Pod that is scheduled onto your chosen node:kubectl apply -f https://k8s.io/examples/pods/pod-nginx-preferred-affinity.yaml Verify that the pod is running on your chosen node:kubectl get pods --output=wide The output is similar to this:NAME READY STATUS RESTARTS AGE IP NODE nginx 1/1 Running 0 13s 10.200.0.4 worker0 What's nextLearn more about Node Affinity.4.3.19 - Configure Pod InitializationThis page shows how to use an Init Container to initialize a Pod before an application Container runs.Before you beginYou need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesTo check the version, enter kubectl version.Create a Pod that has an Init ContainerIn this exercise you create a Pod that has one application Container and one Init Container. The init container runs to completion before the application container starts.Here is the configuration file for the Pod:pods/init-containers.yaml apiVersion: v1 kind: Pod metadata: name: init-demo spec: containers: \- name: nginx image: nginx ports: \- containerPort: 80 volumeMounts: \- name: workdir mountPath: /usr/share/nginx/html # These containers are run during pod initialization initContainers: \- name: install image: busybox:1.28 command: \- wget \- "-O" \- "/work-dir/index.html" \- http://info.cern.ch volumeMounts: \- name: workdir mountPath: "/work-dir" dnsPolicy: Default volumes: \- name: workdir emptyDir: {} In the configuration file, you can see that the Pod has a Volume that the init container and the application container share.The init container mounts the shared Volume at /work-dir, and the application container mounts the shared Volume at /usr/share/nginx/html. The init container runs the following command and then terminates:wget -O /work-dir/index.html http://info.cern.ch Notice that the init container writes the index.html file in the root directory of the nginx server.Create the Pod:kubectl apply -f https://k8s.io/examples/pods/init-containers.yaml Verify that the nginx container is running:kubectl get pod init-demo The output shows that the nginx container is running:NAME READY STATUS RESTARTS AGE init-demo 1/1 Running 0 1m Get a shell into the nginx container running in the init-demo Pod:kubectl exec -it init-demo -- /bin/bash In your shell, send a GET request to the nginx server:root@nginx:~# apt-get update root@nginx:~# apt-get install curl root@nginx:~# curl localhost The output shows that nginx is serving the web page that was written by the init container: http://info.cern.ch

# http://info.cern.ch - home of the first website

... 
* [Browse the first website](http://info.cern.ch/hypertext/WWW/TheProject.html)
... What's nextLearn more about communicating between Containers running in the same Pod.Learn more about Init Containers.Learn more about Volumes.Learn more about Debugging Init Containers4.3.20 - Attach Handlers to Container Lifecycle EventsThis page shows how to attach handlers to Container lifecycle events. Kubernetes supports the postStart and preStop events. Kubernetes sends the postStart event immediately after a Container is started, and it sends the preStop event immediately before the Container is terminated. A Container may specify one handler per event.Before you beginYou need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesTo check the version, enter kubectl version.Define postStart and preStop handlersIn this exercise, you create a Pod that has one Container. The Container has handlers for the postStart and preStop events.Here is the configuration file for the Pod:pods/lifecycle-events.yaml apiVersion: v1 kind: Pod metadata: name: lifecycle-demo spec: containers: \- name: lifecycle-demo-container image: nginx lifecycle: postStart: exec: command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"] preStop: exec: command: ["/bin/sh","-c","nginx -s quit; while killall -0 nginx; do sleep 1; done"] In the configuration file, you can see that the postStart command writes a message file to the Container's /usr/share directory. The preStop command shuts down nginx gracefully. This is helpful if the Container is being terminated because of a failure.Create the Pod:kubectl apply -f https://k8s.io/examples/pods/lifecycle-events.yaml Verify that the Container in the Pod is running:kubectl get pod lifecycle-demo Get a shell into the Container running in your Pod:kubectl exec -it lifecycle-demo -- /bin/bash In your shell, verify that the postStart handler created the message file:root@lifecycle-demo:/# cat /usr/share/message The output shows the text written by the postStart handler:Hello from the postStart handler DiscussionKubernetes sends the postStart event immediately after the Container is created. There is no guarantee, however, that the postStart handler is called before the Container's entrypoint is called. The postStart handler runs asynchronously relative to the Container's code, but Kubernetes' management of the container blocks until the postStart handler completes. The Container's status is not set to RUNNING until the postStart handler completes.Kubernetes sends the preStop event immediately before the Container is terminated. Kubernetes' management of the Container blocks until the preStop handler completes, unless the Pod's grace period expires. For more details, see Pod Lifecycle.Note:Kubernetes only sends the preStop event when a Pod or a container in the Pod is terminated. This means that the preStop hook is not invoked when the Pod is completed. About this limitation, please see Container hooks for the detail.What's nextLearn more about Container lifecycle hooks.Learn more about the lifecycle of a Pod.ReferenceLifecycleContainerSee terminationGracePeriodSeconds in PodSpec4.3.21 - Configure a Pod to Use a ConfigMapMany applications rely on configuration which is used during either application initialization or runtime. Most times, there is a requirement to adjust values assigned to configuration parameters. ConfigMaps are a Kubernetes mechanism that let you inject configuration data into application pods.The ConfigMap concept allow you to decouple configuration artifacts from image content to keep containerized applications portable. For example, you can download and run the same container image to spin up containers for the purposes of local development, system test, or running a live end-user workload.This page provides a series of usage examples demonstrating how to create ConfigMaps and configure Pods using data stored in ConfigMaps.Before you beginYou need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesYou need to have the wget tool installed. If you have a different tool such as curl, and you do not have wget, you will need to adapt the step that downloads example data.Create a ConfigMapYou can use either kubectl create configmap or a ConfigMap generator in kustomization.yaml to create a ConfigMap.Create a ConfigMap using kubectl create configmapUse the kubectl create configmap command to create ConfigMaps from directories, files, or literal values:kubectl create configmap  where  is the name you want to assign to the ConfigMap and  is the directory, file, or literal value to draw the data from. The name of a ConfigMap object must be a valid DNS subdomain name.When you are creating a ConfigMap based on a file, the key in the  defaults to the basename of the file, and the value defaults to the file content.You can use kubectl describe or kubectl get to retrieve information about a ConfigMap.Create a ConfigMap from a directoryYou can use kubectl create configmap to create a ConfigMap from multiple files in the same directory. When you are creating a ConfigMap based on a directory, kubectl identifies files whose filename is a valid key in the directory and packages each of those files into the new ConfigMap. Any directory entries except regular files are ignored (for example: subdirectories, symlinks, devices, pipes, and more).Note:Each filename being used for ConfigMap creation must consist of only acceptable characters, which are: letters (A to Z and a to z), digits (0 to 9), '-', '_', or '.'. If you use kubectl create configmap with a directory where any of the file names contains an unacceptable character, the kubectl command may fail.The kubectl command does not print an error when it encounters an invalid filename.Create the local directory:mkdir -p configure-pod-container/configmap/ Now, download the sample configuration and create the ConfigMap:# Download the sample files into `configure-pod-container/configmap/` directory wget https://kubernetes.io/examples/configmap/game.properties -O configure-pod-container/configmap/game.properties wget https://kubernetes.io/examples/configmap/ui.properties -O configure-pod-container/configmap/ui.properties # Create the ConfigMap kubectl create configmap game-config --from-file=configure-pod-container/configmap/ The above command packages each file, in this case, game.properties and ui.properties in the configure-pod-container/configmap/ directory into the game-config ConfigMap. You can display details of the ConfigMap using the following command:kubectl describe configmaps game-config The output is similar to this:Name: game-config Namespace: default Labels:  Annotations:  Data ==== game.properties: \---- enemies=aliens lives=3 enemies.cheat=true enemies.cheat.level=noGoodRotten secret.code.passphrase=UUDDLRLRBABAS secret.code.allowed=true secret.code.lives=30 ui.properties: \---- color.good=purple color.bad=yellow allow.textmode=true how.nice.to.look=fairlyNice The game.properties and ui.properties files in the configure-pod-container/configmap/ directory are represented in the data section of the ConfigMap.kubectl get configmaps game-config -o yaml The output is similar to this:apiVersion: v1 kind: ConfigMap metadata: creationTimestamp: 2022-02-18T18:52:05Z name: game-config namespace: default resourceVersion: "516" uid: b4952dc3-d670-11e5-8cd0-68f728db1985 data: game.properties: | enemies=aliens lives=3 enemies.cheat=true enemies.cheat.level=noGoodRotten secret.code.passphrase=UUDDLRLRBABAS secret.code.allowed=true secret.code.lives=30 ui.properties: | color.good=purple color.bad=yellow allow.textmode=true how.nice.to.look=fairlyNice Create ConfigMaps from filesYou can use kubectl create configmap to create a ConfigMap from an individual file, or from multiple files.For example,kubectl create configmap game-config-2 --from-file=configure-pod-container/configmap/game.properties would produce the following ConfigMap:kubectl describe configmaps game-config-2 where the output is similar to this:Name: game-config-2 Namespace: default Labels:  Annotations:  Data ==== game.properties: \---- enemies=aliens lives=3 enemies.cheat=true enemies.cheat.level=noGoodRotten secret.code.passphrase=UUDDLRLRBABAS secret.code.allowed=true secret.code.lives=30 You can pass in the --from-file argument multiple times to create a ConfigMap from multiple data sources.kubectl create configmap game-config-2 --from-file=configure-pod-container/configmap/game.properties --from-file=configure-pod-container/configmap/ui.properties You can display details of the game-config-2 ConfigMap using the following command:kubectl describe configmaps game-config-2 The output is similar to this:Name: game-config-2 Namespace: default Labels:  Annotations:  Data ==== game.properties: \---- enemies=aliens lives=3 enemies.cheat=true enemies.cheat.level=noGoodRotten secret.code.passphrase=UUDDLRLRBABAS secret.code.allowed=true secret.code.lives=30 ui.properties: \---- color.good=purple color.bad=yellow allow.textmode=true how.nice.to.look=fairlyNice Use the option --from-env-file to create a ConfigMap from an env-file, for example:# Env-files contain a list of environment variables. # These syntax rules apply: # Each line in an env file has to be in VAR=VAL format. # Lines beginning with # (i.e. comments) are ignored. # Blank lines are ignored. # There is no special handling of quotation marks (i.e. they will be part of the ConfigMap value)). # Download the sample files into `configure-pod-container/configmap/` directory wget https://kubernetes.io/examples/configmap/game-env-file.properties -O configure-pod-container/configmap/game-env-file.properties wget https://kubernetes.io/examples/configmap/ui-env-file.properties -O configure-pod-container/configmap/ui-env-file.properties # The env-file `game-env-file.properties` looks like below cat configure-pod-container/configmap/game-env-file.properties enemies=aliens lives=3 allowed="true" # This comment and the empty line above it are ignored kubectl create configmap game-config-env-file \ \--from-env-file=configure-pod-container/configmap/game-env-file.properties would produce a ConfigMap. View the ConfigMap:kubectl get configmap game-config-env-file -o yaml the output is similar to:apiVersion: v1 kind: ConfigMap metadata: creationTimestamp: 2019-12-27T18:36:28Z name: game-config-env-file namespace: default resourceVersion: "809965" uid: d9d1ca5b-eb34-11e7-887b-42010a8002b8 data: allowed: '"true"' enemies: aliens lives: "3" Starting with Kubernetes v1.23, kubectl supports the --from-env-file argument to be specified multiple times to create a ConfigMap from multiple data sources.kubectl create configmap config-multi-env-files \ \--from-env-file=configure-pod-container/configmap/game-env-file.properties \ \--from-env-file=configure-pod-container/configmap/ui-env-file.properties would produce the following ConfigMap:kubectl get configmap config-multi-env-files -o yaml where the output is similar to this:apiVersion: v1 kind: ConfigMap metadata: creationTimestamp: 2019-12-27T18:38:34Z name: config-multi-env-files namespace: default resourceVersion: "810136" uid: 252c4572-eb35-11e7-887b-42010a8002b8 data: allowed: '"true"' color: purple enemies: aliens how: fairlyNice lives: "3" textmode: "true" Define the key to use when creating a ConfigMap from a fileYou can define a key other than the file name to use in the data section of your ConfigMap when using the --from-file argument:kubectl create configmap game-config-3 --from-file== where  is the key you want to use in the ConfigMap and  is the location of the data source file you want the key to represent.For example:kubectl create configmap game-config-3 --from-file=game-special-key=configure-pod-container/configmap/game.properties would produce the following ConfigMap:kubectl get configmaps game-config-3 -o yaml where the output is similar to this:apiVersion: v1 kind: ConfigMap metadata: creationTimestamp: 2022-02-18T18:54:22Z name: game-config-3 namespace: default resourceVersion: "530" uid: 05f8da22-d671-11e5-8cd0-68f728db1985 data: game-special-key: | enemies=aliens lives=3 enemies.cheat=true enemies.cheat.level=noGoodRotten secret.code.passphrase=UUDDLRLRBABAS secret.code.allowed=true secret.code.lives=30 Create ConfigMaps from literal valuesYou can use kubectl create configmap with the --from-literal argument to define a literal value from the command line:kubectl create configmap special-config --from-literal=special.how=very --from-literal=special.type=charm You can pass in multiple key-value pairs. Each pair provided on the command line is represented as a separate entry in the data section of the ConfigMap.kubectl get configmaps special-config -o yaml The output is similar to this:apiVersion: v1 kind: ConfigMap metadata: creationTimestamp: 2022-02-18T19:14:38Z name: special-config namespace: default resourceVersion: "651" uid: dadce046-d673-11e5-8cd0-68f728db1985 data: special.how: very special.type: charm Create a ConfigMap from generatorYou can also create a ConfigMap from generators and then apply it to create the object in the cluster's API server. You should specify the generators in a kustomization.yaml file within a directory.Generate ConfigMaps from filesFor example, to generate a ConfigMap from files configure-pod-container/configmap/game.properties# Create a kustomization.yaml file with ConfigMapGenerator cat <./kustomization.yaml configMapGenerator: \- name: game-config-4 options: labels: game-config: config-4 files: \- configure-pod-container/configmap/game.properties EOF Apply the kustomization directory to create the ConfigMap object:kubectl apply -k . configmap/game-config-4-m9dm2f92bt created You can check that the ConfigMap was created like this:kubectl get configmap NAME DATA AGE game-config-4-m9dm2f92bt 1 37s and also:kubectl describe configmaps/game-config-4-m9dm2f92bt Name: game-config-4-m9dm2f92bt Namespace: default Labels: game-config=config-4 Annotations: kubectl.kubernetes.io/last-applied-configuration: {"apiVersion":"v1","data":{"game.properties":"enemies=aliens\nlives=3\nenemies.cheat=true\nenemies.cheat.level=noGoodRotten\nsecret.code.p... Data ==== game.properties: \---- enemies=aliens lives=3 enemies.cheat=true enemies.cheat.level=noGoodRotten secret.code.passphrase=UUDDLRLRBABAS secret.code.allowed=true secret.code.lives=30 Events:  Notice that the generated ConfigMap name has a suffix appended by hashing the contents. This ensures that a new ConfigMap is generated each time the content is modified.Define the key to use when generating a ConfigMap from a fileYou can define a key other than the file name to use in the ConfigMap generator. For example, to generate a ConfigMap from files configure-pod-container/configmap/game.properties with the key game-special-key# Create a kustomization.yaml file with ConfigMapGenerator cat <./kustomization.yaml configMapGenerator: \- name: game-config-5 options: labels: game-config: config-5 files: \- game-special-key=configure-pod-container/configmap/game.properties EOF Apply the kustomization directory to create the ConfigMap object.kubectl apply -k . configmap/game-config-5-m67dt67794 created Generate ConfigMaps from literalsThis example shows you how to create a ConfigMap from two literal key/value pairs: special.type=charm and special.how=very, using Kustomize and kubectl. To achieve this, you can specify the ConfigMap generator. Create (or replace) kustomization.yaml so that it has the following contents:--- # kustomization.yaml contents for creating a ConfigMap from literals configMapGenerator: \- name: special-config-2 literals: \- special.how=very \- special.type=charm Apply the kustomization directory to create the ConfigMap object:kubectl apply -k . configmap/special-config-2-c92b5mmcf2 created Interim cleanupBefore proceeding, clean up some of the ConfigMaps you made:kubectl delete configmap special-config kubectl delete configmap env-config kubectl delete configmap -l 'game-config in (config-4,config-5)' Now that you have learned to define ConfigMaps, you can move on to the next section, and learn how to use these objects with Pods.Define container environment variables using ConfigMap dataDefine a container environment variable with data from a single ConfigMapDefine an environment variable as a key-value pair in a ConfigMap:kubectl create configmap special-config --from-literal=special.how=very Assign the special.how value defined in the ConfigMap to the SPECIAL_LEVEL_KEY environment variable in the Pod specification.pods/pod-single-configmap-env-variable.yaml apiVersion: v1 kind: Pod metadata: name: dapi-test-pod spec: containers: \- name: test-container image: registry.k8s.io/busybox:1.27.2 command: [ "/bin/sh", "-c", "env" ] env: # Define the environment variable \- name: SPECIAL_LEVEL_KEY valueFrom: configMapKeyRef: # The ConfigMap containing the value you want to assign to SPECIAL_LEVEL_KEY name: special-config # Specify the key associated with the value key: special.how restartPolicy: Never Create the Pod:kubectl create -f https://kubernetes.io/examples/pods/pod-single-configmap-env-variable.yaml Now, the Pod's output includes environment variable SPECIAL_LEVEL_KEY=very.Define container environment variables with data from multiple ConfigMapsAs with the previous example, create the ConfigMaps first. Here is the manifest you will use:configmap/configmaps.yaml apiVersion: v1 kind: ConfigMap metadata: name: special-config namespace: default data: special.how: very \--- apiVersion: v1 kind: ConfigMap metadata: name: env-config namespace: default data: log_level: INFO Create the ConfigMap:kubectl create -f https://kubernetes.io/examples/configmap/configmaps.yaml Define the environment variables in the Pod specification.pods/pod-multiple-configmap-env-variable.yaml apiVersion: v1 kind: Pod metadata: name: dapi-test-pod spec: containers: \- name: test-container image: registry.k8s.io/busybox:1.27.2 command: [ "/bin/sh", "-c", "env" ] env: \- name: SPECIAL_LEVEL_KEY valueFrom: configMapKeyRef: name: special-config key: special.how \- name: LOG_LEVEL valueFrom: configMapKeyRef: name: env-config key: log_level restartPolicy: Never Create the Pod:kubectl create -f https://kubernetes.io/examples/pods/pod-multiple-configmap-env-variable.yaml Now, the Pod's output includes environment variables SPECIAL_LEVEL_KEY=very and LOG_LEVEL=INFO.Once you're happy to move on, delete that Pod and ConfigMap:kubectl delete pod dapi-test-pod --now kubectl delete configmap special-config kubectl delete configmap env-config Configure all key-value pairs in a ConfigMap as container environment variablesCreate a ConfigMap containing multiple key-value pairs.configmap/configmap-multikeys.yaml apiVersion: v1 kind: ConfigMap metadata: name: special-config namespace: default data: SPECIAL_LEVEL: very SPECIAL_TYPE: charm Create the ConfigMap:kubectl create -f https://kubernetes.io/examples/configmap/configmap-multikeys.yaml Use envFrom to define all of the ConfigMap's data as container environment variables. The key from the ConfigMap becomes the environment variable name in the Pod.pods/pod-configmap-envFrom.yaml apiVersion: v1 kind: Pod metadata: name: dapi-test-pod spec: containers: \- name: test-container image: registry.k8s.io/busybox:1.27.2 command: [ "/bin/sh", "-c", "env" ] envFrom: \- configMapRef: name: special-config restartPolicy: Never Create the Pod:kubectl create -f https://kubernetes.io/examples/pods/pod-configmap-envFrom.yaml Now, the Pod's output includes environment variables SPECIAL_LEVEL=very and SPECIAL_TYPE=charm.Once you're happy to move on, delete that Pod:kubectl delete pod dapi-test-pod --now Use ConfigMap-defined environment variables in Pod commandsYou can use ConfigMap-defined environment variables in the command and args of a container using the $(VAR_NAME) Kubernetes substitution syntax.For example, the following Pod manifest:pods/pod-configmap-env-var-valueFrom.yaml apiVersion: v1 kind: Pod metadata: name: dapi-test-pod spec: containers: \- name: test-container image: registry.k8s.io/busybox:1.27.2 command: [ "/bin/echo", "$(SPECIAL_LEVEL_KEY) $(SPECIAL_TYPE_KEY)" ] env: \- name: SPECIAL_LEVEL_KEY valueFrom: configMapKeyRef: name: special-config key: SPECIAL_LEVEL \- name: SPECIAL_TYPE_KEY valueFrom: configMapKeyRef: name: special-config key: SPECIAL_TYPE restartPolicy: Never Create that Pod, by running:kubectl create -f https://kubernetes.io/examples/pods/pod-configmap-env-var-valueFrom.yaml That pod produces the following output from the test-container container:kubectl logs dapi-test-pod very charm Once you're happy to move on, delete that Pod:kubectl delete pod dapi-test-pod --now Add ConfigMap data to a VolumeAs explained in Create ConfigMaps from files, when you create a ConfigMap using --from-file, the filename becomes a key stored in the data section of the ConfigMap. The file contents become the key's value.The examples in this section refer to a ConfigMap named special-config:configmap/configmap-multikeys.yaml apiVersion: v1 kind: ConfigMap metadata: name: special-config namespace: default data: SPECIAL_LEVEL: very SPECIAL_TYPE: charm Create the ConfigMap:kubectl create -f https://kubernetes.io/examples/configmap/configmap-multikeys.yaml Populate a Volume with data stored in a ConfigMapAdd the ConfigMap name under the volumes section of the Pod specification. This adds the ConfigMap data to the directory specified as volumeMounts.mountPath (in this case, /etc/config). The command section lists directory files with names that match the keys in ConfigMap.pods/pod-configmap-volume.yaml apiVersion: v1 kind: Pod metadata: name: dapi-test-pod spec: containers: \- name: test-container image: registry.k8s.io/busybox:1.27.2 command: [ "/bin/sh", "-c", "ls /etc/config/" ] volumeMounts: \- name: config-volume mountPath: /etc/config volumes: \- name: config-volume configMap: # Provide the name of the ConfigMap containing the files you want # to add to the container name: special-config restartPolicy: Never Create the Pod:kubectl create -f https://kubernetes.io/examples/pods/pod-configmap-volume.yaml When the pod runs, the command ls /etc/config/ produces the output below:SPECIAL_LEVEL SPECIAL_TYPE Text data is exposed as files using the UTF-8 character encoding. To use some other character encoding, use binaryData (see ConfigMap object for more details).Note:If there are any files in the /etc/config directory of that container image, the volume mount will make those files from the image inaccessible.Once you're happy to move on, delete that Pod:kubectl delete pod dapi-test-pod --now Add ConfigMap data to a specific path in the VolumeUse the path field to specify the desired file path for specific ConfigMap items. In this case, the SPECIAL_LEVEL item will be mounted in the config-volume volume at /etc/config/keys.pods/pod-configmap-volume-specific-key.yaml apiVersion: v1 kind: Pod metadata: name: dapi-test-pod spec: containers: \- name: test-container image: registry.k8s.io/busybox:1.27.2 command: [ "/bin/sh","-c","cat /etc/config/keys" ] volumeMounts: \- name: config-volume mountPath: /etc/config volumes: \- name: config-volume configMap: name: special-config items: \- key: SPECIAL_LEVEL path: keys restartPolicy: Never Create the Pod:kubectl create -f https://kubernetes.io/examples/pods/pod-configmap-volume-specific-key.yaml When the pod runs, the command cat /etc/config/keys produces the output below:very Caution:Like before, all previous files in the /etc/config/ directory will be deleted.Delete that Pod:kubectl delete pod dapi-test-pod --now Project keys to specific paths and file permissionsYou can project keys to specific paths. Refer to the corresponding section in the Secrets guide for the syntax.You can set POSIX permissions for keys. Refer to the corresponding section in the Secrets guide for the syntax.Optional referencesA ConfigMap reference may be marked optional. If the ConfigMap is non-existent, the mounted volume will be empty. If the ConfigMap exists, but the referenced key is non-existent, the path will be absent beneath the mount point. See Optional ConfigMaps for more details.Mounted ConfigMaps are updated automaticallyWhen a mounted ConfigMap is updated, the projected content is eventually updated too. This applies in the case where an optionally referenced ConfigMap comes into existence after a pod has started.Kubelet checks whether the mounted ConfigMap is fresh on every periodic sync. However, it uses its local TTL-based cache for getting the current value of the ConfigMap. As a result, the total delay from the moment when the ConfigMap is updated to the moment when new keys are projected to the pod can be as long as kubelet sync period (1 minute by default) + TTL of ConfigMaps cache (1 minute by default) in kubelet. You can trigger an immediate refresh by updating one of the pod's annotations.Note:A container using a ConfigMap as a subPath volume will not receive ConfigMap updates.Understanding ConfigMaps and PodsThe ConfigMap API resource stores configuration data as key-value pairs. The data can be consumed in pods or provide the configurations for system components such as controllers. ConfigMap is similar to Secrets, but provides a means of working with strings that don't contain sensitive information. Users and system components alike can store configuration data in ConfigMap.Note:ConfigMaps should reference properties files, not replace them. Think of the ConfigMap as representing something similar to the Linux /etc directory and its contents. For example, if you create a Kubernetes Volume from a ConfigMap, each data item in the ConfigMap is represented by an individual file in the volume.The ConfigMap's data field contains the configuration data. As shown in the example below, this can be simple (like individual properties defined using --from-literal) or complex (like configuration files or JSON blobs defined using --from-file).apiVersion: v1 kind: ConfigMap metadata: creationTimestamp: 2016-02-18T19:14:38Z name: example-config namespace: default data: # example of a simple property defined using --from-literal example.property.1: hello example.property.2: world # example of a complex property defined using --from-file example.property.file: |- property.1=value-1 property.2=value-2 property.3=value-3 When kubectl creates a ConfigMap from inputs that are not ASCII or UTF-8, the tool puts these into the binaryData field of the ConfigMap, and not in data. Both text and binary data sources can be combined in one ConfigMap.If you want to view the binaryData keys (and their values) in a ConfigMap, you can run kubectl get configmap -o jsonpath='{.binaryData}' .Pods can load data from a ConfigMap that uses either data or binaryData.Optional ConfigMapsYou can mark a reference to a ConfigMap as optional in a Pod specification. If the ConfigMap doesn't exist, the configuration for which it provides data in the Pod (for example: environment variable, mounted volume) will be empty. If the ConfigMap exists, but the referenced key is non-existent the data is also empty.For example, the following Pod specification marks an environment variable from a ConfigMap as optional:apiVersion: v1 kind: Pod metadata: name: dapi-test-pod spec: containers: \- name: test-container image: gcr.io/google_containers/busybox command: ["/bin/sh", "-c", "env"] env: \- name: SPECIAL_LEVEL_KEY valueFrom: configMapKeyRef: name: a-config key: akey optional: true # mark the variable as optional restartPolicy: Never If you run this pod, and there is no ConfigMap named a-config, the output is empty. If you run this pod, and there is a ConfigMap named a-config but that ConfigMap doesn't have a key named akey, the output is also empty. If you do set a value for akey in the a-config ConfigMap, this pod prints that value and then terminates.You can also mark the volumes and files provided by a ConfigMap as optional. Kubernetes always creates the mount paths for the volume, even if the referenced ConfigMap or key doesn't exist. For example, the following Pod specification marks a volume that references a ConfigMap as optional:apiVersion: v1 kind: Pod metadata: name: dapi-test-pod spec: containers: \- name: test-container image: gcr.io/google_containers/busybox command: ["/bin/sh", "-c", "ls /etc/config"] volumeMounts: \- name: config-volume mountPath: /etc/config volumes: \- name: config-volume configMap: name: no-config optional: true # mark the source ConfigMap as optional restartPolicy: Never RestrictionsYou must create the ConfigMap object before you reference it in a Pod specification. Alternatively, mark the ConfigMap reference as optional in the Pod spec (see Optional ConfigMaps). If you reference a ConfigMap that doesn't exist and you don't mark the reference as optional, the Pod won't start. Similarly, references to keys that don't exist in the ConfigMap will also prevent the Pod from starting, unless you mark the key references as optional.If you use envFrom to define environment variables from ConfigMaps, keys that are considered invalid will be skipped. The pod will be allowed to start, but the invalid names will be recorded in the event log (InvalidVariableNames). The log message lists each skipped key. For example:kubectl get events The output is similar to this:LASTSEEN FIRSTSEEN COUNT NAME KIND SUBOBJECT TYPE REASON SOURCE MESSAGE 0s 0s 1 dapi-test-pod Pod Warning InvalidEnvironmentVariableNames {kubelet, 127.0.0.1} Keys [1badkey, 2alsobad] from the EnvFrom configMap default/myconfig were skipped since they are considered invalid environment variable names. ConfigMaps reside in a specific Namespace. Pods can only refer to ConfigMaps that are in the same namespace as the Pod.You can't use ConfigMaps for static pods, because the kubelet does not support this.Cleaning upDelete the ConfigMaps and Pods that you made:kubectl delete configmaps/game-config configmaps/game-config-2 configmaps/game-config-3 \ configmaps/game-config-env-file kubectl delete pod dapi-test-pod --now # You might already have removed the next set kubectl delete configmaps/special-config configmaps/env-config kubectl delete configmap -l 'game-config in (config-4,config-5)' Remove the kustomization.yaml file that you used to generate the ConfigMap:rm kustomization.yaml If you created a directory configure-pod-container and no longer need it, you should remove that too, or move it into the trash can / deleted files location.rm -r configure-pod-container What's nextFollow a real world example of Configuring Redis using a ConfigMap.Follow an example of Updating configuration via a ConfigMap.4.3.22 - Share Process Namespace between Containers in a PodThis page shows how to configure process namespace sharing for a pod. When process namespace sharing is enabled, processes in a container are visible to all other containers in the same pod.You can use this feature to configure cooperating containers, such as a log handler sidecar container, or to troubleshoot container images that don't include debugging utilities like a shell.Before you beginYou need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesConfigure a PodProcess namespace sharing is enabled using the shareProcessNamespace field of .spec for a Pod. For example:pods/share-process-namespace.yaml apiVersion: v1 kind: Pod metadata: name: nginx spec: shareProcessNamespace: true containers: \- name: nginx image: nginx \- name: shell image: busybox:1.28 command: ["sleep", "3600"] securityContext: capabilities: add: \- SYS_PTRACE stdin: true tty: true Create the pod nginx on your cluster:kubectl apply -f https://k8s.io/examples/pods/share-process-namespace.yaml Attach to the shell container and run ps:kubectl exec -it nginx -c shell -- /bin/sh If you don't see a command prompt, try pressing enter. In the container shell:# run this inside the "shell" container ps ax The output is similar to this:PID USER TIME COMMAND 1 root 0:00 /pause 8 root 0:00 nginx: master process nginx -g daemon off; 14 101 0:00 nginx: worker process 15 root 0:00 sh 21 root 0:00 ps ax You can signal processes in other containers. For example, send SIGHUP to nginx to restart the worker process. This requires the SYS_PTRACE capability.# run this inside the "shell" container kill -HUP 8 # change "8" to match the PID of the nginx leader process, if necessary ps ax The output is similar to this:PID USER TIME COMMAND 1 root 0:00 /pause 8 root 0:00 nginx: master process nginx -g daemon off; 15 root 0:00 sh 22 101 0:00 nginx: worker process 23 root 0:00 ps ax It's even possible to access the file system of another container using the /proc/$pid/root link.# run this inside the "shell" container # change "8" to the PID of the Nginx process, if necessary head /proc/8/root/etc/nginx/nginx.conf The output is similar to this:user nginx; worker_processes 1; error_log /var/log/nginx/error.log warn; pid /var/run/nginx.pid; events { worker_connections 1024; Understanding process namespace sharingPods share many resources so it makes sense they would also share a process namespace. Some containers may expect to be isolated from others, though, so it's important to understand the differences:The container process no longer has PID 1. Some containers refuse to start without PID 1 (for example, containers using systemd) or run commands like kill -HUP 1 to signal the container process. In pods with a shared process namespace, kill -HUP 1 will signal the pod sandbox (/pause in the above example).Processes are visible to other containers in the pod. This includes all information visible in /proc, such as passwords that were passed as arguments or environment variables. These are protected only by regular Unix permissions.Container filesystems are visible to other containers in the pod through the /proc/$pid/root link. This makes debugging easier, but it also means that filesystem secrets are protected only by filesystem permissions.4.3.23 - Use a User Namespace With a PodFEATURE STATE: Kubernetes v1.30 [beta] (enabled by default: false)This page shows how to configure a user namespace for pods. This allows you to isolate the user running inside the container from the one in the host.A process running as root in a container can run as a different (non-root) user in the host; in other words, the process has full privileges for operations inside the user namespace, but is unprivileged for operations outside the namespace.You can use this feature to reduce the damage a compromised container can do to the host or other pods in the same node. There are several security vulnerabilities rated either HIGH or CRITICAL that were not exploitable when user namespaces is active. It is expected user namespace will mitigate some future vulnerabilities too.Without using a user namespace a container running as root, in the case of a container breakout, has root privileges on the node. And if some capability were granted to the container, the capabilities are valid on the host too. None of this is true when user namespaces are used.Before you beginYou need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesYour Kubernetes server must be at or later than version v1.25.To check the version, enter kubectl version.🛇 This item links to a third party project or product that is not part of Kubernetes itself. More informationThe node OS needs to be LinuxYou need to exec commands in the hostYou need to be able to exec into podsYou need to enable the UserNamespacesSupport feature gateNote:The feature gate to enable user namespaces was previously named UserNamespacesStatelessPodsSupport, when only stateless pods were supported. Only Kubernetes v1.25 through to v1.27 recognise UserNamespacesStatelessPodsSupport.The cluster that you're using must include at least one node that meets the requirements for using user namespaces with Pods.If you have a mixture of nodes and only some of the nodes provide user namespace support for Pods, you also need to ensure that the user namespace Pods are scheduled to suitable nodes.Run a Pod that uses a user namespaceA user namespace for a pod is enabled setting the hostUsers field of .spec to false. For example:pods/user-namespaces-stateless.yaml apiVersion: v1 kind: Pod metadata: name: userns spec: hostUsers: false containers: \- name: shell command: ["sleep", "infinity"] image: debian Create the pod on your cluster:kubectl apply -f https://k8s.io/examples/pods/user-namespaces-stateless.yaml Add a debugging container and attach to it and run readlink /proc/self/ns/user:kubectl debug userns -it --image=busybox Run this command:readlink /proc/self/ns/user The output is similar to:user:[4026531837] Also run:cat /proc/self/uid_map The output is similar to:0 833617920 65536 Then, open a shell in the host and run the same commands.The readlink command shows the user namespace the process is running in. It should be different when it is run on the host and inside the container.The last number of the uid_map file inside the container must be 65536, on the host it must be a bigger number.If you are running the kubelet inside a user namespace, you need to compare the output from running the command in the pod to the output of running in the host:readlink /proc/$pid/ns/user replacing $pid with the kubelet PID.4.3.24 - Use an Image Volume With a PodFEATURE STATE: Kubernetes v1.33 [beta] (enabled by default: false)This page shows how to configure a pod using image volumes. This allows you to mount content from OCI registries inside containers.Before you beginYou need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesYour Kubernetes server must be at or later than version v1.31.To check the version, enter kubectl version.The container runtime needs to support the image volumes featureYou need to exec commands in the hostYou need to be able to exec into podsYou need to enable the ImageVolume feature gateRun a Pod that uses an image volumeAn image volume for a pod is enabled by setting the volumes.[*].image field of .spec to a valid reference and consuming it in the volumeMounts of the container. For example:pods/image-volumes.yaml apiVersion: v1 kind: Pod metadata: name: image-volume spec: containers: \- name: shell command: ["sleep", "infinity"] image: debian volumeMounts: \- name: volume mountPath: /volume volumes: \- name: volume image: reference: quay.io/crio/artifact:v2 pullPolicy: IfNotPresent Create the pod on your cluster:kubectl apply -f https://k8s.io/examples/pods/image-volumes.yaml Attach to the container:kubectl attach -it image-volume bash Check the content of a file in the volume:cat /volume/dir/file The output is similar to:1 You can also check another file in a different path:cat /volume/file The output is similar to:2 Use subPath (or subPathExpr)It is possible to utilize subPath or subPathExpr from Kubernetes v1.33 when using the image volume feature.pods/image-volumes-subpath.yaml apiVersion: v1 kind: Pod metadata: name: image-volume spec: containers: \- name: shell command: ["sleep", "infinity"] image: debian volumeMounts: \- name: volume mountPath: /volume subPath: dir volumes: \- name: volume image: reference: quay.io/crio/artifact:v2 pullPolicy: IfNotPresent Create the pod on your cluster:kubectl apply -f https://k8s.io/examples/pods/image-volumes-subpath.yaml Attach to the container:kubectl attach -it image-volume bash Check the content of the file from the dir sub path in the volume:cat /volume/file The output is similar to:1 Further readingimage volumes4.3.25 - Create static PodsStatic Pods are managed directly by the kubelet daemon on a specific node, without the API server observing them. Unlike Pods that are managed by the control plane (for example, a Deployment); instead, the kubelet watches each static Pod (and restarts it if it fails).Static Pods are always bound to one Kubelet on a specific node.The kubelet automatically tries to create a mirror Pod on the Kubernetes API server for each static Pod. This means that the Pods running on a node are visible on the API server, but cannot be controlled from there. The Pod names will be suffixed with the node hostname with a leading hyphen.Note:If you are running clustered Kubernetes and are using static Pods to run a Pod on every node, you should probably be using a DaemonSet instead.Note:The spec of a static Pod cannot refer to other API objects (e.g., ServiceAccount, ConfigMap, Secret, etc).Note:Static pods do not support ephemeral containers.Before you beginYou need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesTo check the version, enter kubectl version.This page assumes you're using CRI-O to run Pods, and that your nodes are running the Fedora operating system. Instructions for other distributions or Kubernetes installations may vary.Create a static podYou can configure a static Pod with either a file system hosted configuration file or a web hosted configuration file.Filesystem-hosted static Pod manifestManifests are standard Pod definitions in JSON or YAML format in a specific directory. Use the staticPodPath:  field in the kubelet configuration file, which periodically scans the directory and creates/deletes static Pods as YAML/JSON files appear/disappear there. Note that the kubelet will ignore files starting with dots when scanning the specified directory.For example, this is how to start a simple web server as a static Pod:Choose a node where you want to run the static Pod. In this example, it's my-node1.ssh my-node1 Choose a directory, say /etc/kubernetes/manifests and place a web server Pod definition there, for example /etc/kubernetes/manifests/static-web.yaml:# Run this command on the node where kubelet is running mkdir -p /etc/kubernetes/manifests/ cat </etc/kubernetes/manifests/static-web.yaml apiVersion: v1 kind: Pod metadata: name: static-web labels: role: myrole spec: containers: \- name: web image: nginx ports: \- name: web containerPort: 80 protocol: TCP EOF Configure the kubelet on that node to set a staticPodPath value in the kubelet configuration file.See Set Kubelet Parameters Via A Configuration File for more information.An alternative and deprecated method is to configure the kubelet on that node to look for static Pod manifests locally, using a command line argument. To use the deprecated approach, start the kubelet with the--pod-manifest-path=/etc/kubernetes/manifests/ argument.Restart the kubelet. On Fedora, you would run:# Run this command on the node where the kubelet is running systemctl restart kubelet Web-hosted static pod manifestKubelet periodically downloads a file specified by --manifest-url= argument and interprets it as a JSON/YAML file that contains Pod definitions. Similar to how filesystem-hosted manifests work, the kubelet refetches the manifest on a schedule. If there are changes to the list of static Pods, the kubelet applies them.To use this approach:Create a YAML file and store it on a web server so that you can pass the URL of that file to the kubelet.apiVersion: v1 kind: Pod metadata: name: static-web labels: role: myrole spec: containers: \- name: web image: nginx ports: \- name: web containerPort: 80 protocol: TCP Configure the kubelet on your selected node to use this web manifest by running it with --manifest-url=. On Fedora, edit /etc/kubernetes/kubelet to include this line:KUBELET_ARGS="--cluster-dns=10.254.0.10 --cluster-domain=kube.local --manifest-url=" Restart the kubelet. On Fedora, you would run:# Run this command on the node where the kubelet is running systemctl restart kubelet Observe static pod behaviorWhen the kubelet starts, it automatically starts all defined static Pods. As you have defined a static Pod and restarted the kubelet, the new static Pod should already be running.You can view running containers (including static Pods) by running (on the node):# Run this command on the node where the kubelet is running crictl ps The output might be something like:CONTAINER IMAGE CREATED STATE NAME ATTEMPT POD ID 129fd7d382018 docker.io/library/nginx@sha256:... 11 minutes ago Running web 0 34533c6729106 Note:crictl outputs the image URI and SHA-256 checksum. NAME will look more like: docker.io/library/nginx@sha256:0d17b565c37bcbd895e9d92315a05c1c3c9a29f762b011a10c54a66cd53c9b31.You can see the mirror Pod on the API server:kubectl get pods NAME READY STATUS RESTARTS AGE static-web-my-node1 1/1 Running 0 2m Note:Make sure the kubelet has permission to create the mirror Pod in the API server. If not, the creation request is rejected by the API server.Labels from the static Pod are propagated into the mirror Pod. You can use those labels as normal via selectors, etc.If you try to use kubectl to delete the mirror Pod from the API server, the kubelet doesn't remove the static Pod:kubectl delete pod static-web-my-node1 pod "static-web-my-node1" deleted You can see that the Pod is still running:kubectl get pods NAME READY STATUS RESTARTS AGE static-web-my-node1 1/1 Running 0 4s Back on your node where the kubelet is running, you can try to stop the container manually. You'll see that, after a time, the kubelet will notice and will restart the Pod automatically:# Run these commands on the node where the kubelet is running crictl stop 129fd7d382018 # replace with the ID of your container sleep 20 crictl ps CONTAINER IMAGE CREATED STATE NAME ATTEMPT POD ID 89db4553e1eeb docker.io/library/nginx@sha256:... 19 seconds ago Running web 1 34533c6729106 Once you identify the right container, you can get the logs for that container with crictl:# Run these commands on the node where the container is running crictl logs  10.240.0.48 - - [16/Nov/2022:12:45:49 +0000] "GET / HTTP/1.1" 200 612 "-" "curl/7.47.0" "-" 10.240.0.48 - - [16/Nov/2022:12:45:50 +0000] "GET / HTTP/1.1" 200 612 "-" "curl/7.47.0" "-" 10.240.0.48 - - [16/Nove/2022:12:45:51 +0000] "GET / HTTP/1.1" 200 612 "-" "curl/7.47.0" "-" To find more about how to debug using crictl, please visit Debugging Kubernetes nodes with crictl.Dynamic addition and removal of static podsThe running kubelet periodically scans the configured directory (/etc/kubernetes/manifests in our example) for changes and adds/removes Pods as files appear/disappear in this directory.# This assumes you are using filesystem-hosted static Pod configuration # Run these commands on the node where the container is running # mv /etc/kubernetes/manifests/static-web.yaml /tmp sleep 20 crictl ps # You see that no nginx container is running mv /tmp/static-web.yaml /etc/kubernetes/manifests/ sleep 20 crictl ps CONTAINER IMAGE CREATED STATE NAME ATTEMPT POD ID f427638871c35 docker.io/library/nginx@sha256:... 19 seconds ago Running web 1 34533c6729106 What's nextGenerate static Pod manifests for control plane componentsGenerate static Pod manifest for local etcdDebugging Kubernetes nodes with crictlLearn more about crictl.Map docker CLI commands to crictl.Set up etcd instances as static pods managed by a kubelet4.3.26 - Translate a Docker Compose File to Kubernetes ResourcesWhat's Kompose? It's a conversion tool for all things compose (namely Docker Compose) to container orchestrators (Kubernetes or OpenShift).More information can be found on the Kompose website at http://kompose.io.Before you beginYou need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesTo check the version, enter kubectl version.Install KomposeWe have multiple ways to install Kompose. Our preferred method is downloading the binary from the latest GitHub release.GitHub downloadBuild from sourceHomebrew (macOS)Kompose is released via GitHub on a three-week cycle, you can see all current releases on the GitHub release page.# Linux curl -L https://github.com/kubernetes/kompose/releases/download/v1.34.0/kompose-linux-amd64 -o kompose # macOS curl -L https://github.com/kubernetes/kompose/releases/download/v1.34.0/kompose-darwin-amd64 -o kompose # Windows curl -L https://github.com/kubernetes/kompose/releases/download/v1.34.0/kompose-windows-amd64.exe -o kompose.exe chmod +x kompose sudo mv ./kompose /usr/local/bin/kompose Alternatively, you can download the tarball.Installing using go get pulls from the master branch with the latest development changes.go get -u github.com/kubernetes/kompose On macOS you can install the latest release via Homebrew:brew install kompose Use KomposeIn a few steps, we'll take you from Docker Compose to Kubernetes. All you need is an existing docker-compose.yml file.Go to the directory containing your docker-compose.yml file. If you don't have one, test using this one. services: redis-leader: container_name: redis-leader image: redis ports: \- "6379" redis-replica: container_name: redis-replica image: redis ports: \- "6379" command: redis-server --replicaof redis-leader 6379 --dir /tmp web: container_name: web image: quay.io/kompose/web ports: \- "8080:8080" environment: \- GET_HOSTS_FROM=dns labels: kompose.service.type: LoadBalancer To convert the docker-compose.yml file to files that you can use with kubectl, run kompose convert and then kubectl apply -f .kompose convert The output is similar to:INFO Kubernetes file "redis-leader-service.yaml" created INFO Kubernetes file "redis-replica-service.yaml" created INFO Kubernetes file "web-tcp-service.yaml" created INFO Kubernetes file "redis-leader-deployment.yaml" created INFO Kubernetes file "redis-replica-deployment.yaml" created INFO Kubernetes file "web-deployment.yaml" created kubectl apply -f web-tcp-service.yaml,redis-leader-service.yaml,redis-replica-service.yaml,web-deployment.yaml,redis-leader-deployment.yaml,redis-replica-deployment.yaml The output is similar to:deployment.apps/redis-leader created deployment.apps/redis-replica created deployment.apps/web created service/redis-leader created service/redis-replica created service/web-tcp created Your deployments are running in Kubernetes.Access your application.If you're already using minikube for your development process:minikube service web-tcp Otherwise, let's look up what IP your service is using!kubectl describe svc web-tcp Name: web-tcp Namespace: default Labels: io.kompose.service=web-tcp Annotations: kompose.cmd: kompose convert kompose.service.type: LoadBalancer kompose.version: 1.33.0 (3ce457399) Selector: io.kompose.service=web Type: LoadBalancer IP Family Policy: SingleStack IP Families: IPv4 IP: 10.102.30.3 IPs: 10.102.30.3 Port: 8080 8080/TCP TargetPort: 8080/TCP NodePort: 8080 31624/TCP Endpoints: 10.244.0.5:8080 Session Affinity: None External Traffic Policy: Cluster Events:  If you're using a cloud provider, your IP will be listed next to LoadBalancer Ingress.curl http://192.0.2.89 Clean-up.After you are finished testing out the example application deployment, simply run the following command in your shell to delete the resources used.kubectl delete -f web-tcp-service.yaml,redis-leader-service.yaml,redis-replica-service.yaml,web-deployment.yaml,redis-leader-deployment.yaml,redis-replica-deployment.yaml User GuideCLIkompose convertDocumentationAlternative ConversionsLabelsRestartDocker Compose VersionsKompose has support for two providers: OpenShift and Kubernetes. You can choose a targeted provider using global option --provider. If no provider is specified, Kubernetes is set by default.kompose convertKompose supports conversion of V1, V2, and V3 Docker Compose files into Kubernetes and OpenShift objects.Kubernetes kompose convert examplekompose --file docker-voting.yml convert WARN Unsupported key networks - ignoring WARN Unsupported key build - ignoring INFO Kubernetes file "worker-svc.yaml" created INFO Kubernetes file "db-svc.yaml" created INFO Kubernetes file "redis-svc.yaml" created INFO Kubernetes file "result-svc.yaml" created INFO Kubernetes file "vote-svc.yaml" created INFO Kubernetes file "redis-deployment.yaml" created INFO Kubernetes file "result-deployment.yaml" created INFO Kubernetes file "vote-deployment.yaml" created INFO Kubernetes file "worker-deployment.yaml" created INFO Kubernetes file "db-deployment.yaml" created ls db-deployment.yaml docker-compose.yml docker-gitlab.yml redis-deployment.yaml result-deployment.yaml vote-deployment.yaml worker-deployment.yaml db-svc.yaml docker-voting.yml redis-svc.yaml result-svc.yaml vote-svc.yaml worker-svc.yaml You can also provide multiple docker-compose files at the same time:kompose -f docker-compose.yml -f docker-guestbook.yml convert INFO Kubernetes file "frontend-service.yaml" created INFO Kubernetes file "mlbparks-service.yaml" created INFO Kubernetes file "mongodb-service.yaml" created INFO Kubernetes file "redis-master-service.yaml" created INFO Kubernetes file "redis-slave-service.yaml" created INFO Kubernetes file "frontend-deployment.yaml" created INFO Kubernetes file "mlbparks-deployment.yaml" created INFO Kubernetes file "mongodb-deployment.yaml" created INFO Kubernetes file "mongodb-claim0-persistentvolumeclaim.yaml" created INFO Kubernetes file "redis-master-deployment.yaml" created INFO Kubernetes file "redis-slave-deployment.yaml" created ls mlbparks-deployment.yaml mongodb-service.yaml redis-slave-service.jsonmlbparks-service.yaml frontend-deployment.yaml mongodb-claim0-persistentvolumeclaim.yaml redis-master-service.yaml frontend-service.yaml mongodb-deployment.yaml redis-slave-deployment.yaml redis-master-deployment.yaml When multiple docker-compose files are provided the configuration is merged. Any configuration that is common will be overridden by subsequent file.OpenShift kompose convert examplekompose --provider openshift --file docker-voting.yml convert WARN [worker] Service cannot be created because of missing port. INFO OpenShift file "vote-service.yaml" created INFO OpenShift file "db-service.yaml" created INFO OpenShift file "redis-service.yaml" created INFO OpenShift file "result-service.yaml" created INFO OpenShift file "vote-deploymentconfig.yaml" created INFO OpenShift file "vote-imagestream.yaml" created INFO OpenShift file "worker-deploymentconfig.yaml" created INFO OpenShift file "worker-imagestream.yaml" created INFO OpenShift file "db-deploymentconfig.yaml" created INFO OpenShift file "db-imagestream.yaml" created INFO OpenShift file "redis-deploymentconfig.yaml" created INFO OpenShift file "redis-imagestream.yaml" created INFO OpenShift file "result-deploymentconfig.yaml" created INFO OpenShift file "result-imagestream.yaml" created It also supports creating buildconfig for build directive in a service. By default, it uses the remote repo for the current git branch as the source repo, and the current branch as the source branch for the build. You can specify a different source repo and branch using --build-repo and --build-branch options respectively.kompose --provider openshift --file buildconfig/docker-compose.yml convert WARN [foo] Service cannot be created because of missing port. INFO OpenShift Buildconfig using git@github.com:rtnpro/kompose.git::master as source. INFO OpenShift file "foo-deploymentconfig.yaml" created INFO OpenShift file "foo-imagestream.yaml" created INFO OpenShift file "foo-buildconfig.yaml" created Note:If you are manually pushing the OpenShift artifacts using oc create -f, you need to ensure that you push the imagestream artifact before the buildconfig artifact, to workaround this OpenShift issue: https://github.com/openshift/origin/issues/4518 .Alternative ConversionsThe default kompose transformation will generate Kubernetes Deployments and Services, in yaml format. You have alternative option to generate json with -j. Also, you can alternatively generate Replication Controllers objects, Daemon Sets, or Helm charts.kompose convert -j INFO Kubernetes file "redis-svc.json" created INFO Kubernetes file "web-svc.json" created INFO Kubernetes file "redis-deployment.json" created INFO Kubernetes file "web-deployment.json" created The *-deployment.json files contain the Deployment objects.kompose convert --replication-controller INFO Kubernetes file "redis-svc.yaml" created INFO Kubernetes file "web-svc.yaml" created INFO Kubernetes file "redis-replicationcontroller.yaml" created INFO Kubernetes file "web-replicationcontroller.yaml" created The *-replicationcontroller.yaml files contain the Replication Controller objects. If you want to specify replicas (default is 1), use --replicas flag: kompose convert --replication-controller --replicas 3.kompose convert --daemon-set INFO Kubernetes file "redis-svc.yaml" created INFO Kubernetes file "web-svc.yaml" created INFO Kubernetes file "redis-daemonset.yaml" created INFO Kubernetes file "web-daemonset.yaml" created The *-daemonset.yaml files contain the DaemonSet objects.If you want to generate a Chart to be used with Helm run:kompose convert -c INFO Kubernetes file "web-svc.yaml" created INFO Kubernetes file "redis-svc.yaml" created INFO Kubernetes file "web-deployment.yaml" created INFO Kubernetes file "redis-deployment.yaml" created chart created in "./docker-compose/" tree docker-compose/ docker-compose ├── Chart.yaml ├── README.md └── templates ├── redis-deployment.yaml ├── redis-svc.yaml ├── web-deployment.yaml └── web-svc.yaml The chart structure is aimed at providing a skeleton for building your Helm charts.Labelskompose supports Kompose-specific labels within the docker-compose.yml file in order to explicitly define a service's behavior upon conversion.kompose.service.type defines the type of service to be created.For example:version: "2" services: nginx: image: nginx dockerfile: foobar build: ./foobar cap_add: \- ALL container_name: foobar labels: kompose.service.type: nodeport kompose.service.expose defines if the service needs to be made accessible from outside the cluster or not. If the value is set to "true", the provider sets the endpoint automatically, and for any other value, the value is set as the hostname. If multiple ports are defined in a service, the first one is chosen to be the exposed.For the Kubernetes provider, an ingress resource is created and it is assumed that an ingress controller has already been configured.For the OpenShift provider, a route is created.For example:version: "2" services: web: image: tuna/docker-counter23 ports: \- "5000:5000" links: \- redis labels: kompose.service.expose: "counter.example.com" redis: image: redis:3.0 ports: \- "6379" The currently supported options are:KeyValuekompose.service.typenodeport / clusterip / loadbalancerkompose.service.exposetrue / hostnameNote:The kompose.service.type label should be defined with ports only, otherwise kompose will fail.RestartIf you want to create normal pods without controllers you can use restart construct of docker-compose to define that. Follow table below to see what happens on the restart value.docker-compose restartobject createdPod restartPolicy""controller objectAlwaysalwayscontroller objectAlwayson-failurePodOnFailurenoPodNeverNote:The controller object could be deployment or replicationcontroller.For example, the pival service will become pod down here. This container calculated value of pi.version: '2' services: pival: image: perl command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"] restart: "on-failure" Warning about Deployment ConfigurationsIf the Docker Compose file has a volume specified for a service, the Deployment (Kubernetes) or DeploymentConfig (OpenShift) strategy is changed to "Recreate" instead of "RollingUpdate" (default). This is done to avoid multiple instances of a service from accessing a volume at the same time.If the Docker Compose file has service name with _ in it (for example, web_service), then it will be replaced by - and the service name will be renamed accordingly (for example, web-service). Kompose does this because "Kubernetes" doesn't allow _ in object name.Please note that changing service name might break some docker-compose files.Docker Compose VersionsKompose supports Docker Compose versions: 1, 2 and 3. We have limited support on versions 2.1 and 3.2 due to their experimental nature.A full list on compatibility between all three versions is listed in our conversion document including a list of all incompatible Docker Compose keys.4.3.27 - Enforce Pod Security Standards by Configuring the Built-in Admission ControllerKubernetes provides a built-in admission controller to enforce the Pod Security Standards. You can configure this admission controller to set cluster-wide defaults and exemptions.Before you beginFollowing an alpha release in Kubernetes v1.22, Pod Security Admission became available by default in Kubernetes v1.23, as a beta. From version 1.25 onwards, Pod Security Admission is generally available.To check the version, enter kubectl version.If you are not running Kubernetes 1.33, you can switch to viewing this page in the documentation for the Kubernetes version that you are running.Configure the Admission ControllerNote:pod-security.admission.config.k8s.io/v1 configuration requires v1.25+. For v1.23 and v1.24, use v1beta1. For v1.22, use v1alpha1.apiVersion: apiserver.config.k8s.io/v1 kind: AdmissionConfiguration plugins: \- name: PodSecurity configuration: apiVersion: pod-security.admission.config.k8s.io/v1 # see compatibility note kind: PodSecurityConfiguration # Defaults applied when a mode label is not set. # # Level label values must be one of: # - "privileged" (default) # - "baseline" # - "restricted" # # Version label values must be one of: # - "latest" (default) # - specific version like "v1.33" defaults: enforce: "privileged" enforce-version: "latest" audit: "privileged" audit-version: "latest" warn: "privileged" warn-version: "latest" exemptions: # Array of authenticated usernames to exempt. usernames: [] # Array of runtime class names to exempt. runtimeClasses: [] # Array of namespaces to exempt. namespaces: [] Note:The above manifest needs to be specified via the --admission-control-config-file to kube-apiserver.4.3.28 - Enforce Pod Security Standards with Namespace LabelsNamespaces can be labeled to enforce the Pod Security Standards. The three policies privileged, baseline and restricted broadly cover the security spectrum and are implemented by the Pod Security admission controller.Before you beginPod Security Admission was available by default in Kubernetes v1.23, as a beta. From version 1.25 onwards, Pod Security Admission is generally available.To check the version, enter kubectl version.Requiring the baseline Pod Security Standard with namespace labelsThis manifest defines a Namespace my-baseline-namespace that:Blocks any pods that don't satisfy the baseline policy requirements.Generates a user-facing warning and adds an audit annotation to any created pod that does not meet the restricted policy requirements.Pins the versions of the baseline and restricted policies to v1.33.apiVersion: v1 kind: Namespace metadata: name: my-baseline-namespace labels: pod-security.kubernetes.io/enforce: baseline pod-security.kubernetes.io/enforce-version: v1.33 # We are setting these to our _desired_ `enforce` level. pod-security.kubernetes.io/audit: restricted pod-security.kubernetes.io/audit-version: v1.33 pod-security.kubernetes.io/warn: restricted pod-security.kubernetes.io/warn-version: v1.33 Add labels to existing namespaces with kubectl labelNote:When an enforce policy (or version) label is added or changed, the admission plugin will test each pod in the namespace against the new policy. Violations are returned to the user as warnings.It is helpful to apply the --dry-run flag when initially evaluating security profile changes for namespaces. The Pod Security Standard checks will still be run in dry run mode, giving you information about how the new policy would treat existing pods, without actually updating a policy.kubectl label --dry-run=server --overwrite ns --all \ pod-security.kubernetes.io/enforce=baseline Applying to all namespacesIf you're just getting started with the Pod Security Standards, a suitable first step would be to configure all namespaces with audit annotations for a stricter level such as baseline:kubectl label --overwrite ns --all \ pod-security.kubernetes.io/audit=baseline \ pod-security.kubernetes.io/warn=baseline Note that this is not setting an enforce level, so that namespaces that haven't been explicitly evaluated can be distinguished. You can list namespaces without an explicitly set enforce level using this command:kubectl get namespaces --selector='!pod-security.kubernetes.io/enforce' Applying to a single namespaceYou can update a specific namespace as well. This command adds the enforce=restricted policy to my-existing-namespace, pinning the restricted policy version to v1.33.kubectl label --overwrite ns my-existing-namespace \ pod-security.kubernetes.io/enforce=restricted \ pod-security.kubernetes.io/enforce-version=v1.33 4.3.29 - Migrate from PodSecurityPolicy to the Built-In PodSecurity Admission ControllerThis page describes the process of migrating from PodSecurityPolicies to the built-in PodSecurity admission controller. This can be done effectively using a combination of dry-run and audit and warn modes, although this becomes harder if mutating PSPs are used.Before you beginYour Kubernetes server must be at or later than version v1.22.To check the version, enter kubectl version.If you are currently running a version of Kubernetes other than 1.33, you may want to switch to viewing this page in the documentation for the version of Kubernetes that you are actually running.This page assumes you are already familiar with the basic Pod Security Admission concepts.Overall approachThere are multiple strategies you can take for migrating from PodSecurityPolicy to Pod Security Admission. The following steps are one possible migration path, with a goal of minimizing both the risks of a production outage and of a security gap.Decide whether Pod Security Admission is the right fit for your use case.Review namespace permissionsSimplify & standardize PodSecurityPoliciesUpdate namespacesIdentify an appropriate Pod Security levelVerify the Pod Security levelEnforce the Pod Security levelBypass PodSecurityPolicyReview namespace creation processesDisable PodSecurityPolicy0. Decide whether Pod Security Admission is right for youPod Security Admission was designed to meet the most common security needs out of the box, and to provide a standard set of security levels across clusters. However, it is less flexible than PodSecurityPolicy. Notably, the following features are supported by PodSecurityPolicy but not Pod Security Admission:Setting default security constraints - Pod Security Admission is a non-mutating admission controller, meaning it won't modify pods before validating them. If you were relying on this aspect of PSP, you will need to either modify your workloads to meet the Pod Security constraints, or use a Mutating Admission Webhook to make those changes. See Simplify & Standardize PodSecurityPolicies below for more detail.Fine-grained control over policy definition - Pod Security Admission only supports 3 standard levels. If you require more control over specific constraints, then you will need to use a Validating Admission Webhook to enforce those policies.Sub-namespace policy granularity - PodSecurityPolicy lets you bind different policies to different Service Accounts or users, even within a single namespace. This approach has many pitfalls and is not recommended, but if you require this feature anyway you will need to use a 3rd party webhook instead. The exception to this is if you only need to completely exempt specific users or RuntimeClasses, in which case Pod Security Admission does expose some static configuration for exemptions.Even if Pod Security Admission does not meet all of your needs it was designed to be complementary to other policy enforcement mechanisms, and can provide a useful fallback running alongside other admission webhooks.1. Review namespace permissionsPod Security Admission is controlled by labels on namespaces. This means that anyone who can update (or patch or create) a namespace can also modify the Pod Security level for that namespace, which could be used to bypass a more restrictive policy. Before proceeding, ensure that only trusted, privileged users have these namespace permissions. It is not recommended to grant these powerful permissions to users that shouldn't have elevated permissions, but if you must you will need to use an admission webhook to place additional restrictions on setting Pod Security labels on Namespace objects.2. Simplify & standardize PodSecurityPoliciesIn this section, you will reduce mutating PodSecurityPolicies and remove options that are outside the scope of the Pod Security Standards. You should make the changes recommended here to an offline copy of the original PodSecurityPolicy being modified. The cloned PSP should have a different name that is alphabetically before the original (for example, prepend a 0 to it). Do not create the new policies in Kubernetes yet - that will be covered in the Rollout the updated policies section below.2.a. Eliminate purely mutating fieldsIf a PodSecurityPolicy is mutating pods, then you could end up with pods that don't meet the Pod Security level requirements when you finally turn PodSecurityPolicy off. In order to avoid this, you should eliminate all PSP mutation prior to switching over. Unfortunately PSP does not cleanly separate mutating & validating fields, so this is not a straightforward migration.You can start by eliminating the fields that are purely mutating, and don't have any bearing on the validating policy. These fields (also listed in the Mapping PodSecurityPolicies to Pod Security Standards reference) are:.spec.defaultAllowPrivilegeEscalation.spec.runtimeClass.defaultRuntimeClassName.metadata.annotations['seccomp.security.alpha.kubernetes.io/defaultProfileName'].metadata.annotations['apparmor.security.beta.kubernetes.io/defaultProfileName'].spec.defaultAddCapabilities - Although technically a mutating & validating field, these should be merged into .spec.allowedCapabilities which performs the same validation without mutation.Caution:Removing these could result in workloads missing required configuration, and cause problems. See Rollout the updated policies below for advice on how to roll these changes out safely.2.b. Eliminate options not covered by the Pod Security StandardsThere are several fields in PodSecurityPolicy that are not covered by the Pod Security Standards. If you must enforce these options, you will need to supplement Pod Security Admission with an admission webhook, which is outside the scope of this guide.First, you can remove the purely validating fields that the Pod Security Standards do not cover. These fields (also listed in the Mapping PodSecurityPolicies to Pod Security Standards reference with "no opinion") are:.spec.allowedHostPaths.spec.allowedFlexVolumes.spec.allowedCSIDrivers.spec.forbiddenSysctls.spec.runtimeClassYou can also remove the following fields, that are related to POSIX / UNIX group controls.Caution:If any of these use the MustRunAs strategy they may be mutating! Removing these could result in workloads not setting the required groups, and cause problems. See Rollout the updated policies below for advice on how to roll these changes out safely..spec.runAsGroup.spec.supplementalGroups.spec.fsGroupThe remaining mutating fields are required to properly support the Pod Security Standards, and will need to be handled on a case-by-case basis later:.spec.requiredDropCapabilities - Required to drop ALL for the Restricted profile..spec.seLinux - (Only mutating with the MustRunAs rule) required to enforce the SELinux requirements of the Baseline & Restricted profiles..spec.runAsUser - (Non-mutating with the RunAsAny rule) required to enforce RunAsNonRoot for the Restricted profile..spec.allowPrivilegeEscalation - (Only mutating if set to false) required for the Restricted profile.2.c. Rollout the updated PSPsNext, you can rollout the updated policies to your cluster. You should proceed with caution, as removing the mutating options may result in workloads missing required configuration.For each updated PodSecurityPolicy:Identify pods running under the original PSP. This can be done using the kubernetes.io/psp annotation. For example, using kubectl:PSP_NAME="original" # Set the name of the PSP you're checking for kubectl get pods --all-namespaces -o jsonpath="{range .items[?(@.metadata.annotations.kubernetes\\.io\/psp=='$PSP_NAME')]}{.metadata.namespace} {.metadata.name}{'\n'}{end}" Compare these running pods against the original pod spec to determine whether PodSecurityPolicy has modified the pod. For pods created by a workload resource you can compare the pod with the PodTemplate in the controller resource. If any changes are identified, the original Pod or PodTemplate should be updated with the desired configuration. The fields to review are:.metadata.annotations['container.apparmor.security.beta.kubernetes.io/*'] (replace * with each container name).spec.runtimeClassName.spec.securityContext.fsGroup.spec.securityContext.seccompProfile.spec.securityContext.seLinuxOptions.spec.securityContext.supplementalGroupsOn containers, under .spec.containers[*] and .spec.initContainers[*]:.securityContext.allowPrivilegeEscalation.securityContext.capabilities.add.securityContext.capabilities.drop.securityContext.readOnlyRootFilesystem.securityContext.runAsGroup.securityContext.runAsNonRoot.securityContext.runAsUser.securityContext.seccompProfile.securityContext.seLinuxOptionsCreate the new PodSecurityPolicies. If any Roles or ClusterRoles are granting use on all PSPs this could cause the new PSPs to be used instead of their mutating counter-parts.Update your authorization to grant access to the new PSPs. In RBAC this means updating any Roles or ClusterRoles that grant the use permission on the original PSP to also grant it to the updated PSP.Verify: after some soak time, rerun the command from step 1 to see if any pods are still using the original PSPs. Note that pods need to be recreated after the new policies have been rolled out before they can be fully verified.(optional) Once you have verified that the original PSPs are no longer in use, you can delete them.3. Update NamespacesThe following steps will need to be performed on every namespace in the cluster. Commands referenced in these steps use the $NAMESPACE variable to refer to the namespace being updated.3.a. Identify an appropriate Pod Security levelStart reviewing the Pod Security Standards and familiarizing yourself with the 3 different levels.There are several ways to choose a Pod Security level for your namespace:By security requirements for the namespace - If you are familiar with the expected access level for the namespace, you can choose an appropriate level based on those requirements, similar to how one might approach this on a new cluster.By existing PodSecurityPolicies - Using the Mapping PodSecurityPolicies to Pod Security Standards reference you can map each PSP to a Pod Security Standard level. If your PSPs aren't based on the Pod Security Standards, you may need to decide between choosing a level that is at least as permissive as the PSP, and a level that is at least as restrictive. You can see which PSPs are in use for pods in a given namespace with this command:kubectl get pods -n $NAMESPACE -o jsonpath="{.items[*].metadata.annotations.kubernetes\\.io\/psp}" | tr " " "\n" | sort -u By existing pods - Using the strategies under Verify the Pod Security level, you can test out both the Baseline and Restricted levels to see whether they are sufficiently permissive for existing workloads, and chose the least-privileged valid level.Caution:Options 2 & 3 above are based on existing pods, and may miss workloads that aren't currently running, such as CronJobs, scale-to-zero workloads, or other workloads that haven't rolled out.3.b. Verify the Pod Security levelOnce you have selected a Pod Security level for the namespace (or if you're trying several), it's a good idea to test it out first (you can skip this step if using the Privileged level). Pod Security includes several tools to help test and safely roll out profiles.First, you can dry-run the policy, which will evaluate pods currently running in the namespace against the applied policy, without making the new policy take effect:# $LEVEL is the level to dry-run, either "baseline" or "restricted". kubectl label --dry-run=server --overwrite ns $NAMESPACE pod-security.kubernetes.io/enforce=$LEVEL This command will return a warning for any existing pods that are not valid under the proposed level.The second option is better for catching workloads that are not currently running: audit mode. When running under audit-mode (as opposed to enforcing), pods that violate the policy level are recorded in the audit logs, which can be reviewed later after some soak time, but are not forbidden. Warning mode works similarly, but returns the warning to the user immediately. You can set the audit level on a namespace with this command:kubectl label --overwrite ns $NAMESPACE pod-security.kubernetes.io/audit=$LEVEL If either of these approaches yield unexpected violations, you will need to either update the violating workloads to meet the policy requirements, or relax the namespace Pod Security level.3.c. Enforce the Pod Security levelWhen you are satisfied that the chosen level can safely be enforced on the namespace, you can update the namespace to enforce the desired level:kubectl label --overwrite ns $NAMESPACE pod-security.kubernetes.io/enforce=$LEVEL 3.d. Bypass PodSecurityPolicyFinally, you can effectively bypass PodSecurityPolicy at the namespace level by binding the fully privileged PSP to all service accounts in the namespace.# The following cluster-scoped commands are only needed once. kubectl apply -f privileged-psp.yaml kubectl create clusterrole privileged-psp --verb use --resource podsecuritypolicies.policy --resource-name privileged # Per-namespace disable kubectl create -n $NAMESPACE rolebinding disable-psp --clusterrole privileged-psp --group system:serviceaccounts:$NAMESPACE Since the privileged PSP is non-mutating, and the PSP admission controller always prefers non-mutating PSPs, this will ensure that pods in this namespace are no longer being modified or restricted by PodSecurityPolicy.The advantage to disabling PodSecurityPolicy on a per-namespace basis like this is if a problem arises you can easily roll the change back by deleting the RoleBinding. Just make sure the pre-existing PodSecurityPolicies are still in place!# Undo PodSecurityPolicy disablement. kubectl delete -n $NAMESPACE rolebinding disable-psp 4\. Review namespace creation processesNow that existing namespaces have been updated to enforce Pod Security Admission, you should ensure that your processes and/or policies for creating new namespaces are updated to ensure that an appropriate Pod Security profile is applied to new namespaces.You can also statically configure the Pod Security admission controller to set a default enforce, audit, and/or warn level for unlabeled namespaces. See Configure the Admission Controller for more information.5. Disable PodSecurityPolicyFinally, you're ready to disable PodSecurityPolicy. To do so, you will need to modify the admission configuration of the API server: How do I turn off an admission controller?.To verify that the PodSecurityPolicy admission controller is no longer enabled, you can manually run a test by impersonating a user without access to any PodSecurityPolicies (see the PodSecurityPolicy example), or by verifying in the API server logs. At startup, the API server outputs log lines listing the loaded admission controller plugins:I0218 00:59:44.903329 13 plugins.go:158] Loaded 16 mutating admission controller(s) successfully in the following order: NamespaceLifecycle,LimitRanger,ServiceAccount,NodeRestriction,TaintNodesByCondition,Priority,DefaultTolerationSeconds,ExtendedResourceToleration,PersistentVolumeLabel,DefaultStorageClass,StorageObjectInUseProtection,RuntimeClass,DefaultIngressClass,MutatingAdmissionWebhook. I0218 00:59:44.903350 13 plugins.go:161] Loaded 14 validating admission controller(s) successfully in the following order: LimitRanger,ServiceAccount,PodSecurity,Priority,PersistentVolumeClaimResize,RuntimeClass,CertificateApproval,CertificateSigning,CertificateSubjectRestriction,DenyServiceExternalIPs,ValidatingAdmissionWebhook,ResourceQuota. You should see PodSecurity (in the validating admission controllers), and neither list should contain PodSecurityPolicy.Once you are certain the PSP admission controller is disabled (and after sufficient soak time to be confident you won't need to roll back), you are free to delete your PodSecurityPolicies and any associated Roles, ClusterRoles, RoleBindings and ClusterRoleBindings (just make sure they don't grant any other unrelated permissions).4.4 - Monitoring, Logging, and DebuggingSet up monitoring and logging to troubleshoot a cluster, or debug a containerized application.Sometimes things go wrong. This guide is aimed at making them right. It has two sections:Debugging your application - Useful for users who are deploying code into Kubernetes and wondering why it is not working.Debugging your cluster - Useful for cluster administrators and people whose Kubernetes cluster is unhappy.You should also check the known issues for the release you're using.Getting helpIf your problem isn't answered by any of the guides above, there are variety of ways for you to get help from the Kubernetes community.QuestionsThe documentation on this site has been structured to provide answers to a wide range of questions. Concepts explain the Kubernetes architecture and how each component works, while Setup provides practical instructions for getting started. Tasks show how to accomplish commonly used tasks, and Tutorials are more comprehensive walkthroughs of real-world, industry-specific, or end-to-end development scenarios. The Reference section provides detailed documentation on the Kubernetes API and command-line interfaces (CLIs), such as kubectl.Help! My question isn't covered! I need help now!Stack Exchange, Stack Overflow, or Server FaultIf you have questions related to software development for your containerized app, you can ask those on Stack Overflow.If you have Kubernetes questions related to cluster management or configuration, you can ask those on Server Fault.There are also several more specific Stack Exchange network sites which might be the right place to ask Kubernetes questions in areas such as DevOps, Software Engineering, or InfoSec.Someone else from the community may have already asked a similar question or may be able to help with your problem.The Kubernetes team will also monitor posts tagged Kubernetes. If there aren't any existing questions that help, please ensure that your question is on-topic on Stack Overflow, Server Fault, or the Stack Exchange Network site you're asking on, and read through the guidance on how to ask a new question, before asking a new one!SlackMany people from the Kubernetes community hang out on Kubernetes Slack in the #kubernetes-users channel. Slack requires registration; you can request an invitation, and registration is open to everyone). Feel free to come and ask any and all questions. Once registered, access the Kubernetes organisation in Slack via your web browser or via Slack's own dedicated app.Once you are registered, browse the growing list of channels for various subjects of interest. For example, people new to Kubernetes may also want to join the #kubernetes-novice channel. As another example, developers should join the #kubernetes-contributors channel.There are also many country specific / local language channels. Feel free to join these channels for localized support and info:Country / language specific Slack channelsCountryChannelsChina#cn-users, #cn-eventsFinland#fi-usersFrance#fr-users, #fr-eventsGermany#de-users, #de-eventsIndia#in-users, #in-eventsItaly#it-users, #it-eventsJapan#jp-users, #jp-eventsKorea#kr-usersNetherlands#nl-usersNorway#norw-usersPoland#pl-usersRussia#ru-usersSpain#es-usersSweden#se-usersTurkey#tr-users, #tr-eventsForumYou're welcome to join the official Kubernetes Forum: discuss.kubernetes.io.Bugs and feature requestsIf you have what looks like a bug, or you would like to make a feature request, please use the GitHub issue tracking system.Before you file an issue, please search existing issues to see if your issue is already covered.If filing a bug, please include detailed information about how to reproduce the problem, such as:Kubernetes version: kubectl versionCloud provider, OS distro, network configuration, and container runtime versionSteps to reproduce the problem4.4.1 - Troubleshooting ApplicationsDebugging common containerized application issues.This doc contains a set of resources for fixing issues with containerized applications. It covers things like common issues with Kubernetes resources (like Pods, Services, or StatefulSets), advice on making sense of container termination messages, and ways to debug running containers.4.4.1.1 - Debug PodsThis guide is to help users debug applications that are deployed into Kubernetes and not behaving correctly. This is not a guide for people who want to debug their cluster. For that you should check out this guide.Diagnosing the problemThe first step in troubleshooting is triage. What is the problem? Is it your Pods, your Replication Controller or your Service?Debugging PodsDebugging Replication ControllersDebugging ServicesDebugging PodsThe first step in debugging a Pod is taking a look at it. Check the current state of the Pod and recent events with the following command:kubectl describe pods ${POD_NAME} Look at the state of the containers in the pod. Are they all Running? Have there been recent restarts?Continue debugging depending on the state of the pods.My pod stays pendingIf a Pod is stuck in Pending it means that it can not be scheduled onto a node. Generally this is because there are insufficient resources of one type or another that prevent scheduling. Look at the output of the kubectl describe ... command above. There should be messages from the scheduler about why it can not schedule your pod. Reasons include:You don't have enough resources: You may have exhausted the supply of CPU or Memory in your cluster, in this case you need to delete Pods, adjust resource requests, or add new nodes to your cluster. See Compute Resources document for more information.You are using hostPort: When you bind a Pod to a hostPort there are a limited number of places that pod can be scheduled. In most cases, hostPort is unnecessary, try using a Service object to expose your Pod. If you do require hostPort then you can only schedule as many Pods as there are nodes in your Kubernetes cluster.My pod stays waitingIf a Pod is stuck in the Waiting state, then it has been scheduled to a worker node, but it can't run on that machine. Again, the information from kubectl describe ... should be informative. The most common cause of Waiting pods is a failure to pull the image. There are three things to check:Make sure that you have the name of the image correct.Have you pushed the image to the registry?Try to manually pull the image to see if the image can be pulled. For example, if you use Docker on your PC, run docker pull .My pod stays terminatingIf a Pod is stuck in the Terminating state, it means that a deletion has been issued for the Pod, but the control plane is unable to delete the Pod object.This typically happens if the Pod has a finalizer and there is an admission webhook installed in the cluster that prevents the control plane from removing the finalizer.To identify this scenario, check if your cluster has any ValidatingWebhookConfiguration or MutatingWebhookConfiguration that target UPDATE operations for pods resources.If the webhook is provided by a third-party:Make sure you are using the latest version.Disable the webhook for UPDATE operations.Report an issue with the corresponding provider.If you are the author of the webhook:For a mutating webhook, make sure it never changes immutable fields on UPDATE operations. For example, changes to containers are usually not allowed.For a validating webhook, make sure that your validation policies only apply to new changes. In other words, you should allow Pods with existing violations to pass validation. This allows Pods that were created before the validating webhook was installed to continue running.My pod is crashing or otherwise unhealthyOnce your pod has been scheduled, the methods described in Debug Running Pods are available for debugging.My pod is running but not doing what I told it to doIf your pod is not behaving as you expected, it may be that there was an error in your pod description (e.g. mypod.yaml file on your local machine), and that the error was silently ignored when you created the pod. Often a section of the pod description is nested incorrectly, or a key name is typed incorrectly, and so the key is ignored. For example, if you misspelled command as commnd then the pod will be created but will not use the command line you intended it to use.The first thing to do is to delete your pod and try creating it again with the --validate option. For example, run kubectl apply --validate -f mypod.yaml. If you misspelled command as commnd then will give an error like this:I0805 10:43:25.129850 46757 schema.go:126] unknown field: commnd I0805 10:43:25.129973 46757 schema.go:129] this may be a false alarm, see https://github.com/kubernetes/kubernetes/issues/6842 pods/mypod The next thing to check is whether the pod on the apiserver matches the pod you meant to create (e.g. in a yaml file on your local machine). For example, run kubectl get pods/mypod -o yaml > mypod-on-apiserver.yaml and then manually compare the original pod description, mypod.yaml with the one you got back from apiserver, mypod-on-apiserver.yaml. There will typically be some lines on the "apiserver" version that are not on the original version. This is expected. However, if there are lines on the original that are not on the apiserver version, then this may indicate a problem with your pod spec.Debugging Replication ControllersReplication controllers are fairly straightforward. They can either create Pods or they can't. If they can't create pods, then please refer to the instructions above to debug your pods.You can also use kubectl describe rc ${CONTROLLER_NAME} to introspect events related to the replication controller.Debugging ServicesServices provide load balancing across a set of pods. There are several common problems that can make Services not work properly. The following instructions should help debug Service problems.First, verify that there are endpoints for the service. For every Service object, the apiserver makes one or more EndpointSlice resources available.You can view these resources with:kubectl get endpointslices -l kubernetes.io/service-name=${SERVICE_NAME} Make sure that the endpoints in the EndpointSlices match up with the number of pods that you expect to be members of your service. For example, if your Service is for an nginx container with 3 replicas, you would expect to see three different IP addresses in the Service's endpoint slices.My service is missing endpointsIf you are missing endpoints, try listing pods using the labels that Service uses. Imagine that you have a Service where the labels are:... spec: \- selector: name: nginx type: frontend You can use:kubectl get pods --selector=name=nginx,type=frontend to list pods that match this selector. Verify that the list matches the Pods that you expect to provide your Service. Verify that the pod's containerPort matches up with the Service's targetPortNetwork traffic is not forwardedPlease see debugging service for more information.What's nextIf none of the above solves your problem, follow the instructions in Debugging Service document to make sure that your Service is running, has Endpoints, and your Pods are actually serving; you have DNS working, iptables rules installed, and kube-proxy does not seem to be misbehaving.You may also visit troubleshooting document for more information.4.4.1.2 - Debug ServicesAn issue that comes up rather frequently for new installations of Kubernetes is that a Service is not working properly. You've run your Pods through a Deployment (or other workload controller) and created a Service, but you get no response when you try to access it. This document will hopefully help you to figure out what's going wrong.Running commands in a PodFor many steps here you will want to see what a Pod running in the cluster sees. The simplest way to do this is to run an interactive busybox Pod:kubectl run -it --rm --restart=Never busybox --image=gcr.io/google-containers/busybox sh Note:If you don't see a command prompt, try pressing enter.If you already have a running Pod that you prefer to use, you can run a command in it using:kubectl exec  -c  \--  SetupFor the purposes of this walk-through, let's run some Pods. Since you're probably debugging your own Service you can substitute your own details, or you can follow along and get a second data point.kubectl create deployment hostnames --image=registry.k8s.io/serve_hostname deployment.apps/hostnames created kubectl commands will print the type and name of the resource created or mutated, which can then be used in subsequent commands.Let's scale the deployment to 3 replicas.kubectl scale deployment hostnames --replicas=3 deployment.apps/hostnames scaled Note that this is the same as if you had started the Deployment with the following YAML:apiVersion: apps/v1 kind: Deployment metadata: labels: app: hostnames name: hostnames spec: selector: matchLabels: app: hostnames replicas: 3 template: metadata: labels: app: hostnames spec: containers: \- name: hostnames image: registry.k8s.io/serve_hostname The label "app" is automatically set by kubectl create deployment to the name of the Deployment.You can confirm your Pods are running:kubectl get pods -l app=hostnames NAME READY STATUS RESTARTS AGE hostnames-632524106-bbpiw 1/1 Running 0 2m hostnames-632524106-ly40y 1/1 Running 0 2m hostnames-632524106-tlaok 1/1 Running 0 2m You can also confirm that your Pods are serving. You can get the list of Pod IP addresses and test them directly.kubectl get pods -l app=hostnames \ -o go-template='{{range .items}}{{.status.podIP}}{{"\n"}}{{end}}' 10.244.0.5 10.244.0.6 10.244.0.7 The example container used for this walk-through serves its own hostname via HTTP on port 9376, but if you are debugging your own app, you'll want to use whatever port number your Pods are listening on.From within a pod:for ep in 10.244.0.5:9376 10.244.0.6:9376 10.244.0.7:9376; do wget -qO- $ep done This should produce something like:hostnames-632524106-bbpiw hostnames-632524106-ly40y hostnames-632524106-tlaok If you are not getting the responses you expect at this point, your Pods might not be healthy or might not be listening on the port you think they are. You might find kubectl logs to be useful for seeing what is happening, or perhaps you need to kubectl exec directly into your Pods and debug from there.Assuming everything has gone to plan so far, you can start to investigate why your Service doesn't work.Does the Service exist?The astute reader will have noticed that you did not actually create a Service yet - that is intentional. This is a step that sometimes gets forgotten, and is the first thing to check.What would happen if you tried to access a non-existent Service? If you have another Pod that consumes this Service by name you would get something like:wget -O- hostnames Resolving hostnames (hostnames)... failed: Name or service not known. wget: unable to resolve host address 'hostnames' The first thing to check is whether that Service actually exists:kubectl get svc hostnames No resources found. Error from server (NotFound): services "hostnames" not found Let's create the Service. As before, this is for the walk-through - you can use your own Service's details here.kubectl expose deployment hostnames --port=80 --target-port=9376 service/hostnames exposed And read it back:kubectl get svc hostnames NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE hostnames ClusterIP 10.0.1.175  80/TCP 5s Now you know that the Service exists.As before, this is the same as if you had started the Service with YAML:apiVersion: v1 kind: Service metadata: labels: app: hostnames name: hostnames spec: selector: app: hostnames ports: \- name: default protocol: TCP port: 80 targetPort: 9376 In order to highlight the full range of configuration, the Service you created here uses a different port number than the Pods. For many real-world Services, these values might be the same.Any Network Policy Ingress rules affecting the target Pods?If you have deployed any Network Policy Ingress rules which may affect incoming traffic to hostnames-* Pods, these need to be reviewed.Please refer to Network Policies for more details.Does the Service work by DNS name?One of the most common ways that clients consume a Service is through a DNS name.From a Pod in the same Namespace:nslookup hostnames Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local Name: hostnames Address 1: 10.0.1.175 hostnames.default.svc.cluster.local If this fails, perhaps your Pod and Service are in different Namespaces, try a namespace-qualified name (again, from within a Pod):nslookup hostnames.default Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local Name: hostnames.default Address 1: 10.0.1.175 hostnames.default.svc.cluster.local If this works, you'll need to adjust your app to use a cross-namespace name, or run your app and Service in the same Namespace. If this still fails, try a fully-qualified name:nslookup hostnames.default.svc.cluster.local Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local Name: hostnames.default.svc.cluster.local Address 1: 10.0.1.175 hostnames.default.svc.cluster.local Note the suffix here: "default.svc.cluster.local". The "default" is the Namespace you're operating in. The "svc" denotes that this is a Service. The "cluster.local" is your cluster domain, which COULD be different in your own cluster.You can also try this from a Node in the cluster:Note:10.0.0.10 is the cluster's DNS Service IP, yours might be different.nslookup hostnames.default.svc.cluster.local 10.0.0.10 Server: 10.0.0.10 Address: 10.0.0.10#53 Name: hostnames.default.svc.cluster.local Address: 10.0.1.175 If you are able to do a fully-qualified name lookup but not a relative one, you need to check that your /etc/resolv.conf file in your Pod is correct. From within a Pod:cat /etc/resolv.conf You should see something like:nameserver 10.0.0.10 search default.svc.cluster.local svc.cluster.local cluster.local example.com options ndots:5 The nameserver line must indicate your cluster's DNS Service. This is passed into kubelet with the --cluster-dns flag.The search line must include an appropriate suffix for you to find the Service name. In this case it is looking for Services in the local Namespace ("default.svc.cluster.local"), Services in all Namespaces ("svc.cluster.local"), and lastly for names in the cluster ("cluster.local"). Depending on your own install you might have additional records after that (up to 6 total). The cluster suffix is passed into kubelet with the \--cluster-domain flag. Throughout this document, the cluster suffix is assumed to be "cluster.local". Your own clusters might be configured differently, in which case you should change that in all of the previous commands.The options line must set ndots high enough that your DNS client library considers search paths at all. Kubernetes sets this to 5 by default, which is high enough to cover all of the DNS names it generates.Does any Service work by DNS name?If the above still fails, DNS lookups are not working for your Service. You can take a step back and see what else is not working. The Kubernetes master Service should always work. From within a Pod:nslookup kubernetes.default Server: 10.0.0.10 Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local Name: kubernetes.default Address 1: 10.0.0.1 kubernetes.default.svc.cluster.local If this fails, please see the kube-proxy section of this document, or even go back to the top of this document and start over, but instead of debugging your own Service, debug the DNS Service.Does the Service work by IP?Assuming you have confirmed that DNS works, the next thing to test is whether your Service works by its IP address. From a Pod in your cluster, access the Service's IP (from kubectl get above).for i in $(seq 1 3); do wget -qO- 10.0.1.175:80 done This should produce something like:hostnames-632524106-bbpiw hostnames-632524106-ly40y hostnames-632524106-tlaok If your Service is working, you should get correct responses. If not, there are a number of things that could be going wrong. Read on.Is the Service defined correctly?It might sound silly, but you should really double and triple check that your Service is correct and matches your Pod's port. Read back your Service and verify it:kubectl get service hostnames -o json { "kind": "Service", "apiVersion": "v1", "metadata": { "name": "hostnames", "namespace": "default", "uid": "428c8b6c-24bc-11e5-936d-42010af0a9bc", "resourceVersion": "347189", "creationTimestamp": "2015-07-07T15:24:29Z", "labels": { "app": "hostnames" } }, "spec": { "ports": [ { "name": "default", "protocol": "TCP", "port": 80, "targetPort": 9376, "nodePort": 0 } ], "selector": { "app": "hostnames" }, "clusterIP": "10.0.1.175", "type": "ClusterIP", "sessionAffinity": "None" }, "status": { "loadBalancer": {} } } Is the Service port you are trying to access listed in spec.ports[]?Is the targetPort correct for your Pods (some Pods use a different port than the Service)?If you meant to use a numeric port, is it a number (9376) or a string "9376"?If you meant to use a named port, do your Pods expose a port with the same name?Is the port's protocol correct for your Pods?Does the Service have any EndpointSlices?If you got this far, you have confirmed that your Service is correctly defined and is resolved by DNS. Now let's check that the Pods you ran are actually being selected by the Service.Earlier you saw that the Pods were running. You can re-check that:kubectl get pods -l app=hostnames NAME READY STATUS RESTARTS AGE hostnames-632524106-bbpiw 1/1 Running 0 1h hostnames-632524106-ly40y 1/1 Running 0 1h hostnames-632524106-tlaok 1/1 Running 0 1h The -l app=hostnames argument is a label selector configured on the Service.The "AGE" column says that these Pods are about an hour old, which implies that they are running fine and not crashing.The "RESTARTS" column says that these pods are not crashing frequently or being restarted. Frequent restarts could lead to intermittent connectivity issues. If the restart count is high, read more about how to debug pods.Inside the Kubernetes system is a control loop which evaluates the selector of every Service and saves the results into one or more EndpointSlice objects.kubectl get endpointslices -l k8s.io/service-name=hostnames NAME ADDRESSTYPE PORTS ENDPOINTS hostnames-ytpni IPv4 9376 10.244.0.5,10.244.0.6,10.244.0.7 This confirms that the EndpointSlice controller has found the correct Pods for your Service. If the ENDPOINTS column is , you should check that the spec.selector field of your Service actually selects for metadata.labels values on your Pods. A common mistake is to have a typo or other error, such as the Service selecting for app=hostnames, but the Deployment specifying run=hostnames, as in versions previous to 1.18, where the kubectl run command could have been also used to create a Deployment.Are the Pods working?At this point, you know that your Service exists and has selected your Pods. At the beginning of this walk-through, you verified the Pods themselves. Let's check again that the Pods are actually working - you can bypass the Service mechanism and go straight to the Pods, as listed by the Endpoints above.Note:These commands use the Pod port (9376), rather than the Service port (80).From within a Pod:for ep in 10.244.0.5:9376 10.244.0.6:9376 10.244.0.7:9376; do wget -qO- $ep done This should produce something like:hostnames-632524106-bbpiw hostnames-632524106-ly40y hostnames-632524106-tlaok You expect each Pod in the endpoints list to return its own hostname. If this is not what happens (or whatever the correct behavior is for your own Pods), you should investigate what's happening there.Is the kube-proxy working?If you get here, your Service is running, has EndpointSlices, and your Pods are actually serving. At this point, the whole Service proxy mechanism is suspect. Let's confirm it, piece by piece.The default implementation of Services, and the one used on most clusters, is kube-proxy. This is a program that runs on every node and configures one of a small set of mechanisms for providing the Service abstraction. If your cluster does not use kube-proxy, the following sections will not apply, and you will have to investigate whatever implementation of Services you are using.Is kube-proxy running?Confirm that kube-proxy is running on your Nodes. Running directly on a Node, you should get something like the below:ps auxw | grep kube-proxy root 4194 0.4 0.1 101864 17696 ? Sl Jul04 25:43 /usr/local/bin/kube-proxy --master=https://kubernetes-master --kubeconfig=/var/lib/kube-proxy/kubeconfig --v=2 Next, confirm that it is not failing something obvious, like contacting the master. To do this, you'll have to look at the logs. Accessing the logs depends on your Node OS. On some OSes it is a file, such as /var/log/kube-proxy.log, while other OSes use journalctl to access logs. You should see something like:I1027 22:14:53.995134 5063 server.go:200] Running in resource-only container "/kube-proxy" I1027 22:14:53.998163 5063 server.go:247] Using iptables Proxier. I1027 22:14:54.038140 5063 proxier.go:352] Setting endpoints for "kube-system/kube-dns:dns-tcp" to [10.244.1.3:53] I1027 22:14:54.038164 5063 proxier.go:352] Setting endpoints for "kube-system/kube-dns:dns" to [10.244.1.3:53] I1027 22:14:54.038209 5063 proxier.go:352] Setting endpoints for "default/kubernetes:https" to [10.240.0.2:443] I1027 22:14:54.038238 5063 proxier.go:429] Not syncing iptables until Services and Endpoints have been received from master I1027 22:14:54.040048 5063 proxier.go:294] Adding new service "default/kubernetes:https" at 10.0.0.1:443/TCP I1027 22:14:54.040154 5063 proxier.go:294] Adding new service "kube-system/kube-dns:dns" at 10.0.0.10:53/UDP I1027 22:14:54.040223 5063 proxier.go:294] Adding new service "kube-system/kube-dns:dns-tcp" at 10.0.0.10:53/TCP If you see error messages about not being able to contact the master, you should double-check your Node configuration and installation steps.Kube-proxy can run in one of a few modes. In the log listed above, the line Using iptables Proxier indicates that kube-proxy is running in "iptables" mode. The most common other mode is "ipvs".Iptables modeIn "iptables" mode, you should see something like the following on a Node:iptables-save | grep hostnames -A KUBE-SEP-57KPRZ3JQVENLNBR -s 10.244.3.6/32 -m comment --comment "default/hostnames:" -j MARK --set-xmark 0x00004000/0x00004000 -A KUBE-SEP-57KPRZ3JQVENLNBR -p tcp -m comment --comment "default/hostnames:" -m tcp -j DNAT --to-destination 10.244.3.6:9376 -A KUBE-SEP-WNBA2IHDGP2BOBGZ -s 10.244.1.7/32 -m comment --comment "default/hostnames:" -j MARK --set-xmark 0x00004000/0x00004000 -A KUBE-SEP-WNBA2IHDGP2BOBGZ -p tcp -m comment --comment "default/hostnames:" -m tcp -j DNAT --to-destination 10.244.1.7:9376 -A KUBE-SEP-X3P2623AGDH6CDF3 -s 10.244.2.3/32 -m comment --comment "default/hostnames:" -j MARK --set-xmark 0x00004000/0x00004000 -A KUBE-SEP-X3P2623AGDH6CDF3 -p tcp -m comment --comment "default/hostnames:" -m tcp -j DNAT --to-destination 10.244.2.3:9376 -A KUBE-SERVICES -d 10.0.1.175/32 -p tcp -m comment --comment "default/hostnames: cluster IP" -m tcp --dport 80 -j KUBE-SVC-NWV5X2332I4OT4T3 -A KUBE-SVC-NWV5X2332I4OT4T3 -m comment --comment "default/hostnames:" -m statistic --mode random --probability 0.33332999982 -j KUBE-SEP-WNBA2IHDGP2BOBGZ -A KUBE-SVC-NWV5X2332I4OT4T3 -m comment --comment "default/hostnames:" -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-X3P2623AGDH6CDF3 -A KUBE-SVC-NWV5X2332I4OT4T3 -m comment --comment "default/hostnames:" -j KUBE-SEP-57KPRZ3JQVENLNBR For each port of each Service, there should be 1 rule in KUBE-SERVICES and one KUBE-SVC- chain. For each Pod endpoint, there should be a small number of rules in that KUBE-SVC- and one KUBE-SEP- chain with a small number of rules in it. The exact rules will vary based on your exact config (including node-ports and load-balancers).IPVS modeIn "ipvs" mode, you should see something like the following on a Node:ipvsadm -ln Prot LocalAddress:Port Scheduler Flags -> RemoteAddress:Port Forward Weight ActiveConn InActConn ... TCP 10.0.1.175:80 rr -> 10.244.0.5:9376 Masq 1 0 0 -> 10.244.0.6:9376 Masq 1 0 0 -> 10.244.0.7:9376 Masq 1 0 0 ... For each port of each Service, plus any NodePorts, external IPs, and load-balancer IPs, kube-proxy will create a virtual server. For each Pod endpoint, it will create corresponding real servers. In this example, service hostnames(10.0.1.175:80) has 3 endpoints(10.244.0.5:9376, 10.244.0.6:9376, 10.244.0.7:9376).Is kube-proxy proxying?Assuming you do see one the above cases, try again to access your Service by IP from one of your Nodes:curl 10.0.1.175:80 hostnames-632524106-bbpiw If this still fails, look at the kube-proxy logs for specific lines like:Setting endpoints for default/hostnames:default to [10.244.0.5:9376 10.244.0.6:9376 10.244.0.7:9376] If you don't see those, try restarting kube-proxy with the -v flag set to 4, and then look at the logs again.Edge case: A Pod fails to reach itself via the Service IPThis might sound unlikely, but it does happen and it is supposed to work.This can happen when the network is not properly configured for "hairpin" traffic, usually when kube-proxy is running in iptables mode and Pods are connected with bridge network. The Kubelet exposes a hairpin-mode flag that allows endpoints of a Service to loadbalance back to themselves if they try to access their own Service VIP. The hairpin-mode flag must either be set to hairpin-veth or promiscuous-bridge.The common steps to trouble shoot this are as follows:Confirm hairpin-mode is set to hairpin-veth or promiscuous-bridge. You should see something like the below. hairpin-mode is set to promiscuous-bridge in the following example.ps auxw | grep kubelet root 3392 1.1 0.8 186804 65208 ? Sl 00:51 11:11 /usr/local/bin/kubelet --enable-debugging-handlers=true --config=/etc/kubernetes/manifests --allow-privileged=True --v=4 --cluster-dns=10.0.0.10 --cluster-domain=cluster.local --configure-cbr0=true --cgroup-root=/ --system-cgroups=/system --hairpin-mode=promiscuous-bridge --runtime-cgroups=/docker-daemon --kubelet-cgroups=/kubelet --babysit-daemons=true --max-pods=110 --serialize-image-pulls=false --outofdisk-transition-frequency=0 Confirm the effective hairpin-mode. To do this, you'll have to look at kubelet log. Accessing the logs depends on your Node OS. On some OSes it is a file, such as /var/log/kubelet.log, while other OSes use journalctl to access logs. Please be noted that the effective hairpin mode may not match --hairpin-mode flag due to compatibility. Check if there is any log lines with key word hairpin in kubelet.log. There should be log lines indicating the effective hairpin mode, like something below.I0629 00:51:43.648698 3252 kubelet.go:380] Hairpin mode set to "promiscuous-bridge" If the effective hairpin mode is hairpin-veth, ensure the Kubelet has the permission to operate in /sys on node. If everything works properly, you should see something like:for intf in /sys/devices/virtual/net/cbr0/brif/*; do cat $intf/hairpin_mode; done 1 1 1 1 If the effective hairpin mode is promiscuous-bridge, ensure Kubelet has the permission to manipulate linux bridge on node. If cbr0 bridge is used and configured properly, you should see:ifconfig cbr0 |grep PROMISC UP BROADCAST RUNNING PROMISC MULTICAST MTU:1460 Metric:1 Seek help if none of above works out.Seek helpIf you get this far, something very strange is happening. Your Service is running, has EndpointSlices, and your Pods are actually serving. You have DNS working, and kube-proxy does not seem to be misbehaving. And yet your Service is not working. Please let us know what is going on, so we can help investigate!Contact us on Slack or Forum or GitHub.What's nextVisit the troubleshooting overview document for more information.4.4.1.3 - Debug a StatefulSetThis task shows you how to debug a StatefulSet.Before you beginYou need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster.You should have a StatefulSet running that you want to investigate.Debugging a StatefulSetIn order to list all the pods which belong to a StatefulSet, which have a label app.kubernetes.io/name=MyApp set on them, you can use the following:kubectl get pods -l app.kubernetes.io/name=MyApp If you find that any Pods listed are in Unknown or Terminating state for an extended period of time, refer to the Deleting StatefulSet Pods task for instructions on how to deal with them. You can debug individual Pods in a StatefulSet using the Debugging Pods guide.What's nextLearn more about debugging an init-container.4.4.1.4 - Determine the Reason for Pod FailureThis page shows how to write and read a Container termination message.Termination messages provide a way for containers to write information about fatal events to a location where it can be easily retrieved and surfaced by tools like dashboards and monitoring software. In most cases, information that you put in a termination message should also be written to the general Kubernetes logs.Before you beginYou need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesWriting and reading a termination messageIn this exercise, you create a Pod that runs one container. The manifest for that Pod specifies a command that runs when the container starts:debug/termination.yaml apiVersion: v1 kind: Pod metadata: name: termination-demo spec: containers: \- name: termination-demo-container image: debian command: ["/bin/sh"] args: ["-c", "sleep 10 && echo Sleep expired > /dev/termination-log"] Create a Pod based on the YAML configuration file:kubectl apply -f https://k8s.io/examples/debug/termination.yaml In the YAML file, in the command and args fields, you can see that the container sleeps for 10 seconds and then writes "Sleep expired" to the /dev/termination-log file. After the container writes the "Sleep expired" message, it terminates.Display information about the Pod:kubectl get pod termination-demo Repeat the preceding command until the Pod is no longer running.Display detailed information about the Pod:kubectl get pod termination-demo --output=yaml The output includes the "Sleep expired" message:apiVersion: v1 kind: Pod ... lastState: terminated: containerID: ... exitCode: 0 finishedAt: ... message: | Sleep expired ... Use a Go template to filter the output so that it includes only the termination message:kubectl get pod termination-demo -o go-template="{{range .status.containerStatuses}}{{.lastState.terminated.message}}{{end}}" If you are running a multi-container Pod, you can use a Go template to include the container's name. By doing so, you can discover which of the containers is failing:kubectl get pod multi-container-pod -o go-template='{{range .status.containerStatuses}}{{printf "%s:\n%s\n\n" .name .lastState.terminated.message}}{{end}}' Customizing the termination messageKubernetes retrieves termination messages from the termination message file specified in the terminationMessagePath field of a Container, which has a default value of /dev/termination-log. By customizing this field, you can tell Kubernetes to use a different file. Kubernetes use the contents from the specified file to populate the Container's status message on both success and failure.The termination message is intended to be brief final status, such as an assertion failure message. The kubelet truncates messages that are longer than 4096 bytes.The total message length across all containers is limited to 12KiB, divided equally among each container. For example, if there are 12 containers (initContainers or containers), each has 1024 bytes of available termination message space.The default termination message path is /dev/termination-log. You cannot set the termination message path after a Pod is launched.In the following example, the container writes termination messages to /tmp/my-log for Kubernetes to retrieve:apiVersion: v1 kind: Pod metadata: name: msg-path-demo spec: containers: \- name: msg-path-demo-container image: debian terminationMessagePath: "/tmp/my-log" Moreover, users can set the terminationMessagePolicy field of a Container for further customization. This field defaults to "File" which means the termination messages are retrieved only from the termination message file. By setting the terminationMessagePolicy to "FallbackToLogsOnError", you can tell Kubernetes to use the last chunk of container log output if the termination message file is empty and the container exited with an error. The log output is limited to 2048 bytes or 80 lines, whichever is smaller.What's nextSee the terminationMessagePath field in Container.See ImagePullBackOff in Images.Learn about retrieving logs.Learn about Go templates.Learn about Pod status and Pod phase.Learn about container states.4.4.1.5 - Debug Init ContainersThis page shows how to investigate problems related to the execution of Init Containers. The example command lines below refer to the Pod as  and the Init Containers as  and .Before you beginYou need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesTo check the version, enter kubectl version.You should be familiar with the basics of Init Containers.You should have Configured an Init Container.Checking the status of Init ContainersDisplay the status of your pod:kubectl get pod  For example, a status of Init:1/2 indicates that one of two Init Containers has completed successfully:NAME READY STATUS RESTARTS AGE  0/1 Init:1/2 0 7s See Understanding Pod status for more examples of status values and their meanings.Getting details about Init ContainersView more detailed information about Init Container execution:kubectl describe pod  For example, a Pod with two Init Containers might show the following:Init Containers: : Container ID: ... ... State: Terminated Reason: Completed Exit Code: 0 Started: ... Finished: ... Ready: True Restart Count: 0 ... : Container ID: ... ... State: Waiting Reason: CrashLoopBackOff Last State: Terminated Reason: Error Exit Code: 1 Started: ... Finished: ... Ready: False Restart Count: 3 ... You can also access the Init Container statuses programmatically by reading the status.initContainerStatuses field on the Pod Spec:kubectl get pod nginx --template '{{.status.initContainerStatuses}}' This command will return the same information as above in raw JSON.Accessing logs from Init ContainersPass the Init Container name along with the Pod name to access its logs.kubectl logs  -c  Init Containers that run a shell script print commands as they're executed. For example, you can do this in Bash by running set -x at the beginning of the script.Understanding Pod statusA Pod status beginning with Init: summarizes the status of Init Container execution. The table below describes some example status values that you might see while debugging Init Containers.StatusMeaningInit:N/MThe Pod has M Init Containers, and N have completed so far.Init:ErrorAn Init Container has failed to execute.Init:CrashLoopBackOffAn Init Container has failed repeatedly.PendingThe Pod has not yet begun executing Init Containers.PodInitializing or RunningThe Pod has already finished executing Init Containers.4.4.1.6 - Debug Running PodsThis page explains how to debug Pods running (or crashing) on a Node.Before you beginYour Pod should already be scheduled and running. If your Pod is not yet running, start with Debugging Pods.For some of the advanced debugging steps you need to know on which Node the Pod is running and have shell access to run commands on that Node. You don't need that access to run the standard debug steps that use kubectl.Using kubectl describe pod to fetch details about podsFor this example we'll use a Deployment to create two pods, similar to the earlier example.application/nginx-with-request.yaml apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment spec: selector: matchLabels: app: nginx replicas: 2 template: metadata: labels: app: nginx spec: containers: \- name: nginx image: nginx resources: limits: memory: "128Mi" cpu: "500m" ports: \- containerPort: 80 Create deployment by running following command:kubectl apply -f https://k8s.io/examples/application/nginx-with-request.yaml deployment.apps/nginx-deployment created Check pod status by following command:kubectl get pods NAME READY STATUS RESTARTS AGE nginx-deployment-67d4bdd6f5-cx2nz 1/1 Running 0 13s nginx-deployment-67d4bdd6f5-w6kd7 1/1 Running 0 13s We can retrieve a lot more information about each of these pods using kubectl describe pod. For example:kubectl describe pod nginx-deployment-67d4bdd6f5-w6kd7 Name: nginx-deployment-67d4bdd6f5-w6kd7 Namespace: default Priority: 0 Node: kube-worker-1/192.168.0.113 Start Time: Thu, 17 Feb 2022 16:51:01 -0500 Labels: app=nginx pod-template-hash=67d4bdd6f5 Annotations:  Status: Running IP: 10.88.0.3 IPs: IP: 10.88.0.3 IP: 2001:db8::1 Controlled By: ReplicaSet/nginx-deployment-67d4bdd6f5 Containers: nginx: Container ID: containerd://5403af59a2b46ee5a23fb0ae4b1e077f7ca5c5fb7af16e1ab21c00e0e616462a Image: nginx Image ID: docker.io/library/nginx@sha256:2834dc507516af02784808c5f48b7cbe38b8ed5d0f4837f16e78d00deb7e7767 Port: 80/TCP Host Port: 0/TCP State: Running Started: Thu, 17 Feb 2022 16:51:05 -0500 Ready: True Restart Count: 0 Limits: cpu: 500m memory: 128Mi Requests: cpu: 500m memory: 128Mi Environment:  Mounts: /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-bgsgp (ro) Conditions: Type Status Initialized True Ready True ContainersReady True PodScheduled True Volumes: kube-api-access-bgsgp: Type: Projected (a volume that contains injected data from multiple sources) TokenExpirationSeconds: 3607 ConfigMapName: kube-root-ca.crt ConfigMapOptional:  DownwardAPI: true QoS Class: Guaranteed Node-Selectors:  Tolerations: node.kubernetes.io/not-ready:NoExecute op=Exists for 300s node.kubernetes.io/unreachable:NoExecute op=Exists for 300s Events: Type Reason Age From Message \---- ------ ---- ---- ------- Normal Scheduled 34s default-scheduler Successfully assigned default/nginx-deployment-67d4bdd6f5-w6kd7 to kube-worker-1 Normal Pulling 31s kubelet Pulling image "nginx" Normal Pulled 30s kubelet Successfully pulled image "nginx" in 1.146417389s Normal Created 30s kubelet Created container nginx Normal Started 30s kubelet Started container nginx Here you can see configuration information about the container(s) and Pod (labels, resource requirements, etc.), as well as status information about the container(s) and Pod (state, readiness, restart count, events, etc.).The container state is one of Waiting, Running, or Terminated. Depending on the state, additional information will be provided -- here you can see that for a container in Running state, the system tells you when the container started.Ready tells you whether the container passed its last readiness probe. (In this case, the container does not have a readiness probe configured; the container is assumed to be ready if no readiness probe is configured.)Restart Count tells you how many times the container has been restarted; this information can be useful for detecting crash loops in containers that are configured with a restart policy of 'always.'Currently the only Condition associated with a Pod is the binary Ready condition, which indicates that the pod is able to service requests and should be added to the load balancing pools of all matching services.Lastly, you see a log of recent events related to your Pod. "From" indicates the component that is logging the event. "Reason" and "Message" tell you what happened.Example: debugging Pending PodsA common scenario that you can detect using events is when you've created a Pod that won't fit on any node. For example, the Pod might request more resources than are free on any node, or it might specify a label selector that doesn't match any nodes. Let's say we created the previous Deployment with 5 replicas (instead of 2) and requesting 600 millicores instead of 500, on a four-node cluster where each (virtual) machine has 1 CPU. In that case one of the Pods will not be able to schedule. (Note that because of the cluster addon pods such as fluentd, skydns, etc., that run on each node, if we requested 1000 millicores then none of the Pods would be able to schedule.)kubectl get pods NAME READY STATUS RESTARTS AGE nginx-deployment-1006230814-6winp 1/1 Running 0 7m nginx-deployment-1006230814-fmgu3 1/1 Running 0 7m nginx-deployment-1370807587-6ekbw 1/1 Running 0 1m nginx-deployment-1370807587-fg172 0/1 Pending 0 1m nginx-deployment-1370807587-fz9sd 0/1 Pending 0 1m To find out why the nginx-deployment-1370807587-fz9sd pod is not running, we can use kubectl describe pod on the pending Pod and look at its events:kubectl describe pod nginx-deployment-1370807587-fz9sd Name: nginx-deployment-1370807587-fz9sd Namespace: default Node: / Labels: app=nginx,pod-template-hash=1370807587 Status: Pending IP: Controllers: ReplicaSet/nginx-deployment-1370807587 Containers: nginx: Image: nginx Port: 80/TCP QoS Tier: memory: Guaranteed cpu: Guaranteed Limits: cpu: 1 memory: 128Mi Requests: cpu: 1 memory: 128Mi Environment Variables: Volumes: default-token-4bcbi: Type: Secret (a volume populated by a Secret) SecretName: default-token-4bcbi Events: FirstSeen LastSeen Count From SubobjectPath Type Reason Message \--------- -------- ----- ---- ------------- -------- ------ ------- 1m 48s 7 {default-scheduler } Warning FailedScheduling pod (nginx-deployment-1370807587-fz9sd) failed to fit in any node fit failure on node (kubernetes-node-6ta5): Node didn't have enough resource: CPU, requested: 1000, used: 1420, capacity: 2000 fit failure on node (kubernetes-node-wul5): Node didn't have enough resource: CPU, requested: 1000, used: 1100, capacity: 2000 Here you can see the event generated by the scheduler saying that the Pod failed to schedule for reason FailedScheduling (and possibly others). The message tells us that there were not enough resources for the Pod on any of the nodes.To correct this situation, you can use kubectl scale to update your Deployment to specify four or fewer replicas. (Or you could leave the one Pod pending, which is harmless.)Events such as the ones you saw at the end of kubectl describe pod are persisted in etcd and provide high-level information on what is happening in the cluster. To list all events you can usekubectl get events but you have to remember that events are namespaced. This means that if you're interested in events for some namespaced object (e.g. what happened with Pods in namespace my-namespace) you need to explicitly provide a namespace to the command:kubectl get events --namespace=my-namespace To see events from all namespaces, you can use the --all-namespaces argument.In addition to kubectl describe pod, another way to get extra information about a pod (beyond what is provided by kubectl get pod) is to pass the -o yaml output format flag to kubectl get pod. This will give you, in YAML format, even more information than kubectl describe pod--essentially all of the information the system has about the Pod. Here you will see things like annotations (which are key-value metadata without the label restrictions, that is used internally by Kubernetes system components), restart policy, ports, and volumes.kubectl get pod nginx-deployment-1006230814-6winp -o yaml apiVersion: v1 kind: Pod metadata: creationTimestamp: "2022-02-17T21:51:01Z" generateName: nginx-deployment-67d4bdd6f5- labels: app: nginx pod-template-hash: 67d4bdd6f5 name: nginx-deployment-67d4bdd6f5-w6kd7 namespace: default ownerReferences: \- apiVersion: apps/v1 blockOwnerDeletion: true controller: true kind: ReplicaSet name: nginx-deployment-67d4bdd6f5 uid: 7d41dfd4-84c0-4be4-88ab-cedbe626ad82 resourceVersion: "1364" uid: a6501da1-0447-4262-98eb-c03d4002222e spec: containers: \- image: nginx imagePullPolicy: Always name: nginx ports: \- containerPort: 80 protocol: TCP resources: limits: cpu: 500m memory: 128Mi requests: cpu: 500m memory: 128Mi terminationMessagePath: /dev/termination-log terminationMessagePolicy: File volumeMounts: \- mountPath: /var/run/secrets/kubernetes.io/serviceaccount name: kube-api-access-bgsgp readOnly: true dnsPolicy: ClusterFirst enableServiceLinks: true nodeName: kube-worker-1 preemptionPolicy: PreemptLowerPriority priority: 0 restartPolicy: Always schedulerName: default-scheduler securityContext: {} serviceAccount: default serviceAccountName: default terminationGracePeriodSeconds: 30 tolerations: \- effect: NoExecute key: node.kubernetes.io/not-ready operator: Exists tolerationSeconds: 300 \- effect: NoExecute key: node.kubernetes.io/unreachable operator: Exists tolerationSeconds: 300 volumes: \- name: kube-api-access-bgsgp projected: defaultMode: 420 sources: \- serviceAccountToken: expirationSeconds: 3607 path: token \- configMap: items: \- key: ca.crt path: ca.crt name: kube-root-ca.crt \- downwardAPI: items: \- fieldRef: apiVersion: v1 fieldPath: metadata.namespace path: namespace status: conditions: \- lastProbeTime: null lastTransitionTime: "2022-02-17T21:51:01Z" status: "True" type: Initialized \- lastProbeTime: null lastTransitionTime: "2022-02-17T21:51:06Z" status: "True" type: Ready \- lastProbeTime: null lastTransitionTime: "2022-02-17T21:51:06Z" status: "True" type: ContainersReady \- lastProbeTime: null lastTransitionTime: "2022-02-17T21:51:01Z" status: "True" type: PodScheduled containerStatuses: \- containerID: containerd://5403af59a2b46ee5a23fb0ae4b1e077f7ca5c5fb7af16e1ab21c00e0e616462a image: docker.io/library/nginx:latest imageID: docker.io/library/nginx@sha256:2834dc507516af02784808c5f48b7cbe38b8ed5d0f4837f16e78d00deb7e7767 lastState: {} name: nginx ready: true restartCount: 0 started: true state: running: startedAt: "2022-02-17T21:51:05Z" hostIP: 192.168.0.113 phase: Running podIP: 10.88.0.3 podIPs: \- ip: 10.88.0.3 \- ip: 2001:db8::1 qosClass: Guaranteed startTime: "2022-02-17T21:51:01Z" Examining pod logsFirst, look at the logs of the affected container:kubectl logs ${POD_NAME} ${CONTAINER_NAME} If your container has previously crashed, you can access the previous container's crash log with:kubectl logs --previous ${POD_NAME} ${CONTAINER_NAME} Debugging with container execIf the container image includes debugging utilities, as is the case with images built from Linux and Windows OS base images, you can run commands inside a specific container with kubectl exec:kubectl exec ${POD_NAME} -c ${CONTAINER_NAME} -- ${CMD} ${ARG1} ${ARG2} ... ${ARGN} Note:-c ${CONTAINER_NAME} is optional. You can omit it for Pods that only contain a single container.As an example, to look at the logs from a running Cassandra pod, you might runkubectl exec cassandra -- cat /var/log/cassandra/system.log You can run a shell that's connected to your terminal using the -i and -t arguments to kubectl exec, for example:kubectl exec -it cassandra -- sh For more details, see Get a Shell to a Running Container.Debugging with an ephemeral debug containerFEATURE STATE: Kubernetes v1.25 [stable]Ephemeral containers are useful for interactive troubleshooting when kubectl exec is insufficient because a container has crashed or a container image doesn't include debugging utilities, such as with distroless images.Example debugging using ephemeral containersYou can use the kubectl debug command to add ephemeral containers to a running Pod. First, create a pod for the example:kubectl run ephemeral-demo --image=registry.k8s.io/pause:3.1 --restart=Never The examples in this section use the pause container image because it does not contain debugging utilities, but this method works with all container images.If you attempt to use kubectl exec to create a shell you will see an error because there is no shell in this container image.kubectl exec -it ephemeral-demo -- sh OCI runtime exec failed: exec failed: container_linux.go:346: starting container process caused "exec: \"sh\": executable file not found in $PATH": unknown You can instead add a debugging container using kubectl debug. If you specify the -i/--interactive argument, kubectl will automatically attach to the console of the Ephemeral Container.kubectl debug -it ephemeral-demo --image=busybox:1.28 --target=ephemeral-demo Defaulting debug container name to debugger-8xzrl. If you don't see a command prompt, try pressing enter. / # This command adds a new busybox container and attaches to it. The --target parameter targets the process namespace of another container. It's necessary here because kubectl run does not enable process namespace sharing in the pod it creates.Note:The --target parameter must be supported by the Container Runtime. When not supported, the Ephemeral Container may not be started, or it may be started with an isolated process namespace so that ps does not reveal processes in other containers.You can view the state of the newly created ephemeral container using kubectl describe:kubectl describe pod ephemeral-demo ... Ephemeral Containers: debugger-8xzrl: Container ID: docker://b888f9adfd15bd5739fefaa39e1df4dd3c617b9902082b1cfdc29c4028ffb2eb Image: busybox Image ID: docker-pullable://busybox@sha256:1828edd60c5efd34b2bf5dd3282ec0cc04d47b2ff9caa0b6d4f07a21d1c08084 Port:  Host Port:  State: Running Started: Wed, 12 Feb 2020 14:25:42 +0100 Ready: False Restart Count: 0 Environment:  Mounts:  ... Use kubectl delete to remove the Pod when you're finished:kubectl delete pod ephemeral-demo Debugging using a copy of the PodSometimes Pod configuration options make it difficult to troubleshoot in certain situations. For example, you can't run kubectl exec to troubleshoot your container if your container image does not include a shell or if your application crashes on startup. In these situations you can use kubectl debug to create a copy of the Pod with configuration values changed to aid debugging.Copying a Pod while adding a new containerAdding a new container can be useful when your application is running but not behaving as you expect and you'd like to add additional troubleshooting utilities to the Pod.For example, maybe your application's container images are built on busybox but you need debugging utilities not included in busybox. You can simulate this scenario using kubectl run:kubectl run myapp --image=busybox:1.28 --restart=Never -- sleep 1d Run this command to create a copy of myapp named myapp-debug that adds a new Ubuntu container for debugging:kubectl debug myapp -it --image=ubuntu --share-processes --copy-to=myapp-debug Defaulting debug container name to debugger-w7xmf. If you don't see a command prompt, try pressing enter. root@myapp-debug:/# Note:kubectl debug automatically generates a container name if you don't choose one using the --container flag.The -i flag causes kubectl debug to attach to the new container by default. You can prevent this by specifying --attach=false. If your session becomes disconnected you can reattach using kubectl attach.The --share-processes allows the containers in this Pod to see processes from the other containers in the Pod. For more information about how this works, see Share Process Namespace between Containers in a Pod.Don't forget to clean up the debugging Pod when you're finished with it:kubectl delete pod myapp myapp-debug Copying a Pod while changing its commandSometimes it's useful to change the command for a container, for example to add a debugging flag or because the application is crashing.To simulate a crashing application, use kubectl run to create a container that immediately exits:kubectl run --image=busybox:1.28 myapp -- false You can see using kubectl describe pod myapp that this container is crashing:Containers: myapp: Image: busybox ... Args: false State: Waiting Reason: CrashLoopBackOff Last State: Terminated Reason: Error Exit Code: 1 You can use kubectl debug to create a copy of this Pod with the command changed to an interactive shell:kubectl debug myapp -it --copy-to=myapp-debug --container=myapp -- sh If you don't see a command prompt, try pressing enter. / # Now you have an interactive shell that you can use to perform tasks like checking filesystem paths or running the container command manually.Note:To change the command of a specific container you must specify its name using --container or kubectl debug will instead create a new container to run the command you specified.The -i flag causes kubectl debug to attach to the container by default. You can prevent this by specifying --attach=false. If your session becomes disconnected you can reattach using kubectl attach.Don't forget to clean up the debugging Pod when you're finished with it:kubectl delete pod myapp myapp-debug Copying a Pod while changing container imagesIn some situations you may want to change a misbehaving Pod from its normal production container images to an image containing a debugging build or additional utilities.As an example, create a Pod using kubectl run:kubectl run myapp --image=busybox:1.28 --restart=Never -- sleep 1d Now use kubectl debug to make a copy and change its container image to ubuntu:kubectl debug myapp --copy-to=myapp-debug --set-image=*=ubuntu The syntax of --set-image uses the same container_name=image syntax as kubectl set image. *=ubuntu means change the image of all containers to ubuntu.Don't forget to clean up the debugging Pod when you're finished with it:kubectl delete pod myapp myapp-debug Debugging via a shell on the nodeIf none of these approaches work, you can find the Node on which the Pod is running and create a Pod running on the Node. To create an interactive shell on a Node using kubectl debug, run:kubectl debug node/mynode -it --image=ubuntu Creating debugging pod node-debugger-mynode-pdx84 with container debugger on node mynode. If you don't see a command prompt, try pressing enter. root@ek8s:/# When creating a debugging session on a node, keep in mind that:kubectl debug automatically generates the name of the new Pod based on the name of the Node.The root filesystem of the Node will be mounted at /host.The container runs in the host IPC, Network, and PID namespaces, although the pod isn't privileged, so reading some process information may fail, and chroot /host may fail.If you need a privileged pod, create it manually or use the --profile=sysadmin flag.Don't forget to clean up the debugging Pod when you're finished with it:kubectl delete pod node-debugger-mynode-pdx84 Debugging a Pod or Node while applying a profileWhen using kubectl debug to debug a node via a debugging Pod, a Pod via an ephemeral container, or a copied Pod, you can apply a profile to them. By applying a profile, specific properties such as securityContext are set, allowing for adaptation to various scenarios. There are two types of profiles, static profile and custom profile.Applying a Static ProfileA static profile is a set of predefined properties, and you can apply them using the --profile flag. The available profiles are as follows:ProfileDescriptionlegacyA set of properties backwards compatibility with 1.22 behaviorgeneralA reasonable set of generic properties for each debugging journeybaselineA set of properties compatible with PodSecurityStandard baseline policyrestrictedA set of properties compatible with PodSecurityStandard restricted policynetadminA set of properties including Network Administrator privilegessysadminA set of properties including System Administrator (root) privilegesNote:If you don't specify --profile, the legacy profile is used by default, but it is planned to be deprecated in the near future. So it is recommended to use other profiles such as general.Assume that you create a Pod and debug it. First, create a Pod named myapp as an example:kubectl run myapp --image=busybox:1.28 --restart=Never -- sleep 1d Then, debug the Pod using an ephemeral container. If the ephemeral container needs to have privilege, you can use the sysadmin profile:kubectl debug -it myapp --image=busybox:1.28 --target=myapp --profile=sysadmin Targeting container "myapp". If you don't see processes from this container it may be because the container runtime doesn't support this feature. Defaulting debug container name to debugger-6kg4x. If you don't see a command prompt, try pressing enter. / # Check the capabilities of the ephemeral container process by running the following command inside the container:/ # grep Cap /proc/$$/status ... CapPrm: 000001ffffffffff CapEff: 000001ffffffffff ... This means the container process is granted full capabilities as a privileged container by applying sysadmin profile. See more details about capabilities.You can also check that the ephemeral container was created as a privileged container:kubectl get pod myapp -o jsonpath='{.spec.ephemeralContainers[0].securityContext}' {"privileged":true} Clean up the Pod when you're finished with it:kubectl delete pod myapp Applying Custom ProfileFEATURE STATE: Kubernetes v1.32 [stable]You can define a partial container spec for debugging as a custom profile in either YAML or JSON format, and apply it using the --custom flag.Note:Custom profile only supports the modification of the container spec, but modifications to name, image, command, lifecycle and volumeDevices fields of the container spec are not allowed. It does not support the modification of the Pod spec.Create a Pod named myapp as an example:kubectl run myapp --image=busybox:1.28 --restart=Never -- sleep 1d Create a custom profile in YAML or JSON format. Here, create a YAML format file named custom-profile.yaml:env: \- name: ENV_VAR_1 value: value_1 \- name: ENV_VAR_2 value: value_2 securityContext: capabilities: add: \- NET_ADMIN \- SYS_TIME Run this command to debug the Pod using an ephemeral container with the custom profile:kubectl debug -it myapp --image=busybox:1.28 --target=myapp --profile=general --custom=custom-profile.yaml You can check that the ephemeral container has been added to the target Pod with the custom profile applied:kubectl get pod myapp -o jsonpath='{.spec.ephemeralContainers[0].env}' [{"name":"ENV_VAR_1","value":"value_1"},{"name":"ENV_VAR_2","value":"value_2"}] kubectl get pod myapp -o jsonpath='{.spec.ephemeralContainers[0].securityContext}' {"capabilities":{"add":["NET_ADMIN","SYS_TIME"]}} Clean up the Pod when you're finished with it:kubectl delete pod myapp 4.4.1.7 - Get a Shell to a Running ContainerThis page shows how to use kubectl exec to get a shell to a running container.Before you beginYou need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesGetting a shell to a containerIn this exercise, you create a Pod that has one container. The container runs the nginx image. Here is the configuration file for the Pod:application/shell-demo.yaml apiVersion: v1 kind: Pod metadata: name: shell-demo spec: volumes: \- name: shared-data emptyDir: {} containers: \- name: nginx image: nginx volumeMounts: \- name: shared-data mountPath: /usr/share/nginx/html hostNetwork: true dnsPolicy: Default Create the Pod:kubectl apply -f https://k8s.io/examples/application/shell-demo.yaml Verify that the container is running:kubectl get pod shell-demo Get a shell to the running container:kubectl exec --stdin --tty shell-demo -- /bin/bash Note:The double dash (--) separates the arguments you want to pass to the command from the kubectl arguments.In your shell, list the root directory:# Run this inside the container ls / In your shell, experiment with other commands. Here are some examples:# You can run these example commands inside the container ls / cat /proc/mounts cat /proc/1/maps apt-get update apt-get install -y tcpdump tcpdump apt-get install -y lsof lsof apt-get install -y procps ps aux ps aux | grep nginx Writing the root page for nginxLook again at the configuration file for your Pod. The Pod has an emptyDir volume, and the container mounts the volume at /usr/share/nginx/html.In your shell, create an index.html file in the /usr/share/nginx/html directory:# Run this inside the container echo 'Hello shell demo' > /usr/share/nginx/html/index.html In your shell, send a GET request to the nginx server:# Run this in the shell inside your container apt-get update apt-get install curl curl http://localhost/ The output shows the text that you wrote to the index.html file:Hello shell demo When you are finished with your shell, enter exit.exit # To quit the shell in the container Running individual commands in a containerIn an ordinary command window, not your shell, list the environment variables in the running container:kubectl exec shell-demo -- env Experiment with running other commands. Here are some examples:kubectl exec shell-demo -- ps aux kubectl exec shell-demo -- ls / kubectl exec shell-demo -- cat /proc/1/mounts Opening a shell when a Pod has more than one containerIf a Pod has more than one container, use --container or -c to specify a container in the kubectl exec command. For example, suppose you have a Pod named my-pod, and the Pod has two containers named main-app and helper-app. The following command would open a shell to the main-app container.kubectl exec -i -t my-pod --container main-app -- /bin/bash Note:The short options -i and -t are the same as the long options --stdin and --ttyWhat's nextRead about kubectl exec4.4.2 - Troubleshooting ClustersDebugging common cluster issues.This doc is about cluster troubleshooting; we assume you have already ruled out your application as the root cause of the problem you are experiencing. See the application troubleshooting guide for tips on application debugging. You may also visit the troubleshooting overview document for more information.For troubleshooting kubectl, refer to Troubleshooting kubectl.Listing your clusterThe first thing to debug in your cluster is if your nodes are all registered correctly.Run the following command:kubectl get nodes And verify that all of the nodes you expect to see are present and that they are all in the Ready state.To get detailed information about the overall health of your cluster, you can run:kubectl cluster-info dump Example: debugging a down/unreachable nodeSometimes when debugging it can be useful to look at the status of a node -- for example, because you've noticed strange behavior of a Pod that's running on the node, or to find out why a Pod won't schedule onto the node. As with Pods, you can use kubectl describe node and kubectl get node -o yaml to retrieve detailed information about nodes. For example, here's what you'll see if a node is down (disconnected from the network, or kubelet dies and won't restart, etc.). Notice the events that show the node is NotReady, and also notice that the pods are no longer running (they are evicted after five minutes of NotReady status).kubectl get nodes NAME STATUS ROLES AGE VERSION kube-worker-1 NotReady  1h v1.23.3 kubernetes-node-bols Ready  1h v1.23.3 kubernetes-node-st6x Ready  1h v1.23.3 kubernetes-node-unaj Ready  1h v1.23.3 kubectl describe node kube-worker-1 Name: kube-worker-1 Roles:  Labels: beta.kubernetes.io/arch=amd64 beta.kubernetes.io/os=linux kubernetes.io/arch=amd64 kubernetes.io/hostname=kube-worker-1 kubernetes.io/os=linux Annotations: kubeadm.alpha.kubernetes.io/cri-socket: /run/containerd/containerd.sock node.alpha.kubernetes.io/ttl: 0 volumes.kubernetes.io/controller-managed-attach-detach: true CreationTimestamp: Thu, 17 Feb 2022 16:46:30 -0500 Taints: node.kubernetes.io/unreachable:NoExecute node.kubernetes.io/unreachable:NoSchedule Unschedulable: false Lease: HolderIdentity: kube-worker-1 AcquireTime:  RenewTime: Thu, 17 Feb 2022 17:13:09 -0500 Conditions: Type Status LastHeartbeatTime LastTransitionTime Reason Message \---- ------ ----------------- ------------------ ------ ------- NetworkUnavailable False Thu, 17 Feb 2022 17:09:13 -0500 Thu, 17 Feb 2022 17:09:13 -0500 WeaveIsUp Weave pod has set this MemoryPressure Unknown Thu, 17 Feb 2022 17:12:40 -0500 Thu, 17 Feb 2022 17:13:52 -0500 NodeStatusUnknown Kubelet stopped posting node status. DiskPressure Unknown Thu, 17 Feb 2022 17:12:40 -0500 Thu, 17 Feb 2022 17:13:52 -0500 NodeStatusUnknown Kubelet stopped posting node status. PIDPressure Unknown Thu, 17 Feb 2022 17:12:40 -0500 Thu, 17 Feb 2022 17:13:52 -0500 NodeStatusUnknown Kubelet stopped posting node status. Ready Unknown Thu, 17 Feb 2022 17:12:40 -0500 Thu, 17 Feb 2022 17:13:52 -0500 NodeStatusUnknown Kubelet stopped posting node status. Addresses: InternalIP: 192.168.0.113 Hostname: kube-worker-1 Capacity: cpu: 2 ephemeral-storage: 15372232Ki hugepages-2Mi: 0 memory: 2025188Ki pods: 110 Allocatable: cpu: 2 ephemeral-storage: 14167048988 hugepages-2Mi: 0 memory: 1922788Ki pods: 110 System Info: Machine ID: 9384e2927f544209b5d7b67474bbf92b System UUID: aa829ca9-73d7-064d-9019-df07404ad448 Boot ID: 5a295a03-aaca-4340-af20-1327fa5dab5c Kernel Version: 5.13.0-28-generic OS Image: Ubuntu 21.10 Operating System: linux Architecture: amd64 Container Runtime Version: containerd://1.5.9 Kubelet Version: v1.23.3 Kube-Proxy Version: v1.23.3 Non-terminated Pods: (4 in total) Namespace Name CPU Requests CPU Limits Memory Requests Memory Limits Age \--------- ---- ------------ ---------- --------------- ------------- --- default nginx-deployment-67d4bdd6f5-cx2nz 500m (25%) 500m (25%) 128Mi (6%) 128Mi (6%) 23m default nginx-deployment-67d4bdd6f5-w6kd7 500m (25%) 500m (25%) 128Mi (6%) 128Mi (6%) 23m kube-system kube-proxy-dnxbz 0 (0%) 0 (0%) 0 (0%) 0 (0%) 28m kube-system weave-net-gjxxp 100m (5%) 0 (0%) 200Mi (10%) 0 (0%) 28m Allocated resources: (Total limits may be over 100 percent, i.e., overcommitted.) Resource Requests Limits \-------- -------- ------ cpu 1100m (55%) 1 (50%) memory 456Mi (24%) 256Mi (13%) ephemeral-storage 0 (0%) 0 (0%) hugepages-2Mi 0 (0%) 0 (0%) Events: ... kubectl get node kube-worker-1 -o yaml apiVersion: v1 kind: Node metadata: annotations: kubeadm.alpha.kubernetes.io/cri-socket: /run/containerd/containerd.sock node.alpha.kubernetes.io/ttl: "0" volumes.kubernetes.io/controller-managed-attach-detach: "true" creationTimestamp: "2022-02-17T21:46:30Z" labels: beta.kubernetes.io/arch: amd64 beta.kubernetes.io/os: linux kubernetes.io/arch: amd64 kubernetes.io/hostname: kube-worker-1 kubernetes.io/os: linux name: kube-worker-1 resourceVersion: "4026" uid: 98efe7cb-2978-4a0b-842a-1a7bf12c05f8 spec: {} status: addresses: \- address: 192.168.0.113 type: InternalIP \- address: kube-worker-1 type: Hostname allocatable: cpu: "2" ephemeral-storage: "14167048988" hugepages-2Mi: "0" memory: 1922788Ki pods: "110" capacity: cpu: "2" ephemeral-storage: 15372232Ki hugepages-2Mi: "0" memory: 2025188Ki pods: "110" conditions: \- lastHeartbeatTime: "2022-02-17T22:20:32Z" lastTransitionTime: "2022-02-17T22:20:32Z" message: Weave pod has set this reason: WeaveIsUp status: "False" type: NetworkUnavailable \- lastHeartbeatTime: "2022-02-17T22:20:15Z" lastTransitionTime: "2022-02-17T22:13:25Z" message: kubelet has sufficient memory available reason: KubeletHasSufficientMemory status: "False" type: MemoryPressure \- lastHeartbeatTime: "2022-02-17T22:20:15Z" lastTransitionTime: "2022-02-17T22:13:25Z" message: kubelet has no disk pressure reason: KubeletHasNoDiskPressure status: "False" type: DiskPressure \- lastHeartbeatTime: "2022-02-17T22:20:15Z" lastTransitionTime: "2022-02-17T22:13:25Z" message: kubelet has sufficient PID available reason: KubeletHasSufficientPID status: "False" type: PIDPressure \- lastHeartbeatTime: "2022-02-17T22:20:15Z" lastTransitionTime: "2022-02-17T22:15:15Z" message: kubelet is posting ready status reason: KubeletReady status: "True" type: Ready daemonEndpoints: kubeletEndpoint: Port: 10250 nodeInfo: architecture: amd64 bootID: 22333234-7a6b-44d4-9ce1-67e31dc7e369 containerRuntimeVersion: containerd://1.5.9 kernelVersion: 5.13.0-28-generic kubeProxyVersion: v1.23.3 kubeletVersion: v1.23.3 machineID: 9384e2927f544209b5d7b67474bbf92b operatingSystem: linux osImage: Ubuntu 21.10 systemUUID: aa829ca9-73d7-064d-9019-df07404ad448 Looking at logsFor now, digging deeper into the cluster requires logging into the relevant machines. Here are the locations of the relevant log files. On systemd-based systems, you may need to use journalctl instead of examining log files.Control Plane nodes/var/log/kube-apiserver.log - API Server, responsible for serving the API/var/log/kube-scheduler.log - Scheduler, responsible for making scheduling decisions/var/log/kube-controller-manager.log - a component that runs most Kubernetes built-in controllers, with the notable exception of scheduling (the kube-scheduler handles scheduling).Worker Nodes/var/log/kubelet.log - logs from the kubelet, responsible for running containers on the node/var/log/kube-proxy.log - logs from kube-proxy, which is responsible for directing traffic to Service endpointsCluster failure modesThis is an incomplete list of things that could go wrong, and how to adjust your cluster setup to mitigate the problems.Contributing causesVM(s) shutdownNetwork partition within cluster, or between cluster and usersCrashes in Kubernetes softwareData loss or unavailability of persistent storage (e.g. GCE PD or AWS EBS volume)Operator error, for example, misconfigured Kubernetes software or application softwareSpecific scenariosAPI server VM shutdown or apiserver crashingResultsunable to stop, update, or start new pods, services, replication controllerexisting pods and services should continue to work normally unless they depend on the Kubernetes APIAPI server backing storage lostResultsthe kube-apiserver component fails to start successfully and become healthykubelets will not be able to reach it but will continue to run the same pods and provide the same service proxyingmanual recovery or recreation of apiserver state necessary before apiserver is restartedSupporting services (node controller, replication controller manager, scheduler, etc) VM shutdown or crashescurrently those are colocated with the apiserver, and their unavailability has similar consequences as apiserverin future, these will be replicated as well and may not be co-locatedthey do not have their own persistent stateIndividual node (VM or physical machine) shuts downResultspods on that Node stop runningNetwork partitionResultspartition A thinks the nodes in partition B are down; partition B thinks the apiserver is down. (Assuming the master VM ends up in partition A.)Kubelet software faultResultscrashing kubelet cannot start new pods on the nodekubelet might delete the pods or notnode marked unhealthyreplication controllers start new pods elsewhereCluster operator errorResultsloss of pods, services, etclost of apiserver backing storeusers unable to read APIetc.MitigationsAction: Use the IaaS provider's automatic VM restarting feature for IaaS VMsMitigates: Apiserver VM shutdown or apiserver crashingMitigates: Supporting services VM shutdown or crashesAction: Use IaaS providers reliable storage (e.g. GCE PD or AWS EBS volume) for VMs with apiserver+etcdMitigates: Apiserver backing storage lostAction: Use high-availability configurationMitigates: Control plane node shutdown or control plane components (scheduler, API server, controller-manager) crashingWill tolerate one or more simultaneous node or component failuresMitigates: API server backing storage (i.e., etcd's data directory) lostAssumes HA (highly-available) etcd configurationAction: Snapshot apiserver PDs/EBS-volumes periodicallyMitigates: Apiserver backing storage lostMitigates: Some cases of operator errorMitigates: Some cases of Kubernetes software faultAction: use replication controller and services in front of podsMitigates: Node shutdownMitigates: Kubelet software faultAction: applications (containers) designed to tolerate unexpected restartsMitigates: Node shutdownMitigates: Kubelet software faultWhat's nextLearn about the metrics available in the Resource Metrics PipelineDiscover additional tools for monitoring resource usageUse Node Problem Detector to monitor node healthUse kubectl debug node to debug Kubernetes nodesUse crictl to debug Kubernetes nodesGet more information about Kubernetes auditingUse telepresence to develop and debug services locally4.4.2.1 - Troubleshooting kubectlThis documentation is about investigating and diagnosing kubectl related issues. If you encounter issues accessing kubectl or connecting to your cluster, this document outlines various common scenarios and potential solutions to help identify and address the likely cause.Before you beginYou need to have a Kubernetes cluster.You also need to have kubectl installed - see install toolsVerify kubectl setupMake sure you have installed and configured kubectl correctly on your local machine. Check the kubectl version to ensure it is up-to-date and compatible with your cluster.Check kubectl version:kubectl version You'll see a similar output:Client Version: version.Info{Major:"1", Minor:"27", GitVersion:"v1.27.4",GitCommit:"fa3d7990104d7c1f16943a67f11b154b71f6a132", GitTreeState:"clean",BuildDate:"2023-07-19T12:20:54Z", GoVersion:"go1.20.6", Compiler:"gc", Platform:"linux/amd64"} Kustomize Version: v5.0.1 Server Version: version.Info{Major:"1", Minor:"27", GitVersion:"v1.27.3",GitCommit:"25b4e43193bcda6c7328a6d147b1fb73a33f1598", GitTreeState:"clean",BuildDate:"2023-06-14T09:47:40Z", GoVersion:"go1.20.5", Compiler:"gc", Platform:"linux/amd64"} If you see Unable to connect to the server: dial tcp :8443: i/o timeout, instead of Server Version, you need to troubleshoot kubectl connectivity with your cluster.Make sure you have installed the kubectl by following the official documentation for installing kubectl, and you have properly configured the $PATH environment variable.Check kubeconfigThe kubectl requires a kubeconfig file to connect to a Kubernetes cluster. The kubeconfig file is usually located under the ~/.kube/config directory. Make sure that you have a valid kubeconfig file. If you don't have a kubeconfig file, you can obtain it from your Kubernetes administrator, or you can copy it from your Kubernetes control plane's /etc/kubernetes/admin.conf directory. If you have deployed your Kubernetes cluster on a cloud platform and lost your kubeconfig file, you can re-generate it using your cloud provider's tools. Refer the cloud provider's documentation for re-generating a kubeconfig file.Check if the $KUBECONFIG environment variable is configured correctly. You can set $KUBECONFIGenvironment variable or use the --kubeconfig parameter with the kubectl to specify the directory of a kubeconfig file.Check VPN connectivityIf you are using a Virtual Private Network (VPN) to access your Kubernetes cluster, make sure that your VPN connection is active and stable. Sometimes, VPN disconnections can lead to connection issues with the cluster. Reconnect to the VPN and try accessing the cluster again.Authentication and authorizationIf you are using the token based authentication and the kubectl is returning an error regarding the authentication token or authentication server address, validate the Kubernetes authentication token and the authentication server address are configured properly.If kubectl is returning an error regarding the authorization, make sure that you are using the valid user credentials. And you have the permission to access the resource that you have requested.Verify contextsKubernetes supports multiple clusters and contexts. Ensure that you are using the correct context to interact with your cluster.List available contexts:kubectl config get-contexts Switch to the appropriate context:kubectl config use-context  API server and load balancerThe kube-apiserver server is the central component of a Kubernetes cluster. If the API server or the load balancer that runs in front of your API servers is not reachable or not responding, you won't be able to interact with the cluster.Check the if the API server's host is reachable by using ping command. Check cluster's network connectivity and firewall. If your are using a cloud provider for deploying the cluster, check your cloud provider's health check status for the cluster's API server.Verify the status of the load balancer (if used) to ensure it is healthy and forwarding traffic to the API server.TLS problemsAdditional tools required - base64 and openssl version 3.0 or above.The Kubernetes API server only serves HTTPS requests by default. In that case TLS problems may occur due to various reasons, such as certificate expiry or chain of trust validity.You can find the TLS certificate in the kubeconfig file, located in the ~/.kube/config directory. The certificate-authority attribute contains the CA certificate and the client-certificate attribute contains the client certificate.Verify the expiry of these certificates:kubectl config view --flatten --output 'jsonpath={.clusters[0].cluster.certificate-authority-data}' | base64 -d | openssl x509 -noout -dates output:notBefore=Feb 13 05:57:47 2024 GMT notAfter=Feb 10 06:02:47 2034 GMT kubectl config view --flatten --output 'jsonpath={.users[0].user.client-certificate-data}'| base64 -d | openssl x509 -noout -dates output:notBefore=Feb 13 05:57:47 2024 GMT notAfter=Feb 12 06:02:50 2025 GMT Verify kubectl helpersSome kubectl authentication helpers provide easy access to Kubernetes clusters. If you have used such helpers and are facing connectivity issues, ensure that the necessary configurations are still present.Check kubectl configuration for authentication details:kubectl config view If you previously used a helper tool (for example, kubectl-oidc-login), ensure that it is still installed and configured correctly.4.4.2.2 - Resource metrics pipelineFor Kubernetes, the Metrics API offers a basic set of metrics to support automatic scaling and similar use cases. This API makes information available about resource usage for node and pod, including metrics for CPU and memory. If you deploy the Metrics API into your cluster, clients of the Kubernetes API can then query for this information, and you can use Kubernetes' access control mechanisms to manage permissions to do so.The HorizontalPodAutoscaler (HPA) and VerticalPodAutoscaler (VPA) use data from the metrics API to adjust workload replicas and resources to meet customer demand.You can also view the resource metrics using the kubectl top command.Note:The Metrics API, and the metrics pipeline that it enables, only offers the minimum CPU and memory metrics to enable automatic scaling using HPA and / or VPA. If you would like to provide a more complete set of metrics, you can complement the simpler Metrics API by deploying a second metrics pipeline that uses the Custom Metrics API.Figure 1 illustrates the architecture of the resource metrics pipeline.flowchart RL subgraph cluster[Cluster] direction RL S[] A[Metrics-Server] subgraph B[Nodes] direction TB D[cAdvisor] --> C[kubelet] E[Containerruntime] --> D E1[Containerruntime] --> D P[pod data] -.- C end L[APIserver] W[HPA] C ---->|node levelresource metrics| A -->|metricsAPI| L --> W end L ---> K[kubectltop] classDef box fill:#fff,stroke:#000,stroke-width:1px,color:#000; class W,B,P,K,cluster,D,E,E1 box classDef spacewhite fill:#ffffff,stroke:#fff,stroke-width:0px,color:#000 class S spacewhite classDef k8s fill:#326ce5,stroke:#fff,stroke-width:1px,color:#fff; class A,L,C k8sJavaScript must be enabled to view this contentFigure 1. Resource Metrics PipelineThe architecture components, from right to left in the figure, consist of the following:cAdvisor: Daemon for collecting, aggregating and exposing container metrics included in Kubelet.kubelet: Node agent for managing container resources. Resource metrics are accessible using the /metrics/resource and /stats kubelet API endpoints.node level resource metrics: API provided by the kubelet for discovering and retrieving per-node summarized stats available through the /metrics/resource endpoint.metrics-server: Cluster addon component that collects and aggregates resource metrics pulled from each kubelet. The API server serves Metrics API for use by HPA, VPA, and by the kubectl top command. Metrics Server is a reference implementation of the Metrics API.Metrics API: Kubernetes API supporting access to CPU and memory used for workload autoscaling. To make this work in your cluster, you need an API extension server that provides the Metrics API.Note:cAdvisor supports reading metrics from cgroups, which works with typical container runtimes on Linux. If you use a container runtime that uses another resource isolation mechanism, for example virtualization, then that container runtime must support CRI Container Metrics in order for metrics to be available to the kubelet.Metrics APIFEATURE STATE: Kubernetes 1.8 [beta]The metrics-server implements the Metrics API. This API allows you to access CPU and memory usage for the nodes and pods in your cluster. Its primary role is to feed resource usage metrics to K8s autoscaler components.Here is an example of the Metrics API request for a minikube node piped through jq for easier reading:kubectl get --raw "/apis/metrics.k8s.io/v1beta1/nodes/minikube" | jq '.' Here is the same API call using curl:curl http://localhost:8080/apis/metrics.k8s.io/v1beta1/nodes/minikube Sample response:{ "kind": "NodeMetrics", "apiVersion": "metrics.k8s.io/v1beta1", "metadata": { "name": "minikube", "selfLink": "/apis/metrics.k8s.io/v1beta1/nodes/minikube", "creationTimestamp": "2022-01-27T18:48:43Z" }, "timestamp": "2022-01-27T18:48:33Z", "window": "30s", "usage": { "cpu": "487558164n", "memory": "732212Ki" } } Here is an example of the Metrics API request for a kube-scheduler-minikube pod contained in the kube-system namespace and piped through jq for easier reading:kubectl get --raw "/apis/metrics.k8s.io/v1beta1/namespaces/kube-system/pods/kube-scheduler-minikube" | jq '.' Here is the same API call using curl:curl http://localhost:8080/apis/metrics.k8s.io/v1beta1/namespaces/kube-system/pods/kube-scheduler-minikube Sample response:{ "kind": "PodMetrics", "apiVersion": "metrics.k8s.io/v1beta1", "metadata": { "name": "kube-scheduler-minikube", "namespace": "kube-system", "selfLink": "/apis/metrics.k8s.io/v1beta1/namespaces/kube-system/pods/kube-scheduler-minikube", "creationTimestamp": "2022-01-27T19:25:00Z" }, "timestamp": "2022-01-27T19:24:31Z", "window": "30s", "containers": [ { "name": "kube-scheduler", "usage": { "cpu": "9559630n", "memory": "22244Ki" } } ] } The Metrics API is defined in the k8s.io/metrics repository. You must enable the API aggregation layer and register an APIService for the metrics.k8s.io API.To learn more about the Metrics API, see resource metrics API design, the metrics-server repository and the resource metrics API.Note:You must deploy the metrics-server or alternative adapter that serves the Metrics API to be able to access it.Measuring resource usageCPUCPU is reported as the average core usage measured in cpu units. One cpu, in Kubernetes, is equivalent to 1 vCPU/Core for cloud providers, and 1 hyper-thread on bare-metal Intel processors.This value is derived by taking a rate over a cumulative CPU counter provided by the kernel (in both Linux and Windows kernels). The time window used to calculate CPU is shown under window field in Metrics API.To learn more about how Kubernetes allocates and measures CPU resources, see meaning of CPU.MemoryMemory is reported as the working set, measured in bytes, at the instant the metric was collected.In an ideal world, the "working set" is the amount of memory in-use that cannot be freed under memory pressure. However, calculation of the working set varies by host OS, and generally makes heavy use of heuristics to produce an estimate.The Kubernetes model for a container's working set expects that the container runtime counts anonymous memory associated with the container in question. The working set metric typically also includes some cached (file-backed) memory, because the host OS cannot always reclaim pages.To learn more about how Kubernetes allocates and measures memory resources, see meaning of memory.Metrics ServerThe metrics-server fetches resource metrics from the kubelets and exposes them in the Kubernetes API server through the Metrics API for use by the HPA and VPA. You can also view these metrics using the kubectl top command.The metrics-server uses the Kubernetes API to track nodes and pods in your cluster. The metrics-server queries each node over HTTP to fetch metrics. The metrics-server also builds an internal view of pod metadata, and keeps a cache of pod health. That cached pod health information is available via the extension API that the metrics-server makes available.For example with an HPA query, the metrics-server needs to identify which pods fulfill the label selectors in the deployment.The metrics-server calls the kubelet API to collect metrics from each node. Depending on the metrics-server version it uses:Metrics resource endpoint /metrics/resource in version v0.6.0+ orSummary API endpoint /stats/summary in older versionsWhat's nextTo learn more about the metrics-server, see the metrics-server repository.You can also check out the following:metrics-server designmetrics-server FAQmetrics-server known issuesmetrics-server releasesHorizontal Pod AutoscalingTo learn about how the kubelet serves node metrics, and how you can access those via the Kubernetes API, read Node Metrics Data.4.4.2.3 - Tools for Monitoring ResourcesTo scale an application and provide a reliable service, you need to understand how the application behaves when it is deployed. You can examine application performance in a Kubernetes cluster by examining the containers, pods, services, and the characteristics of the overall cluster. Kubernetes provides detailed information about an application's resource usage at each of these levels. This information allows you to evaluate your application's performance and where bottlenecks can be removed to improve overall performance.In Kubernetes, application monitoring does not depend on a single monitoring solution. On new clusters, you can use resource metrics or full metrics pipelines to collect monitoring statistics.Resource metrics pipelineThe resource metrics pipeline provides a limited set of metrics related to cluster components such as the Horizontal Pod Autoscaler controller, as well as the kubectl top utility. These metrics are collected by the lightweight, short-term, in-memory metrics-server and are exposed via the metrics.k8s.io API.metrics-server discovers all nodes on the cluster and queries each node's kubelet for CPU and memory usage. The kubelet acts as a bridge between the Kubernetes master and the nodes, managing the pods and containers running on a machine. The kubelet translates each pod into its constituent containers and fetches individual container usage statistics from the container runtime through the container runtime interface. If you use a container runtime that uses Linux cgroups and namespaces to implement containers, and the container runtime does not publish usage statistics, then the kubelet can look up those statistics directly (using code from cAdvisor). No matter how those statistics arrive, the kubelet then exposes the aggregated pod resource usage statistics through the metrics-server Resource Metrics API. This API is served at /metrics/resource/v1beta1 on the kubelet's authenticated and read-only ports.Full metrics pipelineA full metrics pipeline gives you access to richer metrics. Kubernetes can respond to these metrics by automatically scaling or adapting the cluster based on its current state, using mechanisms such as the Horizontal Pod Autoscaler. The monitoring pipeline fetches metrics from the kubelet and then exposes them to Kubernetes via an adapter by implementing either the custom.metrics.k8s.io or external.metrics.k8s.io API.Kubernetes is designed to work with OpenMetrics, which is one of the CNCF Observability and Analysis - Monitoring Projects, built upon and carefully extending Prometheus exposition format in almost 100% backwards-compatible ways.If you glance over at the CNCF Landscape, you can see a number of monitoring projects that can work with Kubernetes by scraping metric data and using that to help you observe your cluster. It is up to you to select the tool or tools that suit your needs. The CNCF landscape for observability and analytics includes a mix of open-source software, paid-for software-as-a-service, and other commercial products.When you design and implement a full metrics pipeline you can make that monitoring data available back to Kubernetes. For example, a HorizontalPodAutoscaler can use the processed metrics to work out how many Pods to run for a component of your workload.Integration of a full metrics pipeline into your Kubernetes implementation is outside the scope of Kubernetes documentation because of the very wide scope of possible solutions.The choice of monitoring platform depends heavily on your needs, budget, and technical resources. Kubernetes does not recommend any specific metrics pipeline; many options are available. Your monitoring system should be capable of handling the OpenMetrics metrics transmission standard and needs to be chosen to best fit into your overall design and deployment of your infrastructure platform.What's nextLearn about additional debugging tools, including:LoggingGetting into containers via execConnecting to containers via proxiesConnecting to containers via port forwardingInspect Kubernetes node with crictl4.4.2.4 - Monitor Node HealthNode Problem Detector is a daemon for monitoring and reporting about a node's health. You can run Node Problem Detector as a DaemonSet or as a standalone daemon. Node Problem Detector collects information about node problems from various daemons and reports these conditions to the API server as Node Conditions or as Events.To learn how to install and use Node Problem Detector, see Node Problem Detector project documentation.Before you beginYou need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesLimitationsNode Problem Detector uses the kernel log format for reporting kernel issues. To learn how to extend the kernel log format, see Add support for another log format.Enabling Node Problem DetectorSome cloud providers enable Node Problem Detector as an Addon. You can also enable Node Problem Detector with kubectl or by creating an Addon DaemonSet.Using kubectl to enable Node Problem Detectorkubectl provides the most flexible management of Node Problem Detector. You can overwrite the default configuration to fit it into your environment or to detect customized node problems. For example:Create a Node Problem Detector configuration similar to node-problem-detector.yaml:debug/node-problem-detector.yaml apiVersion: apps/v1 kind: DaemonSet metadata: name: node-problem-detector-v0.1 namespace: kube-system labels: k8s-app: node-problem-detector version: v0.1 kubernetes.io/cluster-service: "true" spec: selector: matchLabels: k8s-app: node-problem-detector version: v0.1 kubernetes.io/cluster-service: "true" template: metadata: labels: k8s-app: node-problem-detector version: v0.1 kubernetes.io/cluster-service: "true" spec: hostNetwork: true containers: \- name: node-problem-detector image: registry.k8s.io/node-problem-detector:v0.1 securityContext: privileged: true resources: limits: cpu: "200m" memory: "100Mi" requests: cpu: "20m" memory: "20Mi" volumeMounts: \- name: log mountPath: /log readOnly: true volumes: \- name: log hostPath: path: /var/log/Note:You should verify that the system log directory is right for your operating system distribution.Start node problem detector with kubectl:kubectl apply -f https://k8s.io/examples/debug/node-problem-detector.yaml Using an Addon pod to enable Node Problem DetectorIf you are using a custom cluster bootstrap solution and don't need to overwrite the default configuration, you can leverage the Addon pod to further automate the deployment.Create node-problem-detector.yaml, and save the configuration in the Addon pod's directory /etc/kubernetes/addons/node-problem-detector on a control plane node.Overwrite the configurationThe default configuration is embedded when building the Docker image of Node Problem Detector.However, you can use a ConfigMap to overwrite the configuration:Change the configuration files in config/Create the ConfigMap node-problem-detector-config:kubectl create configmap node-problem-detector-config --from-file=config/ Change the node-problem-detector.yaml to use the ConfigMap:debug/node-problem-detector-configmap.yaml apiVersion: apps/v1 kind: DaemonSet metadata: name: node-problem-detector-v0.1 namespace: kube-system labels: k8s-app: node-problem-detector version: v0.1 kubernetes.io/cluster-service: "true" spec: selector: matchLabels: k8s-app: node-problem-detector version: v0.1 kubernetes.io/cluster-service: "true" template: metadata: labels: k8s-app: node-problem-detector version: v0.1 kubernetes.io/cluster-service: "true" spec: hostNetwork: true containers: \- name: node-problem-detector image: registry.k8s.io/node-problem-detector:v0.1 securityContext: privileged: true resources: limits: cpu: "200m" memory: "100Mi" requests: cpu: "20m" memory: "20Mi" volumeMounts: \- name: log mountPath: /log readOnly: true \- name: config # Overwrite the config/ directory with ConfigMap volume mountPath: /config readOnly: true volumes: \- name: log hostPath: path: /var/log/ \- name: config # Define ConfigMap volume configMap: name: node-problem-detector-configRecreate the Node Problem Detector with the new configuration file:# If you have a node-problem-detector running, delete before recreating kubectl delete -f https://k8s.io/examples/debug/node-problem-detector.yaml kubectl apply -f https://k8s.io/examples/debug/node-problem-detector-configmap.yaml Note:This approach only applies to a Node Problem Detector started with kubectl.Overwriting a configuration is not supported if a Node Problem Detector runs as a cluster Addon. The Addon manager does not support ConfigMap.Problem DaemonsA problem daemon is a sub-daemon of the Node Problem Detector. It monitors specific kinds of node problems and reports them to the Node Problem Detector. There are several types of supported problem daemons.A SystemLogMonitor type of daemon monitors the system logs and reports problems and metrics according to predefined rules. You can customize the configurations for different log sources such as filelog, kmsg, kernel, abrt, and systemd.A SystemStatsMonitor type of daemon collects various health-related system stats as metrics. You can customize its behavior by updating its configuration file.A CustomPluginMonitor type of daemon invokes and checks various node problems by running user-defined scripts. You can use different custom plugin monitors to monitor different problems and customize the daemon behavior by updating the configuration file.A HealthChecker type of daemon checks the health of the kubelet and container runtime on a node.Adding support for other log formatThe system log monitor currently supports file-based logs, journald, and kmsg. Additional sources can be added by implementing a new log watcher.Adding custom plugin monitorsYou can extend the Node Problem Detector to execute any monitor scripts written in any language by developing a custom plugin. The monitor scripts must conform to the plugin protocol in exit code and standard output. For more information, please refer to the plugin interface proposal.ExporterAn exporter reports the node problems and/or metrics to certain backends. The following exporters are supported:Kubernetes exporter: this exporter reports node problems to the Kubernetes API server. Temporary problems are reported as Events and permanent problems are reported as Node Conditions.Prometheus exporter: this exporter reports node problems and metrics locally as Prometheus (or OpenMetrics) metrics. You can specify the IP address and port for the exporter using command line arguments.Stackdriver exporter: this exporter reports node problems and metrics to the Stackdriver Monitoring API. The exporting behavior can be customized using a configuration file.Recommendations and restrictionsIt is recommended to run the Node Problem Detector in your cluster to monitor node health. When running the Node Problem Detector, you can expect extra resource overhead on each node. Usually this is fine, because:The kernel log grows relatively slowly.A resource limit is set for the Node Problem Detector.Even under high load, the resource usage is acceptable. For more information, see the Node Problem Detector benchmark result.4.4.2.5 - Debugging Kubernetes nodes with crictlFEATURE STATE: Kubernetes v1.11 [stable]crictl is a command-line interface for CRI-compatible container runtimes. You can use it to inspect and debug container runtimes and applications on a Kubernetes node. crictl and its source are hosted in the cri-tools repository.Before you begincrictl requires a Linux operating system with a CRI runtime.Installing crictlYou can download a compressed archive crictl from the cri-tools release page, for several different architectures. Download the version that corresponds to your version of Kubernetes. Extract it and move it to a location on your system path, such as /usr/local/bin/.General usageThe crictl command has several subcommands and runtime flags. Use crictl help or crictl  help for more details.You can set the endpoint for crictl by doing one of the following:Set the --runtime-endpoint and --image-endpoint flags.Set the CONTAINER_RUNTIME_ENDPOINT and IMAGE_SERVICE_ENDPOINT environment variables.Set the endpoint in the configuration file /etc/crictl.yaml. To specify a different file, use the --config=PATH_TO_FILE flag when you run crictl.Note:If you don't set an endpoint, crictl attempts to connect to a list of known endpoints, which might result in an impact to performance.You can also specify timeout values when connecting to the server and enable or disable debugging, by specifying timeout or debug values in the configuration file or using the --timeout and --debug command-line flags.To view or edit the current configuration, view or edit the contents of /etc/crictl.yaml. For example, the configuration when using the containerd container runtime would be similar to this:runtime-endpoint: unix:///var/run/containerd/containerd.sock image-endpoint: unix:///var/run/containerd/containerd.sock timeout: 10 debug: true To learn more about crictl, refer to the crictl documentation.Example crictl commandsThe following examples show some crictl commands and example output.List podsList all pods:crictl pods The output is similar to this:POD ID CREATED STATE NAME NAMESPACE ATTEMPT 926f1b5a1d33a About a minute ago Ready sh-84d7dcf559-4r2gq default 0 4dccb216c4adb About a minute ago Ready nginx-65899c769f-wv2gp default 0 a86316e96fa89 17 hours ago Ready kube-proxy-gblk4 kube-system 0 919630b8f81f1 17 hours ago Ready nvidia-device-plugin-zgbbv kube-system 0 List pods by name:crictl pods --name nginx-65899c769f-wv2gp The output is similar to this:POD ID CREATED STATE NAME NAMESPACE ATTEMPT 4dccb216c4adb 2 minutes ago Ready nginx-65899c769f-wv2gp default 0 List pods by label:crictl pods --label run=nginx The output is similar to this:POD ID CREATED STATE NAME NAMESPACE ATTEMPT 4dccb216c4adb 2 minutes ago Ready nginx-65899c769f-wv2gp default 0 List imagesList all images:crictl images The output is similar to this:IMAGE TAG IMAGE ID SIZE busybox latest 8c811b4aec35f 1.15MB k8s-gcrio.azureedge.net/hyperkube-amd64 v1.10.3 e179bbfe5d238 665MB k8s-gcrio.azureedge.net/pause-amd64 3.1 da86e6ba6ca19 742kB nginx latest cd5239a0906a6 109MB List images by repository:crictl images nginx The output is similar to this:IMAGE TAG IMAGE ID SIZE nginx latest cd5239a0906a6 109MB Only list image IDs:crictl images -q The output is similar to this:sha256:8c811b4aec35f259572d0f79207bc0678df4c736eeec50bc9fec37ed936a472a sha256:e179bbfe5d238de6069f3b03fccbecc3fb4f2019af741bfff1233c4d7b2970c5 sha256:da86e6ba6ca197bf6bc5e9d900febd906b133eaa4750e6bed647b0fbe50ed43e sha256:cd5239a0906a6ccf0562354852fae04bc5b52d72a2aff9a871ddb6bd57553569 List containersList all containers:crictl ps -a The output is similar to this:CONTAINER ID IMAGE CREATED STATE NAME ATTEMPT 1f73f2d81bf98 busybox@sha256:141c253bc4c3fd0a201d32dc1f493bcf3fff003b6df416dea4f41046e0f37d47 7 minutes ago Running sh 1 9c5951df22c78 busybox@sha256:141c253bc4c3fd0a201d32dc1f493bcf3fff003b6df416dea4f41046e0f37d47 8 minutes ago Exited sh 0 87d3992f84f74 nginx@sha256:d0a8828cccb73397acb0073bf34f4d7d8aa315263f1e7806bf8c55d8ac139d5f 8 minutes ago Running nginx 0 1941fb4da154f k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:00d814b1f7763f4ab5be80c58e98140dfc69df107f253d7fdd714b30a714260a 18 hours ago Running kube-proxy 0 List running containers:crictl ps The output is similar to this:CONTAINER ID IMAGE CREATED STATE NAME ATTEMPT 1f73f2d81bf98 busybox@sha256:141c253bc4c3fd0a201d32dc1f493bcf3fff003b6df416dea4f41046e0f37d47 6 minutes ago Running sh 1 87d3992f84f74 nginx@sha256:d0a8828cccb73397acb0073bf34f4d7d8aa315263f1e7806bf8c55d8ac139d5f 7 minutes ago Running nginx 0 1941fb4da154f k8s-gcrio.azureedge.net/hyperkube-amd64@sha256:00d814b1f7763f4ab5be80c58e98140dfc69df107f253d7fdd714b30a714260a 17 hours ago Running kube-proxy 0 Execute a command in a running containercrictl exec -i -t 1f73f2d81bf98 ls The output is similar to this:bin dev etc home proc root sys tmp usr var Get a container's logsGet all container logs:crictl logs 87d3992f84f74 The output is similar to this:10.240.0.96 - - [06/Jun/2018:02:45:49 +0000] "GET / HTTP/1.1" 200 612 "-" "curl/7.47.0" "-" 10.240.0.96 - - [06/Jun/2018:02:45:50 +0000] "GET / HTTP/1.1" 200 612 "-" "curl/7.47.0" "-" 10.240.0.96 - - [06/Jun/2018:02:45:51 +0000] "GET / HTTP/1.1" 200 612 "-" "curl/7.47.0" "-" Get only the latest N lines of logs:crictl logs --tail=1 87d3992f84f74 The output is similar to this:10.240.0.96 - - [06/Jun/2018:02:45:51 +0000] "GET / HTTP/1.1" 200 612 "-" "curl/7.47.0" "-" What's nextLearn more about crictl.4.4.2.6 - AuditingKubernetes auditing provides a security-relevant, chronological set of records documenting the sequence of actions in a cluster. The cluster audits the activities generated by users, by applications that use the Kubernetes API, and by the control plane itself.Auditing allows cluster administrators to answer the following questions:what happened?when did it happen?who initiated it?on what did it happen?where was it observed?from where was it initiated?to where was it going?Audit records begin their lifecycle inside the kube-apiserver component. Each request on each stage of its execution generates an audit event, which is then pre-processed according to a certain policy and written to a backend. The policy determines what's recorded and the backends persist the records. The current backend implementations include logs files and webhooks.Each request can be recorded with an associated stage. The defined stages are:RequestReceived - The stage for events generated as soon as the audit handler receives the request, and before it is delegated down the handler chain.ResponseStarted - Once the response headers are sent, but before the response body is sent. This stage is only generated for long-running requests (e.g. watch).ResponseComplete - The response body has been completed and no more bytes will be sent.Panic - Events generated when a panic occurred.Note:The configuration of an Audit Event configuration is different from the Event API object.The audit logging feature increases the memory consumption of the API server because some context required for auditing is stored for each request. Memory consumption depends on the audit logging configuration.Audit policyAudit policy defines rules about what events should be recorded and what data they should include. The audit policy object structure is defined in the audit.k8s.io API group. When an event is processed, it's compared against the list of rules in order. The first matching rule sets the audit level of the event. The defined audit levels are:None - don't log events that match this rule.Metadata - log events with metadata (requesting user, timestamp, resource, verb, etc.) but not request or response body.Request - log events with request metadata and body but not response body. This does not apply for non-resource requests.RequestResponse - log events with request metadata, request body and response body. This does not apply for non-resource requests.You can pass a file with the policy to kube-apiserver using the --audit-policy-file flag. If the flag is omitted, no events are logged. Note that the rules field must be provided in the audit policy file. A policy with no (0) rules is treated as illegal.Below is an example audit policy file:audit/audit-policy.yaml apiVersion: audit.k8s.io/v1 # This is required. kind: Policy # Don't generate audit events for all requests in RequestReceived stage. omitStages: \- "RequestReceived" rules: # Log pod changes at RequestResponse level \- level: RequestResponse resources: \- group: "" # Resource "pods" doesn't match requests to any subresource of pods, # which is consistent with the RBAC policy. resources: ["pods"] # Log "pods/log", "pods/status" at Metadata level \- level: Metadata resources: \- group: "" resources: ["pods/log", "pods/status"] # Don't log requests to a configmap called "controller-leader" \- level: None resources: \- group: "" resources: ["configmaps"] resourceNames: ["controller-leader"] # Don't log watch requests by the "system:kube-proxy" on endpoints or services \- level: None users: ["system:kube-proxy"] verbs: ["watch"] resources: \- group: "" # core API group resources: ["endpoints", "services"] # Don't log authenticated requests to certain non-resource URL paths. \- level: None userGroups: ["system:authenticated"] nonResourceURLs: \- "/api*" # Wildcard matching. \- "/version" # Log the request body of configmap changes in kube-system. \- level: Request resources: \- group: "" # core API group resources: ["configmaps"] # This rule only applies to resources in the "kube-system" namespace. # The empty string "" can be used to select non-namespaced resources. namespaces: ["kube-system"] # Log configmap and secret changes in all other namespaces at the Metadata level. \- level: Metadata resources: \- group: "" # core API group resources: ["secrets", "configmaps"] # Log all other resources in core and extensions at the Request level. \- level: Request resources: \- group: "" # core API group \- group: "extensions" # Version of group should NOT be included. # A catch-all rule to log all other requests at the Metadata level. \- level: Metadata # Long-running requests like watches that fall under this rule will not # generate an audit event in RequestReceived. omitStages: \- "RequestReceived" You can use a minimal audit policy file to log all requests at the Metadata level:# Log all requests at the Metadata level. apiVersion: audit.k8s.io/v1 kind: Policy rules: \- level: Metadata If you're crafting your own audit profile, you can use the audit profile for Google Container-Optimized OS as a starting point. You can check the configure-helper.sh script, which generates an audit policy file. You can see most of the audit policy file by looking directly at the script.You can also refer to the Policy configuration reference for details about the fields defined.Audit backendsAudit backends persist audit events to an external storage. Out of the box, the kube-apiserver provides two backends:Log backend, which writes events into the filesystemWebhook backend, which sends events to an external HTTP APIIn all cases, audit events follow a structure defined by the Kubernetes API in the audit.k8s.io API group.Note:In case of patches, request body is a JSON array with patch operations, not a JSON object with an appropriate Kubernetes API object. For example, the following request body is a valid patch request to /apis/batch/v1/namespaces/some-namespace/jobs/some-job-name:[ { "op": "replace", "path": "/spec/parallelism", "value": 0 }, { "op": "remove", "path": "/spec/template/spec/containers/0/terminationMessagePolicy" } ] Log backendThe log backend writes audit events to a file in JSONlines format. You can configure the log audit backend using the following kube-apiserver flags:--audit-log-path specifies the log file path that log backend uses to write audit events. Not specifying this flag disables log backend. - means standard out--audit-log-maxage defined the maximum number of days to retain old audit log files--audit-log-maxbackup defines the maximum number of audit log files to retain--audit-log-maxsize defines the maximum size in megabytes of the audit log file before it gets rotatedIf your cluster's control plane runs the kube-apiserver as a Pod, remember to mount the hostPath to the location of the policy file and log file, so that audit records are persisted. For example: - --audit-policy-file=/etc/kubernetes/audit-policy.yaml \- --audit-log-path=/var/log/kubernetes/audit/audit.log then mount the volumes:... volumeMounts: \- mountPath: /etc/kubernetes/audit-policy.yaml name: audit readOnly: true \- mountPath: /var/log/kubernetes/audit/ name: audit-log readOnly: false and finally configure the hostPath:... volumes: \- name: audit hostPath: path: /etc/kubernetes/audit-policy.yaml type: File \- name: audit-log hostPath: path: /var/log/kubernetes/audit/ type: DirectoryOrCreate Webhook backendThe webhook audit backend sends audit events to a remote web API, which is assumed to be a form of the Kubernetes API, including means of authentication. You can configure a webhook audit backend using the following kube-apiserver flags:--audit-webhook-config-file specifies the path to a file with a webhook configuration. The webhook configuration is effectively a specialized kubeconfig.--audit-webhook-initial-backoff specifies the amount of time to wait after the first failed request before retrying. Subsequent requests are retried with exponential backoff.The webhook config file uses the kubeconfig format to specify the remote address of the service and credentials used to connect to it.Event batchingBoth log and webhook backends support batching. Using webhook as an example, here's the list of available flags. To get the same flag for log backend, replace webhook with log in the flag name. By default, batching is enabled in webhook and disabled in log. Similarly, by default throttling is enabled in webhook and disabled in log.--audit-webhook-mode defines the buffering strategy. One of the following:batch - buffer events and asynchronously process them in batches. This is the default.blocking - block API server responses on processing each individual event.blocking-strict - Same as blocking, but when there is a failure during audit logging at the RequestReceived stage, the whole request to the kube-apiserver fails.The following flags are used only in the batch mode:--audit-webhook-batch-buffer-size defines the number of events to buffer before batching. If the rate of incoming events overflows the buffer, events are dropped.--audit-webhook-batch-max-size defines the maximum number of events in one batch.--audit-webhook-batch-max-wait defines the maximum amount of time to wait before unconditionally batching events in the queue.--audit-webhook-batch-throttle-qps defines the maximum average number of batches generated per second.--audit-webhook-batch-throttle-burst defines the maximum number of batches generated at the same moment if the allowed QPS was underutilized previously.Parameter tuningParameters should be set to accommodate the load on the API server.For example, if kube-apiserver receives 100 requests each second, and each request is audited only on ResponseStarted and ResponseComplete stages, you should account for ≅200 audit events being generated each second. Assuming that there are up to 100 events in a batch, you should set throttling level at least 2 queries per second. Assuming that the backend can take up to 5 seconds to write events, you should set the buffer size to hold up to 5 seconds of events; that is: 10 batches, or 1000 events.In most cases however, the default parameters should be sufficient and you don't have to worry about setting them manually. You can look at the following Prometheus metrics exposed by kube-apiserver and in the logs to monitor the state of the auditing subsystem.apiserver_audit_event_total metric contains the total number of audit events exported.apiserver_audit_error_total metric contains the total number of events dropped due to an error during exporting.Log entry truncationBoth log and webhook backends support limiting the size of events that are logged. As an example, the following is the list of flags available for the log backend:audit-log-truncate-enabled whether event and batch truncating is enabled.audit-log-truncate-max-batch-size maximum size in bytes of the batch sent to the underlying backend.audit-log-truncate-max-event-size maximum size in bytes of the audit event sent to the underlying backend.By default truncate is disabled in both webhook and log, a cluster administrator should set audit-log-truncate-enabled or audit-webhook-truncate-enabled to enable the feature.What's nextLearn about Mutating webhook auditing annotations.Learn more about Event and the Policy resource types by reading the Audit configuration reference.4.4.2.7 - Debugging Kubernetes Nodes With KubectlThis page shows how to debug a node running on the Kubernetes cluster using kubectl debug command.Before you beginYou need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesYour Kubernetes server must be at or later than version 1.2.To check the version, enter kubectl version.You need to have permission to create Pods and to assign those new Pods to arbitrary nodes. You also need to be authorized to create Pods that access filesystems from the host.Debugging a Node using kubectl debug nodeUse the kubectl debug node command to deploy a Pod to a Node that you want to troubleshoot. This command is helpful in scenarios where you can't access your Node by using an SSH connection. When the Pod is created, the Pod opens an interactive shell on the Node. To create an interactive shell on a Node named “mynode”, run:kubectl debug node/mynode -it --image=ubuntu Creating debugging pod node-debugger-mynode-pdx84 with container debugger on node mynode. If you don't see a command prompt, try pressing enter. root@mynode:/# The debug command helps to gather information and troubleshoot issues. Commands that you might use include ip, ifconfig, nc, ping, and ps and so on. You can also install other tools, such as mtr, tcpdump, and curl, from the respective package manager.Note:The debug commands may differ based on the image the debugging pod is using and these commands might need to be installed.The debugging Pod can access the root filesystem of the Node, mounted at /host in the Pod. If you run your kubelet in a filesystem namespace, the debugging Pod sees the root for that namespace, not for the entire node. For a typical Linux node, you can look at the following paths to find relevant logs:/host/var/log/kubelet.logLogs from the kubelet, responsible for running containers on the node./host/var/log/kube-proxy.logLogs from kube-proxy, which is responsible for directing traffic to Service endpoints./host/var/log/containerd.logLogs from the containerd process running on the node./host/var/log/syslogShows general messages and information regarding the system./host/var/log/kern.logShows kernel logs.When creating a debugging session on a Node, keep in mind that:kubectl debug automatically generates the name of the new pod, based on the name of the node.The root filesystem of the Node will be mounted at /host.Although the container runs in the host IPC, Network, and PID namespaces, the pod isn't privileged. This means that reading some process information might fail because access to that information is restricted to superusers. For example, chroot /host will fail. If you need a privileged pod, create it manually or use the --profile=sysadmin flag.By applying Debugging Profiles, you can set specific properties such as securityContext to a debugging Pod.Cleaning upWhen you finish using the debugging Pod, delete it:kubectl get pods NAME READY STATUS RESTARTS AGE node-debugger-mynode-pdx84 0/1 Completed 0 8m1s # Change the pod name accordingly kubectl delete pod node-debugger-mynode-pdx84 --now pod "node-debugger-mynode-pdx84" deleted Note:The kubectl debug node command won't work if the Node is down (disconnected from the network, or kubelet dies and won't restart, etc.). Check debugging a down/unreachable node in that case.4.4.2.8 - Developing and debugging services locally using telepresenceNote: This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the content guide before submitting a change. More information.Kubernetes applications usually consist of multiple, separate services, each running in its own container. Developing and debugging these services on a remote Kubernetes cluster can be cumbersome, requiring you to get a shell on a running container in order to run debugging tools.telepresence is a tool to ease the process of developing and debugging services locally while proxying the service to a remote Kubernetes cluster. Using telepresence allows you to use custom tools, such as a debugger and IDE, for a local service and provides the service full access to ConfigMap, secrets, and the services running on the remote cluster.This document describes using telepresence to develop and debug services running on a remote cluster locally.Before you beginKubernetes cluster is installedkubectl is configured to communicate with the clusterTelepresence is installedConnecting your local machine to a remote Kubernetes clusterAfter installing telepresence, run telepresence connect to launch its Daemon and connect your local workstation to the cluster.$ telepresence connect Launching Telepresence Daemon ... Connected to context default (https://) You can curl services using the Kubernetes syntax e.g. curl -ik https://kubernetes.defaultDeveloping or debugging an existing serviceWhen developing an application on Kubernetes, you typically program or debug a single service. The service might require access to other services for testing and debugging. One option is to use the continuous deployment pipeline, but even the fastest deployment pipeline introduces a delay in the program or debug cycle.Use the telepresence intercept $SERVICE_NAME --port $LOCAL_PORT:$REMOTE_PORT command to create an "intercept" for rerouting remote service traffic.Where:$SERVICE_NAME is the name of your local service$LOCAL_PORT is the port that your service is running on your local workstationAnd $REMOTE_PORT is the port your service listens to in the clusterRunning this command tells Telepresence to send remote traffic to your local service instead of the service in the remote Kubernetes cluster. Make edits to your service source code locally, save, and see the corresponding changes when accessing your remote application take effect immediately. You can also run your local service using a debugger or any other local development tool.How does Telepresence work?Telepresence installs a traffic-agent sidecar next to your existing application's container running in the remote cluster. It then captures all traffic requests going into the Pod, and instead of forwarding this to the application in the remote cluster, it routes all traffic (when you create a global intercept or a subset of the traffic (when you create a personal intercept) to your local development environment.What's nextIf you're interested in a hands-on tutorial, check out this tutorial that walks through locally developing the Guestbook application on Google Kubernetes Engine.For further reading, visit the Telepresence website.4.4.2.9 - Windows debugging tipsNode-level troubleshootingMy Pods are stuck at "Container Creating" or restarting over and overEnsure that your pause image is compatible with your Windows OS version. See Pause container to see the latest / recommended pause image and/or get more information.Note:If using containerd as your container runtime the pause image is specified in the plugins.plugins.cri.sandbox_image field of the of config.toml configuration file.My pods show status as ErrImgPull or ImagePullBackOffEnsure that your Pod is getting scheduled to a compatible Windows Node.More information on how to specify a compatible node for your Pod can be found in this guide.Network troubleshootingMy Windows Pods do not have network connectivityIf you are using virtual machines, ensure that MAC spoofing is enabled on all the VM network adapter(s).My Windows Pods cannot ping external resourcesWindows Pods do not have outbound rules programmed for the ICMP protocol. However, TCP/UDP is supported. When trying to demonstrate connectivity to resources outside of the cluster, substitute ping  with corresponding curl  commands.If you are still facing problems, most likely your network configuration in cni.conf deserves some extra attention. You can always edit this static file. The configuration update will apply to any new Kubernetes resources.One of the Kubernetes networking requirements (see Kubernetes model) is for cluster communication to occur without NAT internally. To honor this requirement, there is an ExceptionList for all the communication where you do not want outbound NAT to occur. However, this also means that you need to exclude the external IP you are trying to query from the ExceptionList. Only then will the traffic originating from your Windows pods be SNAT'ed correctly to receive a response from the outside world. In this regard, your ExceptionList in cni.conf should look as follows:"ExceptionList": [ "10.244.0.0/16", # Cluster subnet "10.96.0.0/12", # Service subnet "10.127.130.0/24" # Management (host) subnet ] My Windows node cannot access NodePort type ServicesLocal NodePort access from the node itself fails. This is a known limitation. NodePort access works from other nodes or external clients.vNICs and HNS endpoints of containers are being deletedThis issue can be caused when the hostname-override parameter is not passed to kube-proxy. To resolve it, users need to pass the hostname to kube-proxy as follows:C:\k\kube-proxy.exe --hostname-override=$(hostname) My Windows node cannot access my services using the service IPThis is a known limitation of the networking stack on Windows. However, Windows Pods can access the Service IP.No network adapter is found when starting the kubeletThe Windows networking stack needs a virtual adapter for Kubernetes networking to work. If the following commands return no results (in an admin shell), virtual network creation — a necessary prerequisite for the kubelet to work — has failed:Get-HnsNetwork | ? Name -ieq "cbr0" Get-NetAdapter | ? Name -Like "vEthernet (Ethernet*" Often it is worthwhile to modify the InterfaceName parameter of the start.ps1 script, in cases where the host's network adapter isn't "Ethernet". Otherwise, consult the output of the start-kubelet.ps1 script to see if there are errors during virtual network creation.DNS resolution is not properly workingCheck the DNS limitations for Windows in this section.kubectl port-forward fails with "unable to do port forwarding: wincat not found"This was implemented in Kubernetes 1.15 by including wincat.exe in the pause infrastructure container mcr.microsoft.com/oss/kubernetes/pause:3.6. Be sure to use a supported version of Kubernetes. If you would like to build your own pause infrastructure container be sure to include wincat.My Kubernetes installation is failing because my Windows Server node is behind a proxyIf you are behind a proxy, the following PowerShell environment variables must be defined:[Environment]::SetEnvironmentVariable("HTTP_PROXY", "http://proxy.example.com:80/", [EnvironmentVariableTarget]::Machine) [Environment]::SetEnvironmentVariable("HTTPS_PROXY", "http://proxy.example.com:443/", [EnvironmentVariableTarget]::Machine) Flannel troubleshootingWith Flannel, my nodes are having issues after rejoining a clusterWhenever a previously deleted node is being re-joined to the cluster, flannelD tries to assign a new pod subnet to the node. Users should remove the old pod subnet configuration files in the following paths:Remove-Item C:\k\SourceVip.json Remove-Item C:\k\SourceVipRequest.json Flanneld is stuck in "Waiting for the Network to be created"There are numerous reports of this issue; most likely it is a timing issue for when the management IP of the flannel network is set. A workaround is to relaunch start.ps1 or relaunch it manually as follows:[Environment]::SetEnvironmentVariable("NODE_NAME", "") C:\flannel\flanneld.exe --kubeconfig-file=c:\k\config --iface= \--ip-masq=1 --kube-subnet-mgr=1 My Windows Pods cannot launch because of missing /run/flannel/subnet.envThis indicates that Flannel didn't launch correctly. You can either try to restart flanneld.exe or you can copy the files over manually from /run/flannel/subnet.env on the Kubernetes master to C:\run\flannel\subnet.env on the Windows worker node and modify the FLANNEL_SUBNET row to a different number. For example, if node subnet 10.244.4.1/24 is desired:FLANNEL_NETWORK=10.244.0.0/16 FLANNEL_SUBNET=10.244.4.1/24 FLANNEL_MTU=1500 FLANNEL_IPMASQ=true Further investigationIf these steps don't resolve your problem, you can get help running Windows containers on Windows nodes in Kubernetes through:StackOverflow Windows Server Container topicKubernetes Official Forum discuss.kubernetes.ioKubernetes Slack #SIG-Windows Channel4.5 - Manage Kubernetes ObjectsDeclarative and imperative paradigms for interacting with the Kubernetes API.4.5.1 - Declarative Management of Kubernetes Objects Using Configuration FilesKubernetes objects can be created, updated, and deleted by storing multiple object configuration files in a directory and using kubectl apply to recursively create and update those objects as needed. This method retains writes made to live objects without merging the changes back into the object configuration files. kubectl diff also gives you a preview of what changes apply will make.Before you beginInstall kubectl.You need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesTo check the version, enter kubectl version.Trade-offsThe kubectl tool supports three kinds of object management:Imperative commandsImperative object configurationDeclarative object configurationSee Kubernetes Object Management for a discussion of the advantages and disadvantage of each kind of object management.OverviewDeclarative object configuration requires a firm understanding of the Kubernetes object definitions and configuration. Read and complete the following documents if you have not already:Managing Kubernetes Objects Using Imperative CommandsImperative Management of Kubernetes Objects Using Configuration FilesFollowing are definitions for terms used in this document:object configuration file / configuration file: A file that defines the configuration for a Kubernetes object. This topic shows how to pass configuration files to kubectl apply. Configuration files are typically stored in source control, such as Git.live object configuration / live configuration: The live configuration values of an object, as observed by the Kubernetes cluster. These are kept in the Kubernetes cluster storage, typically etcd.declarative configuration writer / declarative writer: A person or software component that makes updates to a live object. The live writers referred to in this topic make changes to object configuration files and run kubectl apply to write the changes.How to create objectsUse kubectl apply to create all objects, except those that already exist, defined by configuration files in a specified directory:kubectl apply -f  This sets the kubectl.kubernetes.io/last-applied-configuration: '{...}' annotation on each object. The annotation contains the contents of the object configuration file that was used to create the object.Note:Add the -R flag to recursively process directories.Here's an example of an object configuration file:application/simple_deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment spec: selector: matchLabels: app: nginx minReadySeconds: 5 template: metadata: labels: app: nginx spec: containers: \- name: nginx image: nginx:1.14.2 ports: \- containerPort: 80 Run kubectl diff to print the object that will be created:kubectl diff -f https://k8s.io/examples/application/simple_deployment.yaml Note:diff uses server-side dry-run, which needs to be enabled on kube-apiserver.Since diff performs a server-side apply request in dry-run mode, it requires granting PATCH, CREATE, and UPDATE permissions. See Dry-Run Authorization for details.Create the object using kubectl apply:kubectl apply -f https://k8s.io/examples/application/simple_deployment.yaml Print the live configuration using kubectl get:kubectl get -f https://k8s.io/examples/application/simple_deployment.yaml -o yaml The output shows that the kubectl.kubernetes.io/last-applied-configuration annotation was written to the live configuration, and it matches the configuration file:kind: Deployment metadata: annotations: # ... # This is the json representation of simple_deployment.yaml # It was written by kubectl apply when the object was created kubectl.kubernetes.io/last-applied-configuration: | {"apiVersion":"apps/v1","kind":"Deployment", "metadata":{"annotations":{},"name":"nginx-deployment","namespace":"default"}, "spec":{"minReadySeconds":5,"selector":{"matchLabels":{"app":nginx}},"template":{"metadata":{"labels":{"app":"nginx"}}, "spec":{"containers":[{"image":"nginx:1.14.2","name":"nginx", "ports":[{"containerPort":80}]}]}}}} # ... spec: # ... minReadySeconds: 5 selector: matchLabels: # ... app: nginx template: metadata: # ... labels: app: nginx spec: containers: \- image: nginx:1.14.2 # ... name: nginx ports: \- containerPort: 80 # ... # ... # ... # ... How to update objectsYou can also use kubectl apply to update all objects defined in a directory, even if those objects already exist. This approach accomplishes the following:Sets fields that appear in the configuration file in the live configuration.Clears fields removed from the configuration file in the live configuration.kubectl diff -f  kubectl apply -f  Note:Add the -R flag to recursively process directories.Here's an example configuration file:application/simple_deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment spec: selector: matchLabels: app: nginx minReadySeconds: 5 template: metadata: labels: app: nginx spec: containers: \- name: nginx image: nginx:1.14.2 ports: \- containerPort: 80 Create the object using kubectl apply:kubectl apply -f https://k8s.io/examples/application/simple_deployment.yaml Note:For purposes of illustration, the preceding command refers to a single configuration file instead of a directory.Print the live configuration using kubectl get:kubectl get -f https://k8s.io/examples/application/simple_deployment.yaml -o yaml The output shows that the kubectl.kubernetes.io/last-applied-configuration annotation was written to the live configuration, and it matches the configuration file:kind: Deployment metadata: annotations: # ... # This is the json representation of simple_deployment.yaml # It was written by kubectl apply when the object was created kubectl.kubernetes.io/last-applied-configuration: | {"apiVersion":"apps/v1","kind":"Deployment", "metadata":{"annotations":{},"name":"nginx-deployment","namespace":"default"}, "spec":{"minReadySeconds":5,"selector":{"matchLabels":{"app":nginx}},"template":{"metadata":{"labels":{"app":"nginx"}}, "spec":{"containers":[{"image":"nginx:1.14.2","name":"nginx", "ports":[{"containerPort":80}]}]}}}} # ... spec: # ... minReadySeconds: 5 selector: matchLabels: # ... app: nginx template: metadata: # ... labels: app: nginx spec: containers: \- image: nginx:1.14.2 # ... name: nginx ports: \- containerPort: 80 # ... # ... # ... # ... Directly update the replicas field in the live configuration by using kubectl scale. This does not use kubectl apply:kubectl scale deployment/nginx-deployment --replicas=2 Print the live configuration using kubectl get:kubectl get deployment nginx-deployment -o yaml The output shows that the replicas field has been set to 2, and the last-applied-configuration annotation does not contain a replicas field:apiVersion: apps/v1 kind: Deployment metadata: annotations: # ... # note that the annotation does not contain replicas # because it was not updated through apply kubectl.kubernetes.io/last-applied-configuration: | {"apiVersion":"apps/v1","kind":"Deployment", "metadata":{"annotations":{},"name":"nginx-deployment","namespace":"default"}, "spec":{"minReadySeconds":5,"selector":{"matchLabels":{"app":nginx}},"template":{"metadata":{"labels":{"app":"nginx"}}, "spec":{"containers":[{"image":"nginx:1.14.2","name":"nginx", "ports":[{"containerPort":80}]}]}}}} # ... spec: replicas: 2 # written by scale # ... minReadySeconds: 5 selector: matchLabels: # ... app: nginx template: metadata: # ... labels: app: nginx spec: containers: \- image: nginx:1.14.2 # ... name: nginx ports: \- containerPort: 80 # ... Update the simple_deployment.yaml configuration file to change the image from nginx:1.14.2 to nginx:1.16.1, and delete the minReadySeconds field:application/update_deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment spec: selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: \- name: nginx image: nginx:1.16.1 # update the image ports: \- containerPort: 80 Apply the changes made to the configuration file:kubectl diff -f https://k8s.io/examples/application/update_deployment.yaml kubectl apply -f https://k8s.io/examples/application/update_deployment.yaml Print the live configuration using kubectl get:kubectl get -f https://k8s.io/examples/application/update_deployment.yaml -o yaml The output shows the following changes to the live configuration:The replicas field retains the value of 2 set by kubectl scale. This is possible because it is omitted from the configuration file.The image field has been updated to nginx:1.16.1 from nginx:1.14.2.The last-applied-configuration annotation has been updated with the new image.The minReadySeconds field has been cleared.The last-applied-configuration annotation no longer contains the minReadySeconds field.apiVersion: apps/v1 kind: Deployment metadata: annotations: # ... # The annotation contains the updated image to nginx 1.16.1, # but does not contain the updated replicas to 2 kubectl.kubernetes.io/last-applied-configuration: | {"apiVersion":"apps/v1","kind":"Deployment", "metadata":{"annotations":{},"name":"nginx-deployment","namespace":"default"}, "spec":{"selector":{"matchLabels":{"app":nginx}},"template":{"metadata":{"labels":{"app":"nginx"}}, "spec":{"containers":[{"image":"nginx:1.16.1","name":"nginx", "ports":[{"containerPort":80}]}]}}}} # ... spec: replicas: 2 # Set by `kubectl scale`. Ignored by `kubectl apply`. # minReadySeconds cleared by `kubectl apply` # ... selector: matchLabels: # ... app: nginx template: metadata: # ... labels: app: nginx spec: containers: \- image: nginx:1.16.1 # Set by `kubectl apply` # ... name: nginx ports: \- containerPort: 80 # ... # ... # ... # ... Warning:Mixing kubectl apply with the imperative object configuration commands create and replace is not supported. This is because create and replace do not retain the kubectl.kubernetes.io/last-applied-configuration that kubectl apply uses to compute updates.How to delete objectsThere are two approaches to delete objects managed by kubectl apply.Recommended: kubectl delete -f Manually deleting objects using the imperative command is the recommended approach, as it is more explicit about what is being deleted, and less likely to result in the user deleting something unintentionally:kubectl delete -f  Alternative: kubectl apply -f  \--pruneAs an alternative to kubectl delete, you can use kubectl apply to identify objects to be deleted after their manifests have been removed from a directory in the local filesystem.In Kubernetes 1.33, there are two pruning modes available in kubectl apply:Allowlist-based pruning: This mode has existed since kubectl v1.5 but is still in alpha due to usability, correctness and performance issues with its design. The ApplySet-based mode is designed to replace it.ApplySet-based pruning: An apply set is a server-side object (by default, a Secret) that kubectl can use to accurately and efficiently track set membership across apply operations. This mode was introduced in alpha in kubectl v1.27 as a replacement for allowlist-based pruning.Allow listApply setFEATURE STATE: Kubernetes v1.5 [alpha]Warning:Take care when using --prune with kubectl apply in allow list mode. Which objects are pruned depends on the values of the --prune-allowlist, --selector and --namespace flags, and relies on dynamic discovery of the objects in scope. Especially if flag values are changed between invocations, this can lead to objects being unexpectedly deleted or retained.To use allowlist-based pruning, add the following flags to your kubectl apply invocation:--prune: Delete previously applied objects that are not in the set passed to the current invocation.--prune-allowlist: A list of group-version-kinds (GVKs) to consider for pruning. This flag is optional but strongly encouraged, as its default value is a partial list of both namespaced and cluster-scoped types, which can lead to surprising results.--selector/-l: Use a label selector to constrain the set of objects selected for pruning. This flag is optional but strongly encouraged.--all: use instead of --selector/-l to explicitly select all previously applied objects of the allowlisted types.Allowlist-based pruning queries the API server for all objects of the allowlisted GVKs that match the given labels (if any), and attempts to match the returned live object configurations against the object manifest files. If an object matches the query, and it does not have a manifest in the directory, and it has a kubectl.kubernetes.io/last-applied-configuration annotation, it is deleted.kubectl apply -f  \--prune -l  \--prune-allowlist= Warning:Apply with prune should only be run against the root directory containing the object manifests. Running against sub-directories can cause objects to be unintentionally deleted if they were previously applied, have the labels given (if any), and do not appear in the subdirectory.FEATURE STATE: Kubernetes v1.27 [alpha]Caution:kubectl apply --prune --applyset is in alpha, and backwards incompatible changes might be introduced in subsequent releases.To use ApplySet-based pruning, set the KUBECTL_APPLYSET=true environment variable, and add the following flags to your kubectl apply invocation:--prune: Delete previously applied objects that are not in the set passed to the current invocation.--applyset: The name of an object that kubectl can use to accurately and efficiently track set membership across apply operations.KUBECTL_APPLYSET=true kubectl apply -f  \--prune --applyset= By default, the type of the ApplySet parent object used is a Secret. However, ConfigMaps can also be used in the format: --applyset=configmaps/. When using a Secret or ConfigMap, kubectl will create the object if it does not already exist.It is also possible to use custom resources as ApplySet parent objects. To enable this, label the Custom Resource Definition (CRD) that defines the resource you want to use with the following: applyset.kubernetes.io/is-parent-type: true. Then, create the object you want to use as an ApplySet parent (kubectl does not do this automatically for custom resources). Finally, refer to that object in the applyset flag as follows: \--applyset=./ (for example, widgets.custom.example.com/widget-name).With ApplySet-based pruning, kubectl adds the applyset.kubernetes.io/part-of= label to each object in the set before they are sent to the server. For performance reasons, it also collects the list of resource types and namespaces that the set contains and adds these in annotations on the live parent object. Finally, at the end of the apply operation, it queries the API server for objects of those types in those namespaces (or in the cluster scope, as applicable) that belong to the set, as defined by the applyset.kubernetes.io/part-of= label.Caveats and restrictions:Each object may be a member of at most one set.The --namespace flag is required when using any namespaced parent, including the default Secret. This means that ApplySets spanning multiple namespaces must use a cluster-scoped custom resource as the parent object.To safely use ApplySet-based pruning with multiple directories, use a unique ApplySet name for each.How to view an objectYou can use kubectl get with -o yaml to view the configuration of a live object:kubectl get -f  -o yaml How apply calculates differences and merges changesCaution:A patch is an update operation that is scoped to specific fields of an object instead of the entire object. This enables updating only a specific set of fields on an object without reading the object first.When kubectl apply updates the live configuration for an object, it does so by sending a patch request to the API server. The patch defines updates scoped to specific fields of the live object configuration. The kubectl apply command calculates this patch request using the configuration file, the live configuration, and the last-applied-configuration annotation stored in the live configuration.Merge patch calculationThe kubectl apply command writes the contents of the configuration file to the kubectl.kubernetes.io/last-applied-configuration annotation. This is used to identify fields that have been removed from the configuration file and need to be cleared from the live configuration. Here are the steps used to calculate which fields should be deleted or set:Calculate the fields to delete. These are the fields present in last-applied-configuration and missing from the configuration file.Calculate the fields to add or set. These are the fields present in the configuration file whose values don't match the live configuration.Here's an example. Suppose this is the configuration file for a Deployment object:application/update_deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment spec: selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: \- name: nginx image: nginx:1.16.1 # update the image ports: \- containerPort: 80 Also, suppose this is the live configuration for the same Deployment object:apiVersion: apps/v1 kind: Deployment metadata: annotations: # ... # note that the annotation does not contain replicas # because it was not updated through apply kubectl.kubernetes.io/last-applied-configuration: | {"apiVersion":"apps/v1","kind":"Deployment", "metadata":{"annotations":{},"name":"nginx-deployment","namespace":"default"}, "spec":{"minReadySeconds":5,"selector":{"matchLabels":{"app":nginx}},"template":{"metadata":{"labels":{"app":"nginx"}}, "spec":{"containers":[{"image":"nginx:1.14.2","name":"nginx", "ports":[{"containerPort":80}]}]}}}} # ... spec: replicas: 2 # written by scale # ... minReadySeconds: 5 selector: matchLabels: # ... app: nginx template: metadata: # ... labels: app: nginx spec: containers: \- image: nginx:1.14.2 # ... name: nginx ports: \- containerPort: 80 # ... Here are the merge calculations that would be performed by kubectl apply:Calculate the fields to delete by reading values from last-applied-configuration and comparing them to values in the configuration file. Clear fields explicitly set to null in the local object configuration file regardless of whether they appear in the last-applied-configuration. In this example, minReadySeconds appears in the last-applied-configuration annotation, but does not appear in the configuration file. Action: Clear minReadySeconds from the live configuration.Calculate the fields to set by reading values from the configuration file and comparing them to values in the live configuration. In this example, the value of image in the configuration file does not match the value in the live configuration. Action: Set the value of image in the live configuration.Set the last-applied-configuration annotation to match the value of the configuration file.Merge the results from 1, 2, 3 into a single patch request to the API server.Here is the live configuration that is the result of the merge:apiVersion: apps/v1 kind: Deployment metadata: annotations: # ... # The annotation contains the updated image to nginx 1.16.1, # but does not contain the updated replicas to 2 kubectl.kubernetes.io/last-applied-configuration: | {"apiVersion":"apps/v1","kind":"Deployment", "metadata":{"annotations":{},"name":"nginx-deployment","namespace":"default"}, "spec":{"selector":{"matchLabels":{"app":nginx}},"template":{"metadata":{"labels":{"app":"nginx"}}, "spec":{"containers":[{"image":"nginx:1.16.1","name":"nginx", "ports":[{"containerPort":80}]}]}}}} # ... spec: selector: matchLabels: # ... app: nginx replicas: 2 # Set by `kubectl scale`. Ignored by `kubectl apply`. # minReadySeconds cleared by `kubectl apply` # ... template: metadata: # ... labels: app: nginx spec: containers: \- image: nginx:1.16.1 # Set by `kubectl apply` # ... name: nginx ports: \- containerPort: 80 # ... # ... # ... # ... How different types of fields are mergedHow a particular field in a configuration file is merged with the live configuration depends on the type of the field. There are several types of fields:primitive: A field of type string, integer, or boolean. For example, image and replicas are primitive fields. Action: Replace.map, also called object: A field of type map or a complex type that contains subfields. For example, labels, annotations,spec and metadata are all maps. Action: Merge elements or subfields.list: A field containing a list of items that can be either primitive types or maps. For example, containers, ports, and args are lists. Action: Varies.When kubectl apply updates a map or list field, it typically does not replace the entire field, but instead updates the individual subelements. For instance, when merging the spec on a Deployment, the entire spec is not replaced. Instead the subfields of spec, such as replicas, are compared and merged.Merging changes to primitive fieldsPrimitive fields are replaced or cleared.Note:- is used for "not applicable" because the value is not used.Field in object configuration fileField in live object configurationField in last-applied-configurationActionYesYes-Set live to configuration file value.YesNo-Set live to local configuration.No-YesClear from live configuration.No-NoDo nothing. Keep live value.Merging changes to map fieldsFields that represent maps are merged by comparing each of the subfields or elements of the map:Note:- is used for "not applicable" because the value is not used.Key in object configuration fileKey in live object configurationField in last-applied-configurationActionYesYes-Compare sub fields values.YesNo-Set live to local configuration.No-YesDelete from live configuration.No-NoDo nothing. Keep live value.Merging changes for fields of type listMerging changes to a list uses one of three strategies:Replace the list if all its elements are primitives.Merge individual elements in a list of complex elements.Merge a list of primitive elements.The choice of strategy is made on a per-field basis.Replace the list if all its elements are primitivesTreat the list the same as a primitive field. Replace or delete the entire list. This preserves ordering.Example: Use kubectl apply to update the args field of a Container in a Pod. This sets the value of args in the live configuration to the value in the configuration file. Any args elements that had previously been added to the live configuration are lost. The order of the args elements defined in the configuration file is retained in the live configuration.# last-applied-configuration value args: ["a", "b"] # configuration file value args: ["a", "c"] # live configuration args: ["a", "b", "d"] # result after merge args: ["a", "c"] Explanation: The merge used the configuration file value as the new list value.Merge individual elements of a list of complex elements:Treat the list as a map, and treat a specific field of each element as a key. Add, delete, or update individual elements. This does not preserve ordering.This merge strategy uses a special tag on each field called a patchMergeKey. The patchMergeKey is defined for each field in the Kubernetes source code: types.go When merging a list of maps, the field specified as the patchMergeKey for a given element is used like a map key for that element.Example: Use kubectl apply to update the containers field of a PodSpec. This merges the list as though it was a map where each element is keyed by name.# last-applied-configuration value containers: \- name: nginx image: nginx:1.16 \- name: nginx-helper-a # key: nginx-helper-a; will be deleted in result image: helper:1.3 \- name: nginx-helper-b # key: nginx-helper-b; will be retained image: helper:1.3 # configuration file value containers: \- name: nginx image: nginx:1.16 \- name: nginx-helper-b image: helper:1.3 \- name: nginx-helper-c # key: nginx-helper-c; will be added in result image: helper:1.3 # live configuration containers: \- name: nginx image: nginx:1.16 \- name: nginx-helper-a image: helper:1.3 \- name: nginx-helper-b image: helper:1.3 args: ["run"] # Field will be retained \- name: nginx-helper-d # key: nginx-helper-d; will be retained image: helper:1.3 # result after merge containers: \- name: nginx image: nginx:1.16 # Element nginx-helper-a was deleted \- name: nginx-helper-b image: helper:1.3 args: ["run"] # Field was retained \- name: nginx-helper-c # Element was added image: helper:1.3 \- name: nginx-helper-d # Element was ignored image: helper:1.3 Explanation:The container named "nginx-helper-a" was deleted because no container named "nginx-helper-a" appeared in the configuration file.The container named "nginx-helper-b" retained the changes to args in the live configuration. kubectl apply was able to identify that "nginx-helper-b" in the live configuration was the same "nginx-helper-b" as in the configuration file, even though their fields had different values (no args in the configuration file). This is because the patchMergeKey field value (name) was identical in both.The container named "nginx-helper-c" was added because no container with that name appeared in the live configuration, but one with that name appeared in the configuration file.The container named "nginx-helper-d" was retained because no element with that name appeared in the last-applied-configuration.Merge a list of primitive elementsAs of Kubernetes 1.5, merging lists of primitive elements is not supported.Note:Which of the above strategies is chosen for a given field is controlled by the patchStrategy tag in types.go If no patchStrategy is specified for a field of type list, then the list is replaced.Default field valuesThe API server sets certain fields to default values in the live configuration if they are not specified when the object is created.Here's a configuration file for a Deployment. The file does not specify strategy:application/simple_deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment spec: selector: matchLabels: app: nginx minReadySeconds: 5 template: metadata: labels: app: nginx spec: containers: \- name: nginx image: nginx:1.14.2 ports: \- containerPort: 80 Create the object using kubectl apply:kubectl apply -f https://k8s.io/examples/application/simple_deployment.yaml Print the live configuration using kubectl get:kubectl get -f https://k8s.io/examples/application/simple_deployment.yaml -o yaml The output shows that the API server set several fields to default values in the live configuration. These fields were not specified in the configuration file.apiVersion: apps/v1 kind: Deployment # ... spec: selector: matchLabels: app: nginx minReadySeconds: 5 replicas: 1 # defaulted by apiserver strategy: rollingUpdate: # defaulted by apiserver - derived from strategy.type maxSurge: 1 maxUnavailable: 1 type: RollingUpdate # defaulted by apiserver template: metadata: creationTimestamp: null labels: app: nginx spec: containers: \- image: nginx:1.14.2 imagePullPolicy: IfNotPresent # defaulted by apiserver name: nginx ports: \- containerPort: 80 protocol: TCP # defaulted by apiserver resources: {} # defaulted by apiserver terminationMessagePath: /dev/termination-log # defaulted by apiserver dnsPolicy: ClusterFirst # defaulted by apiserver restartPolicy: Always # defaulted by apiserver securityContext: {} # defaulted by apiserver terminationGracePeriodSeconds: 30 # defaulted by apiserver # ... In a patch request, defaulted fields are not re-defaulted unless they are explicitly cleared as part of a patch request. This can cause unexpected behavior for fields that are defaulted based on the values of other fields. When the other fields are later changed, the values defaulted from them will not be updated unless they are explicitly cleared.For this reason, it is recommended that certain fields defaulted by the server are explicitly defined in the configuration file, even if the desired values match the server defaults. This makes it easier to recognize conflicting values that will not be re-defaulted by the server.Example:# last-applied-configuration spec: template: metadata: labels: app: nginx spec: containers: \- name: nginx image: nginx:1.14.2 ports: \- containerPort: 80 # configuration file spec: strategy: type: Recreate # updated value template: metadata: labels: app: nginx spec: containers: \- name: nginx image: nginx:1.14.2 ports: \- containerPort: 80 # live configuration spec: strategy: type: RollingUpdate # defaulted value rollingUpdate: # defaulted value derived from type maxSurge : 1 maxUnavailable: 1 template: metadata: labels: app: nginx spec: containers: \- name: nginx image: nginx:1.14.2 ports: \- containerPort: 80 # result after merge - ERROR! spec: strategy: type: Recreate # updated value: incompatible with rollingUpdate rollingUpdate: # defaulted value: incompatible with "type: Recreate" maxSurge : 1 maxUnavailable: 1 template: metadata: labels: app: nginx spec: containers: \- name: nginx image: nginx:1.14.2 ports: \- containerPort: 80 Explanation:The user creates a Deployment without defining strategy.type.The server defaults strategy.type to RollingUpdate and defaults the strategy.rollingUpdate values.The user changes strategy.type to Recreate. The strategy.rollingUpdate values remain at their defaulted values, though the server expects them to be cleared. If the strategy.rollingUpdate values had been defined initially in the configuration file, it would have been more clear that they needed to be deleted.Apply fails because strategy.rollingUpdate is not cleared. The strategy.rollingupdate field cannot be defined with a strategy.type of Recreate.Recommendation: These fields should be explicitly defined in the object configuration file:Selectors and PodTemplate labels on workloads, such as Deployment, StatefulSet, Job, DaemonSet, ReplicaSet, and ReplicationControllerDeployment rollout strategyHow to clear server-defaulted fields or fields set by other writersFields that do not appear in the configuration file can be cleared by setting their values to null and then applying the configuration file. For fields defaulted by the server, this triggers re-defaulting the values.How to change ownership of a field between the configuration file and direct imperative writersThese are the only methods you should use to change an individual object field:Use kubectl apply.Write directly to the live configuration without modifying the configuration file: for example, use kubectl scale.Changing the owner from a direct imperative writer to a configuration fileAdd the field to the configuration file. For the field, discontinue direct updates to the live configuration that do not go through kubectl apply.Changing the owner from a configuration file to a direct imperative writerAs of Kubernetes 1.5, changing ownership of a field from a configuration file to an imperative writer requires manual steps:Remove the field from the configuration file.Remove the field from the kubectl.kubernetes.io/last-applied-configuration annotation on the live object.Changing management methodsKubernetes objects should be managed using only one method at a time. Switching from one method to another is possible, but is a manual process.Note:It is OK to use imperative deletion with declarative management.Migrating from imperative command management to declarative object configurationMigrating from imperative command management to declarative object configuration involves several manual steps:Export the live object to a local configuration file:kubectl get / -o yaml > _.yaml Manually remove the status field from the configuration file.Note:This step is optional, as kubectl apply does not update the status field even if it is present in the configuration file.Set the kubectl.kubernetes.io/last-applied-configuration annotation on the object:kubectl replace --save-config -f _.yaml Change processes to use kubectl apply for managing the object exclusively.Migrating from imperative object configuration to declarative object configurationSet the kubectl.kubernetes.io/last-applied-configuration annotation on the object:kubectl replace --save-config -f _.yaml Change processes to use kubectl apply for managing the object exclusively.Defining controller selectors and PodTemplate labelsWarning:Updating selectors on controllers is strongly discouraged.The recommended approach is to define a single, immutable PodTemplate label used only by the controller selector with no other semantic meaning.Example:selector: matchLabels: controller-selector: "apps/v1/deployment/nginx" template: metadata: labels: controller-selector: "apps/v1/deployment/nginx" What's nextManaging Kubernetes Objects Using Imperative CommandsImperative Management of Kubernetes Objects Using Configuration FilesKubectl Command ReferenceKubernetes API Reference4.5.2 - Declarative Management of Kubernetes Objects Using KustomizeKustomize is a standalone tool to customize Kubernetes objects through a kustomization file.Since 1.14, kubectl also supports the management of Kubernetes objects using a kustomization file. To view resources found in a directory containing a kustomization file, run the following command:kubectl kustomize  To apply those resources, run kubectl apply with --kustomize or -k flag:kubectl apply -k  Before you beginInstall kubectl.You need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesTo check the version, enter kubectl version.Overview of KustomizeKustomize is a tool for customizing Kubernetes configurations. It has the following features to manage application configuration files:generating resources from other sourcessetting cross-cutting fields for resourcescomposing and customizing collections of resourcesGenerating ResourcesConfigMaps and Secrets hold configuration or sensitive data that are used by other Kubernetes objects, such as Pods. The source of truth of ConfigMaps or Secrets are usually external to a cluster, such as a .properties file or an SSH keyfile. Kustomize has secretGenerator and configMapGenerator, which generate Secret and ConfigMap from files or literals.configMapGeneratorTo generate a ConfigMap from a file, add an entry to the files list in configMapGenerator. Here is an example of generating a ConfigMap with a data item from a .properties file:# Create a application.properties file cat <application.properties FOO=Bar EOF cat <./kustomization.yaml configMapGenerator: \- name: example-configmap-1 files: \- application.properties EOF The generated ConfigMap can be examined with the following command:kubectl kustomize ./ The generated ConfigMap is:apiVersion: v1 data: application.properties: | FOO=Bar kind: ConfigMap metadata: name: example-configmap-1-8mbdf7882g To generate a ConfigMap from an env file, add an entry to the envs list in configMapGenerator. Here is an example of generating a ConfigMap with a data item from a .env file:# Create a .env file cat <.env FOO=Bar EOF cat <./kustomization.yaml configMapGenerator: \- name: example-configmap-1 envs: \- .env EOF The generated ConfigMap can be examined with the following command:kubectl kustomize ./ The generated ConfigMap is:apiVersion: v1 data: FOO: Bar kind: ConfigMap metadata: name: example-configmap-1-42cfbf598f Note:Each variable in the .env file becomes a separate key in the ConfigMap that you generate. This is different from the previous example which embeds a file named application.properties (and all its entries) as the value for a single key.ConfigMaps can also be generated from literal key-value pairs. To generate a ConfigMap from a literal key-value pair, add an entry to the literals list in configMapGenerator. Here is an example of generating a ConfigMap with a data item from a key-value pair:cat <./kustomization.yaml configMapGenerator: \- name: example-configmap-2 literals: \- FOO=Bar EOF The generated ConfigMap can be checked by the following command:kubectl kustomize ./ The generated ConfigMap is:apiVersion: v1 data: FOO: Bar kind: ConfigMap metadata: name: example-configmap-2-g2hdhfc6tk To use a generated ConfigMap in a Deployment, reference it by the name of the configMapGenerator. Kustomize will automatically replace this name with the generated name.This is an example deployment that uses a generated ConfigMap:# Create an application.properties file cat <application.properties FOO=Bar EOF cat <deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: my-app labels: app: my-app spec: selector: matchLabels: app: my-app template: metadata: labels: app: my-app spec: containers: \- name: app image: my-app volumeMounts: \- name: config mountPath: /config volumes: \- name: config configMap: name: example-configmap-1 EOF cat <./kustomization.yaml resources: \- deployment.yaml configMapGenerator: \- name: example-configmap-1 files: \- application.properties EOF Generate the ConfigMap and Deployment:kubectl kustomize ./ The generated Deployment will refer to the generated ConfigMap by name:apiVersion: v1 data: application.properties: | FOO=Bar kind: ConfigMap metadata: name: example-configmap-1-g4hk9g2ff8 \--- apiVersion: apps/v1 kind: Deployment metadata: labels: app: my-app name: my-app spec: selector: matchLabels: app: my-app template: metadata: labels: app: my-app spec: containers: \- image: my-app name: app volumeMounts: \- mountPath: /config name: config volumes: \- configMap: name: example-configmap-1-g4hk9g2ff8 name: config secretGeneratorYou can generate Secrets from files or literal key-value pairs. To generate a Secret from a file, add an entry to the files list in secretGenerator. Here is an example of generating a Secret with a data item from a file:# Create a password.txt file cat <./password.txt username=admin password=secret EOF cat <./kustomization.yaml secretGenerator: \- name: example-secret-1 files: \- password.txt EOF The generated Secret is as follows:apiVersion: v1 data: password.txt: dXNlcm5hbWU9YWRtaW4KcGFzc3dvcmQ9c2VjcmV0Cg== kind: Secret metadata: name: example-secret-1-t2kt65hgtb type: Opaque To generate a Secret from a literal key-value pair, add an entry to literals list in secretGenerator. Here is an example of generating a Secret with a data item from a key-value pair:cat <./kustomization.yaml secretGenerator: \- name: example-secret-2 literals: \- username=admin \- password=secret EOF The generated Secret is as follows:apiVersion: v1 data: password: c2VjcmV0 username: YWRtaW4= kind: Secret metadata: name: example-secret-2-t52t6g96d8 type: Opaque Like ConfigMaps, generated Secrets can be used in Deployments by referring to the name of the secretGenerator:# Create a password.txt file cat <./password.txt username=admin password=secret EOF cat <deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: my-app labels: app: my-app spec: selector: matchLabels: app: my-app template: metadata: labels: app: my-app spec: containers: \- name: app image: my-app volumeMounts: \- name: password mountPath: /secrets volumes: \- name: password secret: secretName: example-secret-1 EOF cat <./kustomization.yaml resources: \- deployment.yaml secretGenerator: \- name: example-secret-1 files: \- password.txt EOF generatorOptionsThe generated ConfigMaps and Secrets have a content hash suffix appended. This ensures that a new ConfigMap or Secret is generated when the contents are changed. To disable the behavior of appending a suffix, one can use generatorOptions. Besides that, it is also possible to specify cross-cutting options for generated ConfigMaps and Secrets.cat <./kustomization.yaml configMapGenerator: \- name: example-configmap-3 literals: \- FOO=Bar generatorOptions: disableNameSuffixHash: true labels: type: generated annotations: note: generated EOF Runkubectl kustomize ./ to view the generated ConfigMap:apiVersion: v1 data: FOO: Bar kind: ConfigMap metadata: annotations: note: generated labels: type: generated name: example-configmap-3 Setting cross-cutting fieldsIt is quite common to set cross-cutting fields for all Kubernetes resources in a project. Some use cases for setting cross-cutting fields:setting the same namespace for all resourcesadding the same name prefix or suffixadding the same set of labelsadding the same set of annotationsHere is an example:# Create a deployment.yaml cat <./deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment labels: app: nginx spec: selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: \- name: nginx image: nginx EOF cat <./kustomization.yaml namespace: my-namespace namePrefix: dev- nameSuffix: "-001" labels: \- pairs: app: bingo includeSelectors: true commonAnnotations: oncallPager: 800-555-1212 resources: \- deployment.yaml EOF Run kubectl kustomize ./ to view those fields are all set in the Deployment Resource:apiVersion: apps/v1 kind: Deployment metadata: annotations: oncallPager: 800-555-1212 labels: app: bingo name: dev-nginx-deployment-001 namespace: my-namespace spec: selector: matchLabels: app: bingo template: metadata: annotations: oncallPager: 800-555-1212 labels: app: bingo spec: containers: \- image: nginx name: nginx Composing and Customizing ResourcesIt is common to compose a set of resources in a project and manage them inside the same file or directory. Kustomize offers composing resources from different files and applying patches or other customization to them.ComposingKustomize supports composition of different resources. The resources field, in the kustomization.yaml file, defines the list of resources to include in a configuration. Set the path to a resource's configuration file in the resources list. Here is an example of an NGINX application comprised of a Deployment and a Service:# Create a deployment.yaml file cat < deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: my-nginx spec: selector: matchLabels: run: my-nginx replicas: 2 template: metadata: labels: run: my-nginx spec: containers: \- name: my-nginx image: nginx ports: \- containerPort: 80 EOF # Create a service.yaml file cat < service.yaml apiVersion: v1 kind: Service metadata: name: my-nginx labels: run: my-nginx spec: ports: \- port: 80 protocol: TCP selector: run: my-nginx EOF # Create a kustomization.yaml composing them cat <./kustomization.yaml resources: \- deployment.yaml \- service.yaml EOF The resources from kubectl kustomize ./ contain both the Deployment and the Service objects.CustomizingPatches can be used to apply different customizations to resources. Kustomize supports different patching mechanisms through StrategicMerge and Json6902 using the patches field. patches may be a file or an inline string, targeting a single or multiple resources.The patches field contains a list of patches applied in the order they are specified. The patch target selects resources by group, version, kind, name, namespace, labelSelector and annotationSelector.Small patches that do one thing are recommended. For example, create one patch for increasing the deployment replica number and another patch for setting the memory limit. The target resource is matched using group, version, kind, and name fields from the patch file.# Create a deployment.yaml file cat < deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: my-nginx spec: selector: matchLabels: run: my-nginx replicas: 2 template: metadata: labels: run: my-nginx spec: containers: \- name: my-nginx image: nginx ports: \- containerPort: 80 EOF # Create a patch increase_replicas.yaml cat < increase_replicas.yaml apiVersion: apps/v1 kind: Deployment metadata: name: my-nginx spec: replicas: 3 EOF # Create another patch set_memory.yaml cat < set_memory.yaml apiVersion: apps/v1 kind: Deployment metadata: name: my-nginx spec: template: spec: containers: \- name: my-nginx resources: limits: memory: 512Mi EOF cat <./kustomization.yaml resources: \- deployment.yaml patches: \- path: increase_replicas.yaml \- path: set_memory.yaml EOF Run kubectl kustomize ./ to view the Deployment:apiVersion: apps/v1 kind: Deployment metadata: name: my-nginx spec: replicas: 3 selector: matchLabels: run: my-nginx template: metadata: labels: run: my-nginx spec: containers: \- image: nginx name: my-nginx ports: \- containerPort: 80 resources: limits: memory: 512Mi Not all resources or fields support strategicMerge patches. To support modifying arbitrary fields in arbitrary resources, Kustomize offers applying JSON patch through Json6902. To find the correct Resource for a Json6902 patch, it is mandatory to specify the target field in kustomization.yaml.For example, increasing the replica number of a Deployment object can also be done through Json6902 patch. The target resource is matched using group, version, kind, and name from the target field.# Create a deployment.yaml file cat < deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: my-nginx spec: selector: matchLabels: run: my-nginx replicas: 2 template: metadata: labels: run: my-nginx spec: containers: \- name: my-nginx image: nginx ports: \- containerPort: 80 EOF # Create a json patch cat < patch.yaml \- op: replace path: /spec/replicas value: 3 EOF # Create a kustomization.yaml cat <./kustomization.yaml resources: \- deployment.yaml patches: \- target: group: apps version: v1 kind: Deployment name: my-nginx path: patch.yaml EOF Run kubectl kustomize ./ to see the replicas field is updated:apiVersion: apps/v1 kind: Deployment metadata: name: my-nginx spec: replicas: 3 selector: matchLabels: run: my-nginx template: metadata: labels: run: my-nginx spec: containers: \- image: nginx name: my-nginx ports: \- containerPort: 80 In addition to patches, Kustomize also offers customizing container images or injecting field values from other objects into containers without creating patches. For example, you can change the image used inside containers by specifying the new image in the images field in kustomization.yaml.cat < deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: my-nginx spec: selector: matchLabels: run: my-nginx replicas: 2 template: metadata: labels: run: my-nginx spec: containers: \- name: my-nginx image: nginx ports: \- containerPort: 80 EOF cat <./kustomization.yaml resources: \- deployment.yaml images: \- name: nginx newName: my.image.registry/nginx newTag: 1.4.0 EOF Run kubectl kustomize ./ to see that the image being used is updated:apiVersion: apps/v1 kind: Deployment metadata: name: my-nginx spec: replicas: 2 selector: matchLabels: run: my-nginx template: metadata: labels: run: my-nginx spec: containers: \- image: my.image.registry/nginx:1.4.0 name: my-nginx ports: \- containerPort: 80 Sometimes, the application running in a Pod may need to use configuration values from other objects. For example, a Pod from a Deployment object need to read the corresponding Service name from Env or as a command argument. Since the Service name may change as namePrefix or nameSuffix is added in the kustomization.yaml file. It is not recommended to hard code the Service name in the command argument. For this usage, Kustomize can inject the Service name into containers through replacements.# Create a deployment.yaml file (quoting the here doc delimiter) cat <<'EOF' > deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: my-nginx spec: selector: matchLabels: run: my-nginx replicas: 2 template: metadata: labels: run: my-nginx spec: containers: \- name: my-nginx image: nginx command: ["start", "--host", "MY_SERVICE_NAME_PLACEHOLDER"] EOF # Create a service.yaml file cat < service.yaml apiVersion: v1 kind: Service metadata: name: my-nginx labels: run: my-nginx spec: ports: \- port: 80 protocol: TCP selector: run: my-nginx EOF cat <./kustomization.yaml namePrefix: dev- nameSuffix: "-001" resources: \- deployment.yaml \- service.yaml replacements: \- source: kind: Service name: my-nginx fieldPath: metadata.name targets: \- select: kind: Deployment name: my-nginx fieldPaths: \- spec.template.spec.containers.0.command.2 EOF Run kubectl kustomize ./ to see that the Service name injected into containers is dev-my-nginx-001:apiVersion: apps/v1 kind: Deployment metadata: name: dev-my-nginx-001 spec: replicas: 2 selector: matchLabels: run: my-nginx template: metadata: labels: run: my-nginx spec: containers: \- command: \- start \- --host \- dev-my-nginx-001 image: nginx name: my-nginx Bases and OverlaysKustomize has the concepts of bases and overlays. A base is a directory with a kustomization.yaml, which contains a set of resources and associated customization. A base could be either a local directory or a directory from a remote repo, as long as a kustomization.yaml is present inside. An overlay is a directory with a kustomization.yaml that refers to other kustomization directories as its bases. A base has no knowledge of an overlay and can be used in multiple overlays.The kustomization.yaml in an overlay directory may refer to multiple bases, combining all the resources defined in these bases into a unified configuration. Additionally, it can apply customizations on top of these resources to meet specific requirements.Here is an example of a base:# Create a directory to hold the base mkdir base # Create a base/deployment.yaml cat < base/deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: my-nginx spec: selector: matchLabels: run: my-nginx replicas: 2 template: metadata: labels: run: my-nginx spec: containers: \- name: my-nginx image: nginx EOF # Create a base/service.yaml file cat < base/service.yaml apiVersion: v1 kind: Service metadata: name: my-nginx labels: run: my-nginx spec: ports: \- port: 80 protocol: TCP selector: run: my-nginx EOF # Create a base/kustomization.yaml cat < base/kustomization.yaml resources: \- deployment.yaml \- service.yaml EOF This base can be used in multiple overlays. You can add different namePrefix or other cross-cutting fields in different overlays. Here are two overlays using the same base.mkdir dev cat < dev/kustomization.yaml resources: \- ../base namePrefix: dev- EOF mkdir prod cat < prod/kustomization.yaml resources: \- ../base namePrefix: prod- EOF How to apply/view/delete objects using KustomizeUse --kustomize or -k in kubectl commands to recognize resources managed by kustomization.yaml. Note that -k should point to a kustomization directory, such askubectl apply -k / Given the following kustomization.yaml,# Create a deployment.yaml file cat < deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: my-nginx spec: selector: matchLabels: run: my-nginx replicas: 2 template: metadata: labels: run: my-nginx spec: containers: \- name: my-nginx image: nginx ports: \- containerPort: 80 EOF # Create a kustomization.yaml cat <./kustomization.yaml namePrefix: dev- labels: \- pairs: app: my-nginx includeSelectors: true resources: \- deployment.yaml EOF Run the following command to apply the Deployment object dev-my-nginx:> kubectl apply -k ./ deployment.apps/dev-my-nginx created Run one of the following commands to view the Deployment object dev-my-nginx:kubectl get -k ./ kubectl describe -k ./ Run the following command to compare the Deployment object dev-my-nginx against the state that the cluster would be in if the manifest was applied:kubectl diff -k ./ Run the following command to delete the Deployment object dev-my-nginx:> kubectl delete -k ./ deployment.apps "dev-my-nginx" deleted Kustomize Feature ListFieldTypeExplanationbases[]stringEach entry in this list should resolve to a directory containing a kustomization.yaml filecommonAnnotationsmap[string]stringannotations to add to all resourcescommonLabelsmap[string]stringlabels to add to all resources and selectorsconfigMapGenerator[]ConfigMapArgsEach entry in this list generates a ConfigMapconfigurations[]stringEach entry in this list should resolve to a file containing Kustomize transformer configurationscrds[]stringEach entry in this list should resolve to an OpenAPI definition file for Kubernetes typesgeneratorOptionsGeneratorOptionsModify behaviors of all ConfigMap and Secret generatorimages[]ImageEach entry is to modify the name, tags and/or digest for one image without creating patcheslabelsmap[string]stringAdd labels without automically injecting corresponding selectorsnamePrefixstringvalue of this field is prepended to the names of all resourcesnameSuffixstringvalue of this field is appended to the names of all resourcespatchesJson6902[]PatchEach entry in this list should resolve to a Kubernetes object and a Json PatchpatchesStrategicMerge[]stringEach entry in this list should resolve a strategic merge patch of a Kubernetes objectreplacements[]Replacementscopy the value from a resource's field into any number of specified targets.resources[]stringEach entry in this list must resolve to an existing resource configuration filesecretGenerator[]SecretArgsEach entry in this list generates a Secretvars[]VarEach entry is to capture text from one resource's fieldWhat's nextKustomizeKubectl BookKubectl Command ReferenceKubernetes API Reference4.5.3 - Managing Kubernetes Objects Using Imperative CommandsKubernetes objects can quickly be created, updated, and deleted directly using imperative commands built into the kubectl command-line tool. This document explains how those commands are organized and how to use them to manage live objects.Before you beginInstall kubectl.You need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesTo check the version, enter kubectl version.Trade-offsThe kubectl tool supports three kinds of object management:Imperative commandsImperative object configurationDeclarative object configurationSee Kubernetes Object Management for a discussion of the advantages and disadvantage of each kind of object management.How to create objectsThe kubectl tool supports verb-driven commands for creating some of the most common object types. The commands are named to be recognizable to users unfamiliar with the Kubernetes object types.run: Create a new Pod to run a Container.expose: Create a new Service object to load balance traffic across Pods.autoscale: Create a new Autoscaler object to automatically horizontally scale a controller, such as a Deployment.The kubectl tool also supports creation commands driven by object type. These commands support more object types and are more explicit about their intent, but require users to know the type of objects they intend to create.create  [] Some objects types have subtypes that you can specify in the create command. For example, the Service object has several subtypes including ClusterIP, LoadBalancer, and NodePort. Here's an example that creates a Service with subtype NodePort:kubectl create service nodeport  In the preceding example, the create service nodeport command is called a subcommand of the create service command.You can use the -h flag to find the arguments and flags supported by a subcommand:kubectl create service nodeport -h How to update objectsThe kubectl command supports verb-driven commands for some common update operations. These commands are named to enable users unfamiliar with Kubernetes objects to perform updates without knowing the specific fields that must be set:scale: Horizontally scale a controller to add or remove Pods by updating the replica count of the controller.annotate: Add or remove an annotation from an object.label: Add or remove a label from an object.The kubectl command also supports update commands driven by an aspect of the object. Setting this aspect may set different fields for different object types:set : Set an aspect of an object.Note:In Kubernetes version 1.5, not every verb-driven command has an associated aspect-driven command.The kubectl tool supports these additional ways to update a live object directly, however they require a better understanding of the Kubernetes object schema.edit: Directly edit the raw configuration of a live object by opening its configuration in an editor.patch: Directly modify specific fields of a live object by using a patch string. For more details on patch strings, see the patch section in API Conventions.How to delete objectsYou can use the delete command to delete an object from a cluster:delete /Note:You can use kubectl delete for both imperative commands and imperative object configuration. The difference is in the arguments passed to the command. To use kubectl delete as an imperative command, pass the object to be deleted as an argument. Here's an example that passes a Deployment object named nginx:kubectl delete deployment/nginx How to view an objectThere are several commands for printing information about an object:get: Prints basic information about matching objects. Use get -h to see a list of options.describe: Prints aggregated detailed information about matching objects.logs: Prints the stdout and stderr for a container running in a Pod.Using set commands to modify objects before creationThere are some object fields that don't have a flag you can use in a create command. In some of those cases, you can use a combination of set and create to specify a value for the field before object creation. This is done by piping the output of the create command to the set command, and then back to the create command. Here's an example:kubectl create service clusterip my-svc --clusterip="None" -o yaml --dry-run=client | kubectl set selector --local -f - 'environment=qa' -o yaml | kubectl create -f - The kubectl create service -o yaml --dry-run=client command creates the configuration for the Service, but prints it to stdout as YAML instead of sending it to the Kubernetes API server.The kubectl set selector --local -f - -o yaml command reads the configuration from stdin, and writes the updated configuration to stdout as YAML.The kubectl create -f - command creates the object using the configuration provided via stdin.Using --edit to modify objects before creationYou can use kubectl create --edit to make arbitrary changes to an object before it is created. Here's an example:kubectl create service clusterip my-svc --clusterip="None" -o yaml --dry-run=client > /tmp/srv.yaml kubectl create --edit -f /tmp/srv.yaml The kubectl create service command creates the configuration for the Service and saves it to /tmp/srv.yaml.The kubectl create --edit command opens the configuration file for editing before it creates the object.What's nextImperative Management of Kubernetes Objects Using Configuration FilesDeclarative Management of Kubernetes Objects Using Configuration FilesKubectl Command ReferenceKubernetes API Reference4.5.4 - Imperative Management of Kubernetes Objects Using Configuration FilesKubernetes objects can be created, updated, and deleted by using the kubectl command-line tool along with an object configuration file written in YAML or JSON. This document explains how to define and manage objects using configuration files.Before you beginInstall kubectl.You need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesTo check the version, enter kubectl version.Trade-offsThe kubectl tool supports three kinds of object management:Imperative commandsImperative object configurationDeclarative object configurationSee Kubernetes Object Management for a discussion of the advantages and disadvantage of each kind of object management.How to create objectsYou can use kubectl create -f to create an object from a configuration file. Refer to the kubernetes API reference for details.kubectl create -f How to update objectsWarning:Updating objects with the replace command drops all parts of the spec not specified in the configuration file. This should not be used with objects whose specs are partially managed by the cluster, such as Services of type LoadBalancer, where the externalIPs field is managed independently from the configuration file. Independently managed fields must be copied to the configuration file to prevent replace from dropping them.You can use kubectl replace -f to update a live object according to a configuration file.kubectl replace -f How to delete objectsYou can use kubectl delete -f to delete an object that is described in a configuration file.kubectl delete -f Note:If configuration file has specified the generateName field in the metadata section instead of the name field, you cannot delete the object using kubectl delete -f . You will have to use other flags for deleting the object. For example:kubectl delete  kubectl delete  -l  How to view an objectYou can use kubectl get -f to view information about an object that is described in a configuration file.kubectl get -f  -o yamlThe -o yaml flag specifies that the full object configuration is printed. Use kubectl get -h to see a list of options.LimitationsThe create, replace, and delete commands work well when each object's configuration is fully defined and recorded in its configuration file. However when a live object is updated, and the updates are not merged into its configuration file, the updates will be lost the next time a replace is executed. This can happen if a controller, such as a HorizontalPodAutoscaler, makes updates directly to a live object. Here's an example:You create an object from a configuration file.Another source updates the object by changing some field.You replace the object from the configuration file. Changes made by the other source in step 2 are lost.If you need to support multiple writers to the same object, you can use kubectl apply to manage the object.Creating and editing an object from a URL without saving the configurationSuppose you have the URL of an object configuration file. You can use kubectl create --edit to make changes to the configuration before the object is created. This is particularly useful for tutorials and tasks that point to a configuration file that could be modified by the reader.kubectl create -f  \--edit Migrating from imperative commands to imperative object configurationMigrating from imperative commands to imperative object configuration involves several manual steps.Export the live object to a local object configuration file:kubectl get / -o yaml > _.yaml Manually remove the status field from the object configuration file.For subsequent object management, use replace exclusively.kubectl replace -f _.yaml Defining controller selectors and PodTemplate labelsWarning:Updating selectors on controllers is strongly discouraged.The recommended approach is to define a single, immutable PodTemplate label used only by the controller selector with no other semantic meaning.Example label:selector: matchLabels: controller-selector: "apps/v1/deployment/nginx" template: metadata: labels: controller-selector: "apps/v1/deployment/nginx" What's nextManaging Kubernetes Objects Using Imperative CommandsDeclarative Management of Kubernetes Objects Using Configuration FilesKubectl Command ReferenceKubernetes API Reference4.5.5 - Update API Objects in Place Using kubectl patchUse kubectl patch to update Kubernetes API objects in place. Do a strategic merge patch or a JSON merge patch.This task shows how to use kubectl patch to update an API object in place. The exercises in this task demonstrate a strategic merge patch and a JSON merge patch.Before you beginYou need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesTo check the version, enter kubectl version.Use a strategic merge patch to update a DeploymentHere's the configuration file for a Deployment that has two replicas. Each replica is a Pod that has one container:application/deployment-patch.yaml apiVersion: apps/v1 kind: Deployment metadata: name: patch-demo spec: replicas: 2 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: \- name: patch-demo-ctr image: nginx tolerations: \- effect: NoSchedule key: dedicated value: test-team Create the Deployment:kubectl apply -f https://k8s.io/examples/application/deployment-patch.yaml View the Pods associated with your Deployment:kubectl get pods The output shows that the Deployment has two Pods. The 1/1 indicates that each Pod has one container:NAME READY STATUS RESTARTS AGE patch-demo-28633765-670qr 1/1 Running 0 23s patch-demo-28633765-j5qs3 1/1 Running 0 23s Make a note of the names of the running Pods. Later, you will see that these Pods get terminated and replaced by new ones.At this point, each Pod has one Container that runs the nginx image. Now suppose you want each Pod to have two containers: one that runs nginx and one that runs redis.Create a file named patch-file.yaml that has this content:spec: template: spec: containers: \- name: patch-demo-ctr-2 image: redis Patch your Deployment:kubectl patch deployment patch-demo --patch-file patch-file.yaml View the patched Deployment:kubectl get deployment patch-demo --output yaml The output shows that the PodSpec in the Deployment has two Containers:containers: \- image: redis imagePullPolicy: Always name: patch-demo-ctr-2 ... \- image: nginx imagePullPolicy: Always name: patch-demo-ctr ... View the Pods associated with your patched Deployment:kubectl get pods The output shows that the running Pods have different names from the Pods that were running previously. The Deployment terminated the old Pods and created two new Pods that comply with the updated Deployment spec. The 2/2 indicates that each Pod has two Containers:NAME READY STATUS RESTARTS AGE patch-demo-1081991389-2wrn5 2/2 Running 0 1m patch-demo-1081991389-jmg7b 2/2 Running 0 1m Take a closer look at one of the patch-demo Pods:kubectl get pod  \--output yaml The output shows that the Pod has two Containers: one running nginx and one running redis:containers: \- image: redis ... \- image: nginx ... Notes on the strategic merge patchThe patch you did in the preceding exercise is called a strategic merge patch. Notice that the patch did not replace the containers list. Instead it added a new Container to the list. In other words, the list in the patch was merged with the existing list. This is not always what happens when you use a strategic merge patch on a list. In some cases, the list is replaced, not merged.With a strategic merge patch, a list is either replaced or merged depending on its patch strategy. The patch strategy is specified by the value of the patchStrategy key in a field tag in the Kubernetes source code. For example, the Containers field of PodSpec struct has a patchStrategy of merge:type PodSpec struct { ... Containers []Container `json:"containers" patchStrategy:"merge" patchMergeKey:"name" ...` ... } You can also see the patch strategy in the OpenApi spec:"io.k8s.api.core.v1.PodSpec": { ..., "containers": { "description": "List of containers belonging to the pod. ...." }, "x-kubernetes-patch-merge-key": "name", "x-kubernetes-patch-strategy": "merge" } And you can see the patch strategy in the Kubernetes API documentation.Create a file named patch-file-tolerations.yaml that has this content:spec: template: spec: tolerations: \- effect: NoSchedule key: disktype value: ssd Patch your Deployment:kubectl patch deployment patch-demo --patch-file patch-file-tolerations.yaml View the patched Deployment:kubectl get deployment patch-demo --output yaml The output shows that the PodSpec in the Deployment has only one Toleration:tolerations: \- effect: NoSchedule key: disktype value: ssd Notice that the tolerations list in the PodSpec was replaced, not merged. This is because the Tolerations field of PodSpec does not have a patchStrategy key in its field tag. So the strategic merge patch uses the default patch strategy, which is replace.type PodSpec struct { ... Tolerations []Toleration `json:"tolerations,omitempty" protobuf:"bytes,22,opt,name=tolerations"` ... } Use a JSON merge patch to update a DeploymentA strategic merge patch is different from a JSON merge patch. With a JSON merge patch, if you want to update a list, you have to specify the entire new list. And the new list completely replaces the existing list.The kubectl patch command has a type parameter that you can set to one of these values:Parameter valueMerge typejsonJSON Patch, RFC 6902mergeJSON Merge Patch, RFC 7386strategicStrategic merge patchFor a comparison of JSON patch and JSON merge patch, see JSON Patch and JSON Merge Patch.The default value for the type parameter is strategic. So in the preceding exercise, you did a strategic merge patch.Next, do a JSON merge patch on your same Deployment. Create a file named patch-file-2.yaml that has this content:spec: template: spec: containers: \- name: patch-demo-ctr-3 image: gcr.io/google-samples/hello-app:2.0 In your patch command, set type to merge:kubectl patch deployment patch-demo --type merge --patch-file patch-file-2.yaml View the patched Deployment:kubectl get deployment patch-demo --output yaml The containers list that you specified in the patch has only one Container. The output shows that your list of one Container replaced the existing containers list.spec: containers: \- image: gcr.io/google-samples/hello-app:2.0 ... name: patch-demo-ctr-3 List the running Pods:kubectl get pods In the output, you can see that the existing Pods were terminated, and new Pods were created. The 1/1 indicates that each new Pod is running only one Container.NAME READY STATUS RESTARTS AGE patch-demo-1307768864-69308 1/1 Running 0 1m patch-demo-1307768864-c86dc 1/1 Running 0 1m Use strategic merge patch to update a Deployment using the retainKeys strategyHere's the configuration file for a Deployment that uses the RollingUpdate strategy:application/deployment-retainkeys.yaml apiVersion: apps/v1 kind: Deployment metadata: name: retainkeys-demo spec: selector: matchLabels: app: nginx strategy: rollingUpdate: maxSurge: 30% template: metadata: labels: app: nginx spec: containers: \- name: retainkeys-demo-ctr image: nginx Create the deployment:kubectl apply -f https://k8s.io/examples/application/deployment-retainkeys.yaml At this point, the deployment is created and is using the RollingUpdate strategy.Create a file named patch-file-no-retainkeys.yaml that has this content:spec: strategy: type: Recreate Patch your Deployment:kubectl patch deployment retainkeys-demo --type strategic --patch-file patch-file-no-retainkeys.yaml In the output, you can see that it is not possible to set type as Recreate when a value is defined for spec.strategy.rollingUpdate:The Deployment "retainkeys-demo" is invalid: spec.strategy.rollingUpdate: Forbidden: may not be specified when strategy `type` is 'Recreate' The way to remove the value for spec.strategy.rollingUpdate when updating the value for type is to use the retainKeys strategy for the strategic merge.Create another file named patch-file-retainkeys.yaml that has this content:spec: strategy: $retainKeys: \- type type: Recreate With this patch, we indicate that we want to retain only the type key of the strategy object. Thus, the rollingUpdate will be removed during the patch operation.Patch your Deployment again with this new patch:kubectl patch deployment retainkeys-demo --type strategic --patch-file patch-file-retainkeys.yaml Examine the content of the Deployment:kubectl get deployment retainkeys-demo --output yaml The output shows that the strategy object in the Deployment does not contain the rollingUpdate key anymore:spec: strategy: type: Recreate template: Notes on the strategic merge patch using the retainKeys strategyThe patch you did in the preceding exercise is called a strategic merge patch with retainKeys strategy. This method introduces a new directive $retainKeys that has the following strategies:It contains a list of strings.All fields needing to be preserved must be present in the $retainKeys list.The fields that are present will be merged with live object.All of the missing fields will be cleared when patching.All fields in the $retainKeys list must be a superset or the same as the fields present in the patch.The retainKeys strategy does not work for all objects. It only works when the value of the patchStrategy key in a field tag in the Kubernetes source code contains retainKeys. For example, the Strategy field of the DeploymentSpec struct has a patchStrategy of retainKeys:type DeploymentSpec struct { ... // +patchStrategy=retainKeys Strategy DeploymentStrategy `json:"strategy,omitempty" patchStrategy:"retainKeys" ...` ... } You can also see the retainKeys strategy in the OpenApi spec:"io.k8s.api.apps.v1.DeploymentSpec": { ..., "strategy": { "$ref": "#/definitions/io.k8s.api.apps.v1.DeploymentStrategy", "description": "The deployment strategy to use to replace existing pods with new ones.", "x-kubernetes-patch-strategy": "retainKeys" }, .... } And you can see the retainKeys strategy in the Kubernetes API documentation.Alternate forms of the kubectl patch commandThe kubectl patch command takes YAML or JSON. It can take the patch as a file or directly on the command line.Create a file named patch-file.json that has this content:{ "spec": { "template": { "spec": { "containers": [ { "name": "patch-demo-ctr-2", "image": "redis" } ] } } } } The following commands are equivalent:kubectl patch deployment patch-demo --patch-file patch-file.yaml kubectl patch deployment patch-demo --patch 'spec:\n template:\n spec:\n containers:\n - name: patch-demo-ctr-2\n image: redis' kubectl patch deployment patch-demo --patch-file patch-file.json kubectl patch deployment patch-demo --patch '{"spec": {"template": {"spec": {"containers": [{"name": "patch-demo-ctr-2","image": "redis"}]}}}}' Update an object's replica count using kubectl patch with --subresourceThe flag --subresource=[subresource-name] is used with kubectl commands like get, patch, edit, apply and replace to fetch and update status, scale and resize subresource of the resources you specify. You can specify a subresource for any of the Kubernetes API resources (built-in and CRs) that have status, scale or resize subresource.For example, a Deployment has a status subresource and a scale subresource, so you can use kubectl to get or modify just the status subresource of a Deployment.Here's a manifest for a Deployment that has two replicas:application/deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment spec: selector: matchLabels: app: nginx replicas: 2 # tells deployment to run 2 pods matching the template template: metadata: labels: app: nginx spec: containers: \- name: nginx image: nginx:1.14.2 ports: \- containerPort: 80 Create the Deployment:kubectl apply -f https://k8s.io/examples/application/deployment.yaml View the Pods associated with your Deployment:kubectl get pods -l app=nginx In the output, you can see that Deployment has two Pods. For example:NAME READY STATUS RESTARTS AGE nginx-deployment-7fb96c846b-22567 1/1 Running 0 47s nginx-deployment-7fb96c846b-mlgns 1/1 Running 0 47s Now, patch that Deployment with --subresource=[subresource-name] flag:kubectl patch deployment nginx-deployment --subresource='scale' --type='merge' -p '{"spec":{"replicas":3}}' The output is:scale.autoscaling/nginx-deployment patched View the Pods associated with your patched Deployment:kubectl get pods -l app=nginx In the output, you can see one new pod is created, so now you have 3 running pods.NAME READY STATUS RESTARTS AGE nginx-deployment-7fb96c846b-22567 1/1 Running 0 107s nginx-deployment-7fb96c846b-lxfr2 1/1 Running 0 14s nginx-deployment-7fb96c846b-mlgns 1/1 Running 0 107s View the patched Deployment:kubectl get deployment nginx-deployment -o yaml ... spec: replicas: 3 ... status: ... availableReplicas: 3 readyReplicas: 3 replicas: 3 Note:If you run kubectl patch and specify --subresource flag for resource that doesn't support that particular subresource, the API server returns a 404 Not Found error.SummaryIn this exercise, you used kubectl patch to change the live configuration of a Deployment object. You did not change the configuration file that you originally used to create the Deployment object. Other commands for updating API objects include kubectl annotate, kubectl edit, kubectl replace, kubectl scale, and kubectl apply.Note:Strategic merge patch is not supported for custom resources.What's nextKubernetes Object ManagementManaging Kubernetes Objects Using Imperative CommandsImperative Management of Kubernetes Objects Using Configuration FilesDeclarative Management of Kubernetes Objects Using Configuration Files4.5.6 - Migrate Kubernetes Objects Using Storage Version MigrationFEATURE STATE: Kubernetes v1.30 [alpha] (enabled by default: false)Kubernetes relies on API data being actively re-written, to support some maintenance activities related to at rest storage. Two prominent examples are the versioned schema of stored resources (that is, the preferred storage schema changing from v1 to v2 for a given resource) and encryption at rest (that is, rewriting stale data based on a change in how the data should be encrypted).Before you beginInstall kubectl.You need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesYour Kubernetes server must be at or later than version v1.30.To check the version, enter kubectl version.Ensure that your cluster has the StorageVersionMigrator and InformerResourceVersion feature gates enabled. You will need control plane administrator access to make that change.Enable storage version migration REST api by setting runtime config storagemigration.k8s.io/v1alpha1 to true for the API server. For more information on how to do that, read enable or disable a Kubernetes API.Re-encrypt Kubernetes secrets using storage version migrationTo begin with, configure KMS provider to encrypt data at rest in etcd using following encryption configuration.kind: EncryptionConfiguration apiVersion: apiserver.config.k8s.io/v1 resources: \- resources: \- secrets providers: \- aescbc: keys: \- name: key1 secret: c2VjcmV0IGlzIHNlY3VyZQ== Make sure to enable automatic reload of encryption configuration file by setting --encryption-provider-config-automatic-reload to true.Create a Secret using kubectl.kubectl create secret generic my-secret --from-literal=key1=supersecret Verify the serialized data for that Secret object is prefixed with k8s:enc:aescbc:v1:key1.Update the encryption configuration file as follows to rotate the encryption key.kind: EncryptionConfiguration apiVersion: apiserver.config.k8s.io/v1 resources: \- resources: \- secrets providers: \- aescbc: keys: \- name: key2 secret: c2VjcmV0IGlzIHNlY3VyZSwgaXMgaXQ/ \- aescbc: keys: \- name: key1 secret: c2VjcmV0IGlzIHNlY3VyZQ== To ensure that previously created secret my-secret is re-encrypted with new key key2, you will use Storage Version Migration.Create a StorageVersionMigration manifest named migrate-secret.yaml as follows:kind: StorageVersionMigration apiVersion: storagemigration.k8s.io/v1alpha1 metadata: name: secrets-migration spec: resource: group: "" version: v1 resource: secrets Create the object using kubectl as follows:kubectl apply -f migrate-secret.yaml Monitor migration of Secrets by checking the .status of the StorageVersionMigration. A successful migration should have its Succeeded condition set to true. Get the StorageVersionMigration object as follows:kubectl get storageversionmigration.storagemigration.k8s.io/secrets-migration -o yaml The output is similar to:kind: StorageVersionMigration apiVersion: storagemigration.k8s.io/v1alpha1 metadata: name: secrets-migration uid: 628f6922-a9cb-4514-b076-12d3c178967c resourceVersion: "90" creationTimestamp: "2024-03-12T20:29:45Z" spec: resource: group: "" version: v1 resource: secrets status: conditions: \- type: Running status: "False" lastUpdateTime: "2024-03-12T20:29:46Z" reason: StorageVersionMigrationInProgress \- type: Succeeded status: "True" lastUpdateTime: "2024-03-12T20:29:46Z" reason: StorageVersionMigrationSucceeded resourceVersion: "84" Verify the stored secret is now prefixed with k8s:enc:aescbc:v1:key2.Update the preferred storage schema of a CRDConsider a scenario where a CustomResourceDefinition (CRD) is created to serve custom resources (CRs) and is set as the preferred storage schema. When it's time to introduce v2 of the CRD, it can be added for serving only with a conversion webhook. This enables a smoother transition where users can create CRs using either the v1 or v2 schema, with the webhook in place to perform the necessary schema conversion between them. Before setting v2 as the preferred storage schema version, it's important to ensure that all existing CRs stored as v1 are migrated to v2. This migration can be achieved through Storage Version Migration to migrate all CRs from v1 to v2.Create a manifest for the CRD, named test-crd.yaml, as follows:apiVersion: apiextensions.k8s.io/v1 kind: CustomResourceDefinition metadata: name: selfierequests.stable.example.com spec: group: stable.example.com names: plural: SelfieRequests singular: SelfieRequest kind: SelfieRequest listKind: SelfieRequestList scope: Namespaced versions: \- name: v1 served: true storage: true schema: openAPIV3Schema: type: object properties: hostPort: type: string conversion: strategy: Webhook webhook: clientConfig: url: "https://127.0.0.1:9443/crdconvert" caBundle:  conversionReviewVersions: \- v1 \- v2 Create CRD using kubectl:kubectl apply -f test-crd.yaml Create a manifest for an example testcrd. Name the manifest cr1.yaml and use these contents:apiVersion: stable.example.com/v1 kind: SelfieRequest metadata: name: cr1 namespace: default Create CR using kubectl:kubectl apply -f cr1.yaml Verify that CR is written and stored as v1 by getting the object from etcd.ETCDCTL_API=3 etcdctl get /kubernetes.io/stable.example.com/testcrds/default/cr1 [...] | hexdump -C where [...] contains the additional arguments for connecting to the etcd server.Update the CRD test-crd.yaml to include v2 version for serving and storage and v1 as serving only, as follows:apiVersion: apiextensions.k8s.io/v1 kind: CustomResourceDefinition metadata: name: selfierequests.stable.example.com spec: group: stable.example.com names: plural: SelfieRequests singular: SelfieRequest kind: SelfieRequest listKind: SelfieRequestList scope: Namespaced versions: \- name: v2 served: true storage: true schema: openAPIV3Schema: type: object properties: host: type: string port: type: string \- name: v1 served: true storage: false schema: openAPIV3Schema: type: object properties: hostPort: type: string conversion: strategy: Webhook webhook: clientConfig: url: "https://127.0.0.1:9443/crdconvert" caBundle:  conversionReviewVersions: \- v1 \- v2 Update CRD using kubectl:kubectl apply -f test-crd.yaml Create CR resource file with name cr2.yaml as follows:apiVersion: stable.example.com/v2 kind: SelfieRequest metadata: name: cr2 namespace: default Create CR using kubectl:kubectl apply -f cr2.yaml Verify that CR is written and stored as v2 by getting the object from etcd.ETCDCTL_API=3 etcdctl get /kubernetes.io/stable.example.com/testcrds/default/cr2 [...] | hexdump -C where [...] contains the additional arguments for connecting to the etcd server.Create a StorageVersionMigration manifest named migrate-crd.yaml, with the contents as follows:kind: StorageVersionMigration apiVersion: storagemigration.k8s.io/v1alpha1 metadata: name: crdsvm spec: resource: group: stable.example.com version: v1 resource: SelfieRequest Create the object using kubectl as follows:kubectl apply -f migrate-crd.yaml Monitor migration of secrets using status. Successful migration should have Succeeded condition set to "True" in the status field. Get the migration resource as follows:kubectl get storageversionmigration.storagemigration.k8s.io/crdsvm -o yaml The output is similar to:kind: StorageVersionMigration apiVersion: storagemigration.k8s.io/v1alpha1 metadata: name: crdsvm uid: 13062fe4-32d7-47cc-9528-5067fa0c6ac8 resourceVersion: "111" creationTimestamp: "2024-03-12T22:40:01Z" spec: resource: group: stable.example.com version: v1 resource: testcrds status: conditions: \- type: Running status: "False" lastUpdateTime: "2024-03-12T22:40:03Z" reason: StorageVersionMigrationInProgress \- type: Succeeded status: "True" lastUpdateTime: "2024-03-12T22:40:03Z" reason: StorageVersionMigrationSucceeded resourceVersion: "106" Verify that previously created cr1 is now written and stored as v2 by getting the object from etcd.ETCDCTL_API=3 etcdctl get /kubernetes.io/stable.example.com/testcrds/default/cr1 [...] | hexdump -C where [...] contains the additional arguments for connecting to the etcd server.4.6 - Managing SecretsManaging confidential settings data using Secrets.4.6.1 - Managing Secrets using kubectlCreating Secret objects using kubectl command line.This page shows you how to create, edit, manage, and delete Kubernetes Secrets using the kubectl command-line tool.Before you beginYou need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesCreate a SecretA Secret object stores sensitive data such as credentials used by Pods to access services. For example, you might need a Secret to store the username and password needed to access a database.You can create the Secret by passing the raw data in the command, or by storing the credentials in files that you pass in the command. The following commands create a Secret that stores the username admin and the password S!B\\*d$zDsb=.Use raw dataRun the following command:kubectl create secret generic db-user-pass \ \--from-literal=username=admin \ \--from-literal=password='S!B\\*d$zDsb=' You must use single quotes '' to escape special characters such as $, \, *, =, and ! in your strings. If you don't, your shell will interpret these characters.Note:The stringData field for a Secret does not work well with server-side apply.Use source filesStore the credentials in files:echo -n 'admin' > ./username.txt echo -n 'S!B\\*d$zDsb=' > ./password.txt The -n flag ensures that the generated files do not have an extra newline character at the end of the text. This is important because when kubectl reads a file and encodes the content into a base64 string, the extra newline character gets encoded too. You do not need to escape special characters in strings that you include in a file.Pass the file paths in the kubectl command:kubectl create secret generic db-user-pass \ \--from-file=./username.txt \ \--from-file=./password.txt The default key name is the file name. You can optionally set the key name using --from-file=[key=]source. For example:kubectl create secret generic db-user-pass \ \--from-file=username=./username.txt \ \--from-file=password=./password.txt With either method, the output is similar to:secret/db-user-pass created Verify the SecretCheck that the Secret was created:kubectl get secrets The output is similar to:NAME TYPE DATA AGE db-user-pass Opaque 2 51s View the details of the Secret:kubectl describe secret db-user-pass The output is similar to:Name: db-user-pass Namespace: default Labels:  Annotations:  Type: Opaque Data ==== password: 12 bytes username: 5 bytes The commands kubectl get and kubectl describe avoid showing the contents of a Secret by default. This is to protect the Secret from being exposed accidentally, or from being stored in a terminal log.Decode the SecretView the contents of the Secret you created:kubectl get secret db-user-pass -o jsonpath='{.data}' The output is similar to:{ "password": "UyFCXCpkJHpEc2I9", "username": "YWRtaW4=" } Decode the password data:echo 'UyFCXCpkJHpEc2I9' | base64 --decode The output is similar to:S!B\\*d$zDsb= Caution:This is an example for documentation purposes. In practice, this method could cause the command with the encoded data to be stored in your shell history. Anyone with access to your computer could find the command and decode the secret. A better approach is to combine the view and decode commands.kubectl get secret db-user-pass -o jsonpath='{.data.password}' | base64 --decode Edit a SecretYou can edit an existing Secret object unless it is immutable. To edit a Secret, run the following command:kubectl edit secrets  This opens your default editor and allows you to update the base64 encoded Secret values in the data field, such as in the following example:# Please edit the object below. Lines beginning with a '#' will be ignored, # and an empty file will abort the edit. If an error occurs while saving this file, it will be # reopened with the relevant failures. # apiVersion: v1 data: password: UyFCXCpkJHpEc2I9 username: YWRtaW4= kind: Secret metadata: creationTimestamp: "2022-06-28T17:44:13Z" name: db-user-pass namespace: default resourceVersion: "12708504" uid: 91becd59-78fa-4c85-823f-6d44436242ac type: Opaque Clean upTo delete a Secret, run the following command:kubectl delete secret db-user-pass What's nextRead more about the Secret conceptLearn how to manage Secrets using config fileLearn how to manage Secrets using kustomize4.6.2 - Managing Secrets using Configuration FileCreating Secret objects using resource configuration file.Before you beginYou need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesCreate the SecretYou can define the Secret object in a manifest first, in JSON or YAML format, and then create that object. The Secret resource contains two maps: data and stringData. The data field is used to store arbitrary data, encoded using base64. The stringData field is provided for convenience, and it allows you to provide the same data as unencoded strings. The keys of data and stringData must consist of alphanumeric characters, -, _ or ..The following example stores two strings in a Secret using the data field.Convert the strings to base64:echo -n 'admin' | base64 echo -n '1f2d1e2e67df' | base64 Note:The serialized JSON and YAML values of Secret data are encoded as base64 strings. Newlines are not valid within these strings and must be omitted. When using the base64 utility on Darwin/macOS, users should avoid using the -b option to split long lines. Conversely, Linux users should add the option -w 0 to base64 commands or the pipeline base64 | tr -d '\n' if the -w option is not available.The output is similar to:YWRtaW4= MWYyZDFlMmU2N2Rm Create the manifest:apiVersion: v1 kind: Secret metadata: name: mysecret type: Opaque data: username: YWRtaW4= password: MWYyZDFlMmU2N2Rm Note that the name of a Secret object must be a valid DNS subdomain name.Create the Secret using kubectl apply:kubectl apply -f ./secret.yaml The output is similar to:secret/mysecret created To verify that the Secret was created and to decode the Secret data, refer to Managing Secrets using kubectl.Specify unencoded data when creating a SecretFor certain scenarios, you may wish to use the stringData field instead. This field allows you to put a non-base64 encoded string directly into the Secret, and the string will be encoded for you when the Secret is created or updated.A practical example of this might be where you are deploying an application that uses a Secret to store a configuration file, and you want to populate parts of that configuration file during your deployment process.For example, if your application uses the following configuration file:apiUrl: "https://my.api.com/api/v1" username: "" password: "" You could store this in a Secret using the following definition:apiVersion: v1 kind: Secret metadata: name: mysecret type: Opaque stringData: config.yaml: | apiUrl: "https://my.api.com/api/v1" username:  password:  Note:The stringData field for a Secret does not work well with server-side apply.When you retrieve the Secret data, the command returns the encoded values, and not the plaintext values you provided in stringData.For example, if you run the following command:kubectl get secret mysecret -o yaml The output is similar to:apiVersion: v1 data: config.yaml: YXBpVXJsOiAiaHR0cHM6Ly9teS5hcGkuY29tL2FwaS92MSIKdXNlcm5hbWU6IHt7dXNlcm5hbWV9fQpwYXNzd29yZDoge3twYXNzd29yZH19 kind: Secret metadata: creationTimestamp: 2018-11-15T20:40:59Z name: mysecret namespace: default resourceVersion: "7225" uid: c280ad2e-e916-11e8-98f2-025000000001 type: Opaque Specify both data and stringDataIf you specify a field in both data and stringData, the value from stringData is used.For example, if you define the following Secret:apiVersion: v1 kind: Secret metadata: name: mysecret type: Opaque data: username: YWRtaW4= stringData: username: administrator Note:The stringData field for a Secret does not work well with server-side apply.The Secret object is created as follows:apiVersion: v1 data: username: YWRtaW5pc3RyYXRvcg== kind: Secret metadata: creationTimestamp: 2018-11-15T20:46:46Z name: mysecret namespace: default resourceVersion: "7579" uid: 91460ecb-e917-11e8-98f2-025000000001 type: Opaque YWRtaW5pc3RyYXRvcg== decodes to administrator.Edit a SecretTo edit the data in the Secret you created using a manifest, modify the data or stringData field in your manifest and apply the file to your cluster. You can edit an existing Secret object unless it is immutable.For example, if you want to change the password from the previous example to birdsarentreal, do the following:Encode the new password string:echo -n 'birdsarentreal' | base64 The output is similar to:YmlyZHNhcmVudHJlYWw= Update the data field with your new password string:apiVersion: v1 kind: Secret metadata: name: mysecret type: Opaque data: username: YWRtaW4= password: YmlyZHNhcmVudHJlYWw= Apply the manifest to your cluster:kubectl apply -f ./secret.yaml The output is similar to:secret/mysecret configured Kubernetes updates the existing Secret object. In detail, the kubectl tool notices that there is an existing Secret object with the same name. kubectl fetches the existing object, plans changes to it, and submits the changed Secret object to your cluster control plane.If you specified kubectl apply --server-side instead, kubectl uses Server Side Apply instead.Clean upTo delete the Secret you have created:kubectl delete secret mysecret What's nextRead more about the Secret conceptLearn how to manage Secrets using kubectlLearn how to manage Secrets using kustomize4.6.3 - Managing Secrets using KustomizeCreating Secret objects using kustomization.yaml file.kubectl supports using the Kustomize object management tool to manage Secrets and ConfigMaps. You create a resource generator using Kustomize, which generates a Secret that you can apply to the API server using kubectl.Before you beginYou need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesCreate a SecretYou can generate a Secret by defining a secretGenerator in a kustomization.yaml file that references other existing files, .env files, or literal values. For example, the following instructions create a kustomization file for the username admin and the password 1f2d1e2e67df.Note:The stringData field for a Secret does not work well with server-side apply.Create the kustomization fileLiteralsFiles.env files secretGenerator: \- name: database-creds literals: \- username=admin \- password=1f2d1e2e67df Store the credentials in files. The filenames are the keys of the secret:echo -n 'admin' > ./username.txt echo -n '1f2d1e2e67df' > ./password.txt The -n flag ensures that there's no newline character at the end of your files.Create the kustomization.yaml file:secretGenerator: \- name: database-creds files: \- username.txt \- password.txt You can also define the secretGenerator in the kustomization.yaml file by providing .env files. For example, the following kustomization.yaml file pulls in data from an .env.secret file:secretGenerator: \- name: db-user-pass envs: \- .env.secret In all cases, you don't need to encode the values in base64. The name of the YAML file must be kustomization.yaml or kustomization.yml.Apply the kustomization fileTo create the Secret, apply the directory that contains the kustomization file:kubectl apply -k  The output is similar to:secret/database-creds-5hdh7hhgfk created When a Secret is generated, the Secret name is created by hashing the Secret data and appending the hash value to the name. This ensures that a new Secret is generated each time the data is modified.To verify that the Secret was created and to decode the Secret data,kubectl get -k  -o jsonpath='{.data}' The output is similar to:{ "password": "MWYyZDFlMmU2N2Rm", "username": "YWRtaW4=" } echo 'MWYyZDFlMmU2N2Rm' | base64 --decode The output is similar to:1f2d1e2e67df For more information, refer to Managing Secrets using kubectl and Declarative Management of Kubernetes Objects Using Kustomize.Edit a SecretIn your kustomization.yaml file, modify the data, such as the password.Apply the directory that contains the kustomization file:kubectl apply -k  The output is similar to:secret/db-user-pass-6f24b56cc8 created The edited Secret is created as a new Secret object, instead of updating the existing Secret object. You might need to update references to the Secret in your Pods.Clean upTo delete a Secret, use kubectl:kubectl delete secret db-user-pass What's nextRead more about the Secret conceptLearn how to manage Secrets using kubectlLearn how to manage Secrets using config file4.7 - Inject Data Into ApplicationsSpecify configuration and other data for the Pods that run your workload.4.7.1 - Define a Command and Arguments for a ContainerThis page shows how to define commands and arguments when you run a container in a Pod.Before you beginYou need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesTo check the version, enter kubectl version.Define a command and arguments when you create a PodWhen you create a Pod, you can define a command and arguments for the containers that run in the Pod. To define a command, include the command field in the configuration file. To define arguments for the command, include the args field in the configuration file. The command and arguments that you define cannot be changed after the Pod is created.The command and arguments that you define in the configuration file override the default command and arguments provided by the container image. If you define args, but do not define a command, the default command is used with your new arguments.Note:The command field corresponds to ENTRYPOINT, and the args field corresponds to CMD in some container runtimes.In this exercise, you create a Pod that runs one container. The configuration file for the Pod defines a command and two arguments:pods/commands.yaml apiVersion: v1 kind: Pod metadata: name: command-demo labels: purpose: demonstrate-command spec: containers: \- name: command-demo-container image: debian command: ["printenv"] args: ["HOSTNAME", "KUBERNETES_PORT"] restartPolicy: OnFailure Create a Pod based on the YAML configuration file:kubectl apply -f https://k8s.io/examples/pods/commands.yaml List the running Pods:kubectl get pods The output shows that the container that ran in the command-demo Pod has completed.To see the output of the command that ran in the container, view the logs from the Pod:kubectl logs command-demo The output shows the values of the HOSTNAME and KUBERNETES_PORT environment variables:command-demo tcp://10.3.240.1:443 Use environment variables to define argumentsIn the preceding example, you defined the arguments directly by providing strings. As an alternative to providing strings directly, you can define arguments by using environment variables:env: \- name: MESSAGE value: "hello world" command: ["/bin/echo"] args: ["$(MESSAGE)"] This means you can define an argument for a Pod using any of the techniques available for defining environment variables, including ConfigMaps and Secrets.Note:The environment variable appears in parentheses, "$(VAR)". This is required for the variable to be expanded in the command or args field.Run a command in a shellIn some cases, you need your command to run in a shell. For example, your command might consist of several commands piped together, or it might be a shell script. To run your command in a shell, wrap it like this:command: ["/bin/sh"] args: ["-c", "while true; do echo hello; sleep 10;done"] What's nextLearn more about configuring pods and containers.Learn more about running commands in a container.See Container.4.7.2 - Define Dependent Environment VariablesThis page shows how to define dependent environment variables for a container in a Kubernetes Pod.Before you beginYou need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesDefine an environment dependent variable for a containerWhen you create a Pod, you can set dependent environment variables for the containers that run in the Pod. To set dependent environment variables, you can use $(VAR_NAME) in the value of env in the configuration file.In this exercise, you create a Pod that runs one container. The configuration file for the Pod defines a dependent environment variable with common usage defined. Here is the configuration manifest for the Pod:pods/inject/dependent-envars.yaml apiVersion: v1 kind: Pod metadata: name: dependent-envars-demo spec: containers: \- name: dependent-envars-demo args: \- while true; do echo -en '\n'; printf UNCHANGED_REFERENCE=$UNCHANGED_REFERENCE'\n'; printf SERVICE_ADDRESS=$SERVICE_ADDRESS'\n';printf ESCAPED_REFERENCE=$ESCAPED_REFERENCE'\n'; sleep 30; done; command: \- sh \- -c image: busybox:1.28 env: \- name: SERVICE_PORT value: "80" \- name: SERVICE_IP value: "172.17.0.1" \- name: UNCHANGED_REFERENCE value: "$(PROTOCOL)://$(SERVICE_IP):$(SERVICE_PORT)" \- name: PROTOCOL value: "https" \- name: SERVICE_ADDRESS value: "$(PROTOCOL)://$(SERVICE_IP):$(SERVICE_PORT)" \- name: ESCAPED_REFERENCE value: "$$(PROTOCOL)://$(SERVICE_IP):$(SERVICE_PORT)" Create a Pod based on that manifest:kubectl apply -f https://k8s.io/examples/pods/inject/dependent-envars.yaml pod/dependent-envars-demo created List the running Pods:kubectl get pods dependent-envars-demo NAME READY STATUS RESTARTS AGE dependent-envars-demo 1/1 Running 0 9s Check the logs for the container running in your Pod:kubectl logs pod/dependent-envars-demo UNCHANGED_REFERENCE=$(PROTOCOL)://172.17.0.1:80 SERVICE_ADDRESS=https://172.17.0.1:80 ESCAPED_REFERENCE=$(PROTOCOL)://172.17.0.1:80 As shown above, you have defined the correct dependency reference of SERVICE_ADDRESS, bad dependency reference of UNCHANGED_REFERENCE and skip dependent references of ESCAPED_REFERENCE.When an environment variable is already defined when being referenced, the reference can be correctly resolved, such as in the SERVICE_ADDRESS case.Note that order matters in the env list. An environment variable is not considered "defined" if it is specified further down the list. That is why UNCHANGED_REFERENCE fails to resolve $(PROTOCOL) in the example above.When the environment variable is undefined or only includes some variables, the undefined environment variable is treated as a normal string, such as UNCHANGED_REFERENCE. Note that incorrectly parsed environment variables, in general, will not block the container from starting.The $(VAR_NAME) syntax can be escaped with a double $, ie: $$(VAR_NAME). Escaped references are never expanded, regardless of whether the referenced variable is defined or not. This can be seen from the ESCAPED_REFERENCE case above.What's nextLearn more about environment variables.See EnvVarSource.4.7.3 - Define Environment Variables for a ContainerThis page shows how to define environment variables for a container in a Kubernetes Pod.Before you beginYou need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesDefine an environment variable for a containerWhen you create a Pod, you can set environment variables for the containers that run in the Pod. To set environment variables, include the env or envFrom field in the configuration file.The env and envFrom fields have different effects.envallows you to set environment variables for a container, specifying a value directly for each variable that you name.envFromallows you to set environment variables for a container by referencing either a ConfigMap or a Secret. When you use envFrom, all the key-value pairs in the referenced ConfigMap or Secret are set as environment variables for the container. You can also specify a common prefix string.You can read more about ConfigMap and Secret.This page explains how to use env.In this exercise, you create a Pod that runs one container. The configuration file for the Pod defines an environment variable with name DEMO_GREETING and value "Hello from the environment". Here is the configuration manifest for the Pod:pods/inject/envars.yaml apiVersion: v1 kind: Pod metadata: name: envar-demo labels: purpose: demonstrate-envars spec: containers: \- name: envar-demo-container image: gcr.io/google-samples/hello-app:2.0 env: \- name: DEMO_GREETING value: "Hello from the environment" \- name: DEMO_FAREWELL value: "Such a sweet sorrow" Create a Pod based on that manifest:kubectl apply -f https://k8s.io/examples/pods/inject/envars.yaml List the running Pods:kubectl get pods -l purpose=demonstrate-envars The output is similar to:NAME READY STATUS RESTARTS AGE envar-demo 1/1 Running 0 9s List the Pod's container environment variables:kubectl exec envar-demo -- printenv The output is similar to this:NODE_VERSION=4.4.2 EXAMPLE_SERVICE_PORT_8080_TCP_ADDR=10.3.245.237 HOSTNAME=envar-demo ... DEMO_GREETING=Hello from the environment DEMO_FAREWELL=Such a sweet sorrow Note:The environment variables set using the env or envFrom field override any environment variables specified in the container image.Note:Environment variables may reference each other, however ordering is important. Variables making use of others defined in the same context must come later in the list. Similarly, avoid circular references.Using environment variables inside of your configEnvironment variables that you define in a Pod's configuration under .spec.containers[*].env[*] can be used elsewhere in the configuration, for example in commands and arguments that you set for the Pod's containers. In the example configuration below, the GREETING, HONORIFIC, and NAME environment variables are set to Warm greetings to, The Most Honorable, and Kubernetes, respectively. The environment variable MESSAGE combines the set of all these environment variables and then uses it as a CLI argument passed to the env-print-demo container.Environment variable names consist of letters, numbers, underscores, dots, or hyphens, but the first character cannot be a digit. If the RelaxedEnvironmentVariableValidation feature gate is enabled, all printable ASCII characters except "=" may be used for environment variable names.apiVersion: v1 kind: Pod metadata: name: print-greeting spec: containers: \- name: env-print-demo image: bash env: \- name: GREETING value: "Warm greetings to" \- name: HONORIFIC value: "The Most Honorable" \- name: NAME value: "Kubernetes" \- name: MESSAGE value: "$(GREETING) $(HONORIFIC) $(NAME)" command: ["echo"] args: ["$(MESSAGE)"] Upon creation, the command echo Warm greetings to The Most Honorable Kubernetes is run on the container.What's nextLearn more about environment variables.Learn about using secrets as environment variables.See EnvVarSource.4.7.4 - Expose Pod Information to Containers Through Environment VariablesThis page shows how a Pod can use environment variables to expose information about itself to containers running in the Pod, using the downward API. You can use environment variables to expose Pod fields, container fields, or both.In Kubernetes, there are two ways to expose Pod and container fields to a running container:Environment variables, as explained in this taskVolume filesTogether, these two ways of exposing Pod and container fields are called the downward API.As Services are the primary mode of communication between containerized applications managed by Kubernetes, it is helpful to be able to discover them at runtime.Read more about accessing Services here.Before you beginYou need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesUse Pod fields as values for environment variablesIn this part of exercise, you create a Pod that has one container, and you project Pod-level fields into the running container as environment variables.pods/inject/dapi-envars-pod.yaml apiVersion: v1 kind: Pod metadata: name: dapi-envars-fieldref spec: containers: \- name: test-container image: registry.k8s.io/busybox:1.27.2 command: [ "sh", "-c"] args: \- while true; do echo -en '\n'; printenv MY_NODE_NAME MY_POD_NAME MY_POD_NAMESPACE; printenv MY_POD_IP MY_POD_SERVICE_ACCOUNT; sleep 10; done; env: \- name: MY_NODE_NAME valueFrom: fieldRef: fieldPath: spec.nodeName \- name: MY_POD_NAME valueFrom: fieldRef: fieldPath: metadata.name \- name: MY_POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace \- name: MY_POD_IP valueFrom: fieldRef: fieldPath: status.podIP \- name: MY_POD_SERVICE_ACCOUNT valueFrom: fieldRef: fieldPath: spec.serviceAccountName restartPolicy: Never In that manifest, you can see five environment variables. The env field is an array of environment variable definitions. The first element in the array specifies that the MY_NODE_NAME environment variable gets its value from the Pod's spec.nodeName field. Similarly, the other environment variables get their names from Pod fields.Note:The fields in this example are Pod fields. They are not fields of the container in the Pod.Create the Pod:kubectl apply -f https://k8s.io/examples/pods/inject/dapi-envars-pod.yaml Verify that the container in the Pod is running:# If the new Pod isn't yet healthy, rerun this command a few times. kubectl get pods View the container's logs:kubectl logs dapi-envars-fieldref The output shows the values of selected environment variables:minikube dapi-envars-fieldref default 172.17.0.4 default To see why these values are in the log, look at the command and args fields in the configuration file. When the container starts, it writes the values of five environment variables to stdout. It repeats this every ten seconds.Next, get a shell into the container that is running in your Pod:kubectl exec -it dapi-envars-fieldref -- sh In your shell, view the environment variables:# Run this in a shell inside the container printenv The output shows that certain environment variables have been assigned the values of Pod fields:MY_POD_SERVICE_ACCOUNT=default ... MY_POD_NAMESPACE=default MY_POD_IP=172.17.0.4 ... MY_NODE_NAME=minikube ... MY_POD_NAME=dapi-envars-fieldref Use container fields as values for environment variablesIn the preceding exercise, you used information from Pod-level fields as the values for environment variables. In this next exercise, you are going to pass fields that are part of the Pod definition, but taken from the specific container rather than from the Pod overall.Here is a manifest for another Pod that again has just one container:pods/inject/dapi-envars-container.yaml apiVersion: v1 kind: Pod metadata: name: dapi-envars-resourcefieldref spec: containers: \- name: test-container image: registry.k8s.io/busybox:1.27.2 command: [ "sh", "-c"] args: \- while true; do echo -en '\n'; printenv MY_CPU_REQUEST MY_CPU_LIMIT; printenv MY_MEM_REQUEST MY_MEM_LIMIT; sleep 10; done; resources: requests: memory: "32Mi" cpu: "125m" limits: memory: "64Mi" cpu: "250m" env: \- name: MY_CPU_REQUEST valueFrom: resourceFieldRef: containerName: test-container resource: requests.cpu \- name: MY_CPU_LIMIT valueFrom: resourceFieldRef: containerName: test-container resource: limits.cpu \- name: MY_MEM_REQUEST valueFrom: resourceFieldRef: containerName: test-container resource: requests.memory \- name: MY_MEM_LIMIT valueFrom: resourceFieldRef: containerName: test-container resource: limits.memory restartPolicy: Never In this manifest, you can see four environment variables. The env field is an array of environment variable definitions. The first element in the array specifies that the MY_CPU_REQUEST environment variable gets its value from the requests.cpu field of a container named test-container. Similarly, the other environment variables get their values from fields that are specific to this container.Create the Pod:kubectl apply -f https://k8s.io/examples/pods/inject/dapi-envars-container.yaml Verify that the container in the Pod is running:# If the new Pod isn't yet healthy, rerun this command a few times. kubectl get pods View the container's logs:kubectl logs dapi-envars-resourcefieldref The output shows the values of selected environment variables:1 1 33554432 67108864 What's nextRead Defining Environment Variables for a ContainerRead the spec API definition for Pod. This includes the definition of Container (part of Pod).Read the list of available fields that you can expose using the downward API.Read about Pods, containers and environment variables in the legacy API reference:PodSpecContainerEnvVarEnvVarSourceObjectFieldSelectorResourceFieldSelector4.7.5 - Expose Pod Information to Containers Through FilesThis page shows how a Pod can use a downwardAPI volume, to expose information about itself to containers running in the Pod. A downwardAPI volume can expose Pod fields and container fields.In Kubernetes, there are two ways to expose Pod and container fields to a running container:Environment variablesVolume files, as explained in this taskTogether, these two ways of exposing Pod and container fields are called the downward API.Before you beginYou need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesStore Pod fieldsIn this part of exercise, you create a Pod that has one container, and you project Pod-level fields into the running container as files. Here is the manifest for the Pod:pods/inject/dapi-volume.yaml apiVersion: v1 kind: Pod metadata: name: kubernetes-downwardapi-volume-example labels: zone: us-est-coast cluster: test-cluster1 rack: rack-22 annotations: build: two builder: john-doe spec: containers: \- name: client-container image: registry.k8s.io/busybox:1.27.2 command: ["sh", "-c"] args: \- while true; do if [[ -e /etc/podinfo/labels ]]; then echo -en '\n\n'; cat /etc/podinfo/labels; fi; if [[ -e /etc/podinfo/annotations ]]; then echo -en '\n\n'; cat /etc/podinfo/annotations; fi; sleep 5; done; volumeMounts: \- name: podinfo mountPath: /etc/podinfo volumes: \- name: podinfo downwardAPI: items: \- path: "labels" fieldRef: fieldPath: metadata.labels \- path: "annotations" fieldRef: fieldPath: metadata.annotations In the manifest, you can see that the Pod has a downwardAPI Volume, and the container mounts the volume at /etc/podinfo.Look at the items array under downwardAPI. Each element of the array defines a downwardAPI volume. The first element specifies that the value of the Pod's metadata.labels field should be stored in a file named labels. The second element specifies that the value of the Pod's annotations field should be stored in a file named annotations.Note:The fields in this example are Pod fields. They are not fields of the container in the Pod.Create the Pod:kubectl apply -f https://k8s.io/examples/pods/inject/dapi-volume.yaml Verify that the container in the Pod is running:kubectl get pods View the container's logs:kubectl logs kubernetes-downwardapi-volume-example The output shows the contents of the labels file and the annotations file:cluster="test-cluster1" rack="rack-22" zone="us-est-coast" build="two" builder="john-doe" Get a shell into the container that is running in your Pod:kubectl exec -it kubernetes-downwardapi-volume-example -- sh In your shell, view the labels file:/# cat /etc/podinfo/labels The output shows that all of the Pod's labels have been written to the labels file:cluster="test-cluster1" rack="rack-22" zone="us-est-coast" Similarly, view the annotations file:/# cat /etc/podinfo/annotations View the files in the /etc/podinfo directory:/# ls -laR /etc/podinfo In the output, you can see that the labels and annotations files are in a temporary subdirectory: in this example, ..2982_06_02_21_47_53.299460680. In the /etc/podinfo directory, ..data is a symbolic link to the temporary subdirectory. Also in the /etc/podinfo directory, labels and annotations are symbolic links.drwxr-xr-x ... Feb 6 21:47 ..2982_06_02_21_47_53.299460680 lrwxrwxrwx ... Feb 6 21:47 ..data -> ..2982_06_02_21_47_53.299460680 lrwxrwxrwx ... Feb 6 21:47 annotations -> ..data/annotations lrwxrwxrwx ... Feb 6 21:47 labels -> ..data/labels /etc/..2982_06_02_21_47_53.299460680: total 8 -rw-r--r-- ... Feb 6 21:47 annotations -rw-r--r-- ... Feb 6 21:47 labels Using symbolic links enables dynamic atomic refresh of the metadata; updates are written to a new temporary directory, and the ..data symlink is updated atomically using rename(2).Note:A container using Downward API as a subPath volume mount will not receive Downward API updates.Exit the shell:/# exit Store container fieldsThe preceding exercise, you made Pod-level fields accessible using the downward API. In this next exercise, you are going to pass fields that are part of the Pod definition, but taken from the specific container rather than from the Pod overall. Here is a manifest for a Pod that again has just one container:pods/inject/dapi-volume-resources.yaml apiVersion: v1 kind: Pod metadata: name: kubernetes-downwardapi-volume-example-2 spec: containers: \- name: client-container image: registry.k8s.io/busybox:1.27.2 command: ["sh", "-c"] args: \- while true; do echo -en '\n'; if [[ -e /etc/podinfo/cpu_limit ]]; then echo -en '\n'; cat /etc/podinfo/cpu_limit; fi; if [[ -e /etc/podinfo/cpu_request ]]; then echo -en '\n'; cat /etc/podinfo/cpu_request; fi; if [[ -e /etc/podinfo/mem_limit ]]; then echo -en '\n'; cat /etc/podinfo/mem_limit; fi; if [[ -e /etc/podinfo/mem_request ]]; then echo -en '\n'; cat /etc/podinfo/mem_request; fi; sleep 5; done; resources: requests: memory: "32Mi" cpu: "125m" limits: memory: "64Mi" cpu: "250m" volumeMounts: \- name: podinfo mountPath: /etc/podinfo volumes: \- name: podinfo downwardAPI: items: \- path: "cpu_limit" resourceFieldRef: containerName: client-container resource: limits.cpu divisor: 1m \- path: "cpu_request" resourceFieldRef: containerName: client-container resource: requests.cpu divisor: 1m \- path: "mem_limit" resourceFieldRef: containerName: client-container resource: limits.memory divisor: 1Mi \- path: "mem_request" resourceFieldRef: containerName: client-container resource: requests.memory divisor: 1Mi In the manifest, you can see that the Pod has a downwardAPI volume, and that the single container in that Pod mounts the volume at /etc/podinfo.Look at the items array under downwardAPI. Each element of the array defines a file in the downward API volume.The first element specifies that in the container named client-container, the value of the limits.cpu field in the format specified by 1m should be published as a file named cpu_limit. The divisor field is optional and has the default value of 1. A divisor of 1 means cores for cpu resources, or bytes for memory resources.Create the Pod:kubectl apply -f https://k8s.io/examples/pods/inject/dapi-volume-resources.yaml Get a shell into the container that is running in your Pod:kubectl exec -it kubernetes-downwardapi-volume-example-2 -- sh In your shell, view the cpu_limit file:# Run this in a shell inside the container cat /etc/podinfo/cpu_limit You can use similar commands to view the cpu_request, mem_limit and mem_request files.Project keys to specific paths and file permissionsYou can project keys to specific paths and specific permissions on a per-file basis. For more information, see Secrets.What's nextRead the spec API definition for Pod. This includes the definition of Container (part of Pod).Read the list of available fields that you can expose using the downward API.Read about volumes in the legacy API reference:Check the Volume API definition which defines a generic volume in a Pod for containers to access.Check the DownwardAPIVolumeSource API definition which defines a volume that contains Downward API information.Check the DownwardAPIVolumeFile API definition which contains references to object or resource fields for populating a file in the Downward API volume.Check the ResourceFieldSelector API definition which specifies the container resources and their output format.4.7.6 - Distribute Credentials Securely Using SecretsThis page shows how to securely inject sensitive data, such as passwords and encryption keys, into Pods.Before you beginYou need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesConvert your secret data to a base-64 representationSuppose you want to have two pieces of secret data: a username my-app and a password 39528$vdg7Jb. First, use a base64 encoding tool to convert your username and password to a base64 representation. Here's an example using the commonly available base64 program:echo -n 'my-app' | base64 echo -n '39528$vdg7Jb' | base64 The output shows that the base-64 representation of your username is bXktYXBw, and the base-64 representation of your password is Mzk1MjgkdmRnN0pi.Caution:Use a local tool trusted by your OS to decrease the security risks of external tools.Create a SecretHere is a configuration file you can use to create a Secret that holds your username and password:pods/inject/secret.yaml apiVersion: v1 kind: Secret metadata: name: test-secret data: username: bXktYXBw password: Mzk1MjgkdmRnN0pi Create the Secretkubectl apply -f https://k8s.io/examples/pods/inject/secret.yaml View information about the Secret:kubectl get secret test-secret Output:NAME TYPE DATA AGE test-secret Opaque 2 1m View more detailed information about the Secret:kubectl describe secret test-secret Output:Name: test-secret Namespace: default Labels:  Annotations:  Type: Opaque Data ==== password: 13 bytes username: 7 bytes Create a Secret directly with kubectlIf you want to skip the Base64 encoding step, you can create the same Secret using the kubectl create secret command. For example:kubectl create secret generic test-secret --from-literal='username=my-app' --from-literal='password=39528$vdg7Jb' This is more convenient. The detailed approach shown earlier runs through each step explicitly to demonstrate what is happening.Create a Pod that has access to the secret data through a VolumeHere is a configuration file you can use to create a Pod:pods/inject/secret-pod.yaml apiVersion: v1 kind: Pod metadata: name: secret-test-pod spec: containers: \- name: test-container image: nginx volumeMounts: # name must match the volume name below \- name: secret-volume mountPath: /etc/secret-volume readOnly: true # The secret data is exposed to Containers in the Pod through a Volume. volumes: \- name: secret-volume secret: secretName: test-secret Create the Pod:kubectl apply -f https://k8s.io/examples/pods/inject/secret-pod.yaml Verify that your Pod is running:kubectl get pod secret-test-pod Output:NAME READY STATUS RESTARTS AGE secret-test-pod 1/1 Running 0 42m Get a shell into the Container that is running in your Pod:kubectl exec -i -t secret-test-pod -- /bin/bash The secret data is exposed to the Container through a Volume mounted under /etc/secret-volume.In your shell, list the files in the /etc/secret-volume directory:# Run this in the shell inside the container ls /etc/secret-volume The output shows two files, one for each piece of secret data:password username In your shell, display the contents of the username and password files:# Run this in the shell inside the container echo "$( cat /etc/secret-volume/username )" echo "$( cat /etc/secret-volume/password )" The output is your username and password:my-app 39528$vdg7Jb Modify your image or command line so that the program looks for files in the mountPath directory. Each key in the Secret data map becomes a file name in this directory.Project Secret keys to specific file pathsYou can also control the paths within the volume where Secret keys are projected. Use the .spec.volumes[].secret.items field to change the target path of each key:apiVersion: v1 kind: Pod metadata: name: mypod spec: containers: \- name: mypod image: redis volumeMounts: \- name: foo mountPath: "/etc/foo" readOnly: true volumes: \- name: foo secret: secretName: mysecret items: \- key: username path: my-group/my-username When you deploy this Pod, the following happens:The username key from mysecret is available to the container at the path /etc/foo/my-group/my-username instead of at /etc/foo/username.The password key from that Secret object is not projected.If you list keys explicitly using .spec.volumes[].secret.items, consider the following:Only keys specified in items are projected.To consume all keys from the Secret, all of them must be listed in the items field.All listed keys must exist in the corresponding Secret. Otherwise, the volume is not created.Set POSIX permissions for Secret keysYou can set the POSIX file access permission bits for a single Secret key. If you don't specify any permissions, 0644 is used by default. You can also set a default POSIX file mode for the entire Secret volume, and you can override per key if needed.For example, you can specify a default mode like this:apiVersion: v1 kind: Pod metadata: name: mypod spec: containers: \- name: mypod image: redis volumeMounts: \- name: foo mountPath: "/etc/foo" volumes: \- name: foo secret: secretName: mysecret defaultMode: 0400 The Secret is mounted on /etc/foo; all the files created by the secret volume mount have permission 0400.Note:If you're defining a Pod or a Pod template using JSON, beware that the JSON specification doesn't support octal literals for numbers because JSON considers 0400 to be the decimal value 400. In JSON, use decimal values for the defaultMode instead. If you're writing YAML, you can write the defaultMode in octal.Define container environment variables using Secret dataYou can consume the data in Secrets as environment variables in your containers.If a container already consumes a Secret in an environment variable, a Secret update will not be seen by the container unless it is restarted. There are third party solutions for triggering restarts when secrets change.Define a container environment variable with data from a single SecretDefine an environment variable as a key-value pair in a Secret:kubectl create secret generic backend-user --from-literal=backend-username='backend-admin' Assign the backend-username value defined in the Secret to the SECRET_USERNAME environment variable in the Pod specification.pods/inject/pod-single-secret-env-variable.yaml apiVersion: v1 kind: Pod metadata: name: env-single-secret spec: containers: \- name: envars-test-container image: nginx env: \- name: SECRET_USERNAME valueFrom: secretKeyRef: name: backend-user key: backend-username Create the Pod:kubectl create -f https://k8s.io/examples/pods/inject/pod-single-secret-env-variable.yaml In your shell, display the content of SECRET_USERNAME container environment variable.kubectl exec -i -t env-single-secret -- /bin/sh -c 'echo $SECRET_USERNAME' The output is similar to:backend-admin Define container environment variables with data from multiple SecretsAs with the previous example, create the Secrets first.kubectl create secret generic backend-user --from-literal=backend-username='backend-admin' kubectl create secret generic db-user --from-literal=db-username='db-admin' Define the environment variables in the Pod specification.pods/inject/pod-multiple-secret-env-variable.yaml apiVersion: v1 kind: Pod metadata: name: envvars-multiple-secrets spec: containers: \- name: envars-test-container image: nginx env: \- name: BACKEND_USERNAME valueFrom: secretKeyRef: name: backend-user key: backend-username \- name: DB_USERNAME valueFrom: secretKeyRef: name: db-user key: db-username Create the Pod:kubectl create -f https://k8s.io/examples/pods/inject/pod-multiple-secret-env-variable.yaml In your shell, display the container environment variables.kubectl exec -i -t envvars-multiple-secrets -- /bin/sh -c 'env | grep _USERNAME' The output is similar to:DB_USERNAME=db-admin BACKEND_USERNAME=backend-admin Configure all key-value pairs in a Secret as container environment variablesNote:This functionality is available in Kubernetes v1.6 and later.Create a Secret containing multiple key-value pairskubectl create secret generic test-secret --from-literal=username='my-app' --from-literal=password='39528$vdg7Jb' Use envFrom to define all of the Secret's data as container environment variables. The key from the Secret becomes the environment variable name in the Pod.pods/inject/pod-secret-envFrom.yaml apiVersion: v1 kind: Pod metadata: name: envfrom-secret spec: containers: \- name: envars-test-container image: nginx envFrom: \- secretRef: name: test-secret Create the Pod:kubectl create -f https://k8s.io/examples/pods/inject/pod-secret-envFrom.yaml In your shell, display username and password container environment variables.kubectl exec -i -t envfrom-secret -- /bin/sh -c 'echo "username: $username\npassword: $password\n"' The output is similar to:username: my-app password: 39528$vdg7Jb Example: Provide prod/test credentials to Pods using SecretsThis example illustrates a Pod which consumes a secret containing production credentials and another Pod which consumes a secret with test environment credentials.Create a secret for prod environment credentials:kubectl create secret generic prod-db-secret --from-literal=username=produser --from-literal=password=Y4nys7f11 The output is similar to:secret "prod-db-secret" created Create a secret for test environment credentials.kubectl create secret generic test-db-secret --from-literal=username=testuser --from-literal=password=iluvtests The output is similar to:secret "test-db-secret" created Note:Special characters such as $, \, *, =, and ! will be interpreted by your shell and require escaping.In most shells, the easiest way to escape the password is to surround it with single quotes ('). For example, if your actual password is S!B\\*d$zDsb=, you should execute the command as follows:kubectl create secret generic dev-db-secret --from-literal=username=devuser --from-literal=password='S!B\\*d$zDsb=' You do not need to escape special characters in passwords from files (--from-file).Create the Pod manifests:cat < pod.yaml apiVersion: v1 kind: List items: \- kind: Pod apiVersion: v1 metadata: name: prod-db-client-pod labels: name: prod-db-client spec: volumes: \- name: secret-volume secret: secretName: prod-db-secret containers: \- name: db-client-container image: myClientImage volumeMounts: \- name: secret-volume readOnly: true mountPath: "/etc/secret-volume" \- kind: Pod apiVersion: v1 metadata: name: test-db-client-pod labels: name: test-db-client spec: volumes: \- name: secret-volume secret: secretName: test-db-secret containers: \- name: db-client-container image: myClientImage volumeMounts: \- name: secret-volume readOnly: true mountPath: "/etc/secret-volume" EOF Note:How the specs for the two Pods differ only in one field; this facilitates creating Pods with different capabilities from a common Pod template.Apply all those objects on the API server by running:kubectl create -f pod.yaml Both containers will have the following files present on their filesystems with the values for each container's environment:/etc/secret-volume/username /etc/secret-volume/password You could further simplify the base Pod specification by using two service accounts:prod-user with the prod-db-secrettest-user with the test-db-secretThe Pod specification is shortened to:apiVersion: v1 kind: Pod metadata: name: prod-db-client-pod labels: name: prod-db-client spec: serviceAccount: prod-db-client containers: \- name: db-client-container image: myClientImage ReferencesSecretVolumePodWhat's nextLearn more about Secrets.Learn about Volumes.4.8 - Run ApplicationsRun and manage both stateless and stateful applications.4.8.1 - Run a Stateless Application Using a DeploymentThis page shows how to run an application using a Kubernetes Deployment object.ObjectivesCreate an nginx deployment.Use kubectl to list information about the deployment.Update the deployment.Before you beginYou need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesYour Kubernetes server must be at or later than version v1.9.To check the version, enter kubectl version.Creating and exploring an nginx deploymentYou can run an application by creating a Kubernetes Deployment object, and you can describe a Deployment in a YAML file. For example, this YAML file describes a Deployment that runs the nginx:1.14.2 Docker image:application/deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment spec: selector: matchLabels: app: nginx replicas: 2 # tells deployment to run 2 pods matching the template template: metadata: labels: app: nginx spec: containers: \- name: nginx image: nginx:1.14.2 ports: \- containerPort: 80 Create a Deployment based on the YAML file:kubectl apply -f https://k8s.io/examples/application/deployment.yaml Display information about the Deployment:kubectl describe deployment nginx-deployment The output is similar to this:Name: nginx-deployment Namespace: default CreationTimestamp: Tue, 30 Aug 2016 18:11:37 -0700 Labels: app=nginx Annotations: deployment.kubernetes.io/revision=1 Selector: app=nginx Replicas: 2 desired | 2 updated | 2 total | 2 available | 0 unavailable StrategyType: RollingUpdate MinReadySeconds: 0 RollingUpdateStrategy: 1 max unavailable, 1 max surge Pod Template: Labels: app=nginx Containers: nginx: Image: nginx:1.14.2 Port: 80/TCP Environment:  Mounts:  Volumes:  Conditions: Type Status Reason \---- ------ ------ Available True MinimumReplicasAvailable Progressing True NewReplicaSetAvailable OldReplicaSets:  NewReplicaSet: nginx-deployment-1771418926 (2/2 replicas created) No events. List the Pods created by the deployment:kubectl get pods -l app=nginx The output is similar to this:NAME READY STATUS RESTARTS AGE nginx-deployment-1771418926-7o5ns 1/1 Running 0 16h nginx-deployment-1771418926-r18az 1/1 Running 0 16h Display information about a Pod:kubectl describe pod  where  is the name of one of your Pods.Updating the deploymentYou can update the deployment by applying a new YAML file. This YAML file specifies that the deployment should be updated to use nginx 1.16.1.application/deployment-update.yaml apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment spec: selector: matchLabels: app: nginx replicas: 2 template: metadata: labels: app: nginx spec: containers: \- name: nginx image: nginx:1.16.1 # Update the version of nginx from 1.14.2 to 1.16.1 ports: \- containerPort: 80 Apply the new YAML file:kubectl apply -f https://k8s.io/examples/application/deployment-update.yaml Watch the deployment create pods with new names and delete the old pods:kubectl get pods -l app=nginx Scaling the application by increasing the replica countYou can increase the number of Pods in your Deployment by applying a new YAML file. This YAML file sets replicas to 4, which specifies that the Deployment should have four Pods:application/deployment-scale.yaml apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment spec: selector: matchLabels: app: nginx replicas: 4 # Update the replicas from 2 to 4 template: metadata: labels: app: nginx spec: containers: \- name: nginx image: nginx:1.16.1 ports: \- containerPort: 80 Apply the new YAML file:kubectl apply -f https://k8s.io/examples/application/deployment-scale.yaml Verify that the Deployment has four Pods:kubectl get pods -l app=nginx The output is similar to this:NAME READY STATUS RESTARTS AGE nginx-deployment-148880595-4zdqq 1/1 Running 0 25s nginx-deployment-148880595-6zgi1 1/1 Running 0 25s nginx-deployment-148880595-fxcez 1/1 Running 0 2m nginx-deployment-148880595-rwovn 1/1 Running 0 2m Deleting a deploymentDelete the deployment by name:kubectl delete deployment nginx-deployment ReplicationControllers -- the Old WayThe preferred way to create a replicated application is to use a Deployment, which in turn uses a ReplicaSet. Before the Deployment and ReplicaSet were added to Kubernetes, replicated applications were configured using a ReplicationController.What's nextLearn more about Deployment objects.4.8.2 - Run a Single-Instance Stateful ApplicationThis page shows you how to run a single-instance stateful application in Kubernetes using a PersistentVolume and a Deployment. The application is MySQL.ObjectivesCreate a PersistentVolume referencing a disk in your environment.Create a MySQL Deployment.Expose MySQL to other pods in the cluster at a known DNS name.Before you beginYou need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesTo check the version, enter kubectl version.You need to either have a dynamic PersistentVolume provisioner with a default StorageClass, or statically provision PersistentVolumes yourself to satisfy the PersistentVolumeClaims used here.Deploy MySQLYou can run a stateful application by creating a Kubernetes Deployment and connecting it to an existing PersistentVolume using a PersistentVolumeClaim. For example, this YAML file describes a Deployment that runs MySQL and references the PersistentVolumeClaim. The file defines a volume mount for /var/lib/mysql, and then creates a PersistentVolumeClaim that looks for a 20G volume. This claim is satisfied by any existing volume that meets the requirements, or by a dynamic provisioner.Note: The password is defined in the config yaml, and this is insecure. See Kubernetes Secrets for a secure solution.application/mysql/mysql-deployment.yaml apiVersion: v1 kind: Service metadata: name: mysql spec: ports: \- port: 3306 selector: app: mysql clusterIP: None \--- apiVersion: apps/v1 kind: Deployment metadata: name: mysql spec: selector: matchLabels: app: mysql strategy: type: Recreate template: metadata: labels: app: mysql spec: containers: \- image: mysql:5.6 name: mysql env: # Use secret in real usage \- name: MYSQL_ROOT_PASSWORD value: password ports: \- containerPort: 3306 name: mysql volumeMounts: \- name: mysql-persistent-storage mountPath: /var/lib/mysql volumes: \- name: mysql-persistent-storage persistentVolumeClaim: claimName: mysql-pv-claim application/mysql/mysql-pv.yaml apiVersion: v1 kind: PersistentVolume metadata: name: mysql-pv-volume labels: type: local spec: storageClassName: manual capacity: storage: 20Gi accessModes: \- ReadWriteOnce hostPath: path: "/mnt/data" \--- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: mysql-pv-claim spec: storageClassName: manual accessModes: \- ReadWriteOnce resources: requests: storage: 20Gi Deploy the PV and PVC of the YAML file:kubectl apply -f https://k8s.io/examples/application/mysql/mysql-pv.yaml Deploy the contents of the YAML file:kubectl apply -f https://k8s.io/examples/application/mysql/mysql-deployment.yaml Display information about the Deployment:kubectl describe deployment mysql The output is similar to this:Name: mysql Namespace: default CreationTimestamp: Tue, 01 Nov 2016 11:18:45 -0700 Labels: app=mysql Annotations: deployment.kubernetes.io/revision=1 Selector: app=mysql Replicas: 1 desired | 1 updated | 1 total | 0 available | 1 unavailable StrategyType: Recreate MinReadySeconds: 0 Pod Template: Labels: app=mysql Containers: mysql: Image: mysql:5.6 Port: 3306/TCP Environment: MYSQL_ROOT_PASSWORD: password Mounts: /var/lib/mysql from mysql-persistent-storage (rw) Volumes: mysql-persistent-storage: Type: PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace) ClaimName: mysql-pv-claim ReadOnly: false Conditions: Type Status Reason \---- ------ ------ Available False MinimumReplicasUnavailable Progressing True ReplicaSetUpdated OldReplicaSets:  NewReplicaSet: mysql-63082529 (1/1 replicas created) Events: FirstSeen LastSeen Count From SubobjectPath Type Reason Message \--------- -------- ----- ---- ------------- -------- ------ ------- 33s 33s 1 {deployment-controller } Normal ScalingReplicaSet Scaled up replica set mysql-63082529 to 1 List the pods created by the Deployment:kubectl get pods -l app=mysql The output is similar to this:NAME READY STATUS RESTARTS AGE mysql-63082529-2z3ki 1/1 Running 0 3m Inspect the PersistentVolumeClaim:kubectl describe pvc mysql-pv-claim The output is similar to this:Name: mysql-pv-claim Namespace: default StorageClass: Status: Bound Volume: mysql-pv-volume Labels:  Annotations: pv.kubernetes.io/bind-completed=yes pv.kubernetes.io/bound-by-controller=yes Capacity: 20Gi Access Modes: RWO Events:  Accessing the MySQL instanceThe preceding YAML file creates a service that allows other Pods in the cluster to access the database. The Service option clusterIP: None lets the Service DNS name resolve directly to the Pod's IP address. This is optimal when you have only one Pod behind a Service and you don't intend to increase the number of Pods.Run a MySQL client to connect to the server:kubectl run -it --rm --image=mysql:5.6 --restart=Never mysql-client -- mysql -h mysql -ppassword This command creates a new Pod in the cluster running a MySQL client and connects it to the server through the Service. If it connects, you know your stateful MySQL database is up and running.Waiting for pod default/mysql-client-274442439-zyp6i to be running, status is Pending, pod ready: false If you don't see a command prompt, try pressing enter. mysql> UpdatingThe image or any other part of the Deployment can be updated as usual with the kubectl apply command. Here are some precautions that are specific to stateful apps:Don't scale the app. This setup is for single-instance apps only. The underlying PersistentVolume can only be mounted to one Pod. For clustered stateful apps, see the StatefulSet documentation.Use strategy: type: Recreate in the Deployment configuration YAML file. This instructs Kubernetes to not use rolling updates. Rolling updates will not work, as you cannot have more than one Pod running at a time. The Recreate strategy will stop the first pod before creating a new one with the updated configuration.Deleting a deploymentDelete the deployed objects by name:kubectl delete deployment,svc mysql kubectl delete pvc mysql-pv-claim kubectl delete pv mysql-pv-volume If you manually provisioned a PersistentVolume, you also need to manually delete it, as well as release the underlying resource. If you used a dynamic provisioner, it automatically deletes the PersistentVolume when it sees that you deleted the PersistentVolumeClaim. Some dynamic provisioners (such as those for EBS and PD) also release the underlying resource upon deleting the PersistentVolume.What's nextLearn more about Deployment objects.Learn more about Deploying applicationskubectl run documentationVolumes and Persistent Volumes4.8.3 - Run a Replicated Stateful ApplicationThis page shows how to run a replicated stateful application using a StatefulSet. This application is a replicated MySQL database. The example topology has a single primary server and multiple replicas, using asynchronous row-based replication.Note:This is not a production configuration. MySQL settings remain on insecure defaults to keep the focus on general patterns for running stateful applications in Kubernetes.Before you beginYou need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesYou need to either have a dynamic PersistentVolume provisioner with a default StorageClass, or statically provision PersistentVolumes yourself to satisfy the PersistentVolumeClaims used here.This tutorial assumes you are familiar with PersistentVolumes and StatefulSets, as well as other core concepts like Pods, Services, and ConfigMaps.Some familiarity with MySQL helps, but this tutorial aims to present general patterns that should be useful for other systems.You are using the default namespace or another namespace that does not contain any conflicting objects.You need to have a AMD64-compatible CPU.ObjectivesDeploy a replicated MySQL topology with a StatefulSet.Send MySQL client traffic.Observe resistance to downtime.Scale the StatefulSet up and down.Deploy MySQLThe example MySQL deployment consists of a ConfigMap, two Services, and a StatefulSet.Create a ConfigMapCreate the ConfigMap from the following YAML configuration file:application/mysql/mysql-configmap.yaml apiVersion: v1 kind: ConfigMap metadata: name: mysql labels: app: mysql app.kubernetes.io/name: mysql data: primary.cnf: | # Apply this config only on the primary. [mysqld] log-bin replica.cnf: | # Apply this config only on replicas. [mysqld] super-read-only kubectl apply -f https://k8s.io/examples/application/mysql/mysql-configmap.yaml This ConfigMap provides my.cnf overrides that let you independently control configuration on the primary MySQL server and its replicas. In this case, you want the primary server to be able to serve replication logs to replicas and you want replicas to reject any writes that don't come via replication.There's nothing special about the ConfigMap itself that causes different portions to apply to different Pods. Each Pod decides which portion to look at as it's initializing, based on information provided by the StatefulSet controller.Create ServicesCreate the Services from the following YAML configuration file:application/mysql/mysql-services.yaml # Headless service for stable DNS entries of StatefulSet members. apiVersion: v1 kind: Service metadata: name: mysql labels: app: mysql app.kubernetes.io/name: mysql spec: ports: \- name: mysql port: 3306 clusterIP: None selector: app: mysql \--- # Client service for connecting to any MySQL instance for reads. # For writes, you must instead connect to the primary: mysql-0.mysql. apiVersion: v1 kind: Service metadata: name: mysql-read labels: app: mysql app.kubernetes.io/name: mysql readonly: "true" spec: ports: \- name: mysql port: 3306 selector: app: mysql kubectl apply -f https://k8s.io/examples/application/mysql/mysql-services.yaml The headless Service provides a home for the DNS entries that the StatefulSet controllers creates for each Pod that's part of the set. Because the headless Service is named mysql, the Pods are accessible by resolving .mysql from within any other Pod in the same Kubernetes cluster and namespace.The client Service, called mysql-read, is a normal Service with its own cluster IP that distributes connections across all MySQL Pods that report being Ready. The set of potential endpoints includes the primary MySQL server and all replicas.Note that only read queries can use the load-balanced client Service. Because there is only one primary MySQL server, clients should connect directly to the primary MySQL Pod (through its DNS entry within the headless Service) to execute writes.Create the StatefulSetFinally, create the StatefulSet from the following YAML configuration file:application/mysql/mysql-statefulset.yaml apiVersion: apps/v1 kind: StatefulSet metadata: name: mysql spec: selector: matchLabels: app: mysql app.kubernetes.io/name: mysql serviceName: mysql replicas: 3 template: metadata: labels: app: mysql app.kubernetes.io/name: mysql spec: initContainers: \- name: init-mysql image: mysql:5.7 command: \- bash \- "-c" \- | set -ex # Generate mysql server-id from pod ordinal index. [[ $HOSTNAME =~ -([0-9]+)$ ]] || exit 1 ordinal=${BASH_REMATCH[1]} echo [mysqld] > /mnt/conf.d/server-id.cnf # Add an offset to avoid reserved server-id=0 value. echo server-id=$((100 + $ordinal)) >> /mnt/conf.d/server-id.cnf # Copy appropriate conf.d files from config-map to emptyDir. if [[ $ordinal -eq 0 ]]; then cp /mnt/config-map/primary.cnf /mnt/conf.d/ else cp /mnt/config-map/replica.cnf /mnt/conf.d/ fi volumeMounts: \- name: conf mountPath: /mnt/conf.d \- name: config-map mountPath: /mnt/config-map \- name: clone-mysql image: gcr.io/google-samples/xtrabackup:1.0 command: \- bash \- "-c" \- | set -ex # Skip the clone if data already exists. [[ -d /var/lib/mysql/mysql ]] && exit 0 # Skip the clone on primary (ordinal index 0). [[ `hostname` =~ -([0-9]+)$ ]] || exit 1 ordinal=${BASH_REMATCH[1]} [[ $ordinal -eq 0 ]] && exit 0 # Clone data from previous peer. ncat --recv-only mysql-$(($ordinal-1)).mysql 3307 | xbstream -x -C /var/lib/mysql # Prepare the backup. xtrabackup --prepare --target-dir=/var/lib/mysql volumeMounts: \- name: data mountPath: /var/lib/mysql subPath: mysql \- name: conf mountPath: /etc/mysql/conf.d containers: \- name: mysql image: mysql:5.7 env: \- name: MYSQL_ALLOW_EMPTY_PASSWORD value: "1" ports: \- name: mysql containerPort: 3306 volumeMounts: \- name: data mountPath: /var/lib/mysql subPath: mysql \- name: conf mountPath: /etc/mysql/conf.d resources: requests: cpu: 500m memory: 1Gi livenessProbe: exec: command: ["mysqladmin", "ping"] initialDelaySeconds: 30 periodSeconds: 10 timeoutSeconds: 5 readinessProbe: exec: # Check we can execute queries over TCP (skip-networking is off). command: ["mysql", "-h", "127.0.0.1", "-e", "SELECT 1"] initialDelaySeconds: 5 periodSeconds: 2 timeoutSeconds: 1 \- name: xtrabackup image: gcr.io/google-samples/xtrabackup:1.0 ports: \- name: xtrabackup containerPort: 3307 command: \- bash \- "-c" \- | set -ex cd /var/lib/mysql # Determine binlog position of cloned data, if any. if [[ -f xtrabackup_slave_info && "x$( change_master_to.sql.in # Ignore xtrabackup_binlog_info in this case (it's useless). rm -f xtrabackup_slave_info xtrabackup_binlog_info elif [[ -f xtrabackup_binlog_info ]]; then # We're cloning directly from primary. Parse binlog position. [[ `cat xtrabackup_binlog_info` =~ ^(.*?)[[:space:]]+(.*?)$ ]] || exit 1 rm -f xtrabackup_binlog_info xtrabackup_slave_info echo "CHANGE MASTER TO MASTER_LOG_FILE='${BASH_REMATCH[1]}',\ MASTER_LOG_POS=${BASH_REMATCH[2]}" > change_master_to.sql.in fi # Check if we need to complete a clone by starting replication. if [[ -f change_master_to.sql.in ]]; then echo "Waiting for mysqld to be ready (accepting connections)" until mysql -h 127.0.0.1 -e "SELECT 1"; do sleep 1; done echo "Initializing replication from clone position" mysql -h 127.0.0.1 \ -e "$(-, which results in Pods named mysql-0, mysql-1, and mysql-2.The Pod template in the above StatefulSet manifest takes advantage of these properties to perform orderly startup of MySQL replication.Generating configurationBefore starting any of the containers in the Pod spec, the Pod first runs any init containers in the order defined.The first init container, named init-mysql, generates special MySQL config files based on the ordinal index.The script determines its own ordinal index by extracting it from the end of the Pod name, which is returned by the hostname command. Then it saves the ordinal (with a numeric offset to avoid reserved values) into a file called server-id.cnf in the MySQL conf.d directory. This translates the unique, stable identity provided by the StatefulSet into the domain of MySQL server IDs, which require the same properties.The script in the init-mysql container also applies either primary.cnf or replica.cnf from the ConfigMap by copying the contents into conf.d. Because the example topology consists of a single primary MySQL server and any number of replicas, the script assigns ordinal 0 to be the primary server, and everyone else to be replicas. Combined with the StatefulSet controller's deployment order guarantee, this ensures the primary MySQL server is Ready before creating replicas, so they can begin replicating.Cloning existing dataIn general, when a new Pod joins the set as a replica, it must assume the primary MySQL server might already have data on it. It also must assume that the replication logs might not go all the way back to the beginning of time. These conservative assumptions are the key to allow a running StatefulSet to scale up and down over time, rather than being fixed at its initial size.The second init container, named clone-mysql, performs a clone operation on a replica Pod the first time it starts up on an empty PersistentVolume. That means it copies all existing data from another running Pod, so its local state is consistent enough to begin replicating from the primary server.MySQL itself does not provide a mechanism to do this, so the example uses a popular open-source tool called Percona XtraBackup. During the clone, the source MySQL server might suffer reduced performance. To minimize impact on the primary MySQL server, the script instructs each Pod to clone from the Pod whose ordinal index is one lower. This works because the StatefulSet controller always ensures Pod N is Ready before starting Pod N+1.Starting replicationAfter the init containers complete successfully, the regular containers run. The MySQL Pods consist of a mysql container that runs the actual mysqld server, and an xtrabackup container that acts as a sidecar.The xtrabackup sidecar looks at the cloned data files and determines if it's necessary to initialize MySQL replication on the replica. If so, it waits for mysqld to be ready and then executes the CHANGE MASTER TO and START SLAVE commands with replication parameters extracted from the XtraBackup clone files.Once a replica begins replication, it remembers its primary MySQL server and reconnects automatically if the server restarts or the connection dies. Also, because replicas look for the primary server at its stable DNS name (mysql-0.mysql), they automatically find the primary server even if it gets a new Pod IP due to being rescheduled.Lastly, after starting replication, the xtrabackup container listens for connections from other Pods requesting a data clone. This server remains up indefinitely in case the StatefulSet scales up, or in case the next Pod loses its PersistentVolumeClaim and needs to redo the clone.Sending client trafficYou can send test queries to the primary MySQL server (hostname mysql-0.mysql) by running a temporary container with the mysql:5.7 image and running the mysql client binary.kubectl run mysql-client --image=mysql:5.7 -i --rm --restart=Never --\ mysql -h mysql-0.mysql < with the name of the Node you found in the last step.Caution:Draining a Node can impact other workloads and applications running on the same node. Only perform the following step in a test cluster.# See above advice about impact on other workloads kubectl drain  \--force --delete-emptydir-data --ignore-daemonsets Now you can watch as the Pod reschedules on a different Node:kubectl get pod mysql-2 -o wide --watch It should look something like this:NAME READY STATUS RESTARTS AGE IP NODE mysql-2 2/2 Terminating 0 15m 10.244.1.56 kubernetes-node-9l2t [...] mysql-2 0/2 Pending 0 0s  kubernetes-node-fjlm mysql-2 0/2 Init:0/2 0 0s  kubernetes-node-fjlm mysql-2 0/2 Init:1/2 0 20s 10.244.5.32 kubernetes-node-fjlm mysql-2 0/2 PodInitializing 0 21s 10.244.5.32 kubernetes-node-fjlm mysql-2 1/2 Running 0 22s 10.244.5.32 kubernetes-node-fjlm mysql-2 2/2 Running 0 30s 10.244.5.32 kubernetes-node-fjlm And again, you should see server ID 102 disappear from the SELECT @@server_id loop output for a while and then return.Now uncordon the Node to return it to a normal state:kubectl uncordon  Scaling the number of replicasWhen you use MySQL replication, you can scale your read query capacity by adding replicas. For a StatefulSet, you can achieve this with a single command:kubectl scale statefulset mysql --replicas=5 Watch the new Pods come up by running:kubectl get pods -l app=mysql --watch Once they're up, you should see server IDs 103 and 104 start appearing in the SELECT @@server_id loop output.You can also verify that these new servers have the data you added before they existed:kubectl run mysql-client --image=mysql:5.7 -i -t --rm --restart=Never --\ mysql -h mysql-3.mysql -e "SELECT * FROM test.messages" Waiting for pod default/mysql-client to be running, status is Pending, pod ready: false +---------+ | message | +---------+ | hello | +---------+ pod "mysql-client" deleted Scaling back down is also seamless:kubectl scale statefulset mysql --replicas=3 Note:Although scaling up creates new PersistentVolumeClaims automatically, scaling down does not automatically delete these PVCs.This gives you the choice to keep those initialized PVCs around to make scaling back up quicker, or to extract data before deleting them.You can see this by running:kubectl get pvc -l app=mysql Which shows that all 5 PVCs still exist, despite having scaled the StatefulSet down to 3:NAME STATUS VOLUME CAPACITY ACCESSMODES AGE data-mysql-0 Bound pvc-8acbf5dc-b103-11e6-93fa-42010a800002 10Gi RWO 20m data-mysql-1 Bound pvc-8ad39820-b103-11e6-93fa-42010a800002 10Gi RWO 20m data-mysql-2 Bound pvc-8ad69a6d-b103-11e6-93fa-42010a800002 10Gi RWO 20m data-mysql-3 Bound pvc-50043c45-b1c5-11e6-93fa-42010a800002 10Gi RWO 2m data-mysql-4 Bound pvc-500a9957-b1c5-11e6-93fa-42010a800002 10Gi RWO 2m If you don't intend to reuse the extra PVCs, you can delete them:kubectl delete pvc data-mysql-3 kubectl delete pvc data-mysql-4 Cleaning upCancel the SELECT @@server_id loop by pressing Ctrl+C in its terminal, or running the following from another terminal:kubectl delete pod mysql-client-loop --now Delete the StatefulSet. This also begins terminating the Pods.kubectl delete statefulset mysql Verify that the Pods disappear. They might take some time to finish terminating.kubectl get pods -l app=mysql You'll know the Pods have terminated when the above returns:No resources found. Delete the ConfigMap, Services, and PersistentVolumeClaims.kubectl delete configmap,service,pvc -l app=mysql If you manually provisioned PersistentVolumes, you also need to manually delete them, as well as release the underlying resources. If you used a dynamic provisioner, it automatically deletes the PersistentVolumes when it sees that you deleted the PersistentVolumeClaims. Some dynamic provisioners (such as those for EBS and PD) also release the underlying resources upon deleting the PersistentVolumes.What's nextLearn more about scaling a StatefulSet.Learn more about debugging a StatefulSet.Learn more about deleting a StatefulSet.Learn more about force deleting StatefulSet Pods.Look in the Helm Charts repository for other stateful application examples.4.8.4 - Scale a StatefulSetThis task shows how to scale a StatefulSet. Scaling a StatefulSet refers to increasing or decreasing the number of replicas.Before you beginStatefulSets are only available in Kubernetes version 1.5 or later. To check your version of Kubernetes, run kubectl version.Not all stateful applications scale nicely. If you are unsure about whether to scale your StatefulSets, see StatefulSet concepts or StatefulSet tutorial for further information.You should perform scaling only when you are confident that your stateful application cluster is completely healthy.Scaling StatefulSetsUse kubectl to scale StatefulSetsFirst, find the StatefulSet you want to scale.kubectl get statefulsets  Change the number of replicas of your StatefulSet:kubectl scale statefulsets  \--replicas= Make in-place updates on your StatefulSetsAlternatively, you can do in-place updates on your StatefulSets.If your StatefulSet was initially created with kubectl apply, update .spec.replicas of the StatefulSet manifests, and then do a kubectl apply:kubectl apply -f  Otherwise, edit that field with kubectl edit:kubectl edit statefulsets  Or use kubectl patch:kubectl patch statefulsets  -p '{"spec":{"replicas":}}' TroubleshootingScaling down does not work rightYou cannot scale down a StatefulSet when any of the stateful Pods it manages is unhealthy. Scaling down only takes place after those stateful Pods become running and ready.If spec.replicas > 1, Kubernetes cannot determine the reason for an unhealthy Pod. It might be the result of a permanent fault or of a transient fault. A transient fault can be caused by a restart required by upgrading or maintenance.If the Pod is unhealthy due to a permanent fault, scaling without correcting the fault may lead to a state where the StatefulSet membership drops below a certain minimum number of replicas that are needed to function correctly. This may cause your StatefulSet to become unavailable.If the Pod is unhealthy due to a transient fault and the Pod might become available again, the transient error may interfere with your scale-up or scale-down operation. Some distributed databases have issues when nodes join and leave at the same time. It is better to reason about scaling operations at the application level in these cases, and perform scaling only when you are sure that your stateful application cluster is completely healthy.What's nextLearn more about deleting a StatefulSet.4.8.5 - Delete a StatefulSetThis task shows you how to delete a StatefulSet.Before you beginThis task assumes you have an application running on your cluster represented by a StatefulSet.Deleting a StatefulSetYou can delete a StatefulSet in the same way you delete other resources in Kubernetes: use the kubectl delete command, and specify the StatefulSet either by file or by name.kubectl delete -f  kubectl delete statefulsets  You may need to delete the associated headless service separately after the StatefulSet itself is deleted.kubectl delete service  When deleting a StatefulSet through kubectl, the StatefulSet scales down to 0. All Pods that are part of this workload are also deleted. If you want to delete only the StatefulSet and not the Pods, use --cascade=orphan. For example:kubectl delete -f  \--cascade=orphan By passing --cascade=orphan to kubectl delete, the Pods managed by the StatefulSet are left behind even after the StatefulSet object itself is deleted. If the pods have a label app.kubernetes.io/name=MyApp, you can then delete them as follows:kubectl delete pods -l app.kubernetes.io/name=MyApp Persistent VolumesDeleting the Pods in a StatefulSet will not delete the associated volumes. This is to ensure that you have the chance to copy data off the volume before deleting it. Deleting the PVC after the pods have terminated might trigger deletion of the backing Persistent Volumes depending on the storage class and reclaim policy. You should never assume ability to access a volume after claim deletion.Note:Use caution when deleting a PVC, as it may lead to data loss.Complete deletion of a StatefulSetTo delete everything in a StatefulSet, including the associated pods, you can run a series of commands similar to the following:grace=$(kubectl get pods  \--template '{{.spec.terminationGracePeriodSeconds}}') kubectl delete statefulset -l app.kubernetes.io/name=MyApp sleep $grace kubectl delete pvc -l app.kubernetes.io/name=MyApp In the example above, the Pods have the label app.kubernetes.io/name=MyApp; substitute your own label as appropriate.Force deletion of StatefulSet podsIf you find that some pods in your StatefulSet are stuck in the 'Terminating' or 'Unknown' states for an extended period of time, you may need to manually intervene to forcefully delete the pods from the apiserver. This is a potentially dangerous task. Refer to Force Delete StatefulSet Pods for details.What's nextLearn more about force deleting StatefulSet Pods.4.8.6 - Force Delete StatefulSet PodsThis page shows how to delete Pods which are part of a stateful set, and explains the considerations to keep in mind when doing so.Before you beginThis is a fairly advanced task and has the potential to violate some of the properties inherent to StatefulSet.Before proceeding, make yourself familiar with the considerations enumerated below.StatefulSet considerationsIn normal operation of a StatefulSet, there is never a need to force delete a StatefulSet Pod. The StatefulSet controller is responsible for creating, scaling and deleting members of the StatefulSet. It tries to ensure that the specified number of Pods from ordinal 0 through N-1 are alive and ready. StatefulSet ensures that, at any time, there is at most one Pod with a given identity running in a cluster. This is referred to as at most one semantics provided by a StatefulSet.Manual force deletion should be undertaken with caution, as it has the potential to violate the at most one semantics inherent to StatefulSet. StatefulSets may be used to run distributed and clustered applications which have a need for a stable network identity and stable storage. These applications often have configuration which relies on an ensemble of a fixed number of members with fixed identities. Having multiple members with the same identity can be disastrous and may lead to data loss (e.g. split brain scenario in quorum-based systems).Delete PodsYou can perform a graceful pod deletion with the following command:kubectl delete pods  For the above to lead to graceful termination, the Pod must not specify a pod.Spec.TerminationGracePeriodSeconds of 0. The practice of setting a pod.Spec.TerminationGracePeriodSeconds of 0 seconds is unsafe and strongly discouraged for StatefulSet Pods. Graceful deletion is safe and will ensure that the Pod shuts down gracefully before the kubelet deletes the name from the apiserver.A Pod is not deleted automatically when a node is unreachable. The Pods running on an unreachable Node enter the 'Terminating' or 'Unknown' state after a timeout. Pods may also enter these states when the user attempts graceful deletion of a Pod on an unreachable Node. The only ways in which a Pod in such a state can be removed from the apiserver are as follows:The Node object is deleted (either by you, or by the Node Controller).The kubelet on the unresponsive Node starts responding, kills the Pod and removes the entry from the apiserver.Force deletion of the Pod by the user.The recommended best practice is to use the first or second approach. If a Node is confirmed to be dead (e.g. permanently disconnected from the network, powered down, etc), then delete the Node object. If the Node is suffering from a network partition, then try to resolve this or wait for it to resolve. When the partition heals, the kubelet will complete the deletion of the Pod and free up its name in the apiserver.Normally, the system completes the deletion once the Pod is no longer running on a Node, or the Node is deleted by an administrator. You may override this by force deleting the Pod.Force DeletionForce deletions do not wait for confirmation from the kubelet that the Pod has been terminated. Irrespective of whether a force deletion is successful in killing a Pod, it will immediately free up the name from the apiserver. This would let the StatefulSet controller create a replacement Pod with that same identity; this can lead to the duplication of a still-running Pod, and if said Pod can still communicate with the other members of the StatefulSet, will violate the at most one semantics that StatefulSet is designed to guarantee.When you force delete a StatefulSet pod, you are asserting that the Pod in question will never again make contact with other Pods in the StatefulSet and its name can be safely freed up for a replacement to be created.If you want to delete a Pod forcibly using kubectl version >= 1.5, do the following:kubectl delete pods  \--grace-period=0 --force If you're using any version of kubectl <= 1.4, you should omit the --force option and use:kubectl delete pods  \--grace-period=0 If even after these commands the pod is stuck on Unknown state, use the following command to remove the pod from the cluster:kubectl patch pod  -p '{"metadata":{"finalizers":null}}' Always perform force deletion of StatefulSet Pods carefully and with complete knowledge of the risks involved.What's nextLearn more about debugging a StatefulSet.4.8.7 - Horizontal Pod AutoscalingIn Kubernetes, a HorizontalPodAutoscaler automatically updates a workload resource (such as a Deployment or StatefulSet), with the aim of automatically scaling the workload to match demand.Horizontal scaling means that the response to increased load is to deploy more Pods. This is different from vertical scaling, which for Kubernetes would mean assigning more resources (for example: memory or CPU) to the Pods that are already running for the workload.If the load decreases, and the number of Pods is above the configured minimum, the HorizontalPodAutoscaler instructs the workload resource (the Deployment, StatefulSet, or other similar resource) to scale back down.Horizontal pod autoscaling does not apply to objects that can't be scaled (for example: a DaemonSet.)The HorizontalPodAutoscaler is implemented as a Kubernetes API resource and a controller. The resource determines the behavior of the controller. The horizontal pod autoscaling controller, running within the Kubernetes control plane, periodically adjusts the desired scale of its target (for example, a Deployment) to match observed metrics such as average CPU utilization, average memory utilization, or any other custom metric you specify.There is walkthrough example of using horizontal pod autoscaling.How does a HorizontalPodAutoscaler work?graph BT hpa[Horizontal Pod Autoscaler] --> scale[Scale] subgraph rc[RC / Deployment] scale end scale -.-> pod1[Pod 1] scale -.-> pod2[Pod 2] scale -.-> pod3[Pod N] classDef hpa fill:#D5A6BD,stroke:#1E1E1D,stroke-width:1px,color:#1E1E1D; classDef rc fill:#F9CB9C,stroke:#1E1E1D,stroke-width:1px,color:#1E1E1D; classDef scale fill:#B6D7A8,stroke:#1E1E1D,stroke-width:1px,color:#1E1E1D; classDef pod fill:#9FC5E8,stroke:#1E1E1D,stroke-width:1px,color:#1E1E1D; class hpa hpa; class rc rc; class scale scale; class pod1,pod2,pod3 podJavaScript must be enabled to view this contentFigure 1. HorizontalPodAutoscaler controls the scale of a Deployment and its ReplicaSetKubernetes implements horizontal pod autoscaling as a control loop that runs intermittently (it is not a continuous process). The interval is set by the \--horizontal-pod-autoscaler-sync-period parameter to the kube-controller-manager (and the default interval is 15 seconds).Once during each period, the controller manager queries the resource utilization against the metrics specified in each HorizontalPodAutoscaler definition. The controller manager finds the target resource defined by the scaleTargetRef, then selects the pods based on the target resource's .spec.selector labels, and obtains the metrics from either the resource metrics API (for per-pod resource metrics), or the custom metrics API (for all other metrics).For per-pod resource metrics (like CPU), the controller fetches the metrics from the resource metrics API for each Pod targeted by the HorizontalPodAutoscaler. Then, if a target utilization value is set, the controller calculates the utilization value as a percentage of the equivalent resource request on the containers in each Pod. If a target raw value is set, the raw metric values are used directly. The controller then takes the mean of the utilization or the raw value (depending on the type of target specified) across all targeted Pods, and produces a ratio used to scale the number of desired replicas.Please note that if some of the Pod's containers do not have the relevant resource request set, CPU utilization for the Pod will not be defined and the autoscaler will not take any action for that metric. See the algorithm details section below for more information about how the autoscaling algorithm works.For per-pod custom metrics, the controller functions similarly to per-pod resource metrics, except that it works with raw values, not utilization values.For object metrics and external metrics, a single metric is fetched, which describes the object in question. This metric is compared to the target value, to produce a ratio as above. In the autoscaling/v2 API version, this value can optionally be divided by the number of Pods before the comparison is made.The common use for HorizontalPodAutoscaler is to configure it to fetch metrics from aggregated APIs (metrics.k8s.io, custom.metrics.k8s.io, or external.metrics.k8s.io). The metrics.k8s.io API is usually provided by an add-on named Metrics Server, which needs to be launched separately. For more information about resource metrics, see Metrics Server.Support for metrics APIs explains the stability guarantees and support status for these different APIs.The HorizontalPodAutoscaler controller accesses corresponding workload resources that support scaling (such as Deployments and StatefulSet). These resources each have a subresource named scale, an interface that allows you to dynamically set the number of replicas and examine each of their current states. For general information about subresources in the Kubernetes API, see Kubernetes API Concepts.Algorithm detailsFrom the most basic perspective, the HorizontalPodAutoscaler controller operates on the ratio between desired metric value and current metric value:$$\begin{equation*} desiredReplicas = ceil\left\lceil currentReplicas \times \frac{currentMetricValue}{desiredMetricValue} \right\rceil \end{equation*}$$For example, if the current metric value is 200m, and the desired value is 100m, the number of replicas will be doubled, since \\( { 200.0 \div 100.0 } = 2.0 \\).If the current value is instead 50m, you'll halve the number of replicas, since \\( { 50.0 \div 100.0 } = 0.5 \\). The control plane skips any scaling action if the ratio is sufficiently close to 1.0 (within a configurable tolerance, 0.1 by default).When a targetAverageValue or targetAverageUtilization is specified, the currentMetricValue is computed by taking the average of the given metric across all Pods in the HorizontalPodAutoscaler's scale target.Before checking the tolerance and deciding on the final values, the control plane also considers whether any metrics are missing, and how many Pods are Ready. All Pods with a deletion timestamp set (objects with a deletion timestamp are in the process of being shut down / removed) are ignored, and all failed Pods are discarded.If a particular Pod is missing metrics, it is set aside for later; Pods with missing metrics will be used to adjust the final scaling amount.When scaling on CPU, if any pod has yet to become ready (it's still initializing, or possibly is unhealthy) or the most recent metric point for the pod was before it became ready, that pod is set aside as well.Due to technical constraints, the HorizontalPodAutoscaler controller cannot exactly determine the first time a pod becomes ready when determining whether to set aside certain CPU metrics. Instead, it considers a Pod "not yet ready" if it's unready and transitioned to ready within a short, configurable window of time since it started. This value is configured with the --horizontal-pod-autoscaler-initial-readiness-delay flag, and its default is 30 seconds. Once a pod has become ready, it considers any transition to ready to be the first if it occurred within a longer, configurable time since it started. This value is configured with the \--horizontal-pod-autoscaler-cpu-initialization-period flag, and its default is 5 minutes.The \\( currentMetricValue \over desiredMetricValue \\) base scale ratio is then calculated, using the remaining pods not set aside or discarded from above.If there were any missing metrics, the control plane recomputes the average more conservatively, assuming those pods were consuming 100% of the desired value in case of a scale down, and 0% in case of a scale up. This dampens the magnitude of any potential scale.Furthermore, if any not-yet-ready pods were present, and the workload would have scaled up without factoring in missing metrics or not-yet-ready pods, the controller conservatively assumes that the not-yet-ready pods are consuming 0% of the desired metric, further dampening the magnitude of a scale up.After factoring in the not-yet-ready pods and missing metrics, the controller recalculates the usage ratio. If the new ratio reverses the scale direction, or is within the tolerance, the controller doesn't take any scaling action. In other cases, the new ratio is used to decide any change to the number of Pods.Note that the original value for the average utilization is reported back via the HorizontalPodAutoscaler status, without factoring in the not-yet-ready pods or missing metrics, even when the new usage ratio is used.If multiple metrics are specified in a HorizontalPodAutoscaler, this calculation is done for each metric, and then the largest of the desired replica counts is chosen. If any of these metrics cannot be converted into a desired replica count (e.g. due to an error fetching the metrics from the metrics APIs) and a scale down is suggested by the metrics which can be fetched, scaling is skipped. This means that the HPA is still capable of scaling up if one or more metrics give a desiredReplicas greater than the current value.Finally, right before HPA scales the target, the scale recommendation is recorded. The controller considers all recommendations within a configurable window choosing the highest recommendation from within that window. This value can be configured using the \--horizontal-pod-autoscaler-downscale-stabilization flag, which defaults to 5 minutes. This means that scaledowns will occur gradually, smoothing out the impact of rapidly fluctuating metric values.API ObjectThe Horizontal Pod Autoscaler is an API resource in the Kubernetes autoscaling API group. The current stable version can be found in the autoscaling/v2 API version which includes support for scaling on memory and custom metrics. The new fields introduced in autoscaling/v2 are preserved as annotations when working with autoscaling/v1.When you create a HorizontalPodAutoscaler API object, make sure the name specified is a valid DNS subdomain name. More details about the API object can be found at HorizontalPodAutoscaler Object.Stability of workload scaleWhen managing the scale of a group of replicas using the HorizontalPodAutoscaler, it is possible that the number of replicas keeps fluctuating frequently due to the dynamic nature of the metrics evaluated. This is sometimes referred to as thrashing, or flapping. It's similar to the concept of hysteresis in cybernetics.Autoscaling during rolling updateKubernetes lets you perform a rolling update on a Deployment. In that case, the Deployment manages the underlying ReplicaSets for you. When you configure autoscaling for a Deployment, you bind a HorizontalPodAutoscaler to a single Deployment. The HorizontalPodAutoscaler manages the replicas field of the Deployment. The deployment controller is responsible for setting the replicas of the underlying ReplicaSets so that they add up to a suitable number during the rollout and also afterwards.If you perform a rolling update of a StatefulSet that has an autoscaled number of replicas, the StatefulSet directly manages its set of Pods (there is no intermediate resource similar to ReplicaSet).Support for resource metricsAny HPA target can be scaled based on the resource usage of the pods in the scaling target. When defining the pod specification the resource requests like cpu and memory should be specified. This is used to determine the resource utilization and used by the HPA controller to scale the target up or down. To use resource utilization based scaling specify a metric source like this:type: Resource resource: name: cpu target: type: Utilization averageUtilization: 60 With this metric the HPA controller will keep the average utilization of the pods in the scaling target at 60%. Utilization is the ratio between the current usage of resource to the requested resources of the pod. See Algorithm for more details about how the utilization is calculated and averaged.Note:Since the resource usages of all the containers are summed up the total pod utilization may not accurately represent the individual container resource usage. This could lead to situations where a single container might be running with high usage and the HPA will not scale out because the overall pod usage is still within acceptable limits.Container resource metricsFEATURE STATE: Kubernetes v1.30 [stable] (enabled by default: true)The HorizontalPodAutoscaler API also supports a container metric source where the HPA can track the resource usage of individual containers across a set of Pods, in order to scale the target resource. This lets you configure scaling thresholds for the containers that matter most in a particular Pod. For example, if you have a web application and a sidecar container that provides logging, you can scale based on the resource use of the web application, ignoring the sidecar container and its resource use.If you revise the target resource to have a new Pod specification with a different set of containers, you should revise the HPA spec if that newly added container should also be used for scaling. If the specified container in the metric source is not present or only present in a subset of the pods then those pods are ignored and the recommendation is recalculated. See Algorithm for more details about the calculation. To use container resources for autoscaling define a metric source as follows:type: ContainerResource containerResource: name: cpu container: application target: type: Utilization averageUtilization: 60 In the above example the HPA controller scales the target such that the average utilization of the cpu in the application container of all the pods is 60%.Note:If you change the name of a container that a HorizontalPodAutoscaler is tracking, you can make that change in a specific order to ensure scaling remains available and effective whilst the change is being applied. Before you update the resource that defines the container (such as a Deployment), you should update the associated HPA to track both the new and old container names. This way, the HPA is able to calculate a scaling recommendation throughout the update process.Once you have rolled out the container name change to the workload resource, tidy up by removing the old container name from the HPA specification.Scaling on custom metricsFEATURE STATE: Kubernetes v1.23 [stable](the autoscaling/v2beta2 API version previously provided this ability as a beta feature)Provided that you use the autoscaling/v2 API version, you can configure a HorizontalPodAutoscaler to scale based on a custom metric (that is not built in to Kubernetes or any Kubernetes component). The HorizontalPodAutoscaler controller then queries for these custom metrics from the Kubernetes API.See Support for metrics APIs for the requirements.Scaling on multiple metricsFEATURE STATE: Kubernetes v1.23 [stable](the autoscaling/v2beta2 API version previously provided this ability as a beta feature)Provided that you use the autoscaling/v2 API version, you can specify multiple metrics for a HorizontalPodAutoscaler to scale on. Then, the HorizontalPodAutoscaler controller evaluates each metric, and proposes a new scale based on that metric. The HorizontalPodAutoscaler takes the maximum scale recommended for each metric and sets the workload to that size (provided that this isn't larger than the overall maximum that you configured).Support for metrics APIsBy default, the HorizontalPodAutoscaler controller retrieves metrics from a series of APIs. In order for it to access these APIs, cluster administrators must ensure that:The API aggregation layer is enabled.The corresponding APIs are registered:For resource metrics, this is the metrics.k8s.io API, generally provided by metrics-server. It can be launched as a cluster add-on.For custom metrics, this is the custom.metrics.k8s.io API. It's provided by "adapter" API servers provided by metrics solution vendors. Check with your metrics pipeline to see if there is a Kubernetes metrics adapter available.For external metrics, this is the external.metrics.k8s.io API. It may be provided by the custom metrics adapters provided above.For more information on these different metrics paths and how they differ please see the relevant design proposals for the HPA V2, custom.metrics.k8s.io and external.metrics.k8s.io.For examples of how to use them see the walkthrough for using custom metrics and the walkthrough for using external metrics.Configurable scaling behaviorFEATURE STATE: Kubernetes v1.23 [stable](the autoscaling/v2beta2 API version previously provided this ability as a beta feature)If you use the v2 HorizontalPodAutoscaler API, you can use the behavior field (see the API reference) to configure separate scale-up and scale-down behaviors. You specify these behaviours by setting scaleUp and / or scaleDown under the behavior field.Scaling policies let you control the rate of change of replicas while scaling. Also two settings can be used to prevent flapping: you can specify a stabilization window for smoothing replica counts, and a tolerance to ignore minor metric fluctuations below a specified threshold.Scaling policiesOne or more scaling policies can be specified in the behavior section of the spec. When multiple policies are specified the policy which allows the highest amount of change is the policy which is selected by default. The following example shows this behavior while scaling down:behavior: scaleDown: policies: \- type: Pods value: 4 periodSeconds: 60 \- type: Percent value: 10 periodSeconds: 60 periodSeconds indicates the length of time in the past for which the policy must hold true. The maximum value that you can set for periodSeconds is 1800 (half an hour). The first policy (Pods) allows at most 4 replicas to be scaled down in one minute. The second policy (Percent) allows at most 10% of the current replicas to be scaled down in one minute.Since by default the policy which allows the highest amount of change is selected, the second policy will only be used when the number of pod replicas is more than 40. With 40 or less replicas, the first policy will be applied. For instance if there are 80 replicas and the target has to be scaled down to 10 replicas then during the first step 8 replicas will be reduced. In the next iteration when the number of replicas is 72, 10% of the pods is 7.2 but the number is rounded up to 8. On each loop of the autoscaler controller the number of pods to be change is re-calculated based on the number of current replicas. When the number of replicas falls below 40 the first policy (Pods) is applied and 4 replicas will be reduced at a time.The policy selection can be changed by specifying the selectPolicy field for a scaling direction. By setting the value to Min which would select the policy which allows the smallest change in the replica count. Setting the value to Disabled completely disables scaling in that direction.Stabilization windowThe stabilization window is used to restrict the flapping of replica count when the metrics used for scaling keep fluctuating. The autoscaling algorithm uses this window to infer a previous desired state and avoid unwanted changes to workload scale.For example, in the following example snippet, a stabilization window is specified for scaleDown.behavior: scaleDown: stabilizationWindowSeconds: 300 When the metrics indicate that the target should be scaled down the algorithm looks into previously computed desired states, and uses the highest value from the specified interval. In the above example, all desired states from the past 5 minutes will be considered.This approximates a rolling maximum, and avoids having the scaling algorithm frequently remove Pods only to trigger recreating an equivalent Pod just moments later.ToleranceFEATURE STATE: Kubernetes v1.33 [alpha] (enabled by default: false)The tolerance field configures a threshold for metric variations, preventing the autoscaler from scaling for changes below that value.This tolerance is defined as the amount of variation around the desired metric value under which no scaling will occur. For example, consider a HorizontalPodAutoscaler configured with a target memory consumption of 100MiB and a scale-up tolerance of 5%:behavior: scaleUp: tolerance: 0.05 # 5% tolerance for scale up With this configuration, the HPA algorithm will only consider scaling up if the memory consumption is higher than 105MiB (that is: 5% above the target).If you don't set this field, the HPA applies the default cluster-wide tolerance of 10%. This default can be updated for both scale-up and scale-down using the kube-controller-manager \--horizontal-pod-autoscaler-tolerance command line argument. (You can't use the Kubernetes API to configure this default value.)Default BehaviorTo use the custom scaling not all fields have to be specified. Only values which need to be customized can be specified. These custom values are merged with default values. The default values match the existing behavior in the HPA algorithm.behavior: scaleDown: stabilizationWindowSeconds: 300 policies: \- type: Percent value: 100 periodSeconds: 15 scaleUp: stabilizationWindowSeconds: 0 policies: \- type: Percent value: 100 periodSeconds: 15 \- type: Pods value: 4 periodSeconds: 15 selectPolicy: Max For scaling down the stabilization window is 300 seconds (or the value of the \--horizontal-pod-autoscaler-downscale-stabilization flag if provided). There is only a single policy for scaling down which allows a 100% of the currently running replicas to be removed which means the scaling target can be scaled down to the minimum allowed replicas. For scaling up there is no stabilization window. When the metrics indicate that the target should be scaled up the target is scaled up immediately. There are 2 policies where 4 pods or a 100% of the currently running replicas may at most be added every 15 seconds till the HPA reaches its steady state.Example: change downscale stabilization windowTo provide a custom downscale stabilization window of 1 minute, the following behavior would be added to the HPA:behavior: scaleDown: stabilizationWindowSeconds: 60 Example: limit scale down rateTo limit the rate at which pods are removed by the HPA to 10% per minute, the following behavior would be added to the HPA:behavior: scaleDown: policies: \- type: Percent value: 10 periodSeconds: 60 To ensure that no more than 5 Pods are removed per minute, you can add a second scale-down policy with a fixed size of 5, and set selectPolicy to minimum. Setting selectPolicy to Min means that the autoscaler chooses the policy that affects the smallest number of Pods:behavior: scaleDown: policies: \- type: Percent value: 10 periodSeconds: 60 \- type: Pods value: 5 periodSeconds: 60 selectPolicy: Min Example: disable scale downThe selectPolicy value of Disabled turns off scaling the given direction. So to prevent downscaling the following policy would be used:behavior: scaleDown: selectPolicy: Disabled Support for HorizontalPodAutoscaler in kubectlHorizontalPodAutoscaler, like every API resource, is supported in a standard way by kubectl. You can create a new autoscaler using kubectl create command. You can list autoscalers by kubectl get hpa or get detailed description by kubectl describe hpa. Finally, you can delete an autoscaler using kubectl delete hpa.In addition, there is a special kubectl autoscale command for creating a HorizontalPodAutoscaler object. For instance, executing kubectl autoscale rs foo --min=2 --max=5 --cpu-percent=80 will create an autoscaler for ReplicaSet foo, with target CPU utilization set to 80% and the number of replicas between 2 and 5.Implicit maintenance-mode deactivationYou can implicitly deactivate the HPA for a target without the need to change the HPA configuration itself. If the target's desired replica count is set to 0, and the HPA's minimum replica count is greater than 0, the HPA stops adjusting the target (and sets the ScalingActive Condition on itself to false) until you reactivate it by manually adjusting the target's desired replica count or HPA's minimum replica count.Migrating Deployments and StatefulSets to horizontal autoscalingWhen an HPA is enabled, it is recommended that the value of spec.replicas of the Deployment and / or StatefulSet be removed from their manifest(s). If this isn't done, any time a change to that object is applied, for example via kubectl apply -f deployment.yaml, this will instruct Kubernetes to scale the current number of Pods to the value of the spec.replicas key. This may not be desired and could be troublesome when an HPA is active, resulting in thrashing or flapping behavior.Keep in mind that the removal of spec.replicas may incur a one-time degradation of Pod counts as the default value of this key is 1 (reference Deployment Replicas). Upon the update, all Pods except 1 will begin their termination procedures. Any deployment application afterwards will behave as normal and respect a rolling update configuration as desired. You can avoid this degradation by choosing one of the following two methods based on how you are modifying your deployments:Client Side Apply (this is the default)Server Side Applykubectl apply edit-last-applied deployment/In the editor, remove spec.replicas. When you save and exit the editor, kubectl applies the update. No changes to Pod counts happen at this step.You can now remove spec.replicas from the manifest. If you use source code management, also commit your changes or take whatever other steps for revising the source code are appropriate for how you track updates.From here on out you can run kubectl apply -f deployment.yamlWhen using the Server-Side Apply you can follow the transferring ownership guidelines, which cover this exact use case.What's nextIf you configure autoscaling in your cluster, you may also want to consider using node autoscaling to ensure you are running the right number of nodes.For more information on HorizontalPodAutoscaler:Read a walkthrough example for horizontal pod autoscaling.Read documentation for kubectl autoscale.If you would like to write your own custom metrics adapter, check out the boilerplate to get started.Read the API reference for HorizontalPodAutoscaler.4.8.8 - HorizontalPodAutoscaler WalkthroughA HorizontalPodAutoscaler (HPA for short) automatically updates a workload resource (such as a Deployment or StatefulSet), with the aim of automatically scaling the workload to match demand.Horizontal scaling means that the response to increased load is to deploy more Pods. This is different from vertical scaling, which for Kubernetes would mean assigning more resources (for example: memory or CPU) to the Pods that are already running for the workload.If the load decreases, and the number of Pods is above the configured minimum, the HorizontalPodAutoscaler instructs the workload resource (the Deployment, StatefulSet, or other similar resource) to scale back down.This document walks you through an example of enabling HorizontalPodAutoscaler to automatically manage scale for an example web app. This example workload is Apache httpd running some PHP code.Before you beginYou need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesYour Kubernetes server must be at or later than version 1.23.To check the version, enter kubectl version.If you're running an older release of Kubernetes, refer to the version of the documentation for that release (see available documentation versions).To follow this walkthrough, you also need to use a cluster that has a Metrics Server deployed and configured. The Kubernetes Metrics Server collects resource metrics from the kubelets in your cluster, and exposes those metrics through the Kubernetes API, using an APIService to add new kinds of resource that represent metric readings.To learn how to deploy the Metrics Server, see the metrics-server documentation.If you are running Minikube, run the following command to enable metrics-server:minikube addons enable metrics-server Run and expose php-apache serverTo demonstrate a HorizontalPodAutoscaler, you will first start a Deployment that runs a container using the hpa-example image, and expose it as a Service using the following manifest:application/php-apache.yaml apiVersion: apps/v1 kind: Deployment metadata: name: php-apache spec: selector: matchLabels: run: php-apache template: metadata: labels: run: php-apache spec: containers: \- name: php-apache image: registry.k8s.io/hpa-example ports: \- containerPort: 80 resources: limits: cpu: 500m requests: cpu: 200m \--- apiVersion: v1 kind: Service metadata: name: php-apache labels: run: php-apache spec: ports: \- port: 80 selector: run: php-apache To do so, run the following command:kubectl apply -f https://k8s.io/examples/application/php-apache.yaml deployment.apps/php-apache created service/php-apache created Create the HorizontalPodAutoscalerNow that the server is running, create the autoscaler using kubectl. The kubectl autoscale subcommand, part of kubectl, helps you do this.You will shortly run a command that creates a HorizontalPodAutoscaler that maintains between 1 and 10 replicas of the Pods controlled by the php-apache Deployment that you created in the first step of these instructions.Roughly speaking, the HPA controller will increase and decrease the number of replicas (by updating the Deployment) to maintain an average CPU utilization across all Pods of 50%. The Deployment then updates the ReplicaSet - this is part of how all Deployments work in Kubernetes - and then the ReplicaSet either adds or removes Pods based on the change to its .spec.Since each pod requests 200 milli-cores by kubectl run, this means an average CPU usage of 100 milli-cores. See Algorithm details for more details on the algorithm.Create the HorizontalPodAutoscaler:kubectl autoscale deployment php-apache --cpu-percent=50 --min=1 --max=10 horizontalpodautoscaler.autoscaling/php-apache autoscaled You can check the current status of the newly-made HorizontalPodAutoscaler, by running:# You can use "hpa" or "horizontalpodautoscaler"; either name works OK. kubectl get hpa The output is similar to:NAME REFERENCE TARGET MINPODS MAXPODS REPLICAS AGE php-apache Deployment/php-apache/scale 0% / 50% 1 10 1 18s (if you see other HorizontalPodAutoscalers with different names, that means they already existed, and isn't usually a problem).Please note that the current CPU consumption is 0% as there are no clients sending requests to the server (the TARGET column shows the average across all the Pods controlled by the corresponding deployment).Increase the loadNext, see how the autoscaler reacts to increased load. To do this, you'll start a different Pod to act as a client. The container within the client Pod runs in an infinite loop, sending queries to the php-apache service.# Run this in a separate terminal # so that the load generation continues and you can carry on with the rest of the steps kubectl run -i --tty load-generator --rm --image=busybox:1.28 --restart=Never -- /bin/sh -c "while sleep 0.01; do wget -q -O- http://php-apache; done" Now run:# type Ctrl+C to end the watch when you're ready kubectl get hpa php-apache --watch Within a minute or so, you should see the higher CPU load; for example:NAME REFERENCE TARGET MINPODS MAXPODS REPLICAS AGE php-apache Deployment/php-apache/scale 305% / 50% 1 10 1 3m and then, more replicas. For example:NAME REFERENCE TARGET MINPODS MAXPODS REPLICAS AGE php-apache Deployment/php-apache/scale 305% / 50% 1 10 7 3m Here, CPU consumption has increased to 305% of the request. As a result, the Deployment was resized to 7 replicas:kubectl get deployment php-apache You should see the replica count matching the figure from the HorizontalPodAutoscalerNAME READY UP-TO-DATE AVAILABLE AGE php-apache 7/7 7 7 19m Note:It may take a few minutes to stabilize the number of replicas. Since the amount of load is not controlled in any way it may happen that the final number of replicas will differ from this example.Stop generating loadTo finish the example, stop sending the load.In the terminal where you created the Pod that runs a busybox image, terminate the load generation by typing  \+ C.Then verify the result state (after a minute or so):# type Ctrl+C to end the watch when you're ready kubectl get hpa php-apache --watch The output is similar to:NAME REFERENCE TARGET MINPODS MAXPODS REPLICAS AGE php-apache Deployment/php-apache/scale 0% / 50% 1 10 1 11m and the Deployment also shows that it has scaled down:kubectl get deployment php-apache NAME READY UP-TO-DATE AVAILABLE AGE php-apache 1/1 1 1 27m Once CPU utilization dropped to 0, the HPA automatically scaled the number of replicas back down to 1.Autoscaling the replicas may take a few minutes.Autoscaling on multiple metrics and custom metricsYou can introduce additional metrics to use when autoscaling the php-apache Deployment by making use of the autoscaling/v2 API version.First, get the YAML of your HorizontalPodAutoscaler in the autoscaling/v2 form:kubectl get hpa php-apache -o yaml > /tmp/hpa-v2.yaml Open the /tmp/hpa-v2.yaml file in an editor, and you should see YAML which looks like this:apiVersion: autoscaling/v2 kind: HorizontalPodAutoscaler metadata: name: php-apache spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: php-apache minReplicas: 1 maxReplicas: 10 metrics: \- type: Resource resource: name: cpu target: type: Utilization averageUtilization: 50 status: observedGeneration: 1 lastScaleTime:  currentReplicas: 1 desiredReplicas: 1 currentMetrics: \- type: Resource resource: name: cpu current: averageUtilization: 0 averageValue: 0 Notice that the targetCPUUtilizationPercentage field has been replaced with an array called metrics. The CPU utilization metric is a resource metric, since it is represented as a percentage of a resource specified on pod containers. Notice that you can specify other resource metrics besides CPU. By default, the only other supported resource metric is memory. These resources do not change names from cluster to cluster, and should always be available, as long as the metrics.k8s.io API is available.You can also specify resource metrics in terms of direct values, instead of as percentages of the requested value, by using a target.type of AverageValue instead of Utilization, and setting the corresponding target.averageValue field instead of the target.averageUtilization. metrics: \- type: Resource resource: name: memory target: type: AverageValue averageValue: 500Mi There are two other types of metrics, both of which are considered custom metrics: pod metrics and object metrics. These metrics may have names which are cluster specific, and require a more advanced cluster monitoring setup.The first of these alternative metric types is pod metrics. These metrics describe Pods, and are averaged together across Pods and compared with a target value to determine the replica count. They work much like resource metrics, except that they only support a target type of AverageValue.Pod metrics are specified using a metric block like this:type: Pods pods: metric: name: packets-per-second target: type: AverageValue averageValue: 1k The second alternative metric type is object metrics. These metrics describe a different object in the same namespace, instead of describing Pods. The metrics are not necessarily fetched from the object; they only describe it. Object metrics support target types of both Value and AverageValue. With Value, the target is compared directly to the returned metric from the API. With AverageValue, the value returned from the custom metrics API is divided by the number of Pods before being compared to the target. The following example is the YAML representation of the requests-per-second metric.type: Object object: metric: name: requests-per-second describedObject: apiVersion: networking.k8s.io/v1 kind: Ingress name: main-route target: type: Value value: 2k If you provide multiple such metric blocks, the HorizontalPodAutoscaler will consider each metric in turn. The HorizontalPodAutoscaler will calculate proposed replica counts for each metric, and then choose the one with the highest replica count.For example, if you had your monitoring system collecting metrics about network traffic, you could update the definition above using kubectl edit to look like this:apiVersion: autoscaling/v2 kind: HorizontalPodAutoscaler metadata: name: php-apache spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: php-apache minReplicas: 1 maxReplicas: 10 metrics: \- type: Resource resource: name: cpu target: type: Utilization averageUtilization: 50 \- type: Pods pods: metric: name: packets-per-second target: type: AverageValue averageValue: 1k \- type: Object object: metric: name: requests-per-second describedObject: apiVersion: networking.k8s.io/v1 kind: Ingress name: main-route target: type: Value value: 10k status: observedGeneration: 1 lastScaleTime:  currentReplicas: 1 desiredReplicas: 1 currentMetrics: \- type: Resource resource: name: cpu current: averageUtilization: 0 averageValue: 0 \- type: Object object: metric: name: requests-per-second describedObject: apiVersion: networking.k8s.io/v1 kind: Ingress name: main-route current: value: 10k Then, your HorizontalPodAutoscaler would attempt to ensure that each pod was consuming roughly 50% of its requested CPU, serving 1000 packets per second, and that all pods behind the main-route Ingress were serving a total of 10000 requests per second.Autoscaling on more specific metricsMany metrics pipelines allow you to describe metrics either by name or by a set of additional descriptors called labels. For all non-resource metric types (pod, object, and external, described below), you can specify an additional label selector which is passed to your metric pipeline. For instance, if you collect a metric http_requests with the verb label, you can specify the following metric block to scale only on GET requests:type: Object object: metric: name: http_requests selector: {matchLabels: {verb: GET}} This selector uses the same syntax as the full Kubernetes label selectors. The monitoring pipeline determines how to collapse multiple series into a single value, if the name and selector match multiple series. The selector is additive, and cannot select metrics that describe objects that are not the target object (the target pods in the case of the Pods type, and the described object in the case of the Object type).Autoscaling on metrics not related to Kubernetes objectsApplications running on Kubernetes may need to autoscale based on metrics that don't have an obvious relationship to any object in the Kubernetes cluster, such as metrics describing a hosted service with no direct correlation to Kubernetes namespaces. In Kubernetes 1.10 and later, you can address this use case with external metrics.Using external metrics requires knowledge of your monitoring system; the setup is similar to that required when using custom metrics. External metrics allow you to autoscale your cluster based on any metric available in your monitoring system. Provide a metric block with a name and selector, as above, and use the External metric type instead of Object. If multiple time series are matched by the metricSelector, the sum of their values is used by the HorizontalPodAutoscaler. External metrics support both the Value and AverageValue target types, which function exactly the same as when you use the Object type.For example if your application processes tasks from a hosted queue service, you could add the following section to your HorizontalPodAutoscaler manifest to specify that you need one worker per 30 outstanding tasks.- type: External external: metric: name: queue_messages_ready selector: matchLabels: queue: "worker_tasks" target: type: AverageValue averageValue: 30 When possible, it's preferable to use the custom metric target types instead of external metrics, since it's easier for cluster administrators to secure the custom metrics API. The external metrics API potentially allows access to any metric, so cluster administrators should take care when exposing it.Appendix: Horizontal Pod Autoscaler Status ConditionsWhen using the autoscaling/v2 form of the HorizontalPodAutoscaler, you will be able to see status conditions set by Kubernetes on the HorizontalPodAutoscaler. These status conditions indicate whether or not the HorizontalPodAutoscaler is able to scale, and whether or not it is currently restricted in any way.The conditions appear in the status.conditions field. To see the conditions affecting a HorizontalPodAutoscaler, we can use kubectl describe hpa:kubectl describe hpa cm-test Name: cm-test Namespace: prom Labels:  Annotations:  CreationTimestamp: Fri, 16 Jun 2017 18:09:22 +0000 Reference: ReplicationController/cm-test Metrics: ( current / target ) "http_requests" on pods: 66m / 500m Min replicas: 1 Max replicas: 4 ReplicationController pods: 1 current / 1 desired Conditions: Type Status Reason Message \---- ------ ------ ------- AbleToScale True ReadyForNewScale the last scale time was sufficiently old as to warrant a new scale ScalingActive True ValidMetricFound the HPA was able to successfully calculate a replica count from pods metric http_requests ScalingLimited False DesiredWithinRange the desired replica count is within the acceptable range Events: For this HorizontalPodAutoscaler, you can see several conditions in a healthy state. The first, AbleToScale, indicates whether or not the HPA is able to fetch and update scales, as well as whether or not any backoff-related conditions would prevent scaling. The second, ScalingActive, indicates whether or not the HPA is enabled (i.e. the replica count of the target is not zero) and is able to calculate desired scales. When it is False, it generally indicates problems with fetching metrics. Finally, the last condition, ScalingLimited, indicates that the desired scale was capped by the maximum or minimum of the HorizontalPodAutoscaler. This is an indication that you may wish to raise or lower the minimum or maximum replica count constraints on your HorizontalPodAutoscaler.QuantitiesAll metrics in the HorizontalPodAutoscaler and metrics APIs are specified using a special whole-number notation known in Kubernetes as a quantity. For example, the quantity 10500m would be written as 10.5 in decimal notation. The metrics APIs will return whole numbers without a suffix when possible, and will generally return quantities in milli-units otherwise. This means you might see your metric value fluctuate between 1 and 1500m, or 1 and 1.5 when written in decimal notation.Other possible scenariosCreating the autoscaler declarativelyInstead of using kubectl autoscale command to create a HorizontalPodAutoscaler imperatively we can use the following manifest to create it declaratively:application/hpa/php-apache.yaml apiVersion: autoscaling/v2 kind: HorizontalPodAutoscaler metadata: name: php-apache spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: php-apache minReplicas: 1 maxReplicas: 10 metrics: \- type: Resource resource: name: cpu target: type: Utilization averageUtilization: 50 Then, create the autoscaler by executing the following command:kubectl create -f https://k8s.io/examples/application/hpa/php-apache.yaml horizontalpodautoscaler.autoscaling/php-apache created 4.8.9 - Specifying a Disruption Budget for your ApplicationFEATURE STATE: Kubernetes v1.21 [stable]This page shows how to limit the number of concurrent disruptions that your application experiences, allowing for higher availability while permitting the cluster administrator to manage the clusters nodes.Before you beginYour Kubernetes server must be at or later than version v1.21.To check the version, enter kubectl version.You are the owner of an application running on a Kubernetes cluster that requires high availability.You should know how to deploy Replicated Stateless Applications and/or Replicated Stateful Applications.You should have read about Pod Disruptions.You should confirm with your cluster owner or service provider that they respect Pod Disruption Budgets.Protecting an Application with a PodDisruptionBudgetIdentify what application you want to protect with a PodDisruptionBudget (PDB).Think about how your application reacts to disruptions.Create a PDB definition as a YAML file.Create the PDB object from the YAML file.Identify an Application to ProtectThe most common use case when you want to protect an application specified by one of the built-in Kubernetes controllers:DeploymentReplicationControllerReplicaSetStatefulSetIn this case, make a note of the controller's .spec.selector; the same selector goes into the PDBs .spec.selector.From version 1.15 PDBs support custom controllers where the scale subresource is enabled.You can also use PDBs with pods which are not controlled by one of the above controllers, or arbitrary groups of pods, but there are some restrictions, described in Arbitrary workloads and arbitrary selectors.Think about how your application reacts to disruptionsDecide how many instances can be down at the same time for a short period due to a voluntary disruption.Stateless frontends:Concern: don't reduce serving capacity by more than 10%.Solution: use PDB with minAvailable 90% for example.Single-instance Stateful Application:Concern: do not terminate this application without talking to me.Possible Solution 1: Do not use a PDB and tolerate occasional downtime.Possible Solution 2: Set PDB with maxUnavailable=0. Have an understanding (outside of Kubernetes) that the cluster operator needs to consult you before termination. When the cluster operator contacts you, prepare for downtime, and then delete the PDB to indicate readiness for disruption. Recreate afterwards.Multiple-instance Stateful application such as Consul, ZooKeeper, or etcd:Concern: Do not reduce number of instances below quorum, otherwise writes fail.Possible Solution 1: set maxUnavailable to 1 (works with varying scale of application).Possible Solution 2: set minAvailable to quorum-size (e.g. 3 when scale is 5). (Allows more disruptions at once).Restartable Batch Job:Concern: Job needs to complete in case of voluntary disruption.Possible solution: Do not create a PDB. The Job controller will create a replacement pod.Rounding logic when specifying percentagesValues for minAvailable or maxUnavailable can be expressed as integers or as a percentage.When you specify an integer, it represents a number of Pods. For instance, if you set minAvailable to 10, then 10 Pods must always be available, even during a disruption.When you specify a percentage by setting the value to a string representation of a percentage (eg. "50%"), it represents a percentage of total Pods. For instance, if you set minAvailable to "50%", then at least 50% of the Pods remain available during a disruption.When you specify the value as a percentage, it may not map to an exact number of Pods. For example, if you have 7 Pods and you set minAvailable to "50%", it's not immediately obvious whether that means 3 Pods or 4 Pods must be available. Kubernetes rounds up to the nearest integer, so in this case, 4 Pods must be available. When you specify the value maxUnavailable as a percentage, Kubernetes rounds up the number of Pods that may be disrupted. Thereby a disruption can exceed your defined maxUnavailable percentage. You can examine the code that controls this behavior.Specifying a PodDisruptionBudgetA PodDisruptionBudget has three fields:A label selector .spec.selector to specify the set of pods to which it applies. This field is required..spec.minAvailable which is a description of the number of pods from that set that must still be available after the eviction, even in the absence of the evicted pod. minAvailable can be either an absolute number or a percentage..spec.maxUnavailable (available in Kubernetes 1.7 and higher) which is a description of the number of pods from that set that can be unavailable after the eviction. It can be either an absolute number or a percentage.Note:The behavior for an empty selector differs between the policy/v1beta1 and policy/v1 APIs for PodDisruptionBudgets. For policy/v1beta1 an empty selector matches zero pods, while for policy/v1 an empty selector matches every pod in the namespace.You can specify only one of maxUnavailable and minAvailable in a single PodDisruptionBudget. maxUnavailable can only be used to control the eviction of pods that have an associated controller managing them. In the examples below, "desired replicas" is the scale of the controller managing the pods being selected by the PodDisruptionBudget.Example 1: With a minAvailable of 5, evictions are allowed as long as they leave behind 5 or more healthy pods among those selected by the PodDisruptionBudget's selector.Example 2: With a minAvailable of 30%, evictions are allowed as long as at least 30% of the number of desired replicas are healthy.Example 3: With a maxUnavailable of 5, evictions are allowed as long as there are at most 5 unhealthy replicas among the total number of desired replicas.Example 4: With a maxUnavailable of 30%, evictions are allowed as long as the number of unhealthy replicas does not exceed 30% of the total number of desired replica rounded up to the nearest integer. If the total number of desired replicas is just one, that single replica is still allowed for disruption, leading to an effective unavailability of 100%.In typical usage, a single budget would be used for a collection of pods managed by a controller—for example, the pods in a single ReplicaSet or StatefulSet.Note:A disruption budget does not truly guarantee that the specified number/percentage of pods will always be up. For example, a node that hosts a pod from the collection may fail when the collection is at the minimum size specified in the budget, thus bringing the number of available pods from the collection below the specified size. The budget can only protect against voluntary evictions, not all causes of unavailability.If you set maxUnavailable to 0% or 0, or you set minAvailable to 100% or the number of replicas, you are requiring zero voluntary evictions. When you set zero voluntary evictions for a workload object such as ReplicaSet, then you cannot successfully drain a Node running one of those Pods. If you try to drain a Node where an unevictable Pod is running, the drain never completes. This is permitted as per the semantics of PodDisruptionBudget.You can find examples of pod disruption budgets defined below. They match pods with the label app: zookeeper.Example PDB Using minAvailable:policy/zookeeper-pod-disruption-budget-minavailable.yaml apiVersion: policy/v1 kind: PodDisruptionBudget metadata: name: zk-pdb spec: minAvailable: 2 selector: matchLabels: app: zookeeper Example PDB Using maxUnavailable:policy/zookeeper-pod-disruption-budget-maxunavailable.yaml apiVersion: policy/v1 kind: PodDisruptionBudget metadata: name: zk-pdb spec: maxUnavailable: 1 selector: matchLabels: app: zookeeper For example, if the above zk-pdb object selects the pods of a StatefulSet of size 3, both specifications have the exact same meaning. The use of maxUnavailable is recommended as it automatically responds to changes in the number of replicas of the corresponding controller.Create the PDB objectYou can create or update the PDB object using kubectl.kubectl apply -f mypdb.yaml Check the status of the PDBUse kubectl to check that your PDB is created.Assuming you don't actually have pods matching app: zookeeper in your namespace, then you'll see something like this:kubectl get poddisruptionbudgets NAME MIN AVAILABLE MAX UNAVAILABLE ALLOWED DISRUPTIONS AGE zk-pdb 2 N/A 0 7s If there are matching pods (say, 3), then you would see something like this:kubectl get poddisruptionbudgets NAME MIN AVAILABLE MAX UNAVAILABLE ALLOWED DISRUPTIONS AGE zk-pdb 2 N/A 1 7s The non-zero value for ALLOWED DISRUPTIONS means that the disruption controller has seen the pods, counted the matching pods, and updated the status of the PDB.You can get more information about the status of a PDB with this command:kubectl get poddisruptionbudgets zk-pdb -o yaml apiVersion: policy/v1 kind: PodDisruptionBudget metadata: annotations: … creationTimestamp: "2020-03-04T04:22:56Z" generation: 1 name: zk-pdb … status: currentHealthy: 3 desiredHealthy: 2 disruptionsAllowed: 1 expectedPods: 3 observedGeneration: 1 Healthiness of a PodThe current implementation considers healthy pods, as pods that have .status.conditions item with type="Ready" and status="True". These pods are tracked via .status.currentHealthy field in the PDB status.Unhealthy Pod Eviction PolicyFEATURE STATE: Kubernetes v1.31 [stable] (enabled by default: true)PodDisruptionBudget guarding an application ensures that .status.currentHealthy number of pods does not fall below the number specified in .status.desiredHealthy by disallowing eviction of healthy pods. By using .spec.unhealthyPodEvictionPolicy, you can also define the criteria when unhealthy pods should be considered for eviction. The default behavior when no policy is specified corresponds to the IfHealthyBudget policy.Policies:IfHealthyBudgetRunning pods (.status.phase="Running"), but not yet healthy can be evicted only if the guarded application is not disrupted (.status.currentHealthy is at least equal to .status.desiredHealthy).This policy ensures that running pods of an already disrupted application have the best chance to become healthy. This has negative implications for draining nodes, which can be blocked by misbehaving applications that are guarded by a PDB. More specifically applications with pods in CrashLoopBackOff state (due to a bug or misconfiguration), or pods that are just failing to report the Ready condition.AlwaysAllowRunning pods (.status.phase="Running"), but not yet healthy are considered disrupted and can be evicted regardless of whether the criteria in a PDB is met.This means prospective running pods of a disrupted application might not get a chance to become healthy. By using this policy, cluster managers can easily evict misbehaving applications that are guarded by a PDB. More specifically applications with pods in CrashLoopBackOff state (due to a bug or misconfiguration), or pods that are just failing to report the Ready condition.Note:Pods in Pending, Succeeded or Failed phase are always considered for eviction.Arbitrary workloads and arbitrary selectorsYou can skip this section if you only use PDBs with the built-in workload resources (Deployment, ReplicaSet, StatefulSet and ReplicationController) or with custom resources that implement a scale subresource, and where the PDB selector exactly matches the selector of the Pod's owning resource.You can use a PDB with pods controlled by another resource, by an "operator", or bare pods, but with these restrictions:only .spec.minAvailable can be used, not .spec.maxUnavailable.only an integer value can be used with .spec.minAvailable, not a percentage.It is not possible to use other availability configurations, because Kubernetes cannot derive a total number of pods without a supported owning resource.You can use a selector which selects a subset or superset of the pods belonging to a workload resource. The eviction API will disallow eviction of any pod covered by multiple PDBs, so most users will want to avoid overlapping selectors. One reasonable use of overlapping PDBs is when pods are being transitioned from one PDB to another.4.8.10 - Accessing the Kubernetes API from a PodThis guide demonstrates how to access the Kubernetes API from within a pod.Before you beginYou need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesAccessing the API from within a PodWhen accessing the API from within a Pod, locating and authenticating to the API server are slightly different to the external client case.The easiest way to use the Kubernetes API from a Pod is to use one of the official client libraries. These libraries can automatically discover the API server and authenticate.Using Official Client LibrariesFrom within a Pod, the recommended ways to connect to the Kubernetes API are:For a Go client, use the official Go client library. The rest.InClusterConfig() function handles API host discovery and authentication automatically. See an example here.For a Python client, use the official Python client library. The config.load_incluster_config() function handles API host discovery and authentication automatically. See an example here.There are a number of other libraries available, please refer to the Client Libraries page.In each case, the service account credentials of the Pod are used to communicate securely with the API server.Directly accessing the REST APIWhile running in a Pod, your container can create an HTTPS URL for the Kubernetes API server by fetching the KUBERNETES_SERVICE_HOST and KUBERNETES_SERVICE_PORT_HTTPS environment variables. The API server's in-cluster address is also published to a Service named kubernetes in the default namespace so that pods may reference kubernetes.default.svc as a DNS name for the local API server.Note:Kubernetes does not guarantee that the API server has a valid certificate for the hostname kubernetes.default.svc; however, the control plane is expected to present a valid certificate for the hostname or IP address that $KUBERNETES_SERVICE_HOST represents.The recommended way to authenticate to the API server is with a service account credential. By default, a Pod is associated with a service account, and a credential (token) for that service account is placed into the filesystem tree of each container in that Pod, at /var/run/secrets/kubernetes.io/serviceaccount/token.If available, a certificate bundle is placed into the filesystem tree of each container at /var/run/secrets/kubernetes.io/serviceaccount/ca.crt, and should be used to verify the serving certificate of the API server.Finally, the default namespace to be used for namespaced API operations is placed in a file at /var/run/secrets/kubernetes.io/serviceaccount/namespace in each container.Using kubectl proxyIf you would like to query the API without an official client library, you can run kubectl proxy as the command of a new sidecar container in the Pod. This way, kubectl proxy will authenticate to the API and expose it on the localhost interface of the Pod, so that other containers in the Pod can use it directly.Without using a proxyIt is possible to avoid using the kubectl proxy by passing the authentication token directly to the API server. The internal certificate secures the connection.# Point to the internal API server hostname APISERVER=https://kubernetes.default.svc # Path to ServiceAccount token SERVICEACCOUNT=/var/run/secrets/kubernetes.io/serviceaccount # Read this Pod's namespace NAMESPACE=$(cat ${SERVICEACCOUNT}/namespace) # Read the ServiceAccount bearer token TOKEN=$(cat ${SERVICEACCOUNT}/token) # Reference the internal certificate authority (CA) CACERT=${SERVICEACCOUNT}/ca.crt # Explore the API with TOKEN curl --cacert ${CACERT} --header "Authorization: Bearer ${TOKEN}" -X GET ${APISERVER}/api The output will be similar to this:{ "kind": "APIVersions", "versions": ["v1"], "serverAddressByClientCIDRs": [ { "clientCIDR": "0.0.0.0/0", "serverAddress": "10.0.1.149:443" } ] } 4.9 - Run JobsRun Jobs using parallel processing.4.9.1 - Running Automated Tasks with a CronJobThis page shows how to run automated tasks using Kubernetes CronJob object.Before you beginYou need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesCreating a CronJobCron jobs require a config file. Here is a manifest for a CronJob that runs a simple demonstration task every minute:application/job/cronjob.yaml apiVersion: batch/v1 kind: CronJob metadata: name: hello spec: schedule: "* * * * *" jobTemplate: spec: template: spec: containers: \- name: hello image: busybox:1.28 imagePullPolicy: IfNotPresent command: \- /bin/sh \- -c \- date; echo Hello from the Kubernetes cluster restartPolicy: OnFailure Run the example CronJob by using this command:kubectl create -f https://k8s.io/examples/application/job/cronjob.yaml The output is similar to this:cronjob.batch/hello created After creating the cron job, get its status using this command:kubectl get cronjob hello The output is similar to this:NAME SCHEDULE SUSPEND ACTIVE LAST SCHEDULE AGE hello */1 * * * * False 0  10s As you can see from the results of the command, the cron job has not scheduled or run any jobs yet. Watch for the job to be created in around one minute:kubectl get jobs --watch The output is similar to this:NAME COMPLETIONS DURATION AGE hello-4111706356 0/1 0s hello-4111706356 0/1 0s 0s hello-4111706356 1/1 5s 5s Now you've seen one running job scheduled by the "hello" cron job. You can stop watching the job and view the cron job again to see that it scheduled the job:kubectl get cronjob hello The output is similar to this:NAME SCHEDULE SUSPEND ACTIVE LAST SCHEDULE AGE hello */1 * * * * False 0 50s 75s You should see that the cron job hello successfully scheduled a job at the time specified in LAST SCHEDULE. There are currently 0 active jobs, meaning that the job has completed or failed.Now, find the pods that the last scheduled job created and view the standard output of one of the pods.Note:The job name is different from the pod name.# Replace "hello-4111706356" with the job name in your system pods=$(kubectl get pods --selector=job-name=hello-4111706356 --output=jsonpath={.items[*].metadata.name}) Show the pod log:kubectl logs $pods The output is similar to this:Fri Feb 22 11:02:09 UTC 2019 Hello from the Kubernetes cluster Deleting a CronJobWhen you don't need a cron job any more, delete it with kubectl delete cronjob :kubectl delete cronjob hello Deleting the cron job removes all the jobs and pods it created and stops it from creating additional jobs. You can read more about removing jobs in garbage collection.4.9.2 - Coarse Parallel Processing Using a Work QueueIn this example, you will run a Kubernetes Job with multiple parallel worker processes.In this example, as each pod is created, it picks up one unit of work from a task queue, completes it, deletes it from the queue, and exits.Here is an overview of the steps in this example:Start a message queue service. In this example, you use RabbitMQ, but you could use another one. In practice you would set up a message queue service once and reuse it for many jobs.Create a queue, and fill it with messages. Each message represents one task to be done. In this example, a message is an integer that we will do a lengthy computation on.Start a Job that works on tasks from the queue. The Job starts several pods. Each pod takes one task from the message queue, processes it, and exits.Before you beginYou should already be familiar with the basic, non-parallel, use of Job.You need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesYou will need a container image registry where you can upload images to run in your cluster.This task example also assumes that you have Docker installed locally.Starting a message queue serviceThis example uses RabbitMQ, however, you can adapt the example to use another AMQP-type message service.In practice you could set up a message queue service once in a cluster and reuse it for many jobs, as well as for long-running services.Start RabbitMQ as follows:# make a Service for the StatefulSet to use kubectl create -f https://kubernetes.io/examples/application/job/rabbitmq/rabbitmq-service.yaml service "rabbitmq-service" created kubectl create -f https://kubernetes.io/examples/application/job/rabbitmq/rabbitmq-statefulset.yaml statefulset "rabbitmq" created Testing the message queue serviceNow, we can experiment with accessing the message queue. We will create a temporary interactive pod, install some tools on it, and experiment with queues.First create a temporary interactive Pod.# Create a temporary interactive container kubectl run -i --tty temp --image ubuntu:22.04 Waiting for pod default/temp-loe07 to be running, status is Pending, pod ready: false ... [ previous line repeats several times .. hit return when it stops ] ... Note that your pod name and command prompt will be different.Next install the amqp-tools so you can work with message queues. The next commands show what you need to run inside the interactive shell in that Pod:apt-get update && apt-get install -y curl ca-certificates amqp-tools python3 dnsutils Later, you will make a container image that includes these packages.Next, you will check that you can discover the Service for RabbitMQ:# Run these commands inside the Pod # Note the rabbitmq-service has a DNS name, provided by Kubernetes: nslookup rabbitmq-service Server: 10.0.0.10 Address: 10.0.0.10#53 Name: rabbitmq-service.default.svc.cluster.local Address: 10.0.147.152 (the IP addresses will vary)If the kube-dns addon is not set up correctly, the previous step may not work for you. You can also find the IP address for that Service in an environment variable:# run this check inside the Pod env | grep RABBITMQ_SERVICE | grep HOST RABBITMQ_SERVICE_SERVICE_HOST=10.0.147.152 (the IP address will vary)Next you will verify that you can create a queue, and publish and consume messages.# Run these commands inside the Pod # In the next line, rabbitmq-service is the hostname where the rabbitmq-service # can be reached. 5672 is the standard port for rabbitmq. export BROKER_URL=amqp://guest:guest@rabbitmq-service:5672 # If you could not resolve "rabbitmq-service" in the previous step, # then use this command instead: BROKER_URL=amqp://guest:guest@$RABBITMQ_SERVICE_SERVICE_HOST:5672 # Now create a queue: /usr/bin/amqp-declare-queue --url=$BROKER_URL -q foo -d foo Publish one message to the queue:/usr/bin/amqp-publish --url=$BROKER_URL -r foo -p -b Hello # And get it back. /usr/bin/amqp-consume --url=$BROKER_URL -q foo -c 1 cat && echo 1>&2 Hello In the last command, the amqp-consume tool took one message (-c 1) from the queue, and passes that message to the standard input of an arbitrary command. In this case, the program cat prints out the characters read from standard input, and the echo adds a carriage return so the example is readable.Fill the queue with tasksNow, fill the queue with some simulated tasks. In this example, the tasks are strings to be printed.In a practice, the content of the messages might be:names of files to that need to be processedextra flags to the programranges of keys in a database tableconfiguration parameters to a simulationframe numbers of a scene to be renderedIf there is large data that is needed in a read-only mode by all pods of the Job, you typically put that in a shared file system like NFS and mount that readonly on all the pods, or write the program in the pod so that it can natively read data from a cluster file system (for example: HDFS).For this example, you will create the queue and fill it using the AMQP command line tools. In practice, you might write a program to fill the queue using an AMQP client library.# Run this on your computer, not in the Pod /usr/bin/amqp-declare-queue --url=$BROKER_URL -q job1 -d job1 Add items to the queue:for f in apple banana cherry date fig grape lemon melon do /usr/bin/amqp-publish --url=$BROKER_URL -r job1 -p -b $f done You added 8 messages to the queue.Create a container imageNow you are ready to create an image that you will run as a Job.The job will use the amqp-consume utility to read the message from the queue and run the actual work. Here is a very simple example program:application/job/rabbitmq/worker.py #!/usr/bin/env python # Just prints standard out and sleeps for 10 seconds. import sys import time print("Processing " + sys.stdin.readlines()[0]) time.sleep(10) Give the script execution permission:chmod +x worker.py Now, build an image. Make a temporary directory, change to it, download the Dockerfile, and worker.py. In either case, build the image with this command:docker build -t job-wq-1 . For the Docker Hub, tag your app image with your username and push to the Hub with the below commands. Replace  with your Hub username.docker tag job-wq-1 /job-wq-1 docker push /job-wq-1 If you are using an alternative container image registry, tag the image and push it there instead.Defining a JobHere is a manifest for a Job. You'll need to make a copy of the Job manifest (call it ./job.yaml), and edit the name of the container image to match the name you used.application/job/rabbitmq/job.yaml apiVersion: batch/v1 kind: Job metadata: name: job-wq-1 spec: completions: 8 parallelism: 2 template: metadata: name: job-wq-1 spec: containers: \- name: c image: gcr.io//job-wq-1 env: \- name: BROKER_URL value: amqp://guest:guest@rabbitmq-service:5672 \- name: QUEUE value: job1 restartPolicy: OnFailure In this example, each pod works on one item from the queue and then exits. So, the completion count of the Job corresponds to the number of work items done. That is why the example manifest has .spec.completions set to 8.Running the JobNow, run the Job:# this assumes you downloaded and then edited the manifest already kubectl apply -f ./job.yaml You can wait for the Job to succeed, with a timeout:# The check for condition name is case insensitive kubectl wait --for=condition=complete --timeout=300s job/job-wq-1 Next, check on the Job:kubectl describe jobs/job-wq-1 Name: job-wq-1 Namespace: default Selector: controller-uid=41d75705-92df-11e7-b85e-fa163ee3c11f Labels: controller-uid=41d75705-92df-11e7-b85e-fa163ee3c11f job-name=job-wq-1 Annotations:  Parallelism: 2 Completions: 8 Start Time: Wed, 06 Sep 2022 16:42:02 +0000 Pods Statuses: 0 Running / 8 Succeeded / 0 Failed Pod Template: Labels: controller-uid=41d75705-92df-11e7-b85e-fa163ee3c11f job-name=job-wq-1 Containers: c: Image: container-registry.example/causal-jigsaw-637/job-wq-1 Port: Environment: BROKER_URL: amqp://guest:guest@rabbitmq-service:5672 QUEUE: job1 Mounts:  Volumes:  Events: FirstSeen LastSeen Count From SubobjectPath Type Reason Message ───────── ──────── ───── ──── ───────────── ────── ────── ─────── 27s 27s 1 {job } Normal SuccessfulCreate Created pod: job-wq-1-hcobb 27s 27s 1 {job } Normal SuccessfulCreate Created pod: job-wq-1-weytj 27s 27s 1 {job } Normal SuccessfulCreate Created pod: job-wq-1-qaam5 27s 27s 1 {job } Normal SuccessfulCreate Created pod: job-wq-1-b67sr 26s 26s 1 {job } Normal SuccessfulCreate Created pod: job-wq-1-xe5hj 15s 15s 1 {job } Normal SuccessfulCreate Created pod: job-wq-1-w2zqe 14s 14s 1 {job } Normal SuccessfulCreate Created pod: job-wq-1-d6ppa 14s 14s 1 {job } Normal SuccessfulCreate Created pod: job-wq-1-p17e0 All the pods for that Job succeeded! You're done.AlternativesThis approach has the advantage that you do not need to modify your "worker" program to be aware that there is a work queue. You can include the worker program unmodified in your container image.Using this approach does require that you run a message queue service. If running a queue service is inconvenient, you may want to consider one of the other job patterns.This approach creates a pod for every work item. If your work items only take a few seconds, though, creating a Pod for every work item may add a lot of overhead. Consider another design, such as in the fine parallel work queue example, that executes multiple work items per Pod.In this example, you used the amqp-consume utility to read the message from the queue and run the actual program. This has the advantage that you do not need to modify your program to be aware of the queue. The fine parallel work queue example shows how to communicate with the work queue using a client library.CaveatsIf the number of completions is set to less than the number of items in the queue, then not all items will be processed.If the number of completions is set to more than the number of items in the queue, then the Job will not appear to be completed, even though all items in the queue have been processed. It will start additional pods which will block waiting for a message. You would need to make your own mechanism to spot when there is work to do and measure the size of the queue, setting the number of completions to match.There is an unlikely race with this pattern. If the container is killed in between the time that the message is acknowledged by the amqp-consume command and the time that the container exits with success, or if the node crashes before the kubelet is able to post the success of the pod back to the API server, then the Job will not appear to be complete, even though all items in the queue have been processed.4.9.3 - Fine Parallel Processing Using a Work QueueIn this example, you will run a Kubernetes Job that runs multiple parallel tasks as worker processes, each running as a separate Pod.In this example, as each pod is created, it picks up one unit of work from a task queue, processes it, and repeats until the end of the queue is reached.Here is an overview of the steps in this example:Start a storage service to hold the work queue. In this example, you will use Redis to store work items. In the previous example, you used RabbitMQ. In this example, you will use Redis and a custom work-queue client library; this is because AMQP does not provide a good way for clients to detect when a finite-length work queue is empty. In practice you would set up a store such as Redis once and reuse it for the work queues of many jobs, and other things.Create a queue, and fill it with messages. Each message represents one task to be done. In this example, a message is an integer that we will do a lengthy computation on.Start a Job that works on tasks from the queue. The Job starts several pods. Each pod takes one task from the message queue, processes it, and repeats until the end of the queue is reached.Before you beginYou need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesYou will need a container image registry where you can upload images to run in your cluster. The example uses Docker Hub, but you could adapt it to a different container image registry.This task example also assumes that you have Docker installed locally. You use Docker to build container images.Be familiar with the basic, non-parallel, use of Job.Starting RedisFor this example, for simplicity, you will start a single instance of Redis. See the Redis Example for an example of deploying Redis scalably and redundantly.You could also download the following files directly:redis-pod.yamlredis-service.yamlDockerfilejob.yamlrediswq.pyworker.pyTo start a single instance of Redis, you need to create the redis pod and redis service:kubectl apply -f https://k8s.io/examples/application/job/redis/redis-pod.yaml kubectl apply -f https://k8s.io/examples/application/job/redis/redis-service.yaml Filling the queue with tasksNow let's fill the queue with some "tasks". In this example, the tasks are strings to be printed.Start a temporary interactive pod for running the Redis CLI.kubectl run -i --tty temp --image redis --command "/bin/sh" Waiting for pod default/redis2-c7h78 to be running, status is Pending, pod ready: false Hit enter for command prompt Now hit enter, start the Redis CLI, and create a list with some work items in it.redis-cli -h redis redis:6379> rpush job2 "apple" (integer) 1 redis:6379> rpush job2 "banana" (integer) 2 redis:6379> rpush job2 "cherry" (integer) 3 redis:6379> rpush job2 "date" (integer) 4 redis:6379> rpush job2 "fig" (integer) 5 redis:6379> rpush job2 "grape" (integer) 6 redis:6379> rpush job2 "lemon" (integer) 7 redis:6379> rpush job2 "melon" (integer) 8 redis:6379> rpush job2 "orange" (integer) 9 redis:6379> lrange job2 0 -1 1) "apple" 2) "banana" 3) "cherry" 4) "date" 5) "fig" 6) "grape" 7) "lemon" 8) "melon" 9) "orange" So, the list with key job2 will be the work queue.Note: if you do not have Kube DNS setup correctly, you may need to change the first step of the above block to redis-cli -h $REDIS_SERVICE_HOST.Create a container imageNow you are ready to create an image that will process the work in that queue.You're going to use a Python worker program with a Redis client to read the messages from the message queue.A simple Redis work queue client library is provided, called rediswq.py (Download).The "worker" program in each Pod of the Job uses the work queue client library to get work. Here it is:application/job/redis/worker.py #!/usr/bin/env python import time import rediswq host="redis" # Uncomment next two lines if you do not have Kube-DNS working. # import os # host = os.getenv("REDIS_SERVICE_HOST") q = rediswq.RedisWQ(name="job2", host=host) print("Worker with sessionID: " + q.sessionID()) print("Initial queue state: empty=" + str(q.empty())) while not q.empty(): item = q.lease(lease_secs=10, block=True, timeout=2) if item is not None: itemstr = item.decode("utf-8") print("Working on " + itemstr) time.sleep(10) # Put your actual work here instead of sleep. q.complete(item) else: print("Waiting for work") print("Queue empty, exiting") You could also download worker.py, rediswq.py, and Dockerfile files, then build the container image. Here's an example using Docker to do the image build:docker build -t job-wq-2 . Push the imageFor the Docker Hub, tag your app image with your username and push to the Hub with the below commands. Replace  with your Hub username.docker tag job-wq-2 /job-wq-2 docker push /job-wq-2 You need to push to a public repository or configure your cluster to be able to access your private repository.Defining a JobHere is a manifest for the Job you will create:application/job/redis/job.yaml apiVersion: batch/v1 kind: Job metadata: name: job-wq-2 spec: parallelism: 2 template: metadata: name: job-wq-2 spec: containers: \- name: c image: gcr.io/myproject/job-wq-2 restartPolicy: OnFailure Note:Be sure to edit the manifest to change gcr.io/myproject to your own path.In this example, each pod works on several items from the queue and then exits when there are no more items. Since the workers themselves detect when the workqueue is empty, and the Job controller does not know about the workqueue, it relies on the workers to signal when they are done working. The workers signal that the queue is empty by exiting with success. So, as soon as any worker exits with success, the controller knows the work is done, and that the Pods will exit soon. So, you need to leave the completion count of the Job unset. The job controller will wait for the other pods to complete too.Running the JobSo, now run the Job:# this assumes you downloaded and then edited the manifest already kubectl apply -f ./job.yaml Now wait a bit, then check on the Job:kubectl describe jobs/job-wq-2 Name: job-wq-2 Namespace: default Selector: controller-uid=b1c7e4e3-92e1-11e7-b85e-fa163ee3c11f Labels: controller-uid=b1c7e4e3-92e1-11e7-b85e-fa163ee3c11f job-name=job-wq-2 Annotations:  Parallelism: 2 Completions:  Start Time: Mon, 11 Jan 2022 17:07:59 +0000 Pods Statuses: 1 Running / 0 Succeeded / 0 Failed Pod Template: Labels: controller-uid=b1c7e4e3-92e1-11e7-b85e-fa163ee3c11f job-name=job-wq-2 Containers: c: Image: container-registry.example/exampleproject/job-wq-2 Port: Environment:  Mounts:  Volumes:  Events: FirstSeen LastSeen Count From SubobjectPath Type Reason Message \--------- -------- ----- ---- ------------- -------- ------ ------- 33s 33s 1 {job-controller } Normal SuccessfulCreate Created pod: job-wq-2-lglf8 You can wait for the Job to succeed, with a timeout:# The check for condition name is case insensitive kubectl wait --for=condition=complete --timeout=300s job/job-wq-2 kubectl logs pods/job-wq-2-7r7b2 Worker with sessionID: bbd72d0a-9e5c-4dd6-abf6-416cc267991f Initial queue state: empty=False Working on banana Working on date Working on lemon As you can see, one of the pods for this Job worked on several work units.AlternativesIf running a queue service or modifying your containers to use a work queue is inconvenient, you may want to consider one of the other job patterns.If you have a continuous stream of background processing work to run, then consider running your background workers with a ReplicaSet instead, and consider running a background processing library such as https://github.com/resque/resque.4.9.4 - Indexed Job for Parallel Processing with Static Work AssignmentFEATURE STATE: Kubernetes v1.24 [stable]In this example, you will run a Kubernetes Job that uses multiple parallel worker processes. Each worker is a different container running in its own Pod. The Pods have an index number that the control plane sets automatically, which allows each Pod to identify which part of the overall task to work on.The pod index is available in the annotation batch.kubernetes.io/job-completion-index as a string representing its decimal value. In order for the containerized task process to obtain this index, you can publish the value of the annotation using the downward API mechanism. For convenience, the control plane automatically sets the downward API to expose the index in the JOB_COMPLETION_INDEX environment variable.Here is an overview of the steps in this example:Define a Job manifest using indexed completion. The downward API allows you to pass the pod index annotation as an environment variable or file to the container.Start an Indexed Job based on that manifest.Before you beginYou should already be familiar with the basic, non-parallel, use of Job.You need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesYour Kubernetes server must be at or later than version v1.21.To check the version, enter kubectl version.Choose an approachTo access the work item from the worker program, you have a few options:Read the JOB_COMPLETION_INDEX environment variable. The Job controller automatically links this variable to the annotation containing the completion index.Read a file that contains the completion index.Assuming that you can't modify the program, you can wrap it with a script that reads the index using any of the methods above and converts it into something that the program can use as input.For this example, imagine that you chose option 3 and you want to run the rev utility. This program accepts a file as an argument and prints its content reversed.rev data.txt You'll use the rev tool from the busybox container image.As this is only an example, each Pod only does a tiny piece of work (reversing a short string). In a real workload you might, for example, create a Job that represents the task of producing 60 seconds of video based on scene data. Each work item in the video rendering Job would be to render a particular frame of that video clip. Indexed completion would mean that each Pod in the Job knows which frame to render and publish, by counting frames from the start of the clip.Define an Indexed JobHere is a sample Job manifest that uses Indexed completion mode:application/job/indexed-job.yaml apiVersion: batch/v1 kind: Job metadata: name: 'indexed-job' spec: completions: 5 parallelism: 3 completionMode: Indexed template: spec: restartPolicy: Never initContainers: \- name: 'input' image: 'docker.io/library/bash' command: \- "bash" \- "-c" \- | items=(foo bar baz qux xyz) echo ${items[$JOB_COMPLETION_INDEX]} > /input/data.txt volumeMounts: \- mountPath: /input name: input containers: \- name: 'worker' image: 'docker.io/library/busybox' command: \- "rev" \- "/input/data.txt" volumeMounts: \- mountPath: /input name: input volumes: \- name: input emptyDir: {} In the example above, you use the builtin JOB_COMPLETION_INDEX environment variable set by the Job controller for all containers. An init container maps the index to a static value and writes it to a file that is shared with the container running the worker through an emptyDir volume. Optionally, you can define your own environment variable through the downward API to publish the index to containers. You can also choose to load a list of values from a ConfigMap as an environment variable or file.Alternatively, you can directly use the downward API to pass the annotation value as a volume file, like shown in the following example:application/job/indexed-job-vol.yaml apiVersion: batch/v1 kind: Job metadata: name: 'indexed-job' spec: completions: 5 parallelism: 3 completionMode: Indexed template: spec: restartPolicy: Never containers: \- name: 'worker' image: 'docker.io/library/busybox' command: \- "rev" \- "/input/data.txt" volumeMounts: \- mountPath: /input name: input volumes: \- name: input downwardAPI: items: \- path: "data.txt" fieldRef: fieldPath: metadata.annotations['batch.kubernetes.io/job-completion-index']Running the JobNow run the Job:# This uses the first approach (relying on $JOB_COMPLETION_INDEX) kubectl apply -f https://kubernetes.io/examples/application/job/indexed-job.yaml When you create this Job, the control plane creates a series of Pods, one for each index you specified. The value of .spec.parallelism determines how many can run at once whereas .spec.completions determines how many Pods the Job creates in total.Because .spec.parallelism is less than .spec.completions, the control plane waits for some of the first Pods to complete before starting more of them.You can wait for the Job to succeed, with a timeout:# The check for condition name is case insensitive kubectl wait --for=condition=complete --timeout=300s job/indexed-job Now, describe the Job and check that it was successful.kubectl describe jobs/indexed-job The output is similar to:Name: indexed-job Namespace: default Selector: controller-uid=bf865e04-0b67-483b-9a90-74cfc4c3e756 Labels: controller-uid=bf865e04-0b67-483b-9a90-74cfc4c3e756 job-name=indexed-job Annotations:  Parallelism: 3 Completions: 5 Start Time: Thu, 11 Mar 2021 15:47:34 +0000 Pods Statuses: 2 Running / 3 Succeeded / 0 Failed Completed Indexes: 0-2 Pod Template: Labels: controller-uid=bf865e04-0b67-483b-9a90-74cfc4c3e756 job-name=indexed-job Init Containers: input: Image: docker.io/library/bash Port:  Host Port:  Command: bash -c items=(foo bar baz qux xyz) echo ${items[$JOB_COMPLETION_INDEX]} > /input/data.txt Environment:  Mounts: /input from input (rw) Containers: worker: Image: docker.io/library/busybox Port:  Host Port:  Command: rev /input/data.txt Environment:  Mounts: /input from input (rw) Volumes: input: Type: EmptyDir (a temporary directory that shares a pod's lifetime) Medium: SizeLimit:  Events: Type Reason Age From Message \---- ------ ---- ---- ------- Normal SuccessfulCreate 4s job-controller Created pod: indexed-job-njkjj Normal SuccessfulCreate 4s job-controller Created pod: indexed-job-9kd4h Normal SuccessfulCreate 4s job-controller Created pod: indexed-job-qjwsz Normal SuccessfulCreate 1s job-controller Created pod: indexed-job-fdhq5 Normal SuccessfulCreate 1s job-controller Created pod: indexed-job-ncslj In this example, you run the Job with custom values for each index. You can inspect the output of one of the pods:kubectl logs indexed-job-fdhq5 # Change this to match the name of a Pod from that Job The output is similar to:xuq 4.9.5 - Job with Pod-to-Pod CommunicationIn this example, you will run a Job in Indexed completion mode configured such that the pods created by the Job can communicate with each other using pod hostnames rather than pod IP addresses.Pods within a Job might need to communicate among themselves. The user workload running in each pod could query the Kubernetes API server to learn the IPs of the other Pods, but it's much simpler to rely on Kubernetes' built-in DNS resolution.Jobs in Indexed completion mode automatically set the pods' hostname to be in the format of ${jobName}-${completionIndex}. You can use this format to deterministically build pod hostnames and enable pod communication without needing to create a client connection to the Kubernetes control plane to obtain pod hostnames/IPs via API requests.This configuration is useful for use cases where pod networking is required but you don't want to depend on a network connection with the Kubernetes API server.Before you beginYou should already be familiar with the basic use of Job.You need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesYour Kubernetes server must be at or later than version v1.21.To check the version, enter kubectl version.Note:If you are using minikube or a similar tool, you may need to take extra steps to ensure you have DNS.Starting a Job with pod-to-pod communicationTo enable pod-to-pod communication using pod hostnames in a Job, you must do the following:Set up a headless Service with a valid label selector for the pods created by your Job. The headless service must be in the same namespace as the Job. One easy way to do this is to use the job-name:  selector, since the job-name label will be automatically added by Kubernetes. This configuration will trigger the DNS system to create records of the hostnames of the pods running your Job.Configure the headless service as subdomain service for the Job pods by including the following value in your Job template spec:subdomain:  ExampleBelow is a working example of a Job with pod-to-pod communication via pod hostnames enabled. The Job is completed only after all pods successfully ping each other using hostnames.Note:In the Bash script executed on each pod in the example below, the pod hostnames can be prefixed by the namespace as well if the pod needs to be reached from outside the namespace.apiVersion: v1 kind: Service metadata: name: headless-svc spec: clusterIP: None # clusterIP must be None to create a headless service selector: job-name: example-job # must match Job name \--- apiVersion: batch/v1 kind: Job metadata: name: example-job spec: completions: 3 parallelism: 3 completionMode: Indexed template: spec: subdomain: headless-svc # has to match Service name restartPolicy: Never containers: \- name: example-workload image: bash:latest command: \- bash \- -c \- | for i in 0 1 2 do gotStatus="-1" wantStatus="0" while [ $gotStatus -ne $wantStatus ] do ping -c 1 example-job-${i}.headless-svc > /dev/null 2>&1 gotStatus=$? if [ $gotStatus -ne $wantStatus ]; then echo "Failed to ping pod example-job-${i}.headless-svc, retrying in 1 second..." sleep 1 fi done echo "Successfully pinged pod: example-job-${i}.headless-svc" done After applying the example above, reach each other over the network using: .. You should see output similar to the following:kubectl logs example-job-0-qws42 Failed to ping pod example-job-0.headless-svc, retrying in 1 second... Successfully pinged pod: example-job-0.headless-svc Successfully pinged pod: example-job-1.headless-svc Successfully pinged pod: example-job-2.headless-svc Note:Keep in mind that the . name format used in this example would not work with DNS policy set to None or Default. Refer to Pod's DNS Policy.4.9.6 - Parallel Processing using ExpansionsThis task demonstrates running multiple Jobs based on a common template. You can use this approach to process batches of work in parallel.For this example there are only three items: apple, banana, and cherry. The sample Jobs process each item by printing a string then pausing.See using Jobs in real workloads to learn about how this pattern fits more realistic use cases.Before you beginYou should be familiar with the basic, non-parallel, use of Job.You need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesFor basic templating you need the command-line utility sed.To follow the advanced templating example, you need a working installation of Python, and the Jinja2 template library for Python.Once you have Python set up, you can install Jinja2 by running:pip install --user jinja2 Create Jobs based on a templateFirst, download the following template of a Job to a file called job-tmpl.yaml. Here's what you'll download:application/job/job-tmpl.yaml apiVersion: batch/v1 kind: Job metadata: name: process-item-$ITEM labels: jobgroup: jobexample spec: template: metadata: name: jobexample labels: jobgroup: jobexample spec: containers: \- name: c image: busybox:1.28 command: ["sh", "-c", "echo Processing item $ITEM && sleep 5"] restartPolicy: Never # Use curl to download job-tmpl.yaml curl -L -s -O https://k8s.io/examples/application/job/job-tmpl.yaml The file you downloaded is not yet a valid Kubernetes manifest. Instead that template is a YAML representation of a Job object with some placeholders that need to be filled in before it can be used. The $ITEM syntax is not meaningful to Kubernetes.Create manifests from the templateThe following shell snippet uses sed to replace the string $ITEM with the loop variable, writing into a temporary directory named jobs. Run this now:# Expand the template into multiple files, one for each item to be processed. mkdir ./jobs for i in apple banana cherry do cat job-tmpl.yaml | sed "s/\$ITEM/$i/" > ./jobs/job-$i.yaml done Check if it worked:ls jobs/ The output is similar to this:job-apple.yaml job-banana.yaml job-cherry.yaml You could use any type of template language (for example: Jinja2; ERB), or write a program to generate the Job manifests.Create Jobs from the manifestsNext, create all the Jobs with one kubectl command:kubectl create -f ./jobs The output is similar to this:job.batch/process-item-apple created job.batch/process-item-banana created job.batch/process-item-cherry created Now, check on the jobs:kubectl get jobs -l jobgroup=jobexample The output is similar to this:NAME COMPLETIONS DURATION AGE process-item-apple 1/1 14s 22s process-item-banana 1/1 12s 21s process-item-cherry 1/1 12s 20s Using the -l option to kubectl selects only the Jobs that are part of this group of jobs (there might be other unrelated jobs in the system).You can check on the Pods as well using the same label selector:kubectl get pods -l jobgroup=jobexample The output is similar to:NAME READY STATUS RESTARTS AGE process-item-apple-kixwv 0/1 Completed 0 4m process-item-banana-wrsf7 0/1 Completed 0 4m process-item-cherry-dnfu9 0/1 Completed 0 4m We can use this single command to check on the output of all jobs at once:kubectl logs -f -l jobgroup=jobexample The output should be:Processing item apple Processing item banana Processing item cherry Clean up# Remove the Jobs you created # Your cluster automatically cleans up their Pods kubectl delete job -l jobgroup=jobexample Use advanced template parametersIn the first example, each instance of the template had one parameter, and that parameter was also used in the Job's name. However, names are restricted to contain only certain characters.This slightly more complex example uses the Jinja template language to generate manifests and then objects from those manifests, with a multiple parameters for each Job.For this part of the task, you are going to use a one-line Python script to convert the template to a set of manifests.First, copy and paste the following template of a Job object, into a file called job.yaml.jinja2:{% set params = [{ "name": "apple", "url": "http://dbpedia.org/resource/Apple", }, { "name": "banana", "url": "http://dbpedia.org/resource/Banana", }, { "name": "cherry", "url": "http://dbpedia.org/resource/Cherry" }] %} {% for p in params %} {% set name = p["name"] %} {% set url = p["url"] %} \--- apiVersion: batch/v1 kind: Job metadata: name: jobexample-{{ name }} labels: jobgroup: jobexample spec: template: metadata: name: jobexample labels: jobgroup: jobexample spec: containers: \- name: c image: busybox:1.28 command: ["sh", "-c", "echo Processing URL {{ url }} && sleep 5"] restartPolicy: Never {% endfor %} The above template defines two parameters for each Job object using a list of python dicts (lines 1-4). A for loop emits one Job manifest for each set of parameters (remaining lines).This example relies on a feature of YAML. One YAML file can contain multiple documents (Kubernetes manifests, in this case), separated by --- on a line by itself. You can pipe the output directly to kubectl to create the Jobs.Next, use this one-line Python program to expand the template:alias render_template='python -c "from jinja2 import Template; import sys; print(Template(sys.stdin.read()).render());"' Use render_template to convert the parameters and template into a single YAML file containing Kubernetes manifests:# This requires the alias you defined earlier cat job.yaml.jinja2 | render_template > jobs.yaml You can view jobs.yaml to verify that the render_template script worked correctly.Once you are happy that render_template is working how you intend, you can pipe its output into kubectl:cat job.yaml.jinja2 | render_template | kubectl apply -f - Kubernetes accepts and runs the Jobs you created.Clean up# Remove the Jobs you created # Your cluster automatically cleans up their Pods kubectl delete job -l jobgroup=jobexample Using Jobs in real workloadsIn a real use case, each Job performs some substantial computation, such as rendering a frame of a movie, or processing a range of rows in a database. If you were rendering a movie you would set $ITEM to the frame number. If you were processing rows from a database table, you would set $ITEM to represent the range of database rows to process.In the task, you ran a command to collect the output from Pods by fetching their logs. In a real use case, each Pod for a Job writes its output to durable storage before completing. You can use a PersistentVolume for each Job, or an external storage service. For example, if you are rendering frames for a movie, use HTTP to PUT the rendered frame data to a URL, using a different URL for each frame.Labels on Jobs and PodsAfter you create a Job, Kubernetes automatically adds additional labels that distinguish one Job's pods from another Job's pods.In this example, each Job and its Pod template have a label: jobgroup=jobexample.Kubernetes itself pays no attention to labels named jobgroup. Setting a label for all the Jobs you create from a template makes it convenient to operate on all those Jobs at once. In the first example you used a template to create several Jobs. The template ensures that each Pod also gets the same label, so you can check on all Pods for these templated Jobs with a single command.Note:The label key jobgroup is not special or reserved. You can pick your own labelling scheme. There are recommended labels that you can use if you wish.AlternativesIf you plan to create a large number of Job objects, you may find that:Even using labels, managing so many Jobs is cumbersome.If you create many Jobs in a batch, you might place high load on the Kubernetes control plane. Alternatively, the Kubernetes API server could rate limit you, temporarily rejecting your requests with a 429 status.You are limited by a resource quota on Jobs: the API server permanently rejects some of your requests when you create a great deal of work in one batch.There are other job patterns that you can use to process large amounts of work without creating very many Job objects.You could also consider writing your own controller to manage Job objects automatically.4.9.7 - Handling retriable and non-retriable pod failures with Pod failure policyFEATURE STATE: Kubernetes v1.31 [stable] (enabled by default: true)This document shows you how to use the Pod failure policy, in combination with the default Pod backoff failure policy, to improve the control over the handling of container- or Pod-level failure within a Job.The definition of Pod failure policy may help you to:better utilize the computational resources by avoiding unnecessary Pod retries.avoid Job failures due to Pod disruptions (such preemption, API-initiated eviction or taint-based eviction).Before you beginYou should already be familiar with the basic use of Job.You need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesYour Kubernetes server must be at or later than version v1.25.To check the version, enter kubectl version.Usage scenariosConsider the following usage scenarios for Jobs that define a Pod failure policy :Avoiding unnecessary Pod retriesIgnoring Pod disruptionsAvoiding unnecessary Pod retries based on custom Pod ConditionsAvoiding unnecessary Pod retries per indexUsing Pod failure policy to avoid unnecessary Pod retriesWith the following example, you can learn how to use Pod failure policy to avoid unnecessary Pod restarts when a Pod failure indicates a non-retriable software bug.Examine the following manifest:/controllers/job-pod-failure-policy-failjob.yaml apiVersion: batch/v1 kind: Job metadata: name: job-pod-failure-policy-failjob spec: completions: 8 parallelism: 2 template: spec: restartPolicy: Never containers: \- name: main image: docker.io/library/bash:5 command: ["bash"] args: \- -c \- echo "Hello world! I'm going to exit with 42 to simulate a software bug." && sleep 30 && exit 42 backoffLimit: 6 podFailurePolicy: rules: \- action: FailJob onExitCodes: containerName: main operator: In values: [42] Apply the manifest:kubectl create -f https://k8s.io/examples/controllers/job-pod-failure-policy-failjob.yaml After around 30 seconds the entire Job should be terminated. Inspect the status of the Job by running:kubectl get jobs -l job-name=job-pod-failure-policy-failjob -o yaml In the Job status, the following conditions display:FailureTarget condition: has a reason field set to PodFailurePolicy and a message field with more information about the termination, like Container main for pod default/job-pod-failure-policy-failjob-8ckj8 failed with exit code 42 matching FailJob rule at index 0. The Job controller adds this condition as soon as the Job is considered a failure. For details, see Termination of Job Pods.Failed condition: same reason and message as the FailureTarget condition. The Job controller adds this condition after all of the Job's Pods are terminated.For comparison, if the Pod failure policy was disabled it would take 6 retries of the Pod, taking at least 2 minutes.Clean upDelete the Job you created:kubectl delete jobs/job-pod-failure-policy-failjob The cluster automatically cleans up the Pods.Using Pod failure policy to ignore Pod disruptionsWith the following example, you can learn how to use Pod failure policy to ignore Pod disruptions from incrementing the Pod retry counter towards the .spec.backoffLimit limit.Caution:Timing is important for this example, so you may want to read the steps before execution. In order to trigger a Pod disruption it is important to drain the node while the Pod is running on it (within 90s since the Pod is scheduled).Examine the following manifest:/controllers/job-pod-failure-policy-ignore.yaml apiVersion: batch/v1 kind: Job metadata: name: job-pod-failure-policy-ignore spec: completions: 4 parallelism: 2 template: spec: restartPolicy: Never containers: \- name: main image: docker.io/library/bash:5 command: ["bash"] args: \- -c \- echo "Hello world! I'm going to exit with 0 (success)." && sleep 90 && exit 0 backoffLimit: 0 podFailurePolicy: rules: \- action: Ignore onPodConditions: \- type: DisruptionTarget Apply the manifest:kubectl create -f https://k8s.io/examples/controllers/job-pod-failure-policy-ignore.yaml Run this command to check the nodeName the Pod is scheduled to:nodeName=$(kubectl get pods -l job-name=job-pod-failure-policy-ignore -o jsonpath='{.items[0].spec.nodeName}') Drain the node to evict the Pod before it completes (within 90s):kubectl drain nodes/$nodeName --ignore-daemonsets --grace-period=0 Inspect the .status.failed to check the counter for the Job is not incremented:kubectl get jobs -l job-name=job-pod-failure-policy-ignore -o yaml Uncordon the node:kubectl uncordon nodes/$nodeName The Job resumes and succeeds.For comparison, if the Pod failure policy was disabled the Pod disruption would result in terminating the entire Job (as the .spec.backoffLimit is set to 0).Cleaning upDelete the Job you created:kubectl delete jobs/job-pod-failure-policy-ignore The cluster automatically cleans up the Pods.Using Pod failure policy to avoid unnecessary Pod retries based on custom Pod ConditionsWith the following example, you can learn how to use Pod failure policy to avoid unnecessary Pod restarts based on custom Pod Conditions.Note:The example below works since version 1.27 as it relies on transitioning of deleted pods, in the Pending phase, to a terminal phase (see: Pod Phase).Examine the following manifest:/controllers/job-pod-failure-policy-config-issue.yaml apiVersion: batch/v1 kind: Job metadata: name: job-pod-failure-policy-config-issue spec: completions: 8 parallelism: 2 template: spec: restartPolicy: Never containers: \- name: main image: "non-existing-repo/non-existing-image:example" backoffLimit: 6 podFailurePolicy: rules: \- action: FailJob onPodConditions: \- type: ConfigIssue Apply the manifest:kubectl create -f https://k8s.io/examples/controllers/job-pod-failure-policy-config-issue.yaml Note that, the image is misconfigured, as it does not exist.Inspect the status of the job's Pods by running:kubectl get pods -l job-name=job-pod-failure-policy-config-issue -o yaml You will see output similar to this:containerStatuses: \- image: non-existing-repo/non-existing-image:example ... state: waiting: message: Back-off pulling image "non-existing-repo/non-existing-image:example" reason: ImagePullBackOff ... phase: Pending Note that the pod remains in the Pending phase as it fails to pull the misconfigured image. This, in principle, could be a transient issue and the image could get pulled. However, in this case, the image does not exist so we indicate this fact by a custom condition.Add the custom condition. First prepare the patch by running:cat < patch.yaml status: conditions: \- type: ConfigIssue status: "True" reason: "NonExistingImage" lastTransitionTime: "$(date -u +"%Y-%m-%dT%H:%M:%SZ")" EOF Second, select one of the pods created by the job by running:podName=$(kubectl get pods -l job-name=job-pod-failure-policy-config-issue -o jsonpath='{.items[0].metadata.name}') Then, apply the patch on one of the pods by running the following command:kubectl patch pod $podName --subresource=status --patch-file=patch.yaml If applied successfully, you will get a notification like this:pod/job-pod-failure-policy-config-issue-k6pvp patched Delete the pod to transition it to Failed phase, by running the command:kubectl delete pods/$podName Inspect the status of the Job by running:kubectl get jobs -l job-name=job-pod-failure-policy-config-issue -o yaml In the Job status, see a job Failed condition with the field reason equal PodFailurePolicy. Additionally, the message field contains a more detailed information about the Job termination, such as: Pod default/job-pod-failure-policy-config-issue-k6pvp has condition ConfigIssue matching FailJob rule at index 0.Note:In a production environment, the steps 3 and 4 should be automated by a user-provided controller.Cleaning upDelete the Job you created:kubectl delete jobs/job-pod-failure-policy-config-issue The cluster automatically cleans up the Pods.Using Pod Failure Policy to avoid unnecessary Pod retries per indexTo avoid unnecessary Pod restarts per index, you can use the Pod failure policy and backoff limit per index features. This section of the page shows how to use these features together.Examine the following manifest:/controllers/job-backoff-limit-per-index-failindex.yaml apiVersion: batch/v1 kind: Job metadata: name: job-backoff-limit-per-index-failindex spec: completions: 4 parallelism: 2 completionMode: Indexed backoffLimitPerIndex: 1 template: spec: restartPolicy: Never containers: \- name: main image: docker.io/library/python:3 command: # The script: # - fails the Pod with index 0 with exit code 1, which results in one retry; # - fails the Pod with index 1 with exit code 42 which results # in failing the index without retry. # - succeeds Pods with any other index. \- python3 \- -c \- | import os, sys index = int(os.environ.get("JOB_COMPLETION_INDEX")) if index == 0: sys.exit(1) elif index == 1: sys.exit(42) else: sys.exit(0) backoffLimit: 6 podFailurePolicy: rules: \- action: FailIndex onExitCodes: containerName: main operator: In values: [42] Apply the manifest:kubectl create -f https://k8s.io/examples/controllers/job-backoff-limit-per-index-failindex.yaml After around 15 seconds, inspect the status of the Pods for the Job. You can do that by running:kubectl get pods -l job-name=job-backoff-limit-per-index-failindex -o yaml You will see output similar to this:NAME READY STATUS RESTARTS AGE job-backoff-limit-per-index-failindex-0-4g4cm 0/1 Error 0 4s job-backoff-limit-per-index-failindex-0-fkdzq 0/1 Error 0 15s job-backoff-limit-per-index-failindex-1-2bgdj 0/1 Error 0 15s job-backoff-limit-per-index-failindex-2-vs6lt 0/1 Completed 0 11s job-backoff-limit-per-index-failindex-3-s7s47 0/1 Completed 0 6s Note that the output shows the following:Two Pods have index 0, because of the backoff limit allowed for one retry of the index.Only one Pod has index 1, because the exit code of the failed Pod matched the Pod failure policy with the FailIndex action.Inspect the status of the Job by running:kubectl get jobs -l job-name=job-backoff-limit-per-index-failindex -o yaml In the Job status, see that the failedIndexes field shows "0,1", because both indexes failed. Because the index 1 was not retried the number of failed Pods, indicated by the status field "failed" equals 3.Cleaning upDelete the Job you created:kubectl delete jobs/job-backoff-limit-per-index-failindex The cluster automatically cleans up the Pods.AlternativesYou could rely solely on the Pod backoff failure policy, by specifying the Job's .spec.backoffLimit field. However, in many situations it is problematic to find a balance between setting a low value for .spec.backoffLimit to avoid unnecessary Pod retries, yet high enough to make sure the Job would not be terminated by Pod disruptions.4.10 - Access Applications in a ClusterConfigure load balancing, port forwarding, or setup firewall or DNS configurations to access applications in a cluster.4.10.1 - Deploy and Access the Kubernetes DashboardDeploy the web UI (Kubernetes Dashboard) and access it.Dashboard is a web-based Kubernetes user interface. You can use Dashboard to deploy containerized applications to a Kubernetes cluster, troubleshoot your containerized application, and manage the cluster resources. You can use Dashboard to get an overview of applications running on your cluster, as well as for creating or modifying individual Kubernetes resources (such as Deployments, Jobs, DaemonSets, etc). For example, you can scale a Deployment, initiate a rolling update, restart a pod or deploy new applications using a deploy wizard.Dashboard also provides information on the state of Kubernetes resources in your cluster and on any errors that may have occurred.Deploying the Dashboard UINote:Kubernetes Dashboard supports only Helm-based installation currently as it is faster and gives us better control over all dependencies required by Dashboard to run.The Dashboard UI is not deployed by default. To deploy it, run the following command:# Add kubernetes-dashboard repository helm repo add kubernetes-dashboard https://kubernetes.github.io/dashboard/ # Deploy a Helm Release named "kubernetes-dashboard" using the kubernetes-dashboard chart helm upgrade --install kubernetes-dashboard kubernetes-dashboard/kubernetes-dashboard --create-namespace --namespace kubernetes-dashboard Accessing the Dashboard UITo protect your cluster data, Dashboard deploys with a minimal RBAC configuration by default. Currently, Dashboard only supports logging in with a Bearer Token. To create a token for this demo, you can follow our guide on creating a sample user.Warning:The sample user created in the tutorial will have administrative privileges and is for educational purposes only.Command line proxyYou can enable access to the Dashboard using the kubectl command-line tool, by running the following command:kubectl -n kubernetes-dashboard port-forward svc/kubernetes-dashboard-kong-proxy 8443:443 Kubectl will make Dashboard available at https://localhost:8443.The UI can only be accessed from the machine where the command is executed. See kubectl port-forward --help for more options.Note:The kubeconfig authentication method does not support external identity providers or X.509 certificate-based authentication.Welcome viewWhen you access Dashboard on an empty cluster, you'll see the welcome page. This page contains a link to this document as well as a button to deploy your first application. In addition, you can view which system applications are running by default in the kube-system namespace of your cluster, for example the Dashboard itself.Deploying containerized applicationsDashboard lets you create and deploy a containerized application as a Deployment and optional Service with a simple wizard. You can either manually specify application details, or upload a YAML or JSON manifest file containing application configuration.Click the CREATE button in the upper right corner of any page to begin.Specifying application detailsThe deploy wizard expects that you provide the following information:App name (mandatory): Name for your application. A label with the name will be added to the Deployment and Service, if any, that will be deployed.The application name must be unique within the selected Kubernetes namespace. It must start with a lowercase character, and end with a lowercase character or a number, and contain only lowercase letters, numbers and dashes (-). It is limited to 24 characters. Leading and trailing spaces are ignored.Container image (mandatory): The URL of a public Docker container image on any registry, or a private image (commonly hosted on the Google Container Registry or Docker Hub). The container image specification must end with a colon.Number of pods (mandatory): The target number of Pods you want your application to be deployed in. The value must be a positive integer.A Deployment will be created to maintain the desired number of Pods across your cluster.Service (optional): For some parts of your application (e.g. frontends) you may want to expose a Service onto an external, maybe public IP address outside of your cluster (external Service).Note:For external Services, you may need to open up one or more ports to do so.Other Services that are only visible from inside the cluster are called internal Services.Irrespective of the Service type, if you choose to create a Service and your container listens on a port (incoming), you need to specify two ports. The Service will be created mapping the port (incoming) to the target port seen by the container. This Service will route to your deployed Pods. Supported protocols are TCP and UDP. The internal DNS name for this Service will be the value you specified as application name above.If needed, you can expand the Advanced options section where you can specify more settings:Description: The text you enter here will be added as an annotation to the Deployment and displayed in the application's details.Labels: Default labels to be used for your application are application name and version. You can specify additional labels to be applied to the Deployment, Service (if any), and Pods, such as release, environment, tier, partition, and release track.Example:release=1.0 tier=frontend environment=pod track=stable Namespace: Kubernetes supports multiple virtual clusters backed by the same physical cluster. These virtual clusters are called namespaces. They let you partition resources into logically named groups.Dashboard offers all available namespaces in a dropdown list, and allows you to create a new namespace. The namespace name may contain a maximum of 63 alphanumeric characters and dashes (-) but can not contain capital letters. Namespace names should not consist of only numbers. If the name is set as a number, such as 10, the pod will be put in the default namespace.In case the creation of the namespace is successful, it is selected by default. If the creation fails, the first namespace is selected.Image Pull Secret: In case the specified Docker container image is private, it may require pull secret credentials.Dashboard offers all available secrets in a dropdown list, and allows you to create a new secret. The secret name must follow the DNS domain name syntax, for example new.image-pull.secret. The content of a secret must be base64-encoded and specified in a .dockercfg file. The secret name may consist of a maximum of 253 characters.In case the creation of the image pull secret is successful, it is selected by default. If the creation fails, no secret is applied.CPU requirement (cores) and Memory requirement (MiB): You can specify the minimum resource limits for the container. By default, Pods run with unbounded CPU and memory limits.Run command and Run command arguments: By default, your containers run the specified Docker image's default entrypoint command. You can use the command options and arguments to override the default.Run as privileged: This setting determines whether processes in privileged containers are equivalent to processes running as root on the host. Privileged containers can make use of capabilities like manipulating the network stack and accessing devices.Environment variables: Kubernetes exposes Services through environment variables. You can compose environment variable or pass arguments to your commands using the values of environment variables. They can be used in applications to find a Service. Values can reference other variables using the $(VAR_NAME) syntax.Uploading a YAML or JSON fileKubernetes supports declarative configuration. In this style, all configuration is stored in manifests (YAML or JSON configuration files). The manifests use Kubernetes API resource schemas.As an alternative to specifying application details in the deploy wizard, you can define your application in one or more manifests, and upload the files using Dashboard.Using DashboardFollowing sections describe views of the Kubernetes Dashboard UI; what they provide and how can they be used.NavigationWhen there are Kubernetes objects defined in the cluster, Dashboard shows them in the initial view. By default only objects from the default namespace are shown and this can be changed using the namespace selector located in the navigation menu.Dashboard shows most Kubernetes object kinds and groups them in a few menu categories.Admin overviewFor cluster and namespace administrators, Dashboard lists Nodes, Namespaces and PersistentVolumes and has detail views for them. Node list view contains CPU and memory usage metrics aggregated across all Nodes. The details view shows the metrics for a Node, its specification, status, allocated resources, events and pods running on the node.WorkloadsShows all applications running in the selected namespace. The view lists applications by workload kind (for example: Deployments, ReplicaSets, StatefulSets). Each workload kind can be viewed separately. The lists summarize actionable information about the workloads, such as the number of ready pods for a ReplicaSet or current memory usage for a Pod.Detail views for workloads show status and specification information and surface relationships between objects. For example, Pods that ReplicaSet is controlling or new ReplicaSets and HorizontalPodAutoscalers for Deployments.ServicesShows Kubernetes resources that allow for exposing services to external world and discovering them within a cluster. For that reason, Service and Ingress views show Pods targeted by them, internal endpoints for cluster connections and external endpoints for external users.StorageStorage view shows PersistentVolumeClaim resources which are used by applications for storing data.ConfigMaps and SecretsShows all Kubernetes resources that are used for live configuration of applications running in clusters. The view allows for editing and managing config objects and displays secrets hidden by default.Logs viewerPod lists and detail pages link to a logs viewer that is built into Dashboard. The viewer allows for drilling down logs from containers belonging to a single Pod.What's nextFor more information, see the Kubernetes Dashboard project page.4.10.2 - Accessing ClustersThis topic discusses multiple ways to interact with clusters.Accessing for the first time with kubectlWhen accessing the Kubernetes API for the first time, we suggest using the Kubernetes CLI, kubectl.To access a cluster, you need to know the location of the cluster and have credentials to access it. Typically, this is automatically set-up when you work through a Getting started guide, or someone else set up the cluster and provided you with credentials and a location.Check the location and credentials that kubectl knows about with this command:kubectl config view Many of the examples provide an introduction to using kubectl, and complete documentation is found in the kubectl reference.Directly accessing the REST APIKubectl handles locating and authenticating to the apiserver. If you want to directly access the REST API with an http client like curl or wget, or a browser, there are several ways to locate and authenticate:Run kubectl in proxy mode.Recommended approach.Uses stored apiserver location.Verifies identity of apiserver using self-signed cert. No MITM possible.Authenticates to apiserver.In future, may do intelligent client-side load-balancing and failover.Provide the location and credentials directly to the http client.Alternate approach.Works with some types of client code that are confused by using a proxy.Need to import a root cert into your browser to protect against MITM.Using kubectl proxyThe following command runs kubectl in a mode where it acts as a reverse proxy. It handles locating the apiserver and authenticating. Run it like this:kubectl proxy --port=8080 See kubectl proxy for more details.Then you can explore the API with curl, wget, or a browser, replacing localhost with [::1] for IPv6, like so:curl http://localhost:8080/api/ The output is similar to this:{ "kind": "APIVersions", "versions": [ "v1" ], "serverAddressByClientCIDRs": [ { "clientCIDR": "0.0.0.0/0", "serverAddress": "10.0.1.149:443" } ] } Without kubectl proxyUse kubectl apply and kubectl describe secret... to create a token for the default service account with grep/cut:First, create the Secret, requesting a token for the default ServiceAccount:kubectl apply -f - </dev/null; do echo "waiting for token..." >&2 sleep 1 done Capture and use the generated token:APISERVER=$(kubectl config view --minify | grep server | cut -f 2- -d ":" | tr -d " ") TOKEN=$(kubectl describe secret default-token | grep -E '^token' | cut -f2 -d':' | tr -d " ") curl $APISERVER/api --header "Authorization: Bearer $TOKEN" --insecure The output is similar to this:{ "kind": "APIVersions", "versions": [ "v1" ], "serverAddressByClientCIDRs": [ { "clientCIDR": "0.0.0.0/0", "serverAddress": "10.0.1.149:443" } ] } Using jsonpath:APISERVER=$(kubectl config view --minify -o jsonpath='{.clusters[0].cluster.server}') TOKEN=$(kubectl get secret default-token -o jsonpath='{.data.token}' | base64 --decode) curl $APISERVER/api --header "Authorization: Bearer $TOKEN" --insecure The output is similar to this:{ "kind": "APIVersions", "versions": [ "v1" ], "serverAddressByClientCIDRs": [ { "clientCIDR": "0.0.0.0/0", "serverAddress": "10.0.1.149:443" } ] } The above examples use the --insecure flag. This leaves it subject to MITM attacks. When kubectl accesses the cluster it uses a stored root certificate and client certificates to access the server. (These are installed in the ~/.kube directory). Since cluster certificates are typically self-signed, it may take special configuration to get your http client to use root certificate.On some clusters, the apiserver does not require authentication; it may serve on localhost, or be protected by a firewall. There is not a standard for this. Controlling Access to the API describes how a cluster admin can configure this.Programmatic access to the APIKubernetes officially supports Go and Python client libraries.Go clientTo get the library, run the following command: go get k8s.io/client-go@kubernetes-, see INSTALL.md for detailed installation instructions. See https://github.com/kubernetes/client-go to see which versions are supported.Write an application atop of the client-go clients. Note that client-go defines its own API objects, so if needed, please import API definitions from client-go rather than from the main repository, e.g., import "k8s.io/client-go/kubernetes" is correct.The Go client can use the same kubeconfig file as the kubectl CLI does to locate and authenticate to the apiserver. See this example.If the application is deployed as a Pod in the cluster, please refer to the next section.Python clientTo use Python client, run the following command: pip install kubernetes. See Python Client Library page for more installation options.The Python client can use the same kubeconfig file as the kubectl CLI does to locate and authenticate to the apiserver. See this example.Other languagesThere are client libraries for accessing the API from other languages. See documentation for other libraries for how they authenticate.Accessing the API from a PodWhen accessing the API from a pod, locating and authenticating to the API server are somewhat different.Please check Accessing the API from within a Pod for more details.Accessing services running on the clusterThe previous section describes how to connect to the Kubernetes API server. For information about connecting to other services running on a Kubernetes cluster, see Access Cluster Services.Requesting redirectsThe redirect capabilities have been deprecated and removed. Please use a proxy (see below) instead.So many proxiesThere are several different proxies you may encounter when using Kubernetes:The kubectl proxy:runs on a user's desktop or in a podproxies from a localhost address to the Kubernetes apiserverclient to proxy uses HTTPproxy to apiserver uses HTTPSlocates apiserveradds authentication headersThe apiserver proxy:is a bastion built into the apiserverconnects a user outside of the cluster to cluster IPs which otherwise might not be reachableruns in the apiserver processesclient to proxy uses HTTPS (or http if apiserver so configured)proxy to target may use HTTP or HTTPS as chosen by proxy using available informationcan be used to reach a Node, Pod, or Servicedoes load balancing when used to reach a ServiceThe kube proxy:runs on each nodeproxies UDP and TCPdoes not understand HTTPprovides load balancingis only used to reach servicesA Proxy/Load-balancer in front of apiserver(s):existence and implementation varies from cluster to cluster (e.g. nginx)sits between all clients and one or more apiserversacts as load balancer if there are several apiservers.Cloud Load Balancers on external services:are provided by some cloud providers (e.g. AWS ELB, Google Cloud Load Balancer)are created automatically when the Kubernetes service has type LoadBalanceruse UDP/TCP onlyimplementation varies by cloud provider.Kubernetes users will typically not need to worry about anything other than the first two types. The cluster admin will typically ensure that the latter types are set up correctly.4.10.3 - Configure Access to Multiple ClustersThis page shows how to configure access to multiple clusters by using configuration files. After your clusters, users, and contexts are defined in one or more configuration files, you can quickly switch between clusters by using the kubectl config use-context command.Note:A file that is used to configure access to a cluster is sometimes called a kubeconfig file. This is a generic way of referring to configuration files. It does not mean that there is a file named kubeconfig.Warning:Only use kubeconfig files from trusted sources. Using a specially-crafted kubeconfig file could result in malicious code execution or file exposure. If you must use an untrusted kubeconfig file, inspect it carefully first, much as you would a shell script.Before you beginYou need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesTo check that kubectl is installed, run kubectl version --client. The kubectl version should be within one minor version of your cluster's API server.Define clusters, users, and contextsSuppose you have two clusters, one for development work and one for test work. In the development cluster, your frontend developers work in a namespace called frontend, and your storage developers work in a namespace called storage. In your test cluster, developers work in the default namespace, or they create auxiliary namespaces as they see fit. Access to the development cluster requires authentication by certificate. Access to the test cluster requires authentication by username and password.Create a directory named config-exercise. In your config-exercise directory, create a file named config-demo with this content:apiVersion: v1 kind: Config preferences: {} clusters: \- cluster: name: development \- cluster: name: test users: \- name: developer \- name: experimenter contexts: \- context: name: dev-frontend \- context: name: dev-storage \- context: name: exp-test A configuration file describes clusters, users, and contexts. Your config-demo file has the framework to describe two clusters, two users, and three contexts.Go to your config-exercise directory. Enter these commands to add cluster details to your configuration file:kubectl config --kubeconfig=config-demo set-cluster development --server=https://1.2.3.4 --certificate-authority=fake-ca-file kubectl config --kubeconfig=config-demo set-cluster test --server=https://5.6.7.8 --insecure-skip-tls-verify Add user details to your configuration file:Caution:Storing passwords in Kubernetes client config is risky. A better alternative would be to use a credential plugin and store them separately. See: client-go credential pluginskubectl config --kubeconfig=config-demo set-credentials developer --client-certificate=fake-cert-file --client-key=fake-key-seefile kubectl config --kubeconfig=config-demo set-credentials experimenter --username=exp --password=some-password Note:To delete a user you can run kubectl --kubeconfig=config-demo config unset users.To remove a cluster, you can run kubectl --kubeconfig=config-demo config unset clusters.To remove a context, you can run kubectl --kubeconfig=config-demo config unset contexts.Add context details to your configuration file:kubectl config --kubeconfig=config-demo set-context dev-frontend --cluster=development --namespace=frontend --user=developer kubectl config --kubeconfig=config-demo set-context dev-storage --cluster=development --namespace=storage --user=developer kubectl config --kubeconfig=config-demo set-context exp-test --cluster=test --namespace=default --user=experimenter Open your config-demo file to see the added details. As an alternative to opening the config-demo file, you can use the config view command.kubectl config --kubeconfig=config-demo view The output shows the two clusters, two users, and three contexts:apiVersion: v1 clusters: \- cluster: certificate-authority: fake-ca-file server: https://1.2.3.4 name: development \- cluster: insecure-skip-tls-verify: true server: https://5.6.7.8 name: test contexts: \- context: cluster: development namespace: frontend user: developer name: dev-frontend \- context: cluster: development namespace: storage user: developer name: dev-storage \- context: cluster: test namespace: default user: experimenter name: exp-test current-context: "" kind: Config preferences: {} users: \- name: developer user: client-certificate: fake-cert-file client-key: fake-key-file \- name: experimenter user: # Documentation note (this comment is NOT part of the command output). # Storing passwords in Kubernetes client config is risky. # A better alternative would be to use a credential plugin # and store the credentials separately. # See https://kubernetes.io/docs/reference/access-authn-authz/authentication/#client-go-credential-plugins password: some-password username: exp The fake-ca-file, fake-cert-file and fake-key-file above are the placeholders for the pathnames of the certificate files. You need to change these to the actual pathnames of certificate files in your environment.Sometimes you may want to use Base64-encoded data embedded here instead of separate certificate files; in that case you need to add the suffix -data to the keys, for example, certificate-authority-data, client-certificate-data, client-key-data.Each context is a triple (cluster, user, namespace). For example, the dev-frontend context says, "Use the credentials of the developer user to access the frontend namespace of the development cluster".Set the current context:kubectl config --kubeconfig=config-demo use-context dev-frontend Now whenever you enter a kubectl command, the action will apply to the cluster, and namespace listed in the dev-frontend context. And the command will use the credentials of the user listed in the dev-frontend context.To see only the configuration information associated with the current context, use the --minify flag.kubectl config --kubeconfig=config-demo view --minify The output shows configuration information associated with the dev-frontend context:apiVersion: v1 clusters: \- cluster: certificate-authority: fake-ca-file server: https://1.2.3.4 name: development contexts: \- context: cluster: development namespace: frontend user: developer name: dev-frontend current-context: dev-frontend kind: Config preferences: {} users: \- name: developer user: client-certificate: fake-cert-file client-key: fake-key-file Now suppose you want to work for a while in the test cluster.Change the current context to exp-test:kubectl config --kubeconfig=config-demo use-context exp-test Now any kubectl command you give will apply to the default namespace of the test cluster. And the command will use the credentials of the user listed in the exp-test context.View configuration associated with the new current context, exp-test.kubectl config --kubeconfig=config-demo view --minify Finally, suppose you want to work for a while in the storage namespace of the development cluster.Change the current context to dev-storage:kubectl config --kubeconfig=config-demo use-context dev-storage View configuration associated with the new current context, dev-storage.kubectl config --kubeconfig=config-demo view --minify Create a second configuration fileIn your config-exercise directory, create a file named config-demo-2 with this content:apiVersion: v1 kind: Config preferences: {} contexts: \- context: cluster: development namespace: ramp user: developer name: dev-ramp-up The preceding configuration file defines a new context named dev-ramp-up.Set the KUBECONFIG environment variableSee whether you have an environment variable named KUBECONFIG. If so, save the current value of your KUBECONFIG environment variable, so you can restore it later. For example:Linuxexport KUBECONFIG_SAVED="$KUBECONFIG" Windows PowerShell$Env:KUBECONFIG_SAVED=$ENV:KUBECONFIG The KUBECONFIG environment variable is a list of paths to configuration files. The list is colon-delimited for Linux and Mac, and semicolon-delimited for Windows. If you have a KUBECONFIG environment variable, familiarize yourself with the configuration files in the list.Temporarily append two paths to your KUBECONFIG environment variable. For example:Linuxexport KUBECONFIG="${KUBECONFIG}:config-demo:config-demo-2" Windows PowerShell$Env:KUBECONFIG=("config-demo;config-demo-2") In your config-exercise directory, enter this command:kubectl config view The output shows merged information from all the files listed in your KUBECONFIG environment variable. In particular, notice that the merged information has the dev-ramp-up context from the config-demo-2 file and the three contexts from the config-demo file:contexts: \- context: cluster: development namespace: frontend user: developer name: dev-frontend \- context: cluster: development namespace: ramp user: developer name: dev-ramp-up \- context: cluster: development namespace: storage user: developer name: dev-storage \- context: cluster: test namespace: default user: experimenter name: exp-test For more information about how kubeconfig files are merged, see Organizing Cluster Access Using kubeconfig FilesExplore the $HOME/.kube directoryIf you already have a cluster, and you can use kubectl to interact with the cluster, then you probably have a file named config in the $HOME/.kube directory.Go to $HOME/.kube, and see what files are there. Typically, there is a file named config. There might also be other configuration files in this directory. Briefly familiarize yourself with the contents of these files.Append $HOME/.kube/config to your KUBECONFIG environment variableIf you have a $HOME/.kube/config file, and it's not already listed in your KUBECONFIG environment variable, append it to your KUBECONFIG environment variable now. For example:Linuxexport KUBECONFIG="${KUBECONFIG}:${HOME}/.kube/config" Windows Powershell$Env:KUBECONFIG="$Env:KUBECONFIG;$HOME\\.kube\config" View configuration information merged from all the files that are now listed in your KUBECONFIG environment variable. In your config-exercise directory, enter:kubectl config view Clean upReturn your KUBECONFIG environment variable to its original value. For example:Linuxexport KUBECONFIG="$KUBECONFIG_SAVED" Windows PowerShell$Env:KUBECONFIG=$ENV:KUBECONFIG_SAVED Check the subject represented by the kubeconfigIt is not always obvious what attributes (username, groups) you will get after authenticating to the cluster. It can be even more challenging if you are managing more than one cluster at the same time.There is a kubectl subcommand to check subject attributes, such as username, for your selected Kubernetes client context: kubectl auth whoami.Read API access to authentication information for a client to learn about this in more detail.What's nextOrganizing Cluster Access Using kubeconfig Fileskubectl config4.10.4 - Use Port Forwarding to Access Applications in a ClusterThis page shows how to use kubectl port-forward to connect to a MongoDB server running in a Kubernetes cluster. This type of connection can be useful for database debugging.Before you beginYou need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesYour Kubernetes server must be at or later than version v1.10.To check the version, enter kubectl version.Install MongoDB Shell.Creating MongoDB deployment and serviceCreate a Deployment that runs MongoDB:kubectl apply -f https://k8s.io/examples/application/mongodb/mongo-deployment.yaml The output of a successful command verifies that the deployment was created:deployment.apps/mongo created View the pod status to check that it is ready:kubectl get pods The output displays the pod created:NAME READY STATUS RESTARTS AGE mongo-75f59d57f4-4nd6q 1/1 Running 0 2m4s View the Deployment's status:kubectl get deployment The output displays that the Deployment was created:NAME READY UP-TO-DATE AVAILABLE AGE mongo 1/1 1 1 2m21s The Deployment automatically manages a ReplicaSet. View the ReplicaSet status using:kubectl get replicaset The output displays that the ReplicaSet was created:NAME DESIRED CURRENT READY AGE mongo-75f59d57f4 1 1 1 3m12s Create a Service to expose MongoDB on the network:kubectl apply -f https://k8s.io/examples/application/mongodb/mongo-service.yaml The output of a successful command verifies that the Service was created:service/mongo created Check the Service created:kubectl get service mongo The output displays the service created:NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE mongo ClusterIP 10.96.41.183  27017/TCP 11s Verify that the MongoDB server is running in the Pod, and listening on port 27017:# Change mongo-75f59d57f4-4nd6q to the name of the Pod kubectl get pod mongo-75f59d57f4-4nd6q --template='{{(index (index .spec.containers 0).ports 0).containerPort}}{{"\n"}}' The output displays the port for MongoDB in that Pod:27017 27017 is the official TCP port for MongoDB.Forward a local port to a port on the Podkubectl port-forward allows using resource name, such as a pod name, to select a matching pod to port forward to.# Change mongo-75f59d57f4-4nd6q to the name of the Pod kubectl port-forward mongo-75f59d57f4-4nd6q 28015:27017 which is the same askubectl port-forward pods/mongo-75f59d57f4-4nd6q 28015:27017 orkubectl port-forward deployment/mongo 28015:27017 orkubectl port-forward replicaset/mongo-75f59d57f4 28015:27017 orkubectl port-forward service/mongo 28015:27017 Any of the above commands works. The output is similar to this:Forwarding from 127.0.0.1:28015 -> 27017 Forwarding from [::1]:28015 -> 27017 Note:kubectl port-forward does not return. To continue with the exercises, you will need to open another terminal.Start the MongoDB command line interface:mongosh --port 28015 At the MongoDB command line prompt, enter the ping command:db.runCommand( { ping: 1 } ) A successful ping request returns:{ ok: 1 } Optionally let kubectl choose the local portIf you don't need a specific local port, you can let kubectl choose and allocate the local port and thus relieve you from having to manage local port conflicts, with the slightly simpler syntax:kubectl port-forward deployment/mongo :27017 The kubectl tool finds a local port number that is not in use (avoiding low ports numbers, because these might be used by other applications). The output is similar to:Forwarding from 127.0.0.1:63753 -> 27017 Forwarding from [::1]:63753 -> 27017 DiscussionConnections made to local port 28015 are forwarded to port 27017 of the Pod that is running the MongoDB server. With this connection in place, you can use your local workstation to debug the database that is running in the Pod.Note:kubectl port-forward is implemented for TCP ports only. The support for UDP protocol is tracked in issue 47862.What's nextLearn more about kubectl port-forward.4.10.5 - Use a Service to Access an Application in a ClusterThis page shows how to create a Kubernetes Service object that external clients can use to access an application running in a cluster. The Service provides load balancing for an application that has two running instances.Before you beginYou need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesObjectivesRun two instances of a Hello World application.Create a Service object that exposes a node port.Use the Service object to access the running application.Creating a service for an application running in two podsHere is the configuration file for the application Deployment:service/access/hello-application.yaml apiVersion: apps/v1 kind: Deployment metadata: name: hello-world spec: selector: matchLabels: run: load-balancer-example replicas: 2 template: metadata: labels: run: load-balancer-example spec: containers: \- name: hello-world image: us-docker.pkg.dev/google-samples/containers/gke/hello-app:2.0 ports: \- containerPort: 8080 protocol: TCP Run a Hello World application in your cluster: Create the application Deployment using the file above:kubectl apply -f https://k8s.io/examples/service/access/hello-application.yaml The preceding command creates a Deployment and an associated ReplicaSet. The ReplicaSet has two Pods each of which runs the Hello World application.Display information about the Deployment:kubectl get deployments hello-world kubectl describe deployments hello-world Display information about your ReplicaSet objects:kubectl get replicasets kubectl describe replicasets Create a Service object that exposes the deployment:kubectl expose deployment hello-world --type=NodePort --name=example-service Display information about the Service:kubectl describe services example-service The output is similar to this:Name: example-service Namespace: default Labels: run=load-balancer-example Annotations:  Selector: run=load-balancer-example Type: NodePort IP: 10.32.0.16 Port:  8080/TCP TargetPort: 8080/TCP NodePort:  31496/TCP Endpoints: 10.200.1.4:8080,10.200.2.5:8080 Session Affinity: None Events:  Make a note of the NodePort value for the Service. For example, in the preceding output, the NodePort value is 31496.List the pods that are running the Hello World application:kubectl get pods --selector="run=load-balancer-example" --output=wide The output is similar to this:NAME READY STATUS ... IP NODE hello-world-2895499144-bsbk5 1/1 Running ... 10.200.1.4 worker1 hello-world-2895499144-m1pwt 1/1 Running ... 10.200.2.5 worker2 Get the public IP address of one of your nodes that is running a Hello World pod. How you get this address depends on how you set up your cluster. For example, if you are using Minikube, you can see the node address by running kubectl cluster-info. If you are using Google Compute Engine instances, you can use the gcloud compute instances list command to see the public addresses of your nodes.On your chosen node, create a firewall rule that allows TCP traffic on your node port. For example, if your Service has a NodePort value of 31568, create a firewall rule that allows TCP traffic on port 31568. Different cloud providers offer different ways of configuring firewall rules.Use the node address and node port to access the Hello World application:curl http://: where  is the public IP address of your node, and  is the NodePort value for your service. The response to a successful request is a hello message:Hello, world! Version: 2.0.0 Hostname: hello-world-cdd4458f4-m47c8 Using a service configuration fileAs an alternative to using kubectl expose, you can use a service configuration file to create a Service.Cleaning upTo delete the Service, enter this command:kubectl delete services example-service To delete the Deployment, the ReplicaSet, and the Pods that are running the Hello World application, enter this command:kubectl delete deployment hello-world What's nextFollow the Connecting Applications with Services tutorial.4.10.6 - Connect a Frontend to a Backend Using ServicesThis task shows how to create a frontend and a backend microservice. The backend microservice is a hello greeter. The frontend exposes the backend using nginx and a Kubernetes Service object.ObjectivesCreate and run a sample hello backend microservice using a Deployment object.Use a Service object to send traffic to the backend microservice's multiple replicas.Create and run a nginx frontend microservice, also using a Deployment object.Configure the frontend microservice to send traffic to the backend microservice.Use a Service object of type=LoadBalancer to expose the frontend microservice outside the cluster.Before you beginYou need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesTo check the version, enter kubectl version.This task uses Services with external load balancers, which require a supported environment. If your environment does not support this, you can use a Service of type NodePort instead.Creating the backend using a DeploymentThe backend is a simple hello greeter microservice. Here is the configuration file for the backend Deployment:service/access/backend-deployment.yaml \--- apiVersion: apps/v1 kind: Deployment metadata: name: backend spec: selector: matchLabels: app: hello tier: backend track: stable replicas: 3 template: metadata: labels: app: hello tier: backend track: stable spec: containers: \- name: hello image: "gcr.io/google-samples/hello-go-gke:1.0" ports: \- name: http containerPort: 80 ...Create the backend Deployment:kubectl apply -f https://k8s.io/examples/service/access/backend-deployment.yaml View information about the backend Deployment:kubectl describe deployment backend The output is similar to this:Name: backend Namespace: default CreationTimestamp: Mon, 24 Oct 2016 14:21:02 -0700 Labels: app=hello tier=backend track=stable Annotations: deployment.kubernetes.io/revision=1 Selector: app=hello,tier=backend,track=stable Replicas: 3 desired | 3 updated | 3 total | 3 available | 0 unavailable StrategyType: RollingUpdate MinReadySeconds: 0 RollingUpdateStrategy: 1 max unavailable, 1 max surge Pod Template: Labels: app=hello tier=backend track=stable Containers: hello: Image: "gcr.io/google-samples/hello-go-gke:1.0" Port: 80/TCP Environment:  Mounts:  Volumes:  Conditions: Type Status Reason \---- ------ ------ Available True MinimumReplicasAvailable Progressing True NewReplicaSetAvailable OldReplicaSets:  NewReplicaSet: hello-3621623197 (3/3 replicas created) Events: ... Creating the hello Service objectThe key to sending requests from a frontend to a backend is the backend Service. A Service creates a persistent IP address and DNS name entry so that the backend microservice can always be reached. A Service uses selectors to find the Pods that it routes traffic to.First, explore the Service configuration file:service/access/backend-service.yaml \--- apiVersion: v1 kind: Service metadata: name: hello spec: selector: app: hello tier: backend ports: \- protocol: TCP port: 80 targetPort: http ...In the configuration file, you can see that the Service, named hello routes traffic to Pods that have the labels app: hello and tier: backend.Create the backend Service:kubectl apply -f https://k8s.io/examples/service/access/backend-service.yaml At this point, you have a backend Deployment running three replicas of your hello application, and you have a Service that can route traffic to them. However, this service is neither available nor resolvable outside the cluster.Creating the frontendNow that you have your backend running, you can create a frontend that is accessible outside the cluster, and connects to the backend by proxying requests to it.The frontend sends requests to the backend worker Pods by using the DNS name given to the backend Service. The DNS name is hello, which is the value of the name field in the examples/service/access/backend-service.yaml configuration file.The Pods in the frontend Deployment run a nginx image that is configured to proxy requests to the hello backend Service. Here is the nginx configuration file:service/access/frontend-nginx.conf # The identifier Backend is internal to nginx, and used to name this specific upstream upstream Backend { # hello is the internal DNS name used by the backend Service inside Kubernetes server hello; } server { listen 80; location / { # The following statement will proxy traffic to the upstream named Backend proxy_pass http://Backend; } }Similar to the backend, the frontend has a Deployment and a Service. An important difference to notice between the backend and frontend services, is that the configuration for the frontend Service has type: LoadBalancer, which means that the Service uses a load balancer provisioned by your cloud provider and will be accessible from outside the cluster.service/access/frontend-service.yaml \--- apiVersion: v1 kind: Service metadata: name: frontend spec: selector: app: hello tier: frontend ports: \- protocol: "TCP" port: 80 targetPort: 80 type: LoadBalancer ...service/access/frontend-deployment.yaml \--- apiVersion: apps/v1 kind: Deployment metadata: name: frontend spec: selector: matchLabels: app: hello tier: frontend track: stable replicas: 1 template: metadata: labels: app: hello tier: frontend track: stable spec: containers: \- name: nginx image: "gcr.io/google-samples/hello-frontend:1.0" lifecycle: preStop: exec: command: ["/usr/sbin/nginx","-s","quit"] ...Create the frontend Deployment and Service:kubectl apply -f https://k8s.io/examples/service/access/frontend-deployment.yaml kubectl apply -f https://k8s.io/examples/service/access/frontend-service.yaml The output verifies that both resources were created:deployment.apps/frontend created service/frontend created Note:The nginx configuration is baked into the container image. A better way to do this would be to use a ConfigMap, so that you can change the configuration more easily.Interact with the frontend ServiceOnce you've created a Service of type LoadBalancer, you can use this command to find the external IP:kubectl get service frontend --watch This displays the configuration for the frontend Service and watches for changes. Initially, the external IP is listed as :NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE frontend LoadBalancer 10.51.252.116  80/TCP 10s As soon as an external IP is provisioned, however, the configuration updates to include the new IP under the EXTERNAL-IP heading:NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE frontend LoadBalancer 10.51.252.116 XXX.XXX.XXX.XXX 80/TCP 1m That IP can now be used to interact with the frontend service from outside the cluster.Send traffic through the frontendThe frontend and backend are now connected. You can hit the endpoint by using the curl command on the external IP of your frontend Service.curl http://${EXTERNAL_IP} # replace this with the EXTERNAL-IP you saw earlier The output shows the message generated by the backend:{"message":"Hello"} Cleaning upTo delete the Services, enter this command:kubectl delete services frontend backend To delete the Deployments, the ReplicaSets and the Pods that are running the backend and frontend applications, enter this command:kubectl delete deployment frontend backend What's nextLearn more about ServicesLearn more about ConfigMapsLearn more about DNS for Service and Pods4.10.7 - Create an External Load BalancerThis page shows how to create an external load balancer.When creating a Service, you have the option of automatically creating a cloud load balancer. This provides an externally-accessible IP address that sends traffic to the correct port on your cluster nodes, provided your cluster runs in a supported environment and is configured with the correct cloud load balancer provider package.You can also use an Ingress in place of Service. For more information, check the Ingress documentation.Before you beginYou need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesYour cluster must be running in a cloud or other environment that already has support for configuring external load balancers.Create a ServiceCreate a Service from a manifestTo create an external load balancer, add the following line to your Service manifest: type: LoadBalancer Your manifest might then look like:apiVersion: v1 kind: Service metadata: name: example-service spec: selector: app: example ports: \- port: 8765 targetPort: 9376 type: LoadBalancer Create a Service using kubectlYou can alternatively create the service with the kubectl expose command and its --type=LoadBalancer flag:kubectl expose deployment example --port=8765 --target-port=9376 \ \--name=example-service --type=LoadBalancer This command creates a new Service using the same selectors as the referenced resource (in the case of the example above, a Deployment named example).For more information, including optional flags, refer to the kubectl expose reference.Finding your IP addressYou can find the IP address created for your service by getting the service information through kubectl:kubectl describe services example-service which should produce output similar to:Name: example-service Namespace: default Labels: app=example Annotations:  Selector: app=example Type: LoadBalancer IP Families:  IP: 10.3.22.96 IPs: 10.3.22.96 LoadBalancer Ingress: 192.0.2.89 Port:  8765/TCP TargetPort: 9376/TCP NodePort:  30593/TCP Endpoints: 172.17.0.3:9376 Session Affinity: None External Traffic Policy: Cluster Events:  The load balancer's IP address is listed next to LoadBalancer Ingress.Note:If you are running your service on Minikube, you can find the assigned IP address and port with:minikube service example-service --url Preserving the client source IPBy default, the source IP seen in the target container is not the original source IP of the client. To enable preservation of the client IP, the following fields can be configured in the .spec of the Service:.spec.externalTrafficPolicy - denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints. There are two available options: Cluster (default) and Local. Cluster obscures the client source IP and may cause a second hop to another node, but should have good overall load-spreading. Local preserves the client source IP and avoids a second hop for LoadBalancer and NodePort type Services, but risks potentially imbalanced traffic spreading..spec.healthCheckNodePort - specifies the health check node port (numeric port number) for the service. If you don't specify healthCheckNodePort, the service controller allocates a port from your cluster's NodePort range.You can configure that range by setting an API server command line option, \--service-node-port-range. The Service will use the user-specified healthCheckNodePort value if you specify it, provided that the Service type is set to LoadBalancer and externalTrafficPolicy is set to Local.Setting externalTrafficPolicy to Local in the Service manifest activates this feature. For example:apiVersion: v1 kind: Service metadata: name: example-service spec: selector: app: example ports: \- port: 8765 targetPort: 9376 externalTrafficPolicy: Local type: LoadBalancer Caveats and limitations when preserving source IPsLoad balancing services from some cloud providers do not let you configure different weights for each target.With each target weighted equally in terms of sending traffic to Nodes, external traffic is not equally load balanced across different Pods. The external load balancer is unaware of the number of Pods on each node that are used as a target.Where NumServicePods << NumNodes or NumServicePods >> NumNodes, a fairly close-to-equal distribution will be seen, even without weights.Internal pod to pod traffic should behave similar to ClusterIP services, with equal probability across all pods.Garbage collecting load balancersFEATURE STATE: Kubernetes v1.17 [stable]In usual case, the correlating load balancer resources in cloud provider should be cleaned up soon after a LoadBalancer type Service is deleted. But it is known that there are various corner cases where cloud resources are orphaned after the associated Service is deleted. Finalizer Protection for Service LoadBalancers was introduced to prevent this from happening. By using finalizers, a Service resource will never be deleted until the correlating load balancer resources are also deleted.Specifically, if a Service has type LoadBalancer, the service controller will attach a finalizer named service.kubernetes.io/load-balancer-cleanup. The finalizer will only be removed after the load balancer resource is cleaned up. This prevents dangling load balancer resources even in corner cases such as the service controller crashing.External load balancer providersIt is important to note that the datapath for this functionality is provided by a load balancer external to the Kubernetes cluster.When the Service type is set to LoadBalancer, Kubernetes provides functionality equivalent to type equals ClusterIP to pods within the cluster and extends it by programming the (external to Kubernetes) load balancer with entries for the nodes hosting the relevant Kubernetes pods. The Kubernetes control plane automates the creation of the external load balancer, health checks (if needed), and packet filtering rules (if needed). Once the cloud provider allocates an IP address for the load balancer, the control plane looks up that external IP address and populates it into the Service object.What's nextFollow the Connecting Applications with Services tutorialRead about ServiceRead about Ingress4.10.8 - List All Container Images Running in a ClusterThis page shows how to use kubectl to list all of the Container images for Pods running in a cluster.Before you beginYou need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesTo check the version, enter kubectl version.In this exercise you will use kubectl to fetch all of the Pods running in a cluster, and format the output to pull out the list of Containers for each.List all Container images in all namespacesFetch all Pods in all namespaces using kubectl get pods --all-namespacesFormat the output to include only the list of Container image names using -o jsonpath={.items[*].spec['initContainers', 'containers'][*].image}. This will recursively parse out the image field from the returned json.See the jsonpath reference for further information on how to use jsonpath.Format the output using standard tools: tr, sort, uniqUse tr to replace spaces with newlinesUse sort to sort the resultsUse uniq to aggregate image countskubectl get pods --all-namespaces -o jsonpath="{.items[*].spec['initContainers', 'containers'][*].image}" |\ tr -s '[[:space:]]' '\n' |\ sort |\ uniq -c The jsonpath is interpreted as follows:.items[*]: for each returned value.spec: get the spec['initContainers', 'containers'][*]: for each container.image: get the imageNote:When fetching a single Pod by name, for example kubectl get pod nginx, the .items[*] portion of the path should be omitted because a single Pod is returned instead of a list of items.List Container images by PodThe formatting can be controlled further by using the range operation to iterate over elements individually.kubectl get pods --all-namespaces -o jsonpath='{range .items[*]}{"\n"}{.metadata.name}{":\t"}{range .spec.containers[*]}{.image}{", "}{end}{end}' |\ sort List Container images filtering by Pod labelTo target only Pods matching a specific label, use the -l flag. The following matches only Pods with labels matching app=nginx.kubectl get pods --all-namespaces -o jsonpath="{.items[*].spec.containers[*].image}" -l app=nginx List Container images filtering by Pod namespaceTo target only pods in a specific namespace, use the namespace flag. The following matches only Pods in the kube-system namespace.kubectl get pods --namespace kube-system -o jsonpath="{.items[*].spec.containers[*].image}" List Container images using a go-template instead of jsonpathAs an alternative to jsonpath, Kubectl supports using go-templates for formatting the output:kubectl get pods --all-namespaces -o go-template --template="{{range .items}}{{range .spec.containers}}{{.image}} {{end}}{{end}}" What's nextReferenceJsonpath reference guideGo template reference guide4.10.9 - Set up Ingress on Minikube with the NGINX Ingress ControllerAn Ingress is an API object that defines rules which allow external access to services in a cluster. An Ingress controller fulfills the rules set in the Ingress.This page shows you how to set up a simple Ingress which routes requests to Service 'web' or 'web2' depending on the HTTP URI.Before you beginThis tutorial assumes that you are using minikube to run a local Kubernetes cluster. Visit Install tools to learn how to install minikube.Note:This tutorial uses a container that requires the AMD64 architecture. If you are using minikube on a computer with a different CPU architecture, you could try using minikube with a driver that can emulate AMD64. For example, the Docker Desktop driver can do this.You need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesYour Kubernetes server must be at or later than version 1.19.To check the version, enter kubectl version.If you are using an older Kubernetes version, switch to the documentation for that version.Create a minikube clusterIf you haven't already set up a cluster locally, run minikube start to create a cluster.Enable the Ingress controllerTo enable the NGINX Ingress controller, run the following command:minikube addons enable ingress Verify that the NGINX Ingress controller is runningkubectl get pods -n ingress-nginx Note:It can take up to a minute before you see these pods running OK.The output is similar to:NAME READY STATUS RESTARTS AGE ingress-nginx-admission-create-g9g49 0/1 Completed 0 11m ingress-nginx-admission-patch-rqp78 0/1 Completed 1 11m ingress-nginx-controller-59b45fb494-26npt 1/1 Running 0 11m Deploy a hello, world appCreate a Deployment using the following command:kubectl create deployment web --image=gcr.io/google-samples/hello-app:1.0 The output should be:deployment.apps/web created Verify that the Deployment is in a Ready state:kubectl get deployment web The output should be similar to:NAME READY UP-TO-DATE AVAILABLE AGE web 1/1 1 1 53s Expose the Deployment:kubectl expose deployment web --type=NodePort --port=8080 The output should be:service/web exposed Verify the Service is created and is available on a node port:kubectl get service web The output is similar to:NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE web NodePort 10.104.133.249  8080:31637/TCP 12m Visit the Service via NodePort, using the minikube service command. Follow the instructions for your platform:LinuxMacOSminikube service web --url The output is similar to:http://172.17.0.15:31637 Invoke the URL obtained in the output of the previous step:curl http://172.17.0.15:31637 # The command must be run in a separate terminal. minikube service web --url The output is similar to:http://127.0.0.1:62445 ! Because you are using a Docker driver on darwin, the terminal needs to be open to run it. From a different terminal, invoke the URL obtained in the output of the previous step:curl http://127.0.0.1:62445 The output is similar to:Hello, world! Version: 1.0.0 Hostname: web-55b8c6998d-8k564 You can now access the sample application via the Minikube IP address and NodePort. The next step lets you access the application using the Ingress resource.Create an IngressThe following manifest defines an Ingress that sends traffic to your Service via hello-world.example.Create example-ingress.yaml from the following file:service/networking/example-ingress.yaml apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: example-ingress spec: ingressClassName: nginx rules: \- host: hello-world.example http: paths: \- path: / pathType: Prefix backend: service: name: web port: number: 8080Create the Ingress object by running the following command:kubectl apply -f https://k8s.io/examples/service/networking/example-ingress.yaml The output should be:ingress.networking.k8s.io/example-ingress created Verify the IP address is set:kubectl get ingress Note:This can take a couple of minutes.You should see an IPv4 address in the ADDRESS column; for example:NAME CLASS HOSTS ADDRESS PORTS AGE example-ingress nginx hello-world.example 172.17.0.15 80 38s Verify that the Ingress controller is directing traffic, by following the instructions for your platform:Note:The network is limited if using the Docker driver on MacOS (Darwin) and the Node IP is not reachable directly. To get ingress to work you’ll need to open a new terminal and run minikube tunnel.sudo permission is required for it, so provide the password when prompted.LinuxMacOScurl --resolve "hello-world.example:80:$( minikube ip )" -i http://hello-world.example minikube tunnel The output is similar to:Tunnel successfully started NOTE: Please do not close this terminal as this process must stay alive for the tunnel to be accessible ... The service/ingress example-ingress requires privileged ports to be exposed: [80 443] sudo permission will be asked for it. Starting tunnel for service example-ingress. From within a new terminal, invoke the following command:curl --resolve "hello-world.example:80:127.0.0.1" -i http://hello-world.example You should see:Hello, world! Version: 1.0.0 Hostname: web-55b8c6998d-8k564 Optionally, you can also visit hello-world.example from your browser.Add a line to the bottom of the /etc/hosts file on your computer (you will need administrator access):LinuxMacOSLook up the external IP address as reported by minikube minikube ip 172.17.0.15 hello-world.example Note:Change the IP address to match the output from minikube ip.127.0.0.1 hello-world.example After you make this change, your web browser sends requests for hello-world.example URLs to Minikube.Create a second DeploymentCreate another Deployment using the following command:kubectl create deployment web2 --image=gcr.io/google-samples/hello-app:2.0 The output should be:deployment.apps/web2 created Verify that the Deployment is in a Ready state:kubectl get deployment web2 The output should be similar to:NAME READY UP-TO-DATE AVAILABLE AGE web2 1/1 1 1 16s Expose the second Deployment:kubectl expose deployment web2 --port=8080 --type=NodePort The output should be:service/web2 exposed Edit the existing IngressEdit the existing example-ingress.yaml manifest, and add the following lines at the end:- path: /v2 pathType: Prefix backend: service: name: web2 port: number: 8080 Apply the changes:kubectl apply -f example-ingress.yaml You should see:ingress.networking/example-ingress configured Test your IngressAccess the 1st version of the Hello World app.LinuxMacOScurl --resolve "hello-world.example:80:$( minikube ip )" -i http://hello-world.example minikube tunnel The output is similar to:Tunnel successfully started NOTE: Please do not close this terminal as this process must stay alive for the tunnel to be accessible ... The service/ingress example-ingress requires privileged ports to be exposed: [80 443] sudo permission will be asked for it. Starting tunnel for service example-ingress. From within a new terminal, invoke the following command:curl --resolve "hello-world.example:80:127.0.0.1" -i http://hello-world.example The output is similar to:Hello, world! Version: 1.0.0 Hostname: web-55b8c6998d-8k564 Access the 2nd version of the Hello World app.LinuxMacOScurl --resolve "hello-world.example:80:$( minikube ip )" -i http://hello-world.example/v2 minikube tunnel The output is similar to:Tunnel successfully started NOTE: Please do not close this terminal as this process must stay alive for the tunnel to be accessible ... The service/ingress example-ingress requires privileged ports to be exposed: [80 443] sudo permission will be asked for it. Starting tunnel for service example-ingress. From within a new terminal, invoke the following command:curl --resolve "hello-world.example:80:127.0.0.1" -i http://hello-world.example/v2 The output is similar to:Hello, world! Version: 2.0.0 Hostname: web2-75cd47646f-t8cjk Note:If you did the optional step to update /etc/hosts, you can also visit hello-world.example and hello-world.example/v2 from your browser.What's nextRead more about IngressRead more about Ingress ControllersRead more about Services4.10.10 - Communicate Between Containers in the Same Pod Using a Shared VolumeThis page shows how to use a Volume to communicate between two Containers running in the same Pod. See also how to allow processes to communicate by sharing process namespace between containers.Before you beginYou need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesTo check the version, enter kubectl version.Creating a Pod that runs two ContainersIn this exercise, you create a Pod that runs two Containers. The two containers share a Volume that they can use to communicate. Here is the configuration file for the Pod:pods/two-container-pod.yaml apiVersion: v1 kind: Pod metadata: name: two-containers spec: restartPolicy: Never volumes: \- name: shared-data emptyDir: {} containers: \- name: nginx-container image: nginx volumeMounts: \- name: shared-data mountPath: /usr/share/nginx/html \- name: debian-container image: debian volumeMounts: \- name: shared-data mountPath: /pod-data command: ["/bin/sh"] args: ["-c", "echo Hello from the debian container > /pod-data/index.html"] In the configuration file, you can see that the Pod has a Volume named shared-data.The first container listed in the configuration file runs an nginx server. The mount path for the shared Volume is /usr/share/nginx/html. The second container is based on the debian image, and has a mount path of /pod-data. The second container runs the following command and then terminates.echo Hello from the debian container > /pod-data/index.html Notice that the second container writes the index.html file in the root directory of the nginx server.Create the Pod and the two Containers:kubectl apply -f https://k8s.io/examples/pods/two-container-pod.yaml View information about the Pod and the Containers:kubectl get pod two-containers --output=yaml Here is a portion of the output:apiVersion: v1 kind: Pod metadata: ... name: two-containers namespace: default ... spec: ... containerStatuses: \- containerID: docker://c1d8abd1 ... image: debian ... lastState: terminated: ... name: debian-container ... \- containerID: docker://96c1ff2c5bb ... image: nginx ... name: nginx-container ... state: running: ... You can see that the debian Container has terminated, and the nginx Container is still running.Get a shell to nginx Container:kubectl exec -it two-containers -c nginx-container -- /bin/bash In your shell, verify that nginx is running:root@two-containers:/# apt-get update root@two-containers:/# apt-get install curl procps root@two-containers:/# ps aux The output is similar to this:USER PID ... STAT START TIME COMMAND root 1 ... Ss 21:12 0:00 nginx: master process nginx -g daemon off; Recall that the debian Container created the index.html file in the nginx root directory. Use curl to send a GET request to the nginx server:root@two-containers:/# curl localhost The output shows that nginx serves a web page written by the debian container:Hello from the debian container DiscussionThe primary reason that Pods can have multiple containers is to support helper applications that assist a primary application. Typical examples of helper applications are data pullers, data pushers, and proxies. Helper and primary applications often need to communicate with each other. Typically this is done through a shared filesystem, as shown in this exercise, or through the loopback network interface, localhost. An example of this pattern is a web server along with a helper program that polls a Git repository for new updates.The Volume in this exercise provides a way for Containers to communicate during the life of the Pod. If the Pod is deleted and recreated, any data stored in the shared Volume is lost.What's nextLearn more about patterns for composite containers.Learn about composite containers for modular architecture.See Configuring a Pod to Use a Volume for Storage.See Configure a Pod to share process namespace between containers in a PodSee Volume.See Pod.4.10.11 - Configure DNS for a ClusterKubernetes offers a DNS cluster addon, which most of the supported environments enable by default. In Kubernetes version 1.11 and later, CoreDNS is recommended and is installed by default with kubeadm.For more information on how to configure CoreDNS for a Kubernetes cluster, see the Customizing DNS Service. An example demonstrating how to use Kubernetes DNS with kube-dns, see the Kubernetes DNS sample plugin.4.10.12 - Access Services Running on ClustersThis page shows how to connect to services running on the Kubernetes cluster.Before you beginYou need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesTo check the version, enter kubectl version.Accessing services running on the clusterIn Kubernetes, nodes, pods and services all have their own IPs. In many cases, the node IPs, pod IPs, and some service IPs on a cluster will not be routable, so they will not be reachable from a machine outside the cluster, such as your desktop machine.Ways to connectYou have several options for connecting to nodes, pods and services from outside the cluster:Access services through public IPs.Use a service with type NodePort or LoadBalancer to make the service reachable outside the cluster. See the services and kubectl expose documentation.Depending on your cluster environment, this may only expose the service to your corporate network, or it may expose it to the internet. Think about whether the service being exposed is secure. Does it do its own authentication?Place pods behind services. To access one specific pod from a set of replicas, such as for debugging, place a unique label on the pod and create a new service which selects this label.In most cases, it should not be necessary for application developer to directly access nodes via their nodeIPs.Access services, nodes, or pods using the Proxy Verb.Does apiserver authentication and authorization prior to accessing the remote service. Use this if the services are not secure enough to expose to the internet, or to gain access to ports on the node IP, or for debugging.Proxies may cause problems for some web applications.Only works for HTTP/HTTPS.Described here.Access from a node or pod in the cluster.Run a pod, and then connect to a shell in it using kubectl exec. Connect to other nodes, pods, and services from that shell.Some clusters may allow you to ssh to a node in the cluster. From there you may be able to access cluster services. This is a non-standard method, and will work on some clusters but not others. Browsers and other tools may or may not be installed. Cluster DNS may not work.Discovering builtin servicesTypically, there are several services which are started on a cluster by kube-system. Get a list of these with the kubectl cluster-info command:kubectl cluster-info The output is similar to this:Kubernetes master is running at https://192.0.2.1 elasticsearch-logging is running at https://192.0.2.1/api/v1/namespaces/kube-system/services/elasticsearch-logging/proxy kibana-logging is running at https://192.0.2.1/api/v1/namespaces/kube-system/services/kibana-logging/proxy kube-dns is running at https://192.0.2.1/api/v1/namespaces/kube-system/services/kube-dns/proxy grafana is running at https://192.0.2.1/api/v1/namespaces/kube-system/services/monitoring-grafana/proxy heapster is running at https://192.0.2.1/api/v1/namespaces/kube-system/services/monitoring-heapster/proxy This shows the proxy-verb URL for accessing each service. For example, this cluster has cluster-level logging enabled (using Elasticsearch), which can be reached at https://192.0.2.1/api/v1/namespaces/kube-system/services/elasticsearch-logging/proxy/ if suitable credentials are passed, or through a kubectl proxy at, for example: http://localhost:8080/api/v1/namespaces/kube-system/services/elasticsearch-logging/proxy/.Note:See Access Clusters Using the Kubernetes API for how to pass credentials or use kubectl proxy.Manually constructing apiserver proxy URLsAs mentioned above, you use the kubectl cluster-info command to retrieve the service's proxy URL. To create proxy URLs that include service endpoints, suffixes, and parameters, you append to the service's proxy URL: http://kubernetes_master_address/api/v1/namespaces/namespace_name/services/[https:]service_name[:port_name]/proxyIf you haven't specified a name for your port, you don't have to specify port_name in the URL. You can also use the port number in place of the port_name for both named and unnamed ports.By default, the API server proxies to your service using HTTP. To use HTTPS, prefix the service name with https:: http:///api/v1/namespaces//services//proxyThe supported formats for the  segment of the URL are: \- proxies to the default or unnamed port using http: \- proxies to the specified port name or port number using httphttps:: - proxies to the default or unnamed port using https (note the trailing colon)https:: \- proxies to the specified port name or port number using httpsExamplesTo access the Elasticsearch service endpoint _search?q=user:kimchy, you would use:http://192.0.2.1/api/v1/namespaces/kube-system/services/elasticsearch-logging/proxy/_search?q=user:kimchy To access the Elasticsearch cluster health information _cluster/health?pretty=true, you would use:https://192.0.2.1/api/v1/namespaces/kube-system/services/elasticsearch-logging/proxy/_cluster/health?pretty=true The health information is similar to this:{ "cluster_name" : "kubernetes_logging", "status" : "yellow", "timed_out" : false, "number_of_nodes" : 1, "number_of_data_nodes" : 1, "active_primary_shards" : 5, "active_shards" : 5, "relocating_shards" : 0, "initializing_shards" : 0, "unassigned_shards" : 5 } To access the https Elasticsearch service health information _cluster/health?pretty=true, you would use:https://192.0.2.1/api/v1/namespaces/kube-system/services/https:elasticsearch-logging:/proxy/_cluster/health?pretty=true Using web browsers to access services running on the clusterYou may be able to put an apiserver proxy URL into the address bar of a browser. However:Web browsers cannot usually pass tokens, so you may need to use basic (password) auth. Apiserver can be configured to accept basic auth, but your cluster may not be configured to accept basic auth.Some web apps may not work, particularly those with client side javascript that construct URLs in a way that is unaware of the proxy path prefix.4.11 - Extend KubernetesUnderstand advanced ways to adapt your Kubernetes cluster to the needs of your work environment.4.11.1 - Configure the Aggregation LayerConfiguring the aggregation layer allows the Kubernetes apiserver to be extended with additional APIs, which are not part of the core Kubernetes APIs.Before you beginYou need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesTo check the version, enter kubectl version.Note:There are a few setup requirements for getting the aggregation layer working in your environment to support mutual TLS auth between the proxy and extension apiservers. Kubernetes and the kube-apiserver have multiple CAs, so make sure that the proxy is signed by the aggregation layer CA and not by something else, like the Kubernetes general CA.Caution:Reusing the same CA for different client types can negatively impact the cluster's ability to function. For more information, see CA Reusage and Conflicts.Authentication FlowUnlike Custom Resource Definitions (CRDs), the Aggregation API involves another server - your Extension apiserver - in addition to the standard Kubernetes apiserver. The Kubernetes apiserver will need to communicate with your extension apiserver, and your extension apiserver will need to communicate with the Kubernetes apiserver. In order for this communication to be secured, the Kubernetes apiserver uses x509 certificates to authenticate itself to the extension apiserver.This section describes how the authentication and authorization flows work, and how to configure them.The high-level flow is as follows:Kubernetes apiserver: authenticate the requesting user and authorize their rights to the requested API path.Kubernetes apiserver: proxy the request to the extension apiserverExtension apiserver: authenticate the request from the Kubernetes apiserverExtension apiserver: authorize the request from the original userExtension apiserver: executeThe rest of this section describes these steps in detail.The flow can be seen in the following diagram.The source for the above swimlanes can be found in the source of this document.Kubernetes Apiserver Authentication and AuthorizationA request to an API path that is served by an extension apiserver begins the same way as all API requests: communication to the Kubernetes apiserver. This path already has been registered with the Kubernetes apiserver by the extension apiserver.The user communicates with the Kubernetes apiserver, requesting access to the path. The Kubernetes apiserver uses standard authentication and authorization configured with the Kubernetes apiserver to authenticate the user and authorize access to the specific path.For an overview of authenticating to a Kubernetes cluster, see "Authenticating to a Cluster". For an overview of authorization of access to Kubernetes cluster resources, see "Authorization Overview".Everything to this point has been standard Kubernetes API requests, authentication and authorization.The Kubernetes apiserver now is prepared to send the request to the extension apiserver.Kubernetes Apiserver Proxies the RequestThe Kubernetes apiserver now will send, or proxy, the request to the extension apiserver that registered to handle the request. In order to do so, it needs to know several things:How should the Kubernetes apiserver authenticate to the extension apiserver, informing the extension apiserver that the request, which comes over the network, is coming from a valid Kubernetes apiserver?How should the Kubernetes apiserver inform the extension apiserver of the username and group for which the original request was authenticated?In order to provide for these two, you must configure the Kubernetes apiserver using several flags.Kubernetes Apiserver Client AuthenticationThe Kubernetes apiserver connects to the extension apiserver over TLS, authenticating itself using a client certificate. You must provide the following to the Kubernetes apiserver upon startup, using the provided flags:private key file via --proxy-client-key-filesigned client certificate file via --proxy-client-cert-filecertificate of the CA that signed the client certificate file via --requestheader-client-ca-filevalid Common Name values (CNs) in the signed client certificate via --requestheader-allowed-namesThe Kubernetes apiserver will use the files indicated by --proxy-client-*-file to authenticate to the extension apiserver. In order for the request to be considered valid by a compliant extension apiserver, the following conditions must be met:The connection must be made using a client certificate that is signed by the CA whose certificate is in --requestheader-client-ca-file.The connection must be made using a client certificate whose CN is one of those listed in --requestheader-allowed-names.Note:You can set this option to blank as --requestheader-allowed-names="". This will indicate to an extension apiserver that any CN is acceptable.When started with these options, the Kubernetes apiserver will:Use them to authenticate to the extension apiserver.Create a configmap in the kube-system namespace called extension-apiserver-authentication, in which it will place the CA certificate and the allowed CNs. These in turn can be retrieved by extension apiservers to validate requests.Note that the same client certificate is used by the Kubernetes apiserver to authenticate against all extension apiservers. It does not create a client certificate per extension apiserver, but rather a single one to authenticate as the Kubernetes apiserver. This same one is reused for all extension apiserver requests.Original Request Username and GroupWhen the Kubernetes apiserver proxies the request to the extension apiserver, it informs the extension apiserver of the username and group with which the original request successfully authenticated. It provides these in http headers of its proxied request. You must inform the Kubernetes apiserver of the names of the headers to be used.the header in which to store the username via --requestheader-username-headersthe header in which to store the group via --requestheader-group-headersthe prefix to append to all extra headers via --requestheader-extra-headers-prefixThese header names are also placed in the extension-apiserver-authentication configmap, so they can be retrieved and used by extension apiservers.Extension Apiserver Authenticates the RequestThe extension apiserver, upon receiving a proxied request from the Kubernetes apiserver, must validate that the request actually did come from a valid authenticating proxy, which role the Kubernetes apiserver is fulfilling. The extension apiserver validates it via:Retrieve the following from the configmap in kube-system, as described above:Client CA certificateList of allowed names (CNs)Header names for username, group and extra infoCheck that the TLS connection was authenticated using a client certificate which:Was signed by the CA whose certificate matches the retrieved CA certificate.Has a CN in the list of allowed CNs, unless the list is blank, in which case all CNs are allowed.Extract the username and group from the appropriate headersIf the above passes, then the request is a valid proxied request from a legitimate authenticating proxy, in this case the Kubernetes apiserver.Note that it is the responsibility of the extension apiserver implementation to provide the above. Many do it by default, leveraging the k8s.io/apiserver/ package. Others may provide options to override it using command-line options.In order to have permission to retrieve the configmap, an extension apiserver requires the appropriate role. There is a default role named extension-apiserver-authentication-reader in the kube-system namespace which can be assigned.Extension Apiserver Authorizes the RequestThe extension apiserver now can validate that the user/group retrieved from the headers are authorized to execute the given request. It does so by sending a standard SubjectAccessReview request to the Kubernetes apiserver.In order for the extension apiserver to be authorized itself to submit the SubjectAccessReview request to the Kubernetes apiserver, it needs the correct permissions. Kubernetes includes a default ClusterRole named system:auth-delegator that has the appropriate permissions. It can be granted to the extension apiserver's service account.Extension Apiserver ExecutesIf the SubjectAccessReview passes, the extension apiserver executes the request.Enable Kubernetes Apiserver flagsEnable the aggregation layer via the following kube-apiserver flags. They may have already been taken care of by your provider.--requestheader-client-ca-file= \--requestheader-allowed-names=front-proxy-client \--requestheader-extra-headers-prefix=X-Remote-Extra- \--requestheader-group-headers=X-Remote-Group \--requestheader-username-headers=X-Remote-User \--proxy-client-cert-file= \--proxy-client-key-file= CA Reusage and ConflictsThe Kubernetes apiserver has two client CA options:--client-ca-file--requestheader-client-ca-fileEach of these functions independently and can conflict with each other, if not used correctly.--client-ca-file: When a request arrives to the Kubernetes apiserver, if this option is enabled, the Kubernetes apiserver checks the certificate of the request. If it is signed by one of the CA certificates in the file referenced by \--client-ca-file, then the request is treated as a legitimate request, and the user is the value of the common name CN=, while the group is the organization O=. See the documentation on TLS authentication.--requestheader-client-ca-file: When a request arrives to the Kubernetes apiserver, if this option is enabled, the Kubernetes apiserver checks the certificate of the request. If it is signed by one of the CA certificates in the file reference by --requestheader-client-ca-file, then the request is treated as a potentially legitimate request. The Kubernetes apiserver then checks if the common name CN= is one of the names in the list provided by --requestheader-allowed-names. If the name is allowed, the request is approved; if it is not, the request is not.If both --client-ca-file and --requestheader-client-ca-file are provided, then the request first checks the --requestheader-client-ca-file CA and then the \--client-ca-file. Normally, different CAs, either root CAs or intermediate CAs, are used for each of these options; regular client requests match against --client-ca-file, while aggregation requests match against --requestheader-client-ca-file. However, if both use the same CA, then client requests that normally would pass via --client-ca-file will fail, because the CA will match the CA in --requestheader-client-ca-file, but the common name CN= will not match one of the acceptable common names in \--requestheader-allowed-names. This can cause your kubelets and other control plane components, as well as end-users, to be unable to authenticate to the Kubernetes apiserver.For this reason, use different CA certs for the --client-ca-file option - to authorize control plane components and end-users - and the --requestheader-client-ca-file option - to authorize aggregation apiserver requests.Warning:Do not reuse a CA that is used in a different context unless you understand the risks and the mechanisms to protect the CA's usage.If you are not running kube-proxy on a host running the API server, then you must make sure that the system is enabled with the following kube-apiserver flag:--enable-aggregator-routing=true Register APIService objectsYou can dynamically configure what client requests are proxied to extension apiserver. The following is an example registration: apiVersion: apiregistration.k8s.io/v1 kind: APIService metadata: name:  spec: group:  version:  groupPriorityMinimum:  versionPriority:  service: namespace:  name:  caBundle:  The name of an APIService object must be a valid path segment name.Contacting the extension apiserverOnce the Kubernetes apiserver has determined a request should be sent to an extension apiserver, it needs to know how to contact it.The service stanza is a reference to the service for an extension apiserver. The service namespace and name are required. The port is optional and defaults to 443.Here is an example of an extension apiserver that is configured to be called on port "1234", and to verify the TLS connection against the ServerName my-service-name.my-service-namespace.svc using a custom CA bundle.apiVersion: apiregistration.k8s.io/v1 kind: APIService ... spec: ... service: namespace: my-service-namespace name: my-service-name port: 1234 caBundle: "Ci0tLS0tQk......tLS0K" ... What's nextSet up an extension api-server to work with the aggregation layer.For a high level overview, see Extending the Kubernetes API with the aggregation layer.Learn how to Extend the Kubernetes API Using Custom Resource Definitions.4.11.2 - Use Custom Resources4.11.2.1 - Extend the Kubernetes API with CustomResourceDefinitionsThis page shows how to install a custom resource into the Kubernetes API by creating a CustomResourceDefinition.Before you beginYou need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesYour Kubernetes server must be at or later than version 1.16.To check the version, enter kubectl version.If you are using an older version of Kubernetes that is still supported, switch to the documentation for that version to see advice that is relevant for your cluster.Create a CustomResourceDefinitionWhen you create a new CustomResourceDefinition (CRD), the Kubernetes API Server creates a new RESTful resource path for each version you specify. The custom resource created from a CRD object can be either namespaced or cluster-scoped, as specified in the CRD's spec.scope field. As with existing built-in objects, deleting a namespace deletes all custom objects in that namespace. CustomResourceDefinitions themselves are non-namespaced and are available to all namespaces.For example, if you save the following CustomResourceDefinition to resourcedefinition.yaml:apiVersion: apiextensions.k8s.io/v1 kind: CustomResourceDefinition metadata: # name must match the spec fields below, and be in the form: . name: crontabs.stable.example.com spec: # group name to use for REST API: /apis// group: stable.example.com # list of versions supported by this CustomResourceDefinition versions: \- name: v1 # Each version can be enabled/disabled by Served flag. served: true # One and only one version must be marked as the storage version. storage: true schema: openAPIV3Schema: type: object properties: spec: type: object properties: cronSpec: type: string image: type: string replicas: type: integer # either Namespaced or Cluster scope: Namespaced names: # plural name to be used in the URL: /apis/// plural: crontabs # singular name to be used as an alias on the CLI and for display singular: crontab # kind is normally the CamelCased singular type. Your resource manifests use this. kind: CronTab # shortNames allow shorter string to match your resource on the CLI shortNames: \- ct and create it:kubectl apply -f resourcedefinition.yaml Then a new namespaced RESTful API endpoint is created at:/apis/stable.example.com/v1/namespaces/*/crontabs/... This endpoint URL can then be used to create and manage custom objects. The kind of these objects will be CronTab from the spec of the CustomResourceDefinition object you created above.It might take a few seconds for the endpoint to be created. You can watch the Established condition of your CustomResourceDefinition to be true or watch the discovery information of the API server for your resource to show up.Create custom objectsAfter the CustomResourceDefinition object has been created, you can create custom objects. Custom objects can contain custom fields. These fields can contain arbitrary JSON. In the following example, the cronSpec and image custom fields are set in a custom object of kind CronTab. The kind CronTab comes from the spec of the CustomResourceDefinition object you created above.If you save the following YAML to my-crontab.yaml:apiVersion: "stable.example.com/v1" kind: CronTab metadata: name: my-new-cron-object spec: cronSpec: "* * * * */5" image: my-awesome-cron-image and create it:kubectl apply -f my-crontab.yaml You can then manage your CronTab objects using kubectl. For example:kubectl get crontab Should print a list like this:NAME AGE my-new-cron-object 6s Resource names are not case-sensitive when using kubectl, and you can use either the singular or plural forms defined in the CRD, as well as any short names.You can also view the raw YAML data:kubectl get ct -o yaml You should see that it contains the custom cronSpec and image fields from the YAML you used to create it:apiVersion: v1 items: \- apiVersion: stable.example.com/v1 kind: CronTab metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | {"apiVersion":"stable.example.com/v1","kind":"CronTab","metadata":{"annotations":{},"name":"my-new-cron-object","namespace":"default"},"spec":{"cronSpec":"* * * * */5","image":"my-awesome-cron-image"}} creationTimestamp: "2021-06-20T07:35:27Z" generation: 1 name: my-new-cron-object namespace: default resourceVersion: "1326" uid: 9aab1d66-628e-41bb-a422-57b8b3b1f5a9 spec: cronSpec: '* * * * */5' image: my-awesome-cron-image kind: List metadata: resourceVersion: "" selfLink: "" Delete a CustomResourceDefinitionWhen you delete a CustomResourceDefinition, the server will uninstall the RESTful API endpoint and delete all custom objects stored in it.kubectl delete -f resourcedefinition.yaml kubectl get crontabs Error from server (NotFound): Unable to list {"stable.example.com" "v1" "crontabs"}: the server could not find the requested resource (get crontabs.stable.example.com) If you later recreate the same CustomResourceDefinition, it will start out empty.Specifying a structural schemaCustomResources store structured data in custom fields (alongside the built-in fields apiVersion, kind and metadata, which the API server validates implicitly). With OpenAPI v3.0 validation a schema can be specified, which is validated during creation and updates, compare below for details and limits of such a schema.With apiextensions.k8s.io/v1 the definition of a structural schema is mandatory for CustomResourceDefinitions. In the beta version of CustomResourceDefinition, the structural schema was optional.A structural schema is an OpenAPI v3.0 validation schema which:specifies a non-empty type (via type in OpenAPI) for the root, for each specified field of an object node (via properties or additionalProperties in OpenAPI) and for each item in an array node (via items in OpenAPI), with the exception of:a node with x-kubernetes-int-or-string: truea node with x-kubernetes-preserve-unknown-fields: truefor each field in an object and each item in an array which is specified within any of allOf, anyOf, oneOf or not, the schema also specifies the field/item outside of those logical junctors (compare example 1 and 2).does not set description, type, default, additionalProperties, nullable within an allOf, anyOf, oneOf or not, with the exception of the two pattern for x-kubernetes-int-or-string: true (see below).if metadata is specified, then only restrictions on metadata.name and metadata.generateName are allowed.Non-structural example 1:allOf: \- properties: foo: ... conflicts with rule 2. The following would be correct:properties: foo: ... allOf: \- properties: foo: ... Non-structural example 2:allOf: \- items: properties: foo: ... conflicts with rule 2. The following would be correct:items: properties: foo: ... allOf: \- items: properties: foo: ... Non-structural example 3:properties: foo: pattern: "abc" metadata: type: object properties: name: type: string pattern: "^a" finalizers: type: array items: type: string pattern: "my-finalizer" anyOf: \- properties: bar: type: integer minimum: 42 required: ["bar"] description: "foo bar object" is not a structural schema because of the following violations:the type at the root is missing (rule 1).the type of foo is missing (rule 1).bar inside of anyOf is not specified outside (rule 2).bar's type is within anyOf (rule 3).the description is set within anyOf (rule 3).metadata.finalizers might not be restricted (rule 4).In contrast, the following, corresponding schema is structural:type: object description: "foo bar object" properties: foo: type: string pattern: "abc" bar: type: integer metadata: type: object properties: name: type: string pattern: "^a" anyOf: \- properties: bar: minimum: 42 required: ["bar"] Violations of the structural schema rules are reported in the NonStructural condition in the CustomResourceDefinition.Field pruningCustomResourceDefinitions store validated resource data in the cluster's persistence store, etcd. As with native Kubernetes resources such as ConfigMap, if you specify a field that the API server does not recognize, the unknown field is pruned (removed) before being persisted.CRDs converted from apiextensions.k8s.io/v1beta1 to apiextensions.k8s.io/v1 might lack structural schemas, and spec.preserveUnknownFields might be true.For legacy CustomResourceDefinition objects created as apiextensions.k8s.io/v1beta1 with spec.preserveUnknownFields set to true, the following is also true:Pruning is not enabled.You can store arbitrary data.For compatibility with apiextensions.k8s.io/v1, update your custom resource definitions to:Use a structural OpenAPI schema.Set spec.preserveUnknownFields to false.If you save the following YAML to my-crontab.yaml:apiVersion: "stable.example.com/v1" kind: CronTab metadata: name: my-new-cron-object spec: cronSpec: "* * * * */5" image: my-awesome-cron-image someRandomField: 42 and create it:kubectl create --validate=false -f my-crontab.yaml -o yaml Your output is similar to:apiVersion: stable.example.com/v1 kind: CronTab metadata: creationTimestamp: 2017-05-31T12:56:35Z generation: 1 name: my-new-cron-object namespace: default resourceVersion: "285" uid: 9423255b-4600-11e7-af6a-28d2447dc82b spec: cronSpec: '* * * * */5' image: my-awesome-cron-image Notice that the field someRandomField was pruned.This example turned off client-side validation to demonstrate the API server's behavior, by adding the --validate=false command line option. Because the OpenAPI validation schemas are also published to clients, kubectl also checks for unknown fields and rejects those objects well before they would be sent to the API server.Controlling pruningBy default, all unspecified fields for a custom resource, across all versions, are pruned. It is possible though to opt-out of that for specific sub-trees of fields by adding x-kubernetes-preserve-unknown-fields: true in the structural OpenAPI v3 validation schema.For example:type: object properties: json: x-kubernetes-preserve-unknown-fields: true The field json can store any JSON value, without anything being pruned.You can also partially specify the permitted JSON; for example:type: object properties: json: x-kubernetes-preserve-unknown-fields: true type: object description: this is arbitrary JSON With this, only object type values are allowed.Pruning is enabled again for each specified property (or additionalProperties):type: object properties: json: x-kubernetes-preserve-unknown-fields: true type: object properties: spec: type: object properties: foo: type: string bar: type: string With this, the value:json: spec: foo: abc bar: def something: x status: something: x is pruned to:json: spec: foo: abc bar: def status: something: x This means that the something field in the specified spec object is pruned, but everything outside is not.IntOrStringNodes in a schema with x-kubernetes-int-or-string: true are excluded from rule 1, such that the following is structural:type: object properties: foo: x-kubernetes-int-or-string: true Also those nodes are partially excluded from rule 3 in the sense that the following two patterns are allowed (exactly those, without variations in order to additional fields):x-kubernetes-int-or-string: true anyOf: \- type: integer \- type: string ... andx-kubernetes-int-or-string: true allOf: \- anyOf: \- type: integer \- type: string \- ... # zero or more ... With one of those specification, both an integer and a string validate.In Validation Schema Publishing, x-kubernetes-int-or-string: true is unfolded to one of the two patterns shown above.RawExtensionRawExtensions (as in runtime.RawExtension) holds complete Kubernetes objects, i.e. with apiVersion and kind fields.It is possible to specify those embedded objects (both completely without constraints or partially specified) by setting x-kubernetes-embedded-resource: true. For example:type: object properties: foo: x-kubernetes-embedded-resource: true x-kubernetes-preserve-unknown-fields: true Here, the field foo holds a complete object, e.g.:foo: apiVersion: v1 kind: Pod spec: ... Because x-kubernetes-preserve-unknown-fields: true is specified alongside, nothing is pruned. The use of x-kubernetes-preserve-unknown-fields: true is optional though.With x-kubernetes-embedded-resource: true, the apiVersion, kind and metadata are implicitly specified and validated.Serving multiple versions of a CRDSee Custom resource definition versioning for more information about serving multiple versions of your CustomResourceDefinition and migrating your objects from one version to another.Advanced topicsFinalizersFinalizers allow controllers to implement asynchronous pre-delete hooks. Custom objects support finalizers similar to built-in objects.You can add a finalizer to a custom object like this:apiVersion: "stable.example.com/v1" kind: CronTab metadata: finalizers: \- stable.example.com/finalizer Identifiers of custom finalizers consist of a domain name, a forward slash and the name of the finalizer. Any controller can add a finalizer to any object's list of finalizers.The first delete request on an object with finalizers sets a value for the metadata.deletionTimestamp field but does not delete it. Once this value is set, entries in the finalizers list can only be removed. While any finalizers remain it is also impossible to force the deletion of an object.When the metadata.deletionTimestamp field is set, controllers watching the object execute any finalizers they handle and remove the finalizer from the list after they are done. It is the responsibility of each controller to remove its finalizer from the list.The value of metadata.deletionGracePeriodSeconds controls the interval between polling updates.Once the list of finalizers is empty, meaning all finalizers have been executed, the resource is deleted by Kubernetes.ValidationCustom resources are validated via OpenAPI v3.0 schemas, by x-kubernetes-validations when the Validation Rules feature is enabled, and you can add additional validation using admission webhooks.Additionally, the following restrictions are applied to the schema:These fields cannot be set:definitions,dependencies,deprecated,discriminator,id,patternProperties,readOnly,writeOnly,xml,$ref.The field uniqueItems cannot be set to true.The field additionalProperties cannot be set to false.The field additionalProperties is mutually exclusive with properties.The x-kubernetes-validations extension can be used to validate custom resources using Common Expression Language (CEL) expressions when the Validation rules feature is enabled and the CustomResourceDefinition schema is a structural schema.Refer to the structural schemas section for other restrictions and CustomResourceDefinition features.The schema is defined in the CustomResourceDefinition. In the following example, the CustomResourceDefinition applies the following validations on the custom object:spec.cronSpec must be a string and must be of the form described by the regular expression.spec.replicas must be an integer and must have a minimum value of 1 and a maximum value of 10.Save the CustomResourceDefinition to resourcedefinition.yaml:apiVersion: apiextensions.k8s.io/v1 kind: CustomResourceDefinition metadata: name: crontabs.stable.example.com spec: group: stable.example.com versions: \- name: v1 served: true storage: true schema: # openAPIV3Schema is the schema for validating custom objects. openAPIV3Schema: type: object properties: spec: type: object properties: cronSpec: type: string pattern: '^(\d+|\\*)(/\d+)?(\s+(\d+|\\*)(/\d+)?){4}$' image: type: string replicas: type: integer minimum: 1 maximum: 10 scope: Namespaced names: plural: crontabs singular: crontab kind: CronTab shortNames: \- ct and create it:kubectl apply -f resourcedefinition.yaml A request to create a custom object of kind CronTab is rejected if there are invalid values in its fields. In the following example, the custom object contains fields with invalid values:spec.cronSpec does not match the regular expression.spec.replicas is greater than 10.If you save the following YAML to my-crontab.yaml:apiVersion: "stable.example.com/v1" kind: CronTab metadata: name: my-new-cron-object spec: cronSpec: "* * * *" image: my-awesome-cron-image replicas: 15 and attempt to create it:kubectl apply -f my-crontab.yaml then you get an error:The CronTab "my-new-cron-object" is invalid: []: Invalid value: map[string]interface {}{"apiVersion":"stable.example.com/v1", "kind":"CronTab", "metadata":map[string]interface {}{"name":"my-new-cron-object", "namespace":"default", "deletionTimestamp":interface {}(nil), "deletionGracePeriodSeconds":(*int64)(nil), "creationTimestamp":"2017-09-05T05:20:07Z", "uid":"e14d79e7-91f9-11e7-a598-f0761cb232d1", "clusterName":""}, "spec":map[string]interface {}{"cronSpec":"* * * *", "image":"my-awesome-cron-image", "replicas":15}}: validation failure list: spec.cronSpec in body should match '^(\d+|\\*)(/\d+)?(\s+(\d+|\\*)(/\d+)?){4}$' spec.replicas in body should be less than or equal to 10 If the fields contain valid values, the object creation request is accepted.Save the following YAML to my-crontab.yaml:apiVersion: "stable.example.com/v1" kind: CronTab metadata: name: my-new-cron-object spec: cronSpec: "* * * * */5" image: my-awesome-cron-image replicas: 5 And create it:kubectl apply -f my-crontab.yaml crontab "my-new-cron-object" created Validation ratchetingFEATURE STATE: Kubernetes v1.33 [stable] (enabled by default: true)If you are using a version of Kubernetes older than v1.30, you need to explicitly enable the CRDValidationRatcheting feature gate to use this behavior, which then applies to all CustomResourceDefinitions in your cluster.Provided you enabled the feature gate, Kubernetes implements validation ratcheting for CustomResourceDefinitions. The API server is willing to accept updates to resources that are not valid after the update, provided that each part of the resource that failed to validate was not changed by the update operation. In other words, any invalid part of the resource that remains invalid must have already been wrong. You cannot use this mechanism to update a valid resource so that it becomes invalid.This feature allows authors of CRDs to confidently add new validations to the OpenAPIV3 schema under certain conditions. Users can update to the new schema safely without bumping the version of the object or breaking workflows.While most validations placed in the OpenAPIV3 schema of a CRD support ratcheting, there are a few exceptions. The following OpenAPIV3 schema validations are not supported by ratcheting under the implementation in Kubernetes 1.33 and if violated will continue to throw an error as normally:QuantorsallOfoneOfanyOfnotany validations in a descendent of one of these fieldsx-kubernetes-validations For Kubernetes 1.28, CRD validation rules are ignored by ratcheting. Starting with Alpha 2 in Kubernetes 1.29, x-kubernetes-validations are ratcheted only if they do not refer to oldSelf.Transition Rules are never ratcheted: only errors raised by rules that do not use oldSelf will be automatically ratcheted if their values are unchanged.To write custom ratcheting logic for CEL expressions, check out optionalOldSelf.x-kubernetes-list-type Errors arising from changing the list type of a subschema will not be ratcheted. For example adding set onto a list with duplicates will always result in an error.x-kubernetes-list-map-keys Errors arising from changing the map keys of a list schema will not be ratcheted.required Errors arising from changing the list of required fields will not be ratcheted.properties Adding/removing/modifying the names of properties is not ratcheted, but changes to validations in each properties' schemas and subschemas may be ratcheted if the name of the property stays the same.additionalProperties To remove a previously specified additionalProperties validation will not be ratcheted.metadata Errors that come from Kubernetes' built-in validation of an object's metadata are not ratcheted (such as object name, or characters in a label value). If you specify your own additional rules for the metadata of a custom resource, that additional validation will be ratcheted.Validation rulesFEATURE STATE: Kubernetes v1.29 [stable]Validation rules use the Common Expression Language (CEL) to validate custom resource values. Validation rules are included in CustomResourceDefinition schemas using the x-kubernetes-validations extension.The Rule is scoped to the location of the x-kubernetes-validations extension in the schema. And self variable in the CEL expression is bound to the scoped value.All validation rules are scoped to the current object: no cross-object or stateful validation rules are supported.For example: ... openAPIV3Schema: type: object properties: spec: type: object x-kubernetes-validations: \- rule: "self.minReplicas <= self.replicas" message: "replicas should be greater than or equal to minReplicas." \- rule: "self.replicas <= self.maxReplicas" message: "replicas should be smaller than or equal to maxReplicas." properties: ... minReplicas: type: integer replicas: type: integer maxReplicas: type: integer required: \- minReplicas \- replicas \- maxReplicas will reject a request to create this custom resource:apiVersion: "stable.example.com/v1" kind: CronTab metadata: name: my-new-cron-object spec: minReplicas: 0 replicas: 20 maxReplicas: 10 with the response:The CronTab "my-new-cron-object" is invalid: * spec: Invalid value: map[string]interface {}{"maxReplicas":10, "minReplicas":0, "replicas":20}: replicas should be smaller than or equal to maxReplicas. x-kubernetes-validations could have multiple rules. The rule under x-kubernetes-validations represents the expression which will be evaluated by CEL. The message represents the message displayed when validation fails. If message is unset, the above response would be:The CronTab "my-new-cron-object" is invalid: * spec: Invalid value: map[string]interface {}{"maxReplicas":10, "minReplicas":0, "replicas":20}: failed rule: self.replicas <= self.maxReplicas Note:You can quickly test CEL expressions in CEL Playground.Validation rules are compiled when CRDs are created/updated. The request of CRDs create/update will fail if compilation of validation rules fail. Compilation process includes type checking as well.The compilation failure:no_matching_overload: this function has no overload for the types of the arguments.For example, a rule like self == true against a field of integer type will get error:Invalid value: apiextensions.ValidationRule{Rule:"self == true", Message:""}: compilation failed: ERROR: \:1:6: found no matching overload for '_==_' applied to '(int, bool)' no_such_field: does not contain the desired field.For example, a rule like self.nonExistingField > 0 against a non-existing field will return the following error:Invalid value: apiextensions.ValidationRule{Rule:"self.nonExistingField > 0", Message:""}: compilation failed: ERROR: \:1:5: undefined field 'nonExistingField' invalid argument: invalid argument to macros.For example, a rule like has(self) will return error:Invalid value: apiextensions.ValidationRule{Rule:"has(self)", Message:""}: compilation failed: ERROR: :1:4: invalid argument to has() macro Validation Rules Examples:RulePurposeself.minReplicas <= self.replicas && self.replicas <= self.maxReplicasValidate that the three fields defining replicas are ordered appropriately'Available' in self.stateCountsValidate that an entry with the 'Available' key exists in a map(size(self.list1) == 0) != (size(self.list2) == 0)Validate that one of two lists is non-empty, but not both!('MY_KEY' in self.map1) || self['MY_KEY'].matches('^[a-zA-Z]*$')Validate the value of a map for a specific key, if it is in the mapself.envars.filter(e, e.name == 'MY_ENV').all(e, e.value.matches('^[a-zA-Z]*$')Validate the 'value' field of a listMap entry where key field 'name' is 'MY_ENV'has(self.expired) && self.created + self.ttl < self.expiredValidate that 'expired' date is after a 'create' date plus a 'ttl' durationself.health.startsWith('ok')Validate a 'health' string field has the prefix 'ok'self.widgets.exists(w, w.key == 'x' && w.foo < 10)Validate that the 'foo' property of a listMap item with a key 'x' is less than 10type(self) == string ? self == '100%' : self == 1000Validate an int-or-string field for both the int and string casesself.metadata.name.startsWith(self.prefix)Validate that an object's name has the prefix of another field valueself.set1.all(e, !(e in self.set2))Validate that two listSets are disjointsize(self.names) == size(self.details) && self.names.all(n, n in self.details)Validate the 'details' map is keyed by the items in the 'names' listSetsize(self.clusters.filter(c, c.name == self.primary)) == 1Validate that the 'primary' property has one and only one occurrence in the 'clusters' listMapXref: Supported evaluation on CELIf the Rule is scoped to the root of a resource, it may make field selection into any fields declared in the OpenAPIv3 schema of the CRD as well as apiVersion, kind, metadata.name and metadata.generateName. This includes selection of fields in both the spec and status in the same expression: ... openAPIV3Schema: type: object x-kubernetes-validations: \- rule: "self.status.availableReplicas >= self.spec.minReplicas" properties: spec: type: object properties: minReplicas: type: integer ... status: type: object properties: availableReplicas: type: integer If the Rule is scoped to an object with properties, the accessible properties of the object are field selectable via self.field and field presence can be checked via has(self.field). Null valued fields are treated as absent fields in CEL expressions. ... openAPIV3Schema: type: object properties: spec: type: object x-kubernetes-validations: \- rule: "has(self.foo)" properties: ... foo: type: integer If the Rule is scoped to an object with additionalProperties (i.e. a map) the value of the map are accessible via self[mapKey], map containment can be checked via mapKey in self and all entries of the map are accessible via CEL macros and functions such as self.all(...). ... openAPIV3Schema: type: object properties: spec: type: object x-kubernetes-validations: \- rule: "self['xyz'].foo > 0" additionalProperties: ... type: object properties: foo: type: integer If the Rule is scoped to an array, the elements of the array are accessible via self[i] and also by macros and functions. ... openAPIV3Schema: type: object properties: ... foo: type: array x-kubernetes-validations: \- rule: "size(self) == 1" items: type: string If the Rule is scoped to a scalar, self is bound to the scalar value. ... openAPIV3Schema: type: object properties: spec: type: object properties: ... foo: type: integer x-kubernetes-validations: \- rule: "self > 0" Examples:type of the field rule scoped toRule exampleroot objectself.status.actual <= self.spec.maxDesiredmap of objectsself.components['Widget'].priority < 10list of integersself.values.all(value, value >= 0 && value < 100)stringself.startsWith('kube')The apiVersion, kind, metadata.name and metadata.generateName are always accessible from the root of the object and from any x-kubernetes-embedded-resource annotated objects. No other metadata properties are accessible.Unknown data preserved in custom resources via x-kubernetes-preserve-unknown-fields is not accessible in CEL expressions. This includes:Unknown field values that are preserved by object schemas with x-kubernetes-preserve-unknown-fields.Object properties where the property schema is of an "unknown type". An "unknown type" is recursively defined as:A schema with no type and x-kubernetes-preserve-unknown-fields set to trueAn array where the items schema is of an "unknown type"An object where the additionalProperties schema is of an "unknown type"Only property names of the form [a-zA-Z_.-/][a-zA-Z0-9_.-/]* are accessible. Accessible property names are escaped according to the following rules when accessed in the expression:escape sequenceproperty name equivalent__underscores______dot__.__dash__-__slash__/__{keyword}__CEL RESERVED keywordNote: CEL RESERVED keyword needs to match the exact property name to be escaped (e.g. int in the word sprint would not be escaped).Examples on escaping:property namerule with escaped property namenamespaceself.__namespace__ > 0x-propself.x__dash__prop > 0redact__dself.redact__underscores__d > 0stringself.startsWith('kube')Equality on arrays with x-kubernetes-list-type of set or map ignores element order, i.e., [1, 2] == [2, 1]. Concatenation on arrays with x-kubernetes-list-type use the semantics of the list type:set: X + Y performs a union where the array positions of all elements in X are preserved and non-intersecting elements in Y are appended, retaining their partial order.map: X + Y performs a merge where the array positions of all keys in X are preserved but the values are overwritten by values in Y when the key sets of X and Y intersect. Elements in Y with non-intersecting keys are appended, retaining their partial order.Here is the declarations type mapping between OpenAPIv3 and CEL type:OpenAPIv3 typeCEL type'object' with Propertiesobject / "message type"'object' with AdditionalPropertiesmap'object' with x-kubernetes-embedded-typeobject / "message type", 'apiVersion', 'kind', 'metadata.name' and 'metadata.generateName' are implicitly included in schema'object' with x-kubernetes-preserve-unknown-fieldsobject / "message type", unknown fields are NOT accessible in CEL expressionx-kubernetes-int-or-stringdynamic object that is either an int or a string, type(value) can be used to check the type'arraylist'array' with x-kubernetes-list-type=maplist with map based Equality & unique key guarantees'array' with x-kubernetes-list-type=setlist with set based Equality & unique entry guarantees'boolean'boolean'number' (all formats)double'integer' (all formats)int (64)'null'null_type'string'string'string' with format=byte (base64 encoded)bytes'string' with format=datetimestamp (google.protobuf.Timestamp)'string' with format=datetimetimestamp (google.protobuf.Timestamp)'string' with format=durationduration (google.protobuf.Duration)xref: CEL types, OpenAPI types, Kubernetes Structural Schemas.The messageExpression fieldSimilar to the message field, which defines the string reported for a validation rule failure, messageExpression allows you to use a CEL expression to construct the message string. This allows you to insert more descriptive information into the validation failure message. messageExpression must evaluate a string and may use the same variables that are available to the rule field. For example:x-kubernetes-validations: \- rule: "self.x <= self.maxLimit" messageExpression: '"x exceeded max limit of " + string(self.maxLimit)' Keep in mind that CEL string concatenation (+ operator) does not auto-cast to string. If you have a non-string scalar, use the string() function to cast the scalar to a string like shown in the above example.messageExpression must evaluate to a string, and this is checked while the CRD is being written. Note that it is possible to set message and messageExpression on the same rule, and if both are present, messageExpression will be used. However, if messageExpression evaluates to an error, the string defined in message will be used instead, and the messageExpression error will be logged. This fallback will also occur if the CEL expression defined in messageExpression generates an empty string, or a string containing line breaks.If one of the above conditions are met and no message has been set, then the default validation failure message will be used instead.messageExpression is a CEL expression, so the restrictions listed in Resource use by validation functions apply. If evaluation halts due to resource constraints during messageExpression execution, then no further validation rules will be executed.Setting messageExpression is optional.The message fieldIf you want to set a static message, you can supply message rather than messageExpression. The value of message is used as an opaque error string if validation fails.Setting message is optional.The reason fieldYou can add a machine-readable validation failure reason within a validation, to be returned whenever a request fails this validation rule.For example:x-kubernetes-validations: \- rule: "self.x <= self.maxLimit" reason: "FieldValueInvalid" The HTTP status code returned to the caller will match the reason of the first failed validation rule. The currently supported reasons are: "FieldValueInvalid", "FieldValueForbidden", "FieldValueRequired", "FieldValueDuplicate". If not set or unknown reasons, default to use "FieldValueInvalid".Setting reason is optional.The fieldPath fieldYou can specify the field path returned when the validation fails.For example:x-kubernetes-validations: \- rule: "self.foo.test.x <= self.maxLimit" fieldPath: ".foo.test.x" In the example above, the validation checks the value of field x should be less than the value of maxLimit. If no fieldPath specified, when validation fails, the fieldPath would be default to wherever self scoped. With fieldPath specified, the returned error will have fieldPath properly refer to the location of field x.The fieldPath value must be a relative JSON path that is scoped to the location of this x-kubernetes-validations extension in the schema. Additionally, it should refer to an existing field within the schema. For example when validation checks if a specific attribute foo under a map testMap, you could set fieldPath to ".testMap.foo" or .testMap['foo']'. If the validation requires checking for unique attributes in two lists, the fieldPath can be set to either of the lists. For example, it can be set to .testList1 or .testList2. It supports child operation to refer to an existing field currently. Refer to JSONPath support in Kubernetes for more info. The fieldPath field does not support indexing arrays numerically.Setting fieldPath is optional.The optionalOldSelf fieldFEATURE STATE: Kubernetes v1.33 [stable] (enabled by default: true)If your cluster does not have CRD validation ratcheting enabled, the CustomResourceDefinition API doesn't include this field, and trying to set it may result in an error.The optionalOldSelf field is a boolean field that alters the behavior of Transition Rules described below. Normally, a transition rule will not evaluate if oldSelf cannot be determined: during object creation or when a new value is introduced in an update.If optionalOldSelf is set to true, then transition rules will always be evaluated and the type of oldSelf be changed to a CEL Optional type.optionalOldSelf is useful in cases where schema authors would like a more control tool than provided by the default equality based behavior of to introduce newer, usually stricter constraints on new values, while still allowing old values to be "grandfathered" or ratcheted using the older validation.Example Usage:CELDescriptionself.foo == "foo" || (oldSelf.hasValue() && oldSelf.value().foo != "foo")Ratcheted rule. Once a value is set to "foo", it must stay foo. But if it existed before the "foo" constraint was introduced, it may use any value[oldSelf.orValue(""), self].all(x, ["OldCase1", "OldCase2"].exists(case, x == case)) || ["NewCase1", "NewCase2"].exists(case, self == case) || ["NewCase"].has(self)"Ratcheted validation for removed enum cases if oldSelf used them"oldSelf.optMap(o, o.size()).orValue(0) < 4 || self.size() >= 4Ratcheted validation of newly increased minimum map or list sizeValidation functionsFunctions available include:CEL standard functions, defined in the list of standard definitionsCEL standard macrosCEL extended string function libraryKubernetes CEL extension libraryTransition rulesA rule that contains an expression referencing the identifier oldSelf is implicitly considered a transition rule. Transition rules allow schema authors to prevent certain transitions between two otherwise valid states. For example:type: string enum: ["low", "medium", "high"] x-kubernetes-validations: \- rule: "!(self == 'high' && oldSelf == 'low') && !(self == 'low' && oldSelf == 'high')" message: cannot transition directly between 'low' and 'high' Unlike other rules, transition rules apply only to operations meeting the following criteria:The operation updates an existing object. Transition rules never apply to create operations.Both an old and a new value exist. It remains possible to check if a value has been added or removed by placing a transition rule on the parent node. Transition rules are never applied to custom resource creation. When placed on an optional field, a transition rule will not apply to update operations that set or unset the field.The path to the schema node being validated by a transition rule must resolve to a node that is comparable between the old object and the new object. For example, list items and their descendants (spec.foo[10].bar) can't necessarily be correlated between an existing object and a later update to the same object.Errors will be generated on CRD writes if a schema node contains a transition rule that can never be applied, e.g. "oldSelf cannot be used on the uncorrelatable portion of the schema within path".Transition rules are only allowed on correlatable portions of a schema. A portion of the schema is correlatable if all array parent schemas are of type x-kubernetes-list-type=map; any setor atomicarray parent schemas make it impossible to unambiguously correlate a self with oldSelf.Here are some examples for transition rules:Transition rules examplesUse CaseRuleImmutabilityself.foo == oldSelf.fooPrevent modification/removal once assignedoldSelf != 'bar' || self == 'bar' or !has(oldSelf.field) || has(self.field)Append-only setself.all(element, element in oldSelf)If previous value was X, new value can only be A or B, not Y or ZoldSelf != 'X' || self in ['A', 'B']Monotonic (non-decreasing) countersself >= oldSelfResource use by validation functionsWhen you create or update a CustomResourceDefinition that uses validation rules, the API server checks the likely impact of running those validation rules. If a rule is estimated to be prohibitively expensive to execute, the API server rejects the create or update operation, and returns an error message. A similar system is used at runtime that observes the actions the interpreter takes. If the interpreter executes too many instructions, execution of the rule will be halted, and an error will result. Each CustomResourceDefinition is also allowed a certain amount of resources to finish executing all of its validation rules. If the sum total of its rules are estimated at creation time to go over that limit, then a validation error will also occur.You are unlikely to encounter issues with the resource budget for validation if you only specify rules that always take the same amount of time regardless of how large their input is. For example, a rule that asserts that self.foo == 1 does not by itself have any risk of rejection on validation resource budget groups. But if foo is a string and you define a validation rule self.foo.contains("someString"), that rule takes longer to execute depending on how long foo is. Another example would be if foo were an array, and you specified a validation rule self.foo.all(x, x > 5). The cost system always assumes the worst-case scenario if a limit on the length of foo is not given, and this will happen for anything that can be iterated over (lists, maps, etc.).Because of this, it is considered best practice to put a limit via maxItems, maxProperties, and maxLength for anything that will be processed in a validation rule in order to prevent validation errors during cost estimation. For example, given this schema with one rule:openAPIV3Schema: type: object properties: foo: type: array items: type: string x-kubernetes-validations: \- rule: "self.all(x, x.contains('a string'))" then the API server rejects this rule on validation budget grounds with error:spec.validation.openAPIV3Schema.properties[spec].properties[foo].x-kubernetes-validations[0].rule: Forbidden: CEL rule exceeded budget by more than 100x (try simplifying the rule, or adding maxItems, maxProperties, and maxLength where arrays, maps, and strings are used) The rejection happens because self.all implies calling contains() on every string in foo, which in turn will check the given string to see if it contains 'a string'. Without limits, this is a very expensive rule.If you do not specify any validation limit, the estimated cost of this rule will exceed the per-rule cost limit. But if you add limits in the appropriate places, the rule will be allowed:openAPIV3Schema: type: object properties: foo: type: array maxItems: 25 items: type: string maxLength: 10 x-kubernetes-validations: \- rule: "self.all(x, x.contains('a string'))" The cost estimation system takes into account how many times the rule will be executed in addition to the estimated cost of the rule itself. For instance, the following rule will have the same estimated cost as the previous example (despite the rule now being defined on the individual array items):openAPIV3Schema: type: object properties: foo: type: array maxItems: 25 items: type: string x-kubernetes-validations: \- rule: "self.contains('a string'))" maxLength: 10 If a list inside of a list has a validation rule that uses self.all, that is significantly more expensive than a non-nested list with the same rule. A rule that would have been allowed on a non-nested list might need lower limits set on both nested lists in order to be allowed. For example, even without having limits set, the following rule is allowed:openAPIV3Schema: type: object properties: foo: type: array items: type: integer x-kubernetes-validations: \- rule: "self.all(x, x == 5)" But the same rule on the following schema (with a nested array added) produces a validation error:openAPIV3Schema: type: object properties: foo: type: array items: type: array items: type: integer x-kubernetes-validations: \- rule: "self.all(x, x == 5)" This is because each item of foo is itself an array, and each subarray in turn calls self.all. Avoid nested lists and maps if possible where validation rules are used.DefaultingNote:To use defaulting, your CustomResourceDefinition must use API version apiextensions.k8s.io/v1.Defaulting allows to specify default values in the OpenAPI v3 validation schema:apiVersion: apiextensions.k8s.io/v1 kind: CustomResourceDefinition metadata: name: crontabs.stable.example.com spec: group: stable.example.com versions: \- name: v1 served: true storage: true schema: # openAPIV3Schema is the schema for validating custom objects. openAPIV3Schema: type: object properties: spec: type: object properties: cronSpec: type: string pattern: '^(\d+|\\*)(/\d+)?(\s+(\d+|\\*)(/\d+)?){4}$' default: "5 0 * * *" image: type: string replicas: type: integer minimum: 1 maximum: 10 default: 1 scope: Namespaced names: plural: crontabs singular: crontab kind: CronTab shortNames: \- ct With this both cronSpec and replicas are defaulted:apiVersion: "stable.example.com/v1" kind: CronTab metadata: name: my-new-cron-object spec: image: my-awesome-cron-image leads toapiVersion: "stable.example.com/v1" kind: CronTab metadata: name: my-new-cron-object spec: cronSpec: "5 0 * * *" image: my-awesome-cron-image replicas: 1 Defaulting happens on the objectin the request to the API server using the request version defaults,when reading from etcd using the storage version defaults,after mutating admission plugins with non-empty patches using the admission webhook object version defaults.Defaults applied when reading data from etcd are not automatically written back to etcd. An update request via the API is required to persist those defaults back into etcd.Default values for non-leaf fields must be pruned (with the exception of defaults for metadata fields) and must validate against a provided schema. For example in the above example, a default of {"replicas": "foo", "badger": 1} for the spec field would be invalid, because badger is an unknown field, and replicas is not a string.Default values for metadata fields of x-kubernetes-embedded-resources: true nodes (or parts of a default value covering metadata) are not pruned during CustomResourceDefinition creation, but through the pruning step during handling of requests.Defaulting and NullableNull values for fields that either don't specify the nullable flag, or give it a false value, will be pruned before defaulting happens. If a default is present, it will be applied. When nullable is true, null values will be conserved and won't be defaulted.For example, given the OpenAPI schema below:type: object properties: spec: type: object properties: foo: type: string nullable: false default: "default" bar: type: string nullable: true baz: type: string creating an object with null values for foo and bar and bazspec: foo: null bar: null baz: null leads tospec: foo: "default" bar: null with foo pruned and defaulted because the field is non-nullable, bar maintaining the null value due to nullable: true, and baz pruned because the field is non-nullable and has no default.Publish Validation Schema in OpenAPICustomResourceDefinition OpenAPI v3 validation schemas which are structural and enable pruning are published as OpenAPI v3 and OpenAPI v2 from Kubernetes API server. It is recommended to use the OpenAPI v3 document as it is a lossless representation of the CustomResourceDefinition OpenAPI v3 validation schema while OpenAPI v2 represents a lossy conversion.The kubectl command-line tool consumes the published schema to perform client-side validation (kubectl create and kubectl apply), schema explanation (kubectl explain) on custom resources. The published schema can be consumed for other purposes as well, like client generation or documentation.Compatibility with OpenAPI V2For compatibility with OpenAPI V2, the OpenAPI v3 validation schema performs a lossy conversion to the OpenAPI v2 schema. The schema show up in definitions and paths fields in the OpenAPI v2 spec.The following modifications are applied during the conversion to keep backwards compatibility with kubectl in previous 1.13 version. These modifications prevent kubectl from being over-strict and rejecting valid OpenAPI schemas that it doesn't understand. The conversion won't modify the validation schema defined in CRD, and therefore won't affect validation in the API server.The following fields are removed as they aren't supported by OpenAPI v2.The fields allOf, anyOf, oneOf and not are removedIf nullable: true is set, we drop type, nullable, items and properties because OpenAPI v2 is not able to express nullable. To avoid kubectl to reject good objects, this is necessary.Additional printer columnsThe kubectl tool relies on server-side output formatting. Your cluster's API server decides which columns are shown by the kubectl get command. You can customize these columns for a CustomResourceDefinition. The following example adds the Spec, Replicas, and Age columns.Save the CustomResourceDefinition to resourcedefinition.yaml:apiVersion: apiextensions.k8s.io/v1 kind: CustomResourceDefinition metadata: name: crontabs.stable.example.com spec: group: stable.example.com scope: Namespaced names: plural: crontabs singular: crontab kind: CronTab shortNames: \- ct versions: \- name: v1 served: true storage: true schema: openAPIV3Schema: type: object properties: spec: type: object properties: cronSpec: type: string image: type: string replicas: type: integer additionalPrinterColumns: \- name: Spec type: string description: The cron spec defining the interval a CronJob is run jsonPath: .spec.cronSpec \- name: Replicas type: integer description: The number of jobs launched by the CronJob jsonPath: .spec.replicas \- name: Age type: date jsonPath: .metadata.creationTimestamp Create the CustomResourceDefinition:kubectl apply -f resourcedefinition.yaml Create an instance using the my-crontab.yaml from the previous section.Invoke the server-side printing:kubectl get crontab my-new-cron-object Notice the NAME, SPEC, REPLICAS, and AGE columns in the output:NAME SPEC REPLICAS AGE my-new-cron-object * * * * * 1 7s Note:The NAME column is implicit and does not need to be defined in the CustomResourceDefinition.PriorityEach column includes a priority field. Currently, the priority differentiates between columns shown in standard view or wide view (using the -o wide flag).Columns with priority 0 are shown in standard view.Columns with priority greater than 0 are shown only in wide view.TypeA column's type field can be any of the following (compare OpenAPI v3 data types):integer – non-floating-point numbersnumber – floating point numbersstring – stringsboolean – true or falsedate – rendered differentially as time since this timestamp.If the value inside a CustomResource does not match the type specified for the column, the value is omitted. Use CustomResource validation to ensure that the value types are correct.FormatA column's format field can be any of the following:int32int64floatdoublebytedatedate-timepasswordThe column's format controls the style used when kubectl prints the value.Field selectorsField Selectors let clients select custom resources based on the value of one or more resource fields.All custom resources support the metadata.name and metadata.namespace field selectors.Fields declared in a CustomResourceDefinition may also be used with field selectors when included in the spec.versions[*].selectableFields field of the CustomResourceDefinition.Selectable fields for custom resourcesFEATURE STATE: Kubernetes v1.32 [stable] (enabled by default: true)The spec.versions[*].selectableFields field of a CustomResourceDefinition may be used to declare which other fields in a custom resource may be used in field selectors with the feature of CustomResourceFieldSelectors feature gate (This feature gate is enabled by default since Kubernetes v1.31). The following example adds the .spec.color and .spec.size fields as selectable fields.Save the CustomResourceDefinition to shirt-resource-definition.yaml:customresourcedefinition/shirt-resource-definition.yaml apiVersion: apiextensions.k8s.io/v1 kind: CustomResourceDefinition metadata: name: shirts.stable.example.com spec: group: stable.example.com scope: Namespaced names: plural: shirts singular: shirt kind: Shirt versions: \- name: v1 served: true storage: true schema: openAPIV3Schema: type: object properties: spec: type: object properties: color: type: string size: type: string selectableFields: \- jsonPath: .spec.color \- jsonPath: .spec.size additionalPrinterColumns: \- jsonPath: .spec.color name: Color type: string \- jsonPath: .spec.size name: Size type: string Create the CustomResourceDefinition:kubectl apply -f https://k8s.io/examples/customresourcedefinition/shirt-resource-definition.yaml Define some Shirts by editing shirt-resources.yaml; for example:customresourcedefinition/shirt-resources.yaml \--- apiVersion: stable.example.com/v1 kind: Shirt metadata: name: example1 spec: color: blue size: S \--- apiVersion: stable.example.com/v1 kind: Shirt metadata: name: example2 spec: color: blue size: M \--- apiVersion: stable.example.com/v1 kind: Shirt metadata: name: example3 spec: color: green size: M Create the custom resources:kubectl apply -f https://k8s.io/examples/customresourcedefinition/shirt-resources.yaml Get all the resources:kubectl get shirts.stable.example.com The output is:NAME COLOR SIZE example1 blue S example2 blue M example3 green M Fetch blue shirts (retrieve Shirts with a color of blue):kubectl get shirts.stable.example.com --field-selector spec.color=blue Should output:NAME COLOR SIZE example1 blue S example2 blue M Get only resources with a color of green and a size of M:kubectl get shirts.stable.example.com --field-selector spec.color=green,spec.size=M Should output:NAME COLOR SIZE example2 blue M SubresourcesCustom resources support /status and /scale subresources.The status and scale subresources can be optionally enabled by defining them in the CustomResourceDefinition.Status subresourceWhen the status subresource is enabled, the /status subresource for the custom resource is exposed.The status and the spec stanzas are represented by the .status and .spec JSONPaths respectively inside of a custom resource.PUT requests to the /status subresource take a custom resource object and ignore changes to anything except the status stanza.PUT requests to the /status subresource only validate the status stanza of the custom resource.PUT/POST/PATCH requests to the custom resource ignore changes to the status stanza.The .metadata.generation value is incremented for all changes, except for changes to .metadata or .status.Only the following constructs are allowed at the root of the CRD OpenAPI validation schema:descriptionexampleexclusiveMaximumexclusiveMinimumexternalDocsformatitemsmaximummaxItemsmaxLengthminimumminItemsminLengthmultipleOfpatternpropertiesrequiredtitletypeuniqueItemsScale subresourceWhen the scale subresource is enabled, the /scale subresource for the custom resource is exposed. The autoscaling/v1.Scale object is sent as the payload for /scale.To enable the scale subresource, the following fields are defined in the CustomResourceDefinition.specReplicasPath defines the JSONPath inside of a custom resource that corresponds to scale.spec.replicas.It is a required value.Only JSONPaths under .spec and with the dot notation are allowed.If there is no value under the specReplicasPath in the custom resource, the /scale subresource will return an error on GET.statusReplicasPath defines the JSONPath inside of a custom resource that corresponds to scale.status.replicas.It is a required value.Only JSONPaths under .status and with the dot notation are allowed.If there is no value under the statusReplicasPath in the custom resource, the status replica value in the /scale subresource will default to 0.labelSelectorPath defines the JSONPath inside of a custom resource that corresponds to Scale.Status.Selector.It is an optional value.It must be set to work with HPA and VPA.Only JSONPaths under .status or .spec and with the dot notation are allowed.If there is no value under the labelSelectorPath in the custom resource, the status selector value in the /scale subresource will default to the empty string.The field pointed by this JSON path must be a string field (not a complex selector struct) which contains a serialized label selector in string form.In the following example, both status and scale subresources are enabled.Save the CustomResourceDefinition to resourcedefinition.yaml:apiVersion: apiextensions.k8s.io/v1 kind: CustomResourceDefinition metadata: name: crontabs.stable.example.com spec: group: stable.example.com versions: \- name: v1 served: true storage: true schema: openAPIV3Schema: type: object properties: spec: type: object properties: cronSpec: type: string image: type: string replicas: type: integer status: type: object properties: replicas: type: integer labelSelector: type: string # subresources describes the subresources for custom resources. subresources: # status enables the status subresource. status: {} # scale enables the scale subresource. scale: # specReplicasPath defines the JSONPath inside of a custom resource that corresponds to Scale.Spec.Replicas. specReplicasPath: .spec.replicas # statusReplicasPath defines the JSONPath inside of a custom resource that corresponds to Scale.Status.Replicas. statusReplicasPath: .status.replicas # labelSelectorPath defines the JSONPath inside of a custom resource that corresponds to Scale.Status.Selector. labelSelectorPath: .status.labelSelector scope: Namespaced names: plural: crontabs singular: crontab kind: CronTab shortNames: \- ct And create it:kubectl apply -f resourcedefinition.yaml After the CustomResourceDefinition object has been created, you can create custom objects.If you save the following YAML to my-crontab.yaml:apiVersion: "stable.example.com/v1" kind: CronTab metadata: name: my-new-cron-object spec: cronSpec: "* * * * */5" image: my-awesome-cron-image replicas: 3 and create it:kubectl apply -f my-crontab.yaml Then new namespaced RESTful API endpoints are created at:/apis/stable.example.com/v1/namespaces/*/crontabs/status and/apis/stable.example.com/v1/namespaces/*/crontabs/scale A custom resource can be scaled using the kubectl scale command. For example, the following command sets .spec.replicas of the custom resource created above to 5:kubectl scale --replicas=5 crontabs/my-new-cron-object crontabs "my-new-cron-object" scaled kubectl get crontabs my-new-cron-object -o jsonpath='{.spec.replicas}' 5 You can use a PodDisruptionBudget to protect custom resources that have the scale subresource enabled.CategoriesCategories is a list of grouped resources the custom resource belongs to (eg. all). You can use kubectl get  to list the resources belonging to the category.The following example adds all in the list of categories in the CustomResourceDefinition and illustrates how to output the custom resource using kubectl get all.Save the following CustomResourceDefinition to resourcedefinition.yaml:apiVersion: apiextensions.k8s.io/v1 kind: CustomResourceDefinition metadata: name: crontabs.stable.example.com spec: group: stable.example.com versions: \- name: v1 served: true storage: true schema: openAPIV3Schema: type: object properties: spec: type: object properties: cronSpec: type: string image: type: string replicas: type: integer scope: Namespaced names: plural: crontabs singular: crontab kind: CronTab shortNames: \- ct # categories is a list of grouped resources the custom resource belongs to. categories: \- all and create it:kubectl apply -f resourcedefinition.yaml After the CustomResourceDefinition object has been created, you can create custom objects.Save the following YAML to my-crontab.yaml:apiVersion: "stable.example.com/v1" kind: CronTab metadata: name: my-new-cron-object spec: cronSpec: "* * * * */5" image: my-awesome-cron-image and create it:kubectl apply -f my-crontab.yaml You can specify the category when using kubectl get:kubectl get all and it will include the custom resources of kind CronTab:NAME AGE crontabs/my-new-cron-object 3s What's nextRead about custom resources.See CustomResourceDefinition.Serve multiple versions of a CustomResourceDefinition.4.11.2.2 - Versions in CustomResourceDefinitionsThis page explains how to add versioning information to CustomResourceDefinitions, to indicate the stability level of your CustomResourceDefinitions or advance your API to a new version with conversion between API representations. It also describes how to upgrade an object from one version to another.Before you beginYou need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesYou should have an initial understanding of custom resources.Your Kubernetes server must be at or later than version v1.16.To check the version, enter kubectl version.OverviewThe CustomResourceDefinition API provides a workflow for introducing and upgrading to new versions of a CustomResourceDefinition.When a CustomResourceDefinition is created, the first version is set in the CustomResourceDefinition spec.versions list to an appropriate stability level and a version number. For example v1beta1 would indicate that the first version is not yet stable. All custom resource objects will initially be stored at this version.Once the CustomResourceDefinition is created, clients may begin using the v1beta1 API.Later it might be necessary to add new version such as v1.Adding a new version:Pick a conversion strategy. Since custom resource objects need the ability to be served at both versions, that means they will sometimes be served in a different version than the one stored. To make this possible, the custom resource objects must sometimes be converted between the version they are stored at and the version they are served at. If the conversion involves schema changes and requires custom logic, a conversion webhook should be used. If there are no schema changes, the default None conversion strategy may be used and only the apiVersion field will be modified when serving different versions.If using conversion webhooks, create and deploy the conversion webhook. See the Webhook conversion for more details.Update the CustomResourceDefinition to include the new version in the spec.versions list with served:true. Also, set spec.conversion field to the selected conversion strategy. If using a conversion webhook, configure spec.conversion.webhookClientConfig field to call the webhook.Once the new version is added, clients may incrementally migrate to the new version. It is perfectly safe for some clients to use the old version while others use the new version.Migrate stored objects to the new version:See the upgrade existing objects to a new stored version section.It is safe for clients to use both the old and new version before, during and after upgrading the objects to a new stored version.Removing an old version:Ensure all clients are fully migrated to the new version. The kube-apiserver logs can be reviewed to help identify any clients that are still accessing via the old version.Set served to false for the old version in the spec.versions list. If any clients are still unexpectedly using the old version they may begin reporting errors attempting to access the custom resource objects at the old version. If this occurs, switch back to using served:true on the old version, migrate the remaining clients to the new version and repeat this step.Ensure the upgrade of existing objects to the new stored version step has been completed.Verify that the storage is set to true for the new version in the spec.versions list in the CustomResourceDefinition.Verify that the old version is no longer listed in the CustomResourceDefinition status.storedVersions.Remove the old version from the CustomResourceDefinition spec.versions list.Drop conversion support for the old version in conversion webhooks.Specify multiple versionsThe CustomResourceDefinition API versions field can be used to support multiple versions of custom resources that you have developed. Versions can have different schemas, and conversion webhooks can convert custom resources between versions. Webhook conversions should follow the Kubernetes API conventions wherever applicable. Specifically, See the API change documentation for a set of useful gotchas and suggestions.Note:In apiextensions.k8s.io/v1beta1, there was a version field instead of versions. The version field is deprecated and optional, but if it is not empty, it must match the first item in the versions field.This example shows a CustomResourceDefinition with two versions. For the first example, the assumption is all versions share the same schema with no conversion between them. The comments in the YAML provide more context.apiextensions.k8s.io/v1apiextensions.k8s.io/v1beta1apiVersion: apiextensions.k8s.io/v1 kind: CustomResourceDefinition metadata: # name must match the spec fields below, and be in the form: . name: crontabs.example.com spec: # group name to use for REST API: /apis// group: example.com # list of versions supported by this CustomResourceDefinition versions: \- name: v1beta1 # Each version can be enabled/disabled by Served flag. served: true # One and only one version must be marked as the storage version. storage: true # A schema is required schema: openAPIV3Schema: type: object properties: host: type: string port: type: string \- name: v1 served: true storage: false schema: openAPIV3Schema: type: object properties: host: type: string port: type: string # The conversion section is introduced in Kubernetes 1.13+ with a default value of # None conversion (strategy sub-field set to None). conversion: # None conversion assumes the same schema for all versions and only sets the apiVersion # field of custom resources to the proper value strategy: None # either Namespaced or Cluster scope: Namespaced names: # plural name to be used in the URL: /apis/// plural: crontabs # singular name to be used as an alias on the CLI and for display singular: crontab # kind is normally the CamelCased singular type. Your resource manifests use this. kind: CronTab # shortNames allow shorter string to match your resource on the CLI shortNames: \- ct # Deprecated in v1.16 in favor of apiextensions.k8s.io/v1 apiVersion: apiextensions.k8s.io/v1beta1 kind: CustomResourceDefinition metadata: # name must match the spec fields below, and be in the form: . name: crontabs.example.com spec: # group name to use for REST API: /apis// group: example.com # list of versions supported by this CustomResourceDefinition versions: \- name: v1beta1 # Each version can be enabled/disabled by Served flag. served: true # One and only one version must be marked as the storage version. storage: true \- name: v1 served: true storage: false validation: openAPIV3Schema: type: object properties: host: type: string port: type: string # The conversion section is introduced in Kubernetes 1.13+ with a default value of # None conversion (strategy sub-field set to None). conversion: # None conversion assumes the same schema for all versions and only sets the apiVersion # field of custom resources to the proper value strategy: None # either Namespaced or Cluster scope: Namespaced names: # plural name to be used in the URL: /apis/// plural: crontabs # singular name to be used as an alias on the CLI and for display singular: crontab # kind is normally the PascalCased singular type. Your resource manifests use this. kind: CronTab # shortNames allow shorter string to match your resource on the CLI shortNames: \- ct You can save the CustomResourceDefinition in a YAML file, then use kubectl apply to create it.kubectl apply -f my-versioned-crontab.yaml After creation, the API server starts to serve each enabled version at an HTTP REST endpoint. In the above example, the API versions are available at /apis/example.com/v1beta1 and /apis/example.com/v1.Version priorityRegardless of the order in which versions are defined in a CustomResourceDefinition, the version with the highest priority is used by kubectl as the default version to access objects. The priority is determined by parsing the name field to determine the version number, the stability (GA, Beta, or Alpha), and the sequence within that stability level.The algorithm used for sorting the versions is designed to sort versions in the same way that the Kubernetes project sorts Kubernetes versions. Versions start with a v followed by a number, an optional beta or alpha designation, and optional additional numeric versioning information. Broadly, a version string might look like v2 or v2beta1. Versions are sorted using the following algorithm:Entries that follow Kubernetes version patterns are sorted before those that do not.For entries that follow Kubernetes version patterns, the numeric portions of the version string is sorted largest to smallest.If the strings beta or alpha follow the first numeric portion, they sorted in that order, after the equivalent string without the beta or alpha suffix (which is presumed to be the GA version).If another number follows the beta, or alpha, those numbers are also sorted from largest to smallest.Strings that don't fit the above format are sorted alphabetically and the numeric portions are not treated specially. Notice that in the example below, foo1 is sorted above foo10. This is different from the sorting of the numeric portion of entries that do follow the Kubernetes version patterns.This might make sense if you look at the following sorted version list:- v10 \- v2 \- v1 \- v11beta2 \- v10beta3 \- v3beta1 \- v12alpha1 \- v11alpha2 \- foo1 \- foo10 For the example in Specify multiple versions, the version sort order is v1, followed by v1beta1. This causes the kubectl command to use v1 as the default version unless the provided object specifies the version.Version deprecationFEATURE STATE: Kubernetes v1.19 [stable]Starting in v1.19, a CustomResourceDefinition can indicate a particular version of the resource it defines is deprecated. When API requests to a deprecated version of that resource are made, a warning message is returned in the API response as a header. The warning message for each deprecated version of the resource can be customized if desired.A customized warning message should indicate the deprecated API group, version, and kind, and should indicate what API group, version, and kind should be used instead, if applicable.apiextensions.k8s.io/v1apiextensions.k8s.io/v1beta1apiVersion: apiextensions.k8s.io/v1 kind: CustomResourceDefinition name: crontabs.example.com spec: group: example.com names: plural: crontabs singular: crontab kind: CronTab scope: Namespaced versions: \- name: v1alpha1 served: true storage: false # This indicates the v1alpha1 version of the custom resource is deprecated. # API requests to this version receive a warning header in the server response. deprecated: true # This overrides the default warning returned to API clients making v1alpha1 API requests. deprecationWarning: "example.com/v1alpha1 CronTab is deprecated; see http://example.com/v1alpha1-v1 for instructions to migrate to example.com/v1 CronTab" schema: ... \- name: v1beta1 served: true # This indicates the v1beta1 version of the custom resource is deprecated. # API requests to this version receive a warning header in the server response. # A default warning message is returned for this version. deprecated: true schema: ... \- name: v1 served: true storage: true schema: ... # Deprecated in v1.16 in favor of apiextensions.k8s.io/v1 apiVersion: apiextensions.k8s.io/v1beta1 kind: CustomResourceDefinition metadata: name: crontabs.example.com spec: group: example.com names: plural: crontabs singular: crontab kind: CronTab scope: Namespaced validation: ... versions: \- name: v1alpha1 served: true storage: false # This indicates the v1alpha1 version of the custom resource is deprecated. # API requests to this version receive a warning header in the server response. deprecated: true # This overrides the default warning returned to API clients making v1alpha1 API requests. deprecationWarning: "example.com/v1alpha1 CronTab is deprecated; see http://example.com/v1alpha1-v1 for instructions to migrate to example.com/v1 CronTab" \- name: v1beta1 served: true # This indicates the v1beta1 version of the custom resource is deprecated. # API requests to this version receive a warning header in the server response. # A default warning message is returned for this version. deprecated: true \- name: v1 served: true storage: true Version removalAn older API version cannot be dropped from a CustomResourceDefinition manifest until existing stored data has been migrated to the newer API version for all clusters that served the older version of the custom resource, and the old version is removed from the status.storedVersions of the CustomResourceDefinition.apiVersion: apiextensions.k8s.io/v1 kind: CustomResourceDefinition name: crontabs.example.com spec: group: example.com names: plural: crontabs singular: crontab kind: CronTab scope: Namespaced versions: \- name: v1beta1 # This indicates the v1beta1 version of the custom resource is no longer served. # API requests to this version receive a not found error in the server response. served: false schema: ... \- name: v1 served: true # The new served version should be set as the storage version storage: true schema: ... Webhook conversionFEATURE STATE: Kubernetes v1.16 [stable]Note:Webhook conversion is available as beta since 1.15, and as alpha since Kubernetes 1.13. The CustomResourceWebhookConversion feature must be enabled, which is the case automatically for many clusters for beta features. Please refer to the feature gate documentation for more information.The above example has a None conversion between versions which only sets the apiVersion field on conversion and does not change the rest of the object. The API server also supports webhook conversions that call an external service in case a conversion is required. For example when:custom resource is requested in a different version than stored version.Watch is created in one version but the changed object is stored in another version.custom resource PUT request is in a different version than storage version.To cover all of these cases and to optimize conversion by the API server, the conversion requests may contain multiple objects in order to minimize the external calls. The webhook should perform these conversions independently.Write a conversion webhook serverPlease refer to the implementation of the custom resource conversion webhook server that is validated in a Kubernetes e2e test. The webhook handles the ConversionReview requests sent by the API servers, and sends back conversion results wrapped in ConversionResponse. Note that the request contains a list of custom resources that need to be converted independently without changing the order of objects. The example server is organized in a way to be reused for other conversions. Most of the common code are located in the framework file that leaves only one function to be implemented for different conversions.Note:The example conversion webhook server leaves the ClientAuth field empty, which defaults to NoClientCert. This means that the webhook server does not authenticate the identity of the clients, supposedly API servers. If you need mutual TLS or other ways to authenticate the clients, see how to authenticate API servers.Permissible mutationsA conversion webhook must not mutate anything inside of metadata of the converted object other than labels and annotations. Attempted changes to name, UID and namespace are rejected and fail the request which caused the conversion. All other changes are ignored.Deploy the conversion webhook serviceDocumentation for deploying the conversion webhook is the same as for the admission webhook example service. The assumption for next sections is that the conversion webhook server is deployed to a service named example-conversion-webhook-server in default namespace and serving traffic on path /crdconvert.Note:When the webhook server is deployed into the Kubernetes cluster as a service, it has to be exposed via a service on port 443 (The server itself can have an arbitrary port but the service object should map it to port 443). The communication between the API server and the webhook service may fail if a different port is used for the service.Configure CustomResourceDefinition to use conversion webhooksThe None conversion example can be extended to use the conversion webhook by modifying conversion section of the spec:apiextensions.k8s.io/v1apiextensions.k8s.io/v1beta1apiVersion: apiextensions.k8s.io/v1 kind: CustomResourceDefinition metadata: # name must match the spec fields below, and be in the form: . name: crontabs.example.com spec: # group name to use for REST API: /apis// group: example.com # list of versions supported by this CustomResourceDefinition versions: \- name: v1beta1 # Each version can be enabled/disabled by Served flag. served: true # One and only one version must be marked as the storage version. storage: true # Each version can define its own schema when there is no top-level # schema is defined. schema: openAPIV3Schema: type: object properties: hostPort: type: string \- name: v1 served: true storage: false schema: openAPIV3Schema: type: object properties: host: type: string port: type: string conversion: # the Webhook strategy instructs the API server to call an external webhook for any conversion between custom resources. strategy: Webhook # webhook is required when strategy is `Webhook` and it configures the webhook endpoint to be called by API server. webhook: # conversionReviewVersions indicates what ConversionReview versions are understood/preferred by the webhook. # The first version in the list understood by the API server is sent to the webhook. # The webhook must respond with a ConversionReview object in the same version it received. conversionReviewVersions: ["v1","v1beta1"] clientConfig: service: namespace: default name: example-conversion-webhook-server path: /crdconvert caBundle: "Ci0tLS0tQk......tLS0K" # either Namespaced or Cluster scope: Namespaced names: # plural name to be used in the URL: /apis/// plural: crontabs # singular name to be used as an alias on the CLI and for display singular: crontab # kind is normally the CamelCased singular type. Your resource manifests use this. kind: CronTab # shortNames allow shorter string to match your resource on the CLI shortNames: \- ct # Deprecated in v1.16 in favor of apiextensions.k8s.io/v1 apiVersion: apiextensions.k8s.io/v1beta1 kind: CustomResourceDefinition metadata: # name must match the spec fields below, and be in the form: . name: crontabs.example.com spec: # group name to use for REST API: /apis// group: example.com # prunes object fields that are not specified in OpenAPI schemas below. preserveUnknownFields: false # list of versions supported by this CustomResourceDefinition versions: \- name: v1beta1 # Each version can be enabled/disabled by Served flag. served: true # One and only one version must be marked as the storage version. storage: true # Each version can define its own schema when there is no top-level # schema is defined. schema: openAPIV3Schema: type: object properties: hostPort: type: string \- name: v1 served: true storage: false schema: openAPIV3Schema: type: object properties: host: type: string port: type: string conversion: # the Webhook strategy instructs the API server to call an external webhook for any conversion between custom resources. strategy: Webhook # webhookClientConfig is required when strategy is `Webhook` and it configures the webhook endpoint to be called by API server. webhookClientConfig: service: namespace: default name: example-conversion-webhook-server path: /crdconvert caBundle: "Ci0tLS0tQk......tLS0K" # either Namespaced or Cluster scope: Namespaced names: # plural name to be used in the URL: /apis/// plural: crontabs # singular name to be used as an alias on the CLI and for display singular: crontab # kind is normally the CamelCased singular type. Your resource manifests use this. kind: CronTab # shortNames allow shorter string to match your resource on the CLI shortNames: \- ct You can save the CustomResourceDefinition in a YAML file, then use kubectl apply to apply it.kubectl apply -f my-versioned-crontab-with-conversion.yaml Make sure the conversion service is up and running before applying new changes.Contacting the webhookOnce the API server has determined a request should be sent to a conversion webhook, it needs to know how to contact the webhook. This is specified in the webhookClientConfig stanza of the webhook configuration.Conversion webhooks can either be called via a URL or a service reference, and can optionally include a custom CA bundle to use to verify the TLS connection.URLurl gives the location of the webhook, in standard URL form (scheme://host:port/path).The host should not refer to a service running in the cluster; use a service reference by specifying the service field instead. The host might be resolved via external DNS in some apiservers (i.e., kube-apiserver cannot resolve in-cluster DNS as that would be a layering violation). host may also be an IP address.Please note that using localhost or 127.0.0.1 as a host is risky unless you take great care to run this webhook on all hosts which run an apiserver which might need to make calls to this webhook. Such installations are likely to be non-portable or not readily run in a new cluster.The scheme must be "https"; the URL must begin with "https://".Attempting to use a user or basic auth (for example "user:password@") is not allowed. Fragments ("#...") and query parameters ("?...") are also not allowed.Here is an example of a conversion webhook configured to call a URL (and expects the TLS certificate to be verified using system trust roots, so does not specify a caBundle):apiextensions.k8s.io/v1apiextensions.k8s.io/v1beta1apiVersion: apiextensions.k8s.io/v1 kind: CustomResourceDefinition ... spec: ... conversion: strategy: Webhook webhook: clientConfig: url: "https://my-webhook.example.com:9443/my-webhook-path" ... # Deprecated in v1.16 in favor of apiextensions.k8s.io/v1 apiVersion: apiextensions.k8s.io/v1beta1 kind: CustomResourceDefinition ... spec: ... conversion: strategy: Webhook webhookClientConfig: url: "https://my-webhook.example.com:9443/my-webhook-path" ... Service ReferenceThe service stanza inside webhookClientConfig is a reference to the service for a conversion webhook. If the webhook is running within the cluster, then you should use service instead of url. The service namespace and name are required. The port is optional and defaults to 443. The path is optional and defaults to "/".Here is an example of a webhook that is configured to call a service on port "1234" at the subpath "/my-path", and to verify the TLS connection against the ServerName my-service-name.my-service-namespace.svc using a custom CA bundle.apiextensions.k8s.io/v1apiextensions.k8s.io/v1beta1apiVersion: apiextensions.k8s.io/v1 kind: CustomResourceDefinition ... spec: ... conversion: strategy: Webhook webhook: clientConfig: service: namespace: my-service-namespace name: my-service-name path: /my-path port: 1234 caBundle: "Ci0tLS0tQk......tLS0K" ... # Deprecated in v1.16 in favor of apiextensions.k8s.io/v1 apiVersion: apiextensions.k8s.io/v1beta1 kind: CustomResourceDefinition ... spec: ... conversion: strategy: Webhook webhookClientConfig: service: namespace: my-service-namespace name: my-service-name path: /my-path port: 1234 caBundle: "Ci0tLS0tQk......tLS0K" ... Webhook request and responseRequestWebhooks are sent a POST request, with Content-Type: application/json, with a ConversionReview API object in the apiextensions.k8s.io API group serialized to JSON as the body.Webhooks can specify what versions of ConversionReview objects they accept with the conversionReviewVersions field in their CustomResourceDefinition:apiextensions.k8s.io/v1apiextensions.k8s.io/v1beta1apiVersion: apiextensions.k8s.io/v1 kind: CustomResourceDefinition ... spec: ... conversion: strategy: Webhook webhook: conversionReviewVersions: ["v1", "v1beta1"] ... conversionReviewVersions is a required field when creating apiextensions.k8s.io/v1 custom resource definitions. Webhooks are required to support at least one ConversionReview version understood by the current and previous API server.# Deprecated in v1.16 in favor of apiextensions.k8s.io/v1 apiVersion: apiextensions.k8s.io/v1beta1 kind: CustomResourceDefinition ... spec: ... conversion: strategy: Webhook conversionReviewVersions: ["v1", "v1beta1"] ... If no conversionReviewVersions are specified, the default when creating apiextensions.k8s.io/v1beta1 custom resource definitions is v1beta1.API servers send the first ConversionReview version in the conversionReviewVersions list they support. If none of the versions in the list are supported by the API server, the custom resource definition will not be allowed to be created. If an API server encounters a conversion webhook configuration that was previously created and does not support any of the ConversionReview versions the API server knows how to send, attempts to call to the webhook will fail.This example shows the data contained in an ConversionReview object for a request to convert CronTab objects to example.com/v1:apiextensions.k8s.io/v1apiextensions.k8s.io/v1beta1{ "apiVersion": "apiextensions.k8s.io/v1", "kind": "ConversionReview", "request": { # Random uid uniquely identifying this conversion call "uid": "705ab4f5-6393-11e8-b7cc-42010a800002", # The API group and version the objects should be converted to "desiredAPIVersion": "example.com/v1", # The list of objects to convert. # May contain one or more objects, in one or more versions. "objects": [ { "kind": "CronTab", "apiVersion": "example.com/v1beta1", "metadata": { "creationTimestamp": "2019-09-04T14:03:02Z", "name": "local-crontab", "namespace": "default", "resourceVersion": "143", "uid": "3415a7fc-162b-4300-b5da-fd6083580d66" }, "hostPort": "localhost:1234" }, { "kind": "CronTab", "apiVersion": "example.com/v1beta1", "metadata": { "creationTimestamp": "2019-09-03T13:02:01Z", "name": "remote-crontab", "resourceVersion": "12893", "uid": "359a83ec-b575-460d-b553-d859cedde8a0" }, "hostPort": "example.com:2345" } ] } } { # Deprecated in v1.16 in favor of apiextensions.k8s.io/v1 "apiVersion": "apiextensions.k8s.io/v1beta1", "kind": "ConversionReview", "request": { # Random uid uniquely identifying this conversion call "uid": "705ab4f5-6393-11e8-b7cc-42010a800002", # The API group and version the objects should be converted to "desiredAPIVersion": "example.com/v1", # The list of objects to convert. # May contain one or more objects, in one or more versions. "objects": [ { "kind": "CronTab", "apiVersion": "example.com/v1beta1", "metadata": { "creationTimestamp": "2019-09-04T14:03:02Z", "name": "local-crontab", "namespace": "default", "resourceVersion": "143", "uid": "3415a7fc-162b-4300-b5da-fd6083580d66" }, "hostPort": "localhost:1234" }, { "kind": "CronTab", "apiVersion": "example.com/v1beta1", "metadata": { "creationTimestamp": "2019-09-03T13:02:01Z", "name": "remote-crontab", "resourceVersion": "12893", "uid": "359a83ec-b575-460d-b553-d859cedde8a0" }, "hostPort": "example.com:2345" } ] } } ResponseWebhooks respond with a 200 HTTP status code, Content-Type: application/json, and a body containing a ConversionReview object (in the same version they were sent), with the response stanza populated, serialized to JSON.If conversion succeeds, a webhook should return a response stanza containing the following fields:uid, copied from the request.uid sent to the webhookresult, set to {"status":"Success"}convertedObjects, containing all of the objects from request.objects, converted to request.desiredAPIVersionExample of a minimal successful response from a webhook:apiextensions.k8s.io/v1apiextensions.k8s.io/v1beta1{ "apiVersion": "apiextensions.k8s.io/v1", "kind": "ConversionReview", "response": { # must match  "uid": "705ab4f5-6393-11e8-b7cc-42010a800002", "result": { "status": "Success" }, # Objects must match the order of request.objects, and have apiVersion set to . # kind, metadata.uid, metadata.name, and metadata.namespace fields must not be changed by the webhook. # metadata.labels and metadata.annotations fields may be changed by the webhook. # All other changes to metadata fields by the webhook are ignored. "convertedObjects": [ { "kind": "CronTab", "apiVersion": "example.com/v1", "metadata": { "creationTimestamp": "2019-09-04T14:03:02Z", "name": "local-crontab", "namespace": "default", "resourceVersion": "143", "uid": "3415a7fc-162b-4300-b5da-fd6083580d66" }, "host": "localhost", "port": "1234" }, { "kind": "CronTab", "apiVersion": "example.com/v1", "metadata": { "creationTimestamp": "2019-09-03T13:02:01Z", "name": "remote-crontab", "resourceVersion": "12893", "uid": "359a83ec-b575-460d-b553-d859cedde8a0" }, "host": "example.com", "port": "2345" } ] } } { # Deprecated in v1.16 in favor of apiextensions.k8s.io/v1 "apiVersion": "apiextensions.k8s.io/v1beta1", "kind": "ConversionReview", "response": { # must match  "uid": "705ab4f5-6393-11e8-b7cc-42010a800002", "result": { "status": "Failed" }, # Objects must match the order of request.objects, and have apiVersion set to . # kind, metadata.uid, metadata.name, and metadata.namespace fields must not be changed by the webhook. # metadata.labels and metadata.annotations fields may be changed by the webhook. # All other changes to metadata fields by the webhook are ignored. "convertedObjects": [ { "kind": "CronTab", "apiVersion": "example.com/v1", "metadata": { "creationTimestamp": "2019-09-04T14:03:02Z", "name": "local-crontab", "namespace": "default", "resourceVersion": "143", "uid": "3415a7fc-162b-4300-b5da-fd6083580d66" }, "host": "localhost", "port": "1234" }, { "kind": "CronTab", "apiVersion": "example.com/v1", "metadata": { "creationTimestamp": "2019-09-03T13:02:01Z", "name": "remote-crontab", "resourceVersion": "12893", "uid": "359a83ec-b575-460d-b553-d859cedde8a0" }, "host": "example.com", "port": "2345" } ] } } If conversion fails, a webhook should return a response stanza containing the following fields:uid, copied from the request.uid sent to the webhookresult, set to {"status":"Failed"}Warning:Failing conversion can disrupt read and write access to the custom resources, including the ability to update or delete the resources. Conversion failures should be avoided whenever possible, and should not be used to enforce validation constraints (use validation schemas or webhook admission instead).Example of a response from a webhook indicating a conversion request failed, with an optional message:apiextensions.k8s.io/v1apiextensions.k8s.io/v1beta1{ "apiVersion": "apiextensions.k8s.io/v1", "kind": "ConversionReview", "response": { "uid": "", "result": { "status": "Failed", "message": "hostPort could not be parsed into a separate host and port" } } } { # Deprecated in v1.16 in favor of apiextensions.k8s.io/v1 "apiVersion": "apiextensions.k8s.io/v1beta1", "kind": "ConversionReview", "response": { "uid": "", "result": { "status": "Failed", "message": "hostPort could not be parsed into a separate host and port" } } } Writing, reading, and updating versioned CustomResourceDefinition objectsWhen an object is written, it is stored at the version designated as the storage version at the time of the write. If the storage version changes, existing objects are never converted automatically. However, newly-created or updated objects are written at the new storage version. It is possible for an object to have been written at a version that is no longer served.When you read an object, you specify the version as part of the path. You can request an object at any version that is currently served. If you specify a version that is different from the object's stored version, Kubernetes returns the object to you at the version you requested, but the stored object is not changed on disk.What happens to the object that is being returned while serving the read request depends on what is specified in the CRD's spec.conversion:if the default strategy value None is specified, the only modifications to the object are changing the apiVersion string and perhaps pruning unknown fields (depending on the configuration). Note that this is unlikely to lead to good results if the schemas differ between the storage and requested version. In particular, you should not use this strategy if the same data is represented in different fields between versions.if webhook conversion is specified, then this mechanism controls the conversion.If you update an existing object, it is rewritten at the version that is currently the storage version. This is the only way that objects can change from one version to another.To illustrate this, consider the following hypothetical series of events:The storage version is v1beta1. You create an object. It is stored at version v1beta1You add version v1 to your CustomResourceDefinition and designate it as the storage version. Here the schemas for v1 and v1beta1 are identical, which is typically the case when promoting an API to stable in the Kubernetes ecosystem.You read your object at version v1beta1, then you read the object again at version v1. Both returned objects are identical except for the apiVersion field.You create a new object. It is stored at version v1. You now have two objects, one of which is at v1beta1, and the other of which is at v1.You update the first object. It is now stored at version v1 since that is the current storage version.Previous storage versionsThe API server records each version which has ever been marked as the storage version in the status field storedVersions. Objects may have been stored at any version that has ever been designated as a storage version. No objects can exist in storage at a version that has never been a storage version.Upgrade existing objects to a new stored versionWhen deprecating versions and dropping support, select a storage upgrade procedure.Option 1: Use the Storage Version MigratorRun the storage Version migratorRemove the old version from the CustomResourceDefinition status.storedVersions field.Option 2: Manually upgrade the existing objects to a new stored versionThe following is an example procedure to upgrade from v1beta1 to v1.Set v1 as the storage in the CustomResourceDefinition file and apply it using kubectl. The storedVersions is now v1beta1, v1.Write an upgrade procedure to list all existing objects and write them with the same content. This forces the backend to write objects in the current storage version, which is v1.Remove v1beta1 from the CustomResourceDefinition status.storedVersions field.Note:Here is an example of how to patch the status subresource for a CRD object using kubectl:kubectl patch customresourcedefinitions  \--subresource='status' --type='merge' -p '{"status":{"storedVersions":["v1"]}}' 4.11.3 - Set up an Extension API ServerSetting up an extension API server to work with the aggregation layer allows the Kubernetes apiserver to be extended with additional APIs, which are not part of the core Kubernetes APIs.Before you beginYou need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesTo check the version, enter kubectl version.You must configure the aggregation layer and enable the apiserver flags.Set up an extension api-server to work with the aggregation layerThe following steps describe how to set up an extension-apiserver at a high level. These steps apply regardless if you're using YAML configs or using APIs. An attempt is made to specifically identify any differences between the two. For a concrete example of how they can be implemented using YAML configs, you can look at the sample-apiserver in the Kubernetes repo.Alternatively, you can use an existing 3rd party solution, such as apiserver-builder, which should generate a skeleton and automate all of the following steps for you.Make sure the APIService API is enabled (check --runtime-config). It should be on by default, unless it's been deliberately turned off in your cluster.You may need to make an RBAC rule allowing you to add APIService objects, or get your cluster administrator to make one. (Since API extensions affect the entire cluster, it is not recommended to do testing/development/debug of an API extension in a live cluster.)Create the Kubernetes namespace you want to run your extension api-service in.Create/get a CA cert to be used to sign the server cert the extension api-server uses for HTTPS.Create a server cert/key for the api-server to use for HTTPS. This cert should be signed by the above CA. It should also have a CN of the Kube DNS name. This is derived from the Kubernetes service and be of the form ..svcCreate a Kubernetes secret with the server cert/key in your namespace.Create a Kubernetes deployment for the extension api-server and make sure you are loading the secret as a volume. It should contain a reference to a working image of your extension api-server. The deployment should also be in your namespace.Make sure that your extension-apiserver loads those certs from that volume and that they are used in the HTTPS handshake.Create a Kubernetes service account in your namespace.Create a Kubernetes cluster role for the operations you want to allow on your resources.Create a Kubernetes cluster role binding from the service account in your namespace to the cluster role you created.Create a Kubernetes cluster role binding from the service account in your namespace to the system:auth-delegator cluster role to delegate auth decisions to the Kubernetes core API server.Create a Kubernetes role binding from the service account in your namespace to the extension-apiserver-authentication-reader role. This allows your extension api-server to access the extension-apiserver-authentication configmap.Create a Kubernetes apiservice. The CA cert above should be base64 encoded, stripped of new lines and used as the spec.caBundle in the apiservice. This should not be namespaced. If using the kube-aggregator API, only pass in the PEM encoded CA bundle because the base 64 encoding is done for you.Use kubectl to get your resource. When run, kubectl should return "No resources found.". This message indicates that everything worked but you currently have no objects of that resource type created.What's nextWalk through the steps to configure the API aggregation layer and enable the apiserver flags.For a high level overview, see Extending the Kubernetes API with the aggregation layer.Learn how to Extend the Kubernetes API using Custom Resource Definitions.4.11.4 - Configure Multiple SchedulersKubernetes ships with a default scheduler that is described here. If the default scheduler does not suit your needs you can implement your own scheduler. Moreover, you can even run multiple schedulers simultaneously alongside the default scheduler and instruct Kubernetes what scheduler to use for each of your pods. Let's learn how to run multiple schedulers in Kubernetes with an example.A detailed description of how to implement a scheduler is outside the scope of this document. Please refer to the kube-scheduler implementation in pkg/scheduler in the Kubernetes source directory for a canonical example.Before you beginYou need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesTo check the version, enter kubectl version.Package the schedulerPackage your scheduler binary into a container image. For the purposes of this example, you can use the default scheduler (kube-scheduler) as your second scheduler. Clone the Kubernetes source code from GitHub and build the source.git clone https://github.com/kubernetes/kubernetes.git cd kubernetes make Create a container image containing the kube-scheduler binary. Here is the Dockerfile to build the image:FROM busybox ADD ./_output/local/bin/linux/amd64/kube-scheduler /usr/local/bin/kube-scheduler Save the file as Dockerfile, build the image and push it to a registry. This example pushes the image to Google Container Registry (GCR). For more details, please read the GCR documentation. Alternatively you can also use the docker hub. For more details refer to the docker hub documentation.docker build -t gcr.io/my-gcp-project/my-kube-scheduler:1.0 . # The image name and the repository gcloud docker -- push gcr.io/my-gcp-project/my-kube-scheduler:1.0 # used in here is just an example Define a Kubernetes Deployment for the schedulerNow that you have your scheduler in a container image, create a pod configuration for it and run it in your Kubernetes cluster. But instead of creating a pod directly in the cluster, you can use a Deployment for this example. A Deployment manages a Replica Set which in turn manages the pods, thereby making the scheduler resilient to failures. Here is the deployment config. Save it as my-scheduler.yaml:admin/sched/my-scheduler.yaml apiVersion: v1 kind: ServiceAccount metadata: name: my-scheduler namespace: kube-system \--- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: my-scheduler-as-kube-scheduler subjects: \- kind: ServiceAccount name: my-scheduler namespace: kube-system roleRef: kind: ClusterRole name: system:kube-scheduler apiGroup: rbac.authorization.k8s.io \--- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: my-scheduler-as-volume-scheduler subjects: \- kind: ServiceAccount name: my-scheduler namespace: kube-system roleRef: kind: ClusterRole name: system:volume-scheduler apiGroup: rbac.authorization.k8s.io \--- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: my-scheduler-extension-apiserver-authentication-reader namespace: kube-system roleRef: kind: Role name: extension-apiserver-authentication-reader apiGroup: rbac.authorization.k8s.io subjects: \- kind: ServiceAccount name: my-scheduler namespace: kube-system \--- apiVersion: v1 kind: ConfigMap metadata: name: my-scheduler-config namespace: kube-system data: my-scheduler-config.yaml: | apiVersion: kubescheduler.config.k8s.io/v1beta2 kind: KubeSchedulerConfiguration profiles: \- schedulerName: my-scheduler leaderElection: leaderElect: false \--- apiVersion: apps/v1 kind: Deployment metadata: labels: component: scheduler tier: control-plane name: my-scheduler namespace: kube-system spec: selector: matchLabels: component: scheduler tier: control-plane replicas: 1 template: metadata: labels: component: scheduler tier: control-plane version: second spec: serviceAccountName: my-scheduler containers: \- command: \- /usr/local/bin/kube-scheduler \- --config=/etc/kubernetes/my-scheduler/my-scheduler-config.yaml image: gcr.io/my-gcp-project/my-kube-scheduler:1.0 livenessProbe: httpGet: path: /healthz port: 10259 scheme: HTTPS initialDelaySeconds: 15 name: kube-second-scheduler readinessProbe: httpGet: path: /healthz port: 10259 scheme: HTTPS resources: requests: cpu: '0.1' securityContext: privileged: false volumeMounts: \- name: config-volume mountPath: /etc/kubernetes/my-scheduler hostNetwork: false hostPID: false volumes: \- name: config-volume configMap: name: my-scheduler-config In the above manifest, you use a KubeSchedulerConfiguration to customize the behavior of your scheduler implementation. This configuration has been passed to the kube-scheduler during initialization with the --config option. The my-scheduler-config ConfigMap stores the configuration file. The Pod of themy-scheduler Deployment mounts the my-scheduler-config ConfigMap as a volume.In the aforementioned Scheduler Configuration, your scheduler implementation is represented via a KubeSchedulerProfile.Note:To determine if a scheduler is responsible for scheduling a specific Pod, the spec.schedulerName field in a PodTemplate or Pod manifest must match the schedulerName field of the KubeSchedulerProfile. All schedulers running in the cluster must have unique names.Also, note that you create a dedicated service account my-scheduler and bind the ClusterRole system:kube-scheduler to it so that it can acquire the same privileges as kube-scheduler.Please see the kube-scheduler documentation for detailed description of other command line arguments and Scheduler Configuration reference for detailed description of other customizable kube-scheduler configurations.Run the second scheduler in the clusterIn order to run your scheduler in a Kubernetes cluster, create the deployment specified in the config above in a Kubernetes cluster:kubectl create -f my-scheduler.yaml Verify that the scheduler pod is running:kubectl get pods --namespace=kube-system NAME READY STATUS RESTARTS AGE .... my-scheduler-lnf4s-4744f 1/1 Running 0 2m ... You should see a "Running" my-scheduler pod, in addition to the default kube-scheduler pod in this list.Enable leader electionTo run multiple-scheduler with leader election enabled, you must do the following:Update the following fields for the KubeSchedulerConfiguration in the my-scheduler-config ConfigMap in your YAML file:leaderElection.leaderElect to trueleaderElection.resourceNamespace to leaderElection.resourceName to Note:The control plane creates the lock objects for you, but the namespace must already exist. You can use the kube-system namespace.If RBAC is enabled on your cluster, you must update the system:kube-scheduler cluster role. Add your scheduler name to the resourceNames of the rule applied for endpoints and leases resources, as in the following example:kubectl edit clusterrole system:kube-scheduler admin/sched/clusterrole.yaml apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: annotations: rbac.authorization.kubernetes.io/autoupdate: "true" labels: kubernetes.io/bootstrapping: rbac-defaults name: system:kube-scheduler rules: \- apiGroups: \- coordination.k8s.io resources: \- leases verbs: \- create \- apiGroups: \- coordination.k8s.io resourceNames: \- kube-scheduler \- my-scheduler resources: \- leases verbs: \- get \- update \- apiGroups: \- "" resourceNames: \- kube-scheduler \- my-scheduler resources: \- endpoints verbs: \- delete \- get \- patch \- update Specify schedulers for podsNow that your second scheduler is running, create some pods, and direct them to be scheduled by either the default scheduler or the one you deployed. In order to schedule a given pod using a specific scheduler, specify the name of the scheduler in that pod spec. Let's look at three examples.Pod spec without any scheduler nameadmin/sched/pod1.yaml apiVersion: v1 kind: Pod metadata: name: no-annotation labels: name: multischeduler-example spec: containers: \- name: pod-with-no-annotation-container image: registry.k8s.io/pause:3.8When no scheduler name is supplied, the pod is automatically scheduled using the default-scheduler.Save this file as pod1.yaml and submit it to the Kubernetes cluster.kubectl create -f pod1.yaml Pod spec with default-scheduleradmin/sched/pod2.yaml apiVersion: v1 kind: Pod metadata: name: annotation-default-scheduler labels: name: multischeduler-example spec: schedulerName: default-scheduler containers: \- name: pod-with-default-annotation-container image: registry.k8s.io/pause:3.8 A scheduler is specified by supplying the scheduler name as a value to spec.schedulerName. In this case, we supply the name of the default scheduler which is default-scheduler.Save this file as pod2.yaml and submit it to the Kubernetes cluster.kubectl create -f pod2.yaml Pod spec with my-scheduleradmin/sched/pod3.yaml apiVersion: v1 kind: Pod metadata: name: annotation-second-scheduler labels: name: multischeduler-example spec: schedulerName: my-scheduler containers: \- name: pod-with-second-annotation-container image: registry.k8s.io/pause:3.8 In this case, we specify that this pod should be scheduled using the scheduler that we deployed - my-scheduler. Note that the value of spec.schedulerName should match the name supplied for the scheduler in the schedulerName field of the mapping KubeSchedulerProfile.Save this file as pod3.yaml and submit it to the Kubernetes cluster.kubectl create -f pod3.yaml Verify that all three pods are running.kubectl get pods Verifying that the pods were scheduled using the desired schedulersIn order to make it easier to work through these examples, we did not verify that the pods were actually scheduled using the desired schedulers. We can verify that by changing the order of pod and deployment config submissions above. If we submit all the pod configs to a Kubernetes cluster before submitting the scheduler deployment config, we see that the pod annotation-second-scheduler remains in "Pending" state forever while the other two pods get scheduled. Once we submit the scheduler deployment config and our new scheduler starts running, the annotation-second-scheduler pod gets scheduled as well.Alternatively, you can look at the "Scheduled" entries in the event logs to verify that the pods were scheduled by the desired schedulers.kubectl get events You can also use a custom scheduler configuration or a custom container image for the cluster's main scheduler by modifying its static pod manifest on the relevant control plane nodes.4.11.5 - Use an HTTP Proxy to Access the Kubernetes APIThis page shows how to use an HTTP proxy to access the Kubernetes API.Before you beginYou need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesTo check the version, enter kubectl version.If you do not already have an application running in your cluster, start a Hello world application by entering this command:kubectl create deployment hello-app --image=gcr.io/google-samples/hello-app:2.0 --port=8080 Using kubectl to start a proxy serverThis command starts a proxy to the Kubernetes API server:kubectl proxy --port=8080 Exploring the Kubernetes APIWhen the proxy server is running, you can explore the API using curl, wget, or a browser.Get the API versions:curl http://localhost:8080/api/ The output should look similar to this:{ "kind": "APIVersions", "versions": [ "v1" ], "serverAddressByClientCIDRs": [ { "clientCIDR": "0.0.0.0/0", "serverAddress": "10.0.2.15:8443" } ] } Get a list of pods:curl http://localhost:8080/api/v1/namespaces/default/pods The output should look similar to this:{ "kind": "PodList", "apiVersion": "v1", "metadata": { "resourceVersion": "33074" }, "items": [ { "metadata": { "name": "kubernetes-bootcamp-2321272333-ix8pt", "generateName": "kubernetes-bootcamp-2321272333-", "namespace": "default", "uid": "ba21457c-6b1d-11e6-85f7-1ef9f1dab92b", "resourceVersion": "33003", "creationTimestamp": "2016-08-25T23:43:30Z", "labels": { "pod-template-hash": "2321272333", "run": "kubernetes-bootcamp" }, ... } What's nextLearn more about kubectl proxy.4.11.6 - Use a SOCKS5 Proxy to Access the Kubernetes APIFEATURE STATE: Kubernetes v1.24 [stable]This page shows how to use a SOCKS5 proxy to access the API of a remote Kubernetes cluster. This is useful when the cluster you want to access does not expose its API directly on the public internet.Before you beginYou need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesYour Kubernetes server must be at or later than version v1.24.To check the version, enter kubectl version.You need SSH client software (the ssh tool), and an SSH service running on the remote server. You must be able to log in to the SSH service on the remote server.Task contextNote:This example tunnels traffic using SSH, with the SSH client and server acting as a SOCKS proxy. You can instead use any other kind of SOCKS5 proxies.Figure 1 represents what you're going to achieve in this task.You have a client computer, referred to as local in the steps ahead, from where you're going to create requests to talk to the Kubernetes API.The Kubernetes server/API is hosted on a remote server.You will use SSH client and server software to create a secure SOCKS5 tunnel between the local and the remote server. The HTTPS traffic between the client and the Kubernetes API will flow over the SOCKS5 tunnel, which is itself tunnelled over SSH.graph LR; subgraph local[Local client machine] client([client])-. localtraffic .-> local_ssh[Local SSHSOCKS5 proxy]; end local_ssh[SSHSOCKS5proxy]-- SSH Tunnel -->sshd subgraph remote[Remote server] sshd[SSHserver]-- local traffic -->service1; end client([client])-. proxied HTTPs trafficgoing through the proxy .->service1[Kubernetes API]; classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000; classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff; classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5; class ingress,service1,service2,pod1,pod2,pod3,pod4 k8s; class client plain; class cluster cluster;JavaScript must be enabled to view this contentFigure 1. SOCKS5 tutorial componentsUsing ssh to create a SOCKS5 proxyThe following command starts a SOCKS5 proxy between your client machine and the remote SOCKS server:# The SSH tunnel continues running in the foreground after you run this ssh -D 1080 -q -N username@kubernetes-remote-server.example The SOCKS5 proxy lets you connect to your cluster's API server based on the following configuration:-D 1080: opens a SOCKS proxy on local port :1080.-q: quiet mode. Causes most warning and diagnostic messages to be suppressed.-N: Do not execute a remote command. Useful for just forwarding ports.username@kubernetes-remote-server.example: the remote SSH server behind which the Kubernetes cluster is running (eg: a bastion host).Client configurationTo access the Kubernetes API server through the proxy you must instruct kubectl to send queries through the SOCKS proxy we created earlier. Do this by either setting the appropriate environment variable, or via the proxy-url attribute in the kubeconfig file. Using an environment variable:export HTTPS_PROXY=socks5://localhost:1080 To always use this setting on a specific kubectl context, specify the proxy-url attribute in the relevant cluster entry within the ~/.kube/config file. For example:apiVersion: v1 clusters: \- cluster: certificate-authority-data: LRMEMMW2 # shortened for readability server: https://:6443 # the "Kubernetes API" server, in other words the IP address of kubernetes-remote-server.example proxy-url: socks5://localhost:1080 # the "SSH SOCKS5 proxy" in the diagram above name: default contexts: \- context: cluster: default user: default name: default current-context: default kind: Config preferences: {} users: \- name: default user: client-certificate-data: LS0tLS1CR== # shortened for readability client-key-data: LS0tLS1CRUdJT= # shortened for readability Once you have created the tunnel via the ssh command mentioned earlier, and defined either the environment variable or the proxy-url attribute, you can interact with your cluster through that proxy. For example:kubectl get pods NAMESPACE NAME READY STATUS RESTARTS AGE kube-system coredns-85cb69466-klwq8 1/1 Running 0 5m46s Note:Before kubectl 1.24, most kubectl commands worked when using a socks proxy, except kubectl exec.kubectl supports both HTTPS_PROXY and https_proxy environment variables. These are used by other programs that support SOCKS, such as curl. Therefore in some cases it will be better to define the environment variable on the command line:HTTPS_PROXY=socks5://localhost:1080 kubectl get pods When using proxy-url, the proxy is used only for the relevant kubectl context, whereas the environment variable will affect all contexts.The k8s API server hostname can be further protected from DNS leakage by using the socks5h protocol name instead of the more commonly known socks5 protocol shown above. In this case, kubectl will ask the proxy server (such as an ssh bastion) to resolve the k8s API server domain name, instead of resolving it on the system running kubectl. Note also that with socks5h, a k8s API server URL like https://localhost:6443/api does not refer to your local client computer. Instead, it refers to localhost as known on the proxy server (eg the ssh bastion).Clean upStop the ssh port-forwarding process by pressing CTRL+C on the terminal where it is running.Type unset https_proxy in a terminal to stop forwarding http traffic through the proxy.Further readingOpenSSH remote login client4.11.7 - Set up Konnectivity serviceThe Konnectivity service provides a TCP level proxy for the control plane to cluster communication.Before you beginYou need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube.Configure the Konnectivity serviceThe following steps require an egress configuration, for example:admin/konnectivity/egress-selector-configuration.yaml apiVersion: apiserver.k8s.io/v1beta1 kind: EgressSelectorConfiguration egressSelections: # Since we want to control the egress traffic to the cluster, we use the # "cluster" as the name. Other supported values are "etcd", and "controlplane". \- name: cluster connection: # This controls the protocol between the API Server and the Konnectivity # server. Supported values are "GRPC" and "HTTPConnect". There is no # end user visible difference between the two modes. You need to set the # Konnectivity server to work in the same mode. proxyProtocol: GRPC transport: # This controls what transport the API Server uses to communicate with the # Konnectivity server. UDS is recommended if the Konnectivity server # locates on the same machine as the API Server. You need to configure the # Konnectivity server to listen on the same UDS socket. # The other supported transport is "tcp". You will need to set up TLS # config to secure the TCP transport. uds: udsName: /etc/kubernetes/konnectivity-server/konnectivity-server.socket You need to configure the API Server to use the Konnectivity service and direct the network traffic to the cluster nodes:Make sure that Service Account Token Volume Projection feature enabled in your cluster. It is enabled by default since Kubernetes v1.20.Create an egress configuration file such as admin/konnectivity/egress-selector-configuration.yaml.Set the --egress-selector-config-file flag of the API Server to the path of your API Server egress configuration file.If you use UDS connection, add volumes config to the kube-apiserver:spec: containers: volumeMounts: \- name: konnectivity-uds mountPath: /etc/kubernetes/konnectivity-server readOnly: false volumes: \- name: konnectivity-uds hostPath: path: /etc/kubernetes/konnectivity-server type: DirectoryOrCreate Generate or obtain a certificate and kubeconfig for konnectivity-server. For example, you can use the OpenSSL command line tool to issue a X.509 certificate, using the cluster CA certificate /etc/kubernetes/pki/ca.crt from a control-plane host.openssl req -subj "/CN=system:konnectivity-server" -new -newkey rsa:2048 -nodes -out konnectivity.csr -keyout konnectivity.key openssl x509 -req -in konnectivity.csr -CA /etc/kubernetes/pki/ca.crt -CAkey /etc/kubernetes/pki/ca.key -CAcreateserial -out konnectivity.crt -days 375 -sha256 SERVER=$(kubectl config view -o jsonpath='{.clusters..server}') kubectl --kubeconfig /etc/kubernetes/konnectivity-server.conf config set-credentials system:konnectivity-server --client-certificate konnectivity.crt --client-key konnectivity.key --embed-certs=true kubectl --kubeconfig /etc/kubernetes/konnectivity-server.conf config set-cluster kubernetes --server "$SERVER" --certificate-authority /etc/kubernetes/pki/ca.crt --embed-certs=true kubectl --kubeconfig /etc/kubernetes/konnectivity-server.conf config set-context system:konnectivity-server@kubernetes --cluster kubernetes --user system:konnectivity-server kubectl --kubeconfig /etc/kubernetes/konnectivity-server.conf config use-context system:konnectivity-server@kubernetes rm -f konnectivity.crt konnectivity.key konnectivity.csr Next, you need to deploy the Konnectivity server and agents. kubernetes-sigs/apiserver-network-proxy is a reference implementation.Deploy the Konnectivity server on your control plane node. The provided konnectivity-server.yaml manifest assumes that the Kubernetes components are deployed as a static Pod in your cluster. If not, you can deploy the Konnectivity server as a DaemonSet.admin/konnectivity/konnectivity-server.yaml apiVersion: v1 kind: Pod metadata: name: konnectivity-server namespace: kube-system spec: priorityClassName: system-cluster-critical hostNetwork: true containers: \- name: konnectivity-server-container image: registry.k8s.io/kas-network-proxy/proxy-server:v0.0.37 command: ["/proxy-server"] args: [ "--logtostderr=true", # This needs to be consistent with the value set in egressSelectorConfiguration. "--uds-name=/etc/kubernetes/konnectivity-server/konnectivity-server.socket", "--delete-existing-uds-file", # The following two lines assume the Konnectivity server is # deployed on the same machine as the apiserver, and the certs and # key of the API Server are at the specified location. "--cluster-cert=/etc/kubernetes/pki/apiserver.crt", "--cluster-key=/etc/kubernetes/pki/apiserver.key", # This needs to be consistent with the value set in egressSelectorConfiguration. "--mode=grpc", "--server-port=0", "--agent-port=8132", "--admin-port=8133", "--health-port=8134", "--agent-namespace=kube-system", "--agent-service-account=konnectivity-agent", "--kubeconfig=/etc/kubernetes/konnectivity-server.conf", "--authentication-audience=system:konnectivity-server" ] livenessProbe: httpGet: scheme: HTTP host: 127.0.0.1 port: 8134 path: /healthz initialDelaySeconds: 30 timeoutSeconds: 60 ports: \- name: agentport containerPort: 8132 hostPort: 8132 \- name: adminport containerPort: 8133 hostPort: 8133 \- name: healthport containerPort: 8134 hostPort: 8134 volumeMounts: \- name: k8s-certs mountPath: /etc/kubernetes/pki readOnly: true \- name: kubeconfig mountPath: /etc/kubernetes/konnectivity-server.conf readOnly: true \- name: konnectivity-uds mountPath: /etc/kubernetes/konnectivity-server readOnly: false volumes: \- name: k8s-certs hostPath: path: /etc/kubernetes/pki \- name: kubeconfig hostPath: path: /etc/kubernetes/konnectivity-server.conf type: FileOrCreate \- name: konnectivity-uds hostPath: path: /etc/kubernetes/konnectivity-server type: DirectoryOrCreate Then deploy the Konnectivity agents in your cluster:admin/konnectivity/konnectivity-agent.yaml apiVersion: apps/v1 # Alternatively, you can deploy the agents as Deployments. It is not necessary # to have an agent on each node. kind: DaemonSet metadata: labels: addonmanager.kubernetes.io/mode: Reconcile k8s-app: konnectivity-agent namespace: kube-system name: konnectivity-agent spec: selector: matchLabels: k8s-app: konnectivity-agent template: metadata: labels: k8s-app: konnectivity-agent spec: priorityClassName: system-cluster-critical tolerations: \- key: "CriticalAddonsOnly" operator: "Exists" containers: \- image: us.gcr.io/k8s-artifacts-prod/kas-network-proxy/proxy-agent:v0.0.37 name: konnectivity-agent command: ["/proxy-agent"] args: [ "--logtostderr=true", "--ca-cert=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt", # Since the konnectivity server runs with hostNetwork=true, # this is the IP address of the master machine. "--proxy-server-host=35.225.206.7", "--proxy-server-port=8132", "--admin-server-port=8133", "--health-server-port=8134", "--service-account-token-path=/var/run/secrets/tokens/konnectivity-agent-token" ] volumeMounts: \- mountPath: /var/run/secrets/tokens name: konnectivity-agent-token livenessProbe: httpGet: port: 8134 path: /healthz initialDelaySeconds: 15 timeoutSeconds: 15 serviceAccountName: konnectivity-agent volumes: \- name: konnectivity-agent-token projected: sources: \- serviceAccountToken: path: konnectivity-agent-token audience: system:konnectivity-server Last, if RBAC is enabled in your cluster, create the relevant RBAC rules:admin/konnectivity/konnectivity-rbac.yaml apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: system:konnectivity-server labels: kubernetes.io/cluster-service: "true" addonmanager.kubernetes.io/mode: Reconcile roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:auth-delegator subjects: \- apiGroup: rbac.authorization.k8s.io kind: User name: system:konnectivity-server \--- apiVersion: v1 kind: ServiceAccount metadata: name: konnectivity-agent namespace: kube-system labels: kubernetes.io/cluster-service: "true" addonmanager.kubernetes.io/mode: Reconcile 4.12 - TLSUnderstand how to protect traffic within your cluster using Transport Layer Security (TLS).4.12.1 - Issue a Certificate for a Kubernetes API Client Using A CertificateSigningRequestKubernetes lets you use a public key infrastructure (PKI) to authenticate to your cluster as a client.A few steps are required in order to get a normal user to be able to authenticate and invoke an API. First, this user must have an X.509 certificate issued by an authority that your Kubernetes cluster trusts. The client must then present that certificate to the Kubernetes API.You use a CertificateSigningRequest as part of this process, and either you or some other principal must approve the request.You will create a private key, and then get a certificate issued, and finally configure that private key for a client.Before you beginYou need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesYou need the kubectl, openssl and base64 utilities.This page assumes you are using Kubernetes role based access control (RBAC). If you have alternative or additional security mechanisms around authorization, you need to account for those as well.Create private keyIn this step, you create a private key. You need to keep this document secret; anyone who has it can impersonate the user.# Create a private key openssl genrsa -out myuser.key 3072 Create an X.509 certificate signing requestNote:This is not the same as the similarly-named CertificateSigningRequest API; the file you generate here goes into the CertificateSigningRequest.It is important to set CN and O attribute of the CSR. CN is the name of the user and O is the group that this user will belong to. You can refer to RBAC for standard groups.# Change the common name "myuser" to the actual username that you want to use openssl req -new -key myuser.key -out myuser.csr -subj "/CN=myuser" Create a Kubernetes CertificateSigningRequestEncode the CSR document using this command:cat myuser.csr | base64 | tr -d "\n" Create a CertificateSigningRequest and submit it to a Kubernetes Cluster via kubectl. Below is a snippet of shell that you can use to generate the CertificateSigningRequest.cat < myuser.crt Configure the certificate into kubeconfigThe next step is to add this user into the kubeconfig file.First, you need to add new credentials:kubectl config set-credentials myuser --client-key=myuser.key --client-certificate=myuser.crt --embed-certs=true Then, you need to add the context:kubectl config set-context myuser --cluster=kubernetes --user=myuser To test it:kubectl --context myuser auth whoami You should see output confirming that you are “myuser“.Create Role and RoleBindingNote:If you don't use Kubernetes RBAC, skip this step and make the appropriate changes for the authorization mechanism your cluster actually uses.With the certificate created it is time to define the Role and RoleBinding for this user to access Kubernetes cluster resources.This is a sample command to create a Role for this new user:kubectl create role developer --verb=create --verb=get --verb=list --verb=update --verb=delete --resource=pods This is a sample command to create a RoleBinding for this new user:kubectl create rolebinding developer-binding-myuser --role=developer --user=myuser What's nextRead Manage TLS Certificates in a ClusterFor details of X.509 itself, refer to RFC 5280 section 3.1For information on the syntax of PKCS#10 certificate signing requests, refer to RFC 2986Read about ClusterTrustBundles4.12.2 - Configure Certificate Rotation for the KubeletThis page shows how to enable and configure certificate rotation for the kubelet.FEATURE STATE: Kubernetes v1.19 [stable]Before you beginKubernetes version 1.8.0 or later is requiredOverviewThe kubelet uses certificates for authenticating to the Kubernetes API. By default, these certificates are issued with one year expiration so that they do not need to be renewed too frequently.Kubernetes contains kubelet certificate rotation, that will automatically generate a new key and request a new certificate from the Kubernetes API as the current certificate approaches expiration. Once the new certificate is available, it will be used for authenticating connections to the Kubernetes API.Enabling client certificate rotationThe kubelet process accepts an argument --rotate-certificates that controls if the kubelet will automatically request a new certificate as the expiration of the certificate currently in use approaches.The kube-controller-manager process accepts an argument \--cluster-signing-duration (--experimental-cluster-signing-duration prior to 1.19) that controls how long certificates will be issued for.Understanding the certificate rotation configurationWhen a kubelet starts up, if it is configured to bootstrap (using the \--bootstrap-kubeconfig flag), it will use its initial certificate to connect to the Kubernetes API and issue a certificate signing request. You can view the status of certificate signing requests using:kubectl get csr Initially a certificate signing request from the kubelet on a node will have a status of Pending. If the certificate signing requests meets specific criteria, it will be auto approved by the controller manager, then it will have a status of Approved. Next, the controller manager will sign a certificate, issued for the duration specified by the \--cluster-signing-duration parameter, and the signed certificate will be attached to the certificate signing request.The kubelet will retrieve the signed certificate from the Kubernetes API and write that to disk, in the location specified by --cert-dir. Then the kubelet will use the new certificate to connect to the Kubernetes API.As the expiration of the signed certificate approaches, the kubelet will automatically issue a new certificate signing request, using the Kubernetes API. This can happen at any point between 30% and 10% of the time remaining on the certificate. Again, the controller manager will automatically approve the certificate request and attach a signed certificate to the certificate signing request. The kubelet will retrieve the new signed certificate from the Kubernetes API and write that to disk. Then it will update the connections it has to the Kubernetes API to reconnect using the new certificate.4.12.3 - Manage TLS Certificates in a ClusterKubernetes provides a certificates.k8s.io API, which lets you provision TLS certificates signed by a Certificate Authority (CA) that you control. These CA and certificates can be used by your workloads to establish trust.certificates.k8s.io API uses a protocol that is similar to the ACME draft.Note:Certificates created using the certificates.k8s.io API are signed by a dedicated CA. It is possible to configure your cluster to use the cluster root CA for this purpose, but you should never rely on this. Do not assume that these certificates will validate against the cluster root CA.Before you beginYou need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesYou need the cfssl tool. You can download cfssl from https://github.com/cloudflare/cfssl/releases.Some steps in this page use the jq tool. If you don't have jq, you can install it via your operating system's software sources, or fetch it from https://jqlang.github.io/jq/.Trusting TLS in a clusterTrusting the custom CA from an application running as a pod usually requires some extra application configuration. You will need to add the CA certificate bundle to the list of CA certificates that the TLS client or server trusts. For example, you would do this with a golang TLS config by parsing the certificate chain and adding the parsed certificates to the RootCAs field in the tls.Config struct.Note:Even though the custom CA certificate may be included in the filesystem (in the ConfigMap kube-root-ca.crt), you should not use that certificate authority for any purpose other than to verify internal Kubernetes endpoints. An example of an internal Kubernetes endpoint is the Service named kubernetes in the default namespace.If you want to use a custom certificate authority for your workloads, you should generate that CA separately, and distribute its CA certificate using a ConfigMap that your pods have access to read.Requesting a certificateThe following section demonstrates how to create a TLS certificate for a Kubernetes service accessed through DNS.Note:This tutorial uses CFSSL: Cloudflare's PKI and TLS toolkit click here to know more.Create a certificate signing requestGenerate a private key and certificate signing request (or CSR) by running the following command:cat < Annotations:  CreationTimestamp: Tue, 01 Feb 2022 11:49:15 -0500 Requesting User: yourname@example.com Signer: example.com/serving Status: Pending Subject: Common Name: my-pod.my-namespace.pod.cluster.local Serial Number: Subject Alternative Names: DNS Names: my-pod.my-namespace.pod.cluster.local my-svc.my-namespace.svc.cluster.local IP Addresses: 192.0.2.24 10.0.34.2 Events:  Get the CertificateSigningRequest approvedApproving the certificate signing request is either done by an automated approval process or on a one off basis by a cluster administrator. If you're authorized to approve a certificate request, you can do that manually using kubectl; for example:kubectl certificate approve my-svc.my-namespace certificatesigningrequest.certificates.k8s.io/my-svc.my-namespace approved You should now see the following:kubectl get csr NAME AGE SIGNERNAME REQUESTOR REQUESTEDDURATION CONDITION my-svc.my-namespace 10m example.com/serving yourname@example.com  Approved This means the certificate request has been approved and is waiting for the requested signer to sign it.Sign the CertificateSigningRequestNext, you'll play the part of a certificate signer, issue the certificate, and upload it to the API.A signer would typically watch the CertificateSigningRequest API for objects with its signerName, check that they have been approved, sign certificates for those requests, and update the API object status with the issued certificate.Create a Certificate AuthorityYou need an authority to provide the digital signature on the new certificate.First, create a signing certificate by running the following:cat < Approved,Issued Download the certificate and use itNow, as the requesting user, you can download the issued certificate and save it to a server.crt file by running the following:kubectl get csr my-svc.my-namespace -o jsonpath='{.status.certificate}' \ | base64 --decode > server.crt Now you can populate server.crt and server-key.pem in a Secret that you could later mount into a Pod (for example, to use with a webserver that serves HTTPS).kubectl create secret tls server --cert server.crt --key server-key.pem secret/server created Finally, you can populate ca.pem into a ConfigMap and use it as the trust root to verify the serving certificate:kubectl create configmap example-serving-ca --from-file ca.crt=ca.pem configmap/example-serving-ca created Approving CertificateSigningRequestsA Kubernetes administrator (with appropriate permissions) can manually approve (or deny) CertificateSigningRequests by using the kubectl certificate approve and kubectl certificate deny commands. However if you intend to make heavy usage of this API, you might consider writing an automated certificates controller.Caution:The ability to approve CSRs decides who trusts whom within your environment. The ability to approve CSRs should not be granted broadly or lightly.You should make sure that you confidently understand both the verification requirements that fall on the approver and the repercussions of issuing a specific certificate before you grant the approve permission.Whether a machine or a human using kubectl as above, the role of the approver is to verify that the CSR satisfies two requirements:The subject of the CSR controls the private key used to sign the CSR. This addresses the threat of a third party masquerading as an authorized subject. In the above example, this step would be to verify that the pod controls the private key used to generate the CSR.The subject of the CSR is authorized to act in the requested context. This addresses the threat of an undesired subject joining the cluster. In the above example, this step would be to verify that the pod is allowed to participate in the requested service.If and only if these two requirements are met, the approver should approve the CSR and otherwise should deny the CSR.For more information on certificate approval and access control, read the Certificate Signing Requests reference page.Configuring your cluster to provide signingThis page assumes that a signer is set up to serve the certificates API. The Kubernetes controller manager provides a default implementation of a signer. To enable it, pass the --cluster-signing-cert-file and \--cluster-signing-key-file parameters to the controller manager with paths to your Certificate Authority's keypair.4.12.4 - Manual Rotation of CA CertificatesThis page shows how to manually rotate the certificate authority (CA) certificates.Before you beginYou need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesFor more information about authentication in Kubernetes, see Authenticating.For more information about best practices for CA certificates, see Single root CA.Rotate the CA certificates manuallyCaution:Make sure to back up your certificate directory along with configuration files and any other necessary files.This approach assumes operation of the Kubernetes control plane in a HA configuration with multiple API servers. Graceful termination of the API server is also assumed so clients can cleanly disconnect from one API server and reconnect to another.Configurations with a single API server will experience unavailability while the API server is being restarted.Distribute the new CA certificates and private keys (for example: ca.crt, ca.key, front-proxy-ca.crt, and front-proxy-ca.key) to all your control plane nodes in the Kubernetes certificates directory.Update the --root-ca-file flag for the kube-controller-manager to include both old and new CA, then restart the kube-controller-manager.Any ServiceAccount created after this point will get Secrets that include both old and new CAs.Note:The files specified by the kube-controller-manager flags --client-ca-file and --cluster-signing-cert-file cannot be CA bundles. If these flags and --root-ca-file point to the same ca.crt file which is now a bundle (includes both old and new CA) you will face an error. To workaround this problem you can copy the new CA to a separate file and make the flags --client-ca-file and --cluster-signing-cert-file point to the copy. Once ca.crt is no longer a bundle you can restore the problem flags to point to ca.crt and delete the copy.Issue 1350 for kubeadm tracks an bug with the kube-controller-manager being unable to accept a CA bundle.Wait for the controller manager to update ca.crt in the service account Secrets to include both old and new CA certificates.If any Pods are started before new CA is used by API servers, the new Pods get this update and will trust both old and new CAs.Restart all pods using in-cluster configurations (for example: kube-proxy, CoreDNS, etc) so they can use the updated certificate authority data from Secrets that link to ServiceAccounts.Make sure CoreDNS, kube-proxy and other Pods using in-cluster configurations are working as expected.Append the both old and new CA to the file against --client-ca-file and --kubelet-certificate-authority flag in the kube-apiserver configuration.Append the both old and new CA to the file against --client-ca-file flag in the kube-scheduler configuration.Update certificates for user accounts by replacing the content of client-certificate-data and client-key-data respectively.For information about creating certificates for individual user accounts, see Configure certificates for user accounts.Additionally, update the certificate-authority-data section in the kubeconfig files, respectively with Base64-encoded old and new certificate authority dataUpdate the --root-ca-file flag for the Cloud Controller Manager to include both old and new CA, then restart the cloud-controller-manager.Note:If your cluster does not have a cloud-controller-manager, you can skip this step.Follow the steps below in a rolling fashion.Restart any other aggregated API servers or webhook handlers to trust the new CA certificates.Restart the kubelet by update the file against clientCAFile in kubelet configuration and certificate-authority-data in kubelet.conf to use both the old and new CA on all nodes.If your kubelet is not using client certificate rotation, update client-certificate-data and client-key-data in kubelet.conf on all nodes along with the kubelet client certificate file usually found in /var/lib/kubelet/pki.Restart API servers with the certificates (apiserver.crt, apiserver-kubelet-client.crt and front-proxy-client.crt) signed by new CA. You can use the existing private keys or new private keys. If you changed the private keys then update these in the Kubernetes certificates directory as well.Since the Pods in your cluster trust both old and new CAs, there will be a momentarily disconnection after which pods' Kubernetes clients reconnect to the new API server. The new API server uses a certificate signed by the new CA.Restart the kube-scheduler to use and trust the new CAs.Make sure control plane components logs no TLS errors.Note: To generate certificates and private keys for your cluster using the `openssl` command line tool, see [Certificates (`openssl`)](/docs/tasks/administer-cluster/certificates/#openssl). You can also use [`cfssl`](/docs/tasks/administer-cluster/certificates/#cfssl). Annotate any DaemonSets and Deployments to trigger pod replacement in a safer rolling fashion.for namespace in $(kubectl get namespace -o jsonpath='{.items[*].metadata.name}'); do for name in $(kubectl get deployments -n $namespace -o jsonpath='{.items[*].metadata.name}'); do kubectl patch deployment -n ${namespace} ${name} -p '{"spec":{"template":{"metadata":{"annotations":{"ca-rotation": "1"}}}}}'; done for name in $(kubectl get daemonset -n $namespace -o jsonpath='{.items[*].metadata.name}'); do kubectl patch daemonset -n ${namespace} ${name} -p '{"spec":{"template":{"metadata":{"annotations":{"ca-rotation": "1"}}}}}'; done done Note: To limit the number of concurrent disruptions that your application experiences, see [configure pod disruption budget](/docs/tasks/run-application/configure-pdb/). Depending on how you use StatefulSets you may also need to perform similar rolling replacement. If your cluster is using bootstrap tokens to join nodes, update the ConfigMap cluster-info in the kube-public namespace with new CA.base64_encoded_ca="$(base64 -w0 /etc/kubernetes/pki/ca.crt)" kubectl get cm/cluster-info --namespace kube-public -o yaml | \ /bin/sed "s/\\(certificate-authority-data:\\).*/\1 ${base64_encoded_ca}/" | \ kubectl apply -f - Verify the cluster functionality.Check the logs from control plane components, along with the kubelet and the kube-proxy. Ensure those components are not reporting any TLS errors; see looking at the logs for more details.Validate logs from any aggregated api servers and pods using in-cluster config.Once the cluster functionality is successfully verified:Update all service account tokens to include new CA certificate only.All pods using an in-cluster kubeconfig will eventually need to be restarted to pick up the new Secret, so that no Pods are relying on the old cluster CA.Restart the control plane components by removing the old CA from the kubeconfig files and the files against \--client-ca-file, --root-ca-file flags resp.On each node, restart the kubelet by removing the old CA from file against the clientCAFile flag and from the kubelet kubeconfig file. You should carry this out as a rolling update.If your cluster lets you make this change, you can also roll it out by replacing nodes rather than reconfiguring them.4.13 - Manage Cluster DaemonsPerform common tasks for managing a DaemonSet, such as performing a rolling update.4.13.1 - Building a Basic DaemonSetThis page demonstrates how to build a basic DaemonSet that runs a Pod on every node in a Kubernetes cluster. It covers a simple use case of mounting a file from the host, logging its contents using an init container, and utilizing a pause container.Before you beginYou need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesA Kubernetes cluster with at least two nodes (one control plane node and one worker node) to demonstrate the behavior of DaemonSets.Define the DaemonSetIn this task, a basic DaemonSet is created which ensures that the copy of a Pod is scheduled on every node. The Pod will use an init container to read and log the contents of /etc/machine-id from the host, while the main container will be a pause container, which keeps the Pod running.application/basic-daemonset.yaml apiVersion: apps/v1 kind: DaemonSet metadata: name: example-daemonset spec: selector: matchLabels: app.kubernetes.io/name: example template: metadata: labels: app.kubernetes.io/name: example spec: containers: \- name: pause image: registry.k8s.io/pause initContainers: \- name: log-machine-id image: busybox:1.37 command: ['sh', '-c', 'cat /etc/machine-id > /var/log/machine-id.log'] volumeMounts: \- name: machine-id mountPath: /etc/machine-id readOnly: true \- name: log-dir mountPath: /var/log volumes: \- name: machine-id hostPath: path: /etc/machine-id type: File \- name: log-dir hostPath: path: /var/logCreate a DaemonSet based on the (YAML) manifest:kubectl apply -f https://k8s.io/examples/application/basic-daemonset.yaml Once applied, you can verify that the DaemonSet is running a Pod on every node in the cluster:kubectl get pods -o wide The output will list one Pod per node, similar to:NAME READY STATUS RESTARTS AGE IP NODE example-daemonset-xxxxx 1/1 Running 0 5m x.x.x.x node-1 example-daemonset-yyyyy 1/1 Running 0 5m x.x.x.x node-2 You can inspect the contents of the logged /etc/machine-id file by checking the log directory mounted from the host:kubectl exec  \-- cat /var/log/machine-id.log Where  is the name of one of your Pods.Cleaning upTo delete the DaemonSet, run this command:kubectl delete --cascade=foreground --ignore-not-found --now daemonsets/example-daemonset This simple DaemonSet example introduces key components like init containers and host path volumes, which can be expanded upon for more advanced use cases. For more details refer to DaemonSet.What's nextSee Performing a rolling update on a DaemonSetSee Creating a DaemonSet to adopt existing DaemonSet pods4.13.2 - Perform a Rolling Update on a DaemonSetThis page shows how to perform a rolling update on a DaemonSet.Before you beginYou need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesDaemonSet Update StrategyDaemonSet has two update strategy types:OnDelete: With OnDelete update strategy, after you update a DaemonSet template, new DaemonSet pods will only be created when you manually delete old DaemonSet pods. This is the same behavior of DaemonSet in Kubernetes version 1.5 or before.RollingUpdate: This is the default update strategy.With RollingUpdate update strategy, after you update a DaemonSet template, old DaemonSet pods will be killed, and new DaemonSet pods will be created automatically, in a controlled fashion. At most one pod of the DaemonSet will be running on each node during the whole update process.Performing a Rolling UpdateTo enable the rolling update feature of a DaemonSet, you must set its .spec.updateStrategy.type to RollingUpdate.You may want to set .spec.updateStrategy.rollingUpdate.maxUnavailable (default to 1), .spec.minReadySeconds (default to 0) and .spec.updateStrategy.rollingUpdate.maxSurge (defaults to 0) as well.Creating a DaemonSet with RollingUpdate update strategyThis YAML file specifies a DaemonSet with an update strategy as 'RollingUpdate'controllers/fluentd-daemonset.yaml apiVersion: apps/v1 kind: DaemonSet metadata: name: fluentd-elasticsearch namespace: kube-system labels: k8s-app: fluentd-logging spec: selector: matchLabels: name: fluentd-elasticsearch updateStrategy: type: RollingUpdate rollingUpdate: maxUnavailable: 1 template: metadata: labels: name: fluentd-elasticsearch spec: tolerations: # these tolerations are to have the daemonset runnable on control plane nodes # remove them if your control plane nodes should not run pods \- key: node-role.kubernetes.io/control-plane operator: Exists effect: NoSchedule \- key: node-role.kubernetes.io/master operator: Exists effect: NoSchedule containers: \- name: fluentd-elasticsearch image: quay.io/fluentd_elasticsearch/fluentd:v2.5.2 volumeMounts: \- name: varlog mountPath: /var/log \- name: varlibdockercontainers mountPath: /var/lib/docker/containers readOnly: true terminationGracePeriodSeconds: 30 volumes: \- name: varlog hostPath: path: /var/log \- name: varlibdockercontainers hostPath: path: /var/lib/docker/containers After verifying the update strategy of the DaemonSet manifest, create the DaemonSet:kubectl create -f https://k8s.io/examples/controllers/fluentd-daemonset.yaml Alternatively, use kubectl apply to create the same DaemonSet if you plan to update the DaemonSet with kubectl apply.kubectl apply -f https://k8s.io/examples/controllers/fluentd-daemonset.yaml Checking DaemonSet RollingUpdate update strategyCheck the update strategy of your DaemonSet, and make sure it's set to RollingUpdate:kubectl get ds/fluentd-elasticsearch -o go-template='{{.spec.updateStrategy.type}}{{"\n"}}' -n kube-system If you haven't created the DaemonSet in the system, check your DaemonSet manifest with the following command instead:kubectl apply -f https://k8s.io/examples/controllers/fluentd-daemonset.yaml --dry-run=client -o go-template='{{.spec.updateStrategy.type}}{{"\n"}}' The output from both commands should be:RollingUpdate If the output isn't RollingUpdate, go back and modify the DaemonSet object or manifest accordingly.Updating a DaemonSet templateAny updates to a RollingUpdate DaemonSet .spec.template will trigger a rolling update. Let's update the DaemonSet by applying a new YAML file. This can be done with several different kubectl commands.controllers/fluentd-daemonset-update.yaml apiVersion: apps/v1 kind: DaemonSet metadata: name: fluentd-elasticsearch namespace: kube-system labels: k8s-app: fluentd-logging spec: selector: matchLabels: name: fluentd-elasticsearch updateStrategy: type: RollingUpdate rollingUpdate: maxUnavailable: 1 template: metadata: labels: name: fluentd-elasticsearch spec: tolerations: # these tolerations are to have the daemonset runnable on control plane nodes # remove them if your control plane nodes should not run pods \- key: node-role.kubernetes.io/control-plane operator: Exists effect: NoSchedule \- key: node-role.kubernetes.io/master operator: Exists effect: NoSchedule containers: \- name: fluentd-elasticsearch image: quay.io/fluentd_elasticsearch/fluentd:v2.5.2 resources: limits: memory: 200Mi requests: cpu: 100m memory: 200Mi volumeMounts: \- name: varlog mountPath: /var/log \- name: varlibdockercontainers mountPath: /var/lib/docker/containers readOnly: true terminationGracePeriodSeconds: 30 volumes: \- name: varlog hostPath: path: /var/log \- name: varlibdockercontainers hostPath: path: /var/lib/docker/containers Declarative commandsIf you update DaemonSets using configuration files, use kubectl apply:kubectl apply -f https://k8s.io/examples/controllers/fluentd-daemonset-update.yaml Imperative commandsIf you update DaemonSets using imperative commands, use kubectl edit :kubectl edit ds/fluentd-elasticsearch -n kube-system Updating only the container imageIf you only need to update the container image in the DaemonSet template, i.e. .spec.template.spec.containers[*].image, use kubectl set image:kubectl set image ds/fluentd-elasticsearch fluentd-elasticsearch=quay.io/fluentd_elasticsearch/fluentd:v2.6.0 -n kube-system Watching the rolling update statusFinally, watch the rollout status of the latest DaemonSet rolling update:kubectl rollout status ds/fluentd-elasticsearch -n kube-system When the rollout is complete, the output is similar to this:daemonset "fluentd-elasticsearch" successfully rolled out TroubleshootingDaemonSet rolling update is stuckSometimes, a DaemonSet rolling update may be stuck. Here are some possible causes:Some nodes run out of resourcesThe rollout is stuck because new DaemonSet pods can't be scheduled on at least one node. This is possible when the node is running out of resources.When this happens, find the nodes that don't have the DaemonSet pods scheduled on by comparing the output of kubectl get nodes and the output of:kubectl get pods -l name=fluentd-elasticsearch -o wide -n kube-system Once you've found those nodes, delete some non-DaemonSet pods from the node to make room for new DaemonSet pods.Note:This will cause service disruption when deleted pods are not controlled by any controllers or pods are not replicated. This does not respect PodDisruptionBudget either.Broken rolloutIf the recent DaemonSet template update is broken, for example, the container is crash looping, or the container image doesn't exist (often due to a typo), DaemonSet rollout won't progress.To fix this, update the DaemonSet template again. New rollout won't be blocked by previous unhealthy rollouts.Clock skewIf .spec.minReadySeconds is specified in the DaemonSet, clock skew between master and nodes will make DaemonSet unable to detect the right rollout progress.Clean upDelete DaemonSet from a namespace :kubectl delete ds fluentd-elasticsearch -n kube-system What's nextSee Performing a rollback on a DaemonSetSee Creating a DaemonSet to adopt existing DaemonSet pods4.13.3 - Perform a Rollback on a DaemonSetThis page shows how to perform a rollback on a DaemonSet.Before you beginYou need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesYour Kubernetes server must be at or later than version 1.7.To check the version, enter kubectl version.You should already know how to perform a rolling update on a DaemonSet.Performing a rollback on a DaemonSetStep 1: Find the DaemonSet revision you want to roll back toYou can skip this step if you only want to roll back to the last revision.List all revisions of a DaemonSet:kubectl rollout history daemonset  This returns a list of DaemonSet revisions:daemonsets "" REVISION CHANGE-CAUSE 1 ... 2 ... ... Change cause is copied from DaemonSet annotation kubernetes.io/change-cause to its revisions upon creation. You may specify --record=true in kubectl to record the command executed in the change cause annotation.To see the details of a specific revision:kubectl rollout history daemonset  \--revision=1 This returns the details of that revision:daemonsets "" with revision #1 Pod Template: Labels: foo=bar Containers: app: Image: ... Port: ... Environment: ... Mounts: ... Volumes: ... Step 2: Roll back to a specific revision# Specify the revision number you get from Step 1 in --to-revision kubectl rollout undo daemonset  \--to-revision= If it succeeds, the command returns:daemonset "" rolled back Note:If --to-revision flag is not specified, kubectl picks the most recent revision.Step 3: Watch the progress of the DaemonSet rollbackkubectl rollout undo daemonset tells the server to start rolling back the DaemonSet. The real rollback is done asynchronously inside the cluster control plane.To watch the progress of the rollback:kubectl rollout status ds/ When the rollback is complete, the output is similar to:daemonset "" successfully rolled out Understanding DaemonSet revisionsIn the previous kubectl rollout history step, you got a list of DaemonSet revisions. Each revision is stored in a resource named ControllerRevision.To see what is stored in each revision, find the DaemonSet revision raw resources:kubectl get controllerrevision -l = This returns a list of ControllerRevisions:NAME CONTROLLER REVISION AGE - DaemonSet/ 1 1h - DaemonSet/ 2 1h Each ControllerRevision stores the annotations and template of a DaemonSet revision.kubectl rollout undo takes a specific ControllerRevision and replaces DaemonSet template with the template stored in the ControllerRevision. kubectl rollout undo is equivalent to updating DaemonSet template to a previous revision through other commands, such as kubectl edit or kubectl apply.Note:DaemonSet revisions only roll forward. That is to say, after a rollback completes, the revision number (.revision field) of the ControllerRevision being rolled back to will advance. For example, if you have revision 1 and 2 in the system, and roll back from revision 2 to revision 1, the ControllerRevision with .revision: 1 will become .revision: 3.TroubleshootingSee troubleshooting DaemonSet rolling update.4.13.4 - Running Pods on Only Some NodesThis page demonstrates how can you run Pods on only some Nodes as part of a DaemonSetBefore you beginYou need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesRunning Pods on only some NodesImagine that you want to run a DaemonSet, but you only need to run those daemon pods on nodes that have local solid state (SSD) storage. For example, the Pod might provide cache service to the node, and the cache is only useful when low-latency local storage is available.Step 1: Add labels to your nodesAdd the label ssd=true to the nodes which have SSDs.kubectl label nodes example-node-1 example-node-2 ssd=true Step 2: Create the manifestLet's create a DaemonSet which will provision the daemon pods on the SSD labeled nodes only.Next, use a nodeSelector to ensure that the DaemonSet only runs Pods on nodes with the ssd label set to "true".controllers/daemonset-label-selector.yaml apiVersion: apps/v1 kind: DaemonSet metadata: name: ssd-driver labels: app: nginx spec: selector: matchLabels: app: ssd-driver-pod template: metadata: labels: app: ssd-driver-pod spec: nodeSelector: ssd: "true" containers: \- name: example-container image: example-imageStep 3: Create the DaemonSetCreate the DaemonSet from the manifest by using kubectl create or kubectl applyLet's label another node as ssd=true.kubectl label nodes example-node-3 ssd=true Labelling the node automatically triggers the control plane (specifically, the DaemonSet controller) to run a new daemon pod on that node.kubectl get pods -o wide The output is similar to:NAME READY STATUS RESTARTS AGE IP NODE  1/1 Running 0 13s ..... example-node-1  1/1 Running 0 13s ..... example-node-2  1/1 Running 0 5s ..... example-node-3 4.14 - NetworkingLearn how to configure networking for your cluster.4.14.1 - Adding entries to Pod /etc/hosts with HostAliasesAdding entries to a Pod's /etc/hosts file provides Pod-level override of hostname resolution when DNS and other options are not applicable. You can add these custom entries with the HostAliases field in PodSpec.The Kubernetes project recommends modifying DNS configuration using the hostAliases field (part of the .spec for a Pod), and not by using an init container or other means to edit /etc/hosts directly. Change made in other ways may be overwritten by the kubelet during Pod creation or restart.Default hosts file contentStart an Nginx Pod which is assigned a Pod IP:kubectl run nginx --image nginx pod/nginx created Examine a Pod IP:kubectl get pods --output=wide NAME READY STATUS RESTARTS AGE IP NODE nginx 1/1 Running 0 13s 10.200.0.4 worker0 The hosts file content would look like this:kubectl exec nginx -- cat /etc/hosts # Kubernetes-managed hosts file. 127.0.0.1 localhost ::1 localhost ip6-localhost ip6-loopback fe00::0 ip6-localnet fe00::0 ip6-mcastprefix fe00::1 ip6-allnodes fe00::2 ip6-allrouters 10.200.0.4 nginx By default, the hosts file only includes IPv4 and IPv6 boilerplates like localhost and its own hostname.Adding additional entries with hostAliasesIn addition to the default boilerplate, you can add additional entries to the hosts file. For example: to resolve foo.local, bar.local to 127.0.0.1 and foo.remote, bar.remote to 10.1.2.3, you can configure HostAliases for a Pod under .spec.hostAliases:service/networking/hostaliases-pod.yaml apiVersion: v1 kind: Pod metadata: name: hostaliases-pod spec: restartPolicy: Never hostAliases: \- ip: "127.0.0.1" hostnames: \- "foo.local" \- "bar.local" \- ip: "10.1.2.3" hostnames: \- "foo.remote" \- "bar.remote" containers: \- name: cat-hosts image: busybox:1.28 command: \- cat args: \- "/etc/hosts" You can start a Pod with that configuration by running:kubectl apply -f https://k8s.io/examples/service/networking/hostaliases-pod.yaml pod/hostaliases-pod created Examine a Pod's details to see its IPv4 address and its status:kubectl get pod --output=wide NAME READY STATUS RESTARTS AGE IP NODE hostaliases-pod 0/1 Completed 0 6s 10.200.0.5 worker0 The hosts file content looks like this:kubectl logs hostaliases-pod # Kubernetes-managed hosts file. 127.0.0.1 localhost ::1 localhost ip6-localhost ip6-loopback fe00::0 ip6-localnet fe00::0 ip6-mcastprefix fe00::1 ip6-allnodes fe00::2 ip6-allrouters 10.200.0.5 hostaliases-pod # Entries added by HostAliases. 127.0.0.1 foo.local bar.local 10.1.2.3 foo.remote bar.remote with the additional entries specified at the bottom.Why does the kubelet manage the hosts file?The kubelet manages the hosts file for each container of the Pod to prevent the container runtime from modifying the file after the containers have already been started. Historically, Kubernetes always used Docker Engine as its container runtime, and Docker Engine would then modify the /etc/hosts file after each container had started.Current Kubernetes can use a variety of container runtimes; even so, the kubelet manages the hosts file within each container so that the outcome is as intended regardless of which container runtime you use.Caution:Avoid making manual changes to the hosts file inside a container.If you make manual changes to the hosts file, those changes are lost when the container exits.4.14.2 - Extend Service IP RangesFEATURE STATE: Kubernetes v1.33 [stable] (enabled by default: true)This document shares how to extend the existing Service IP range assigned to a cluster.Before you beginYou need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesYour Kubernetes server must be at or later than version v1.29.To check the version, enter kubectl version.APIKubernetes clusters with kube-apiservers that have enabled the MultiCIDRServiceAllocator feature gate and have the networking.k8s.io/v1beta1 API group active, will create a ServiceCIDR object that takes the well-known name kubernetes, and that specifies an IP address range based on the value of the --service-cluster-ip-range command line argument to kube-apiserver.kubectl get servicecidr NAME CIDRS AGE kubernetes 10.96.0.0/28 17d The well-known kubernetes Service, that exposes the kube-apiserver endpoint to the Pods, calculates the first IP address from the default ServiceCIDR range and uses that IP address as its cluster IP address.kubectl get service kubernetes NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1  443/TCP 17d The default Service, in this case, uses the ClusterIP 10.96.0.1, that has the corresponding IPAddress object.kubectl get ipaddress 10.96.0.1 NAME PARENTREF 10.96.0.1 services/default/kubernetes The ServiceCIDRs are protected with finalizers, to avoid leaving Service ClusterIPs orphans; the finalizer is only removed if there is another subnet that contains the existing IPAddresses or there are no IPAddresses belonging to the subnet.Extend the number of available IPs for ServicesThere are cases that users will need to increase the number addresses available to Services, previously, increasing the Service range was a disruptive operation that could also cause data loss. With this new feature users only need to add a new ServiceCIDR to increase the number of available addresses.Adding a new ServiceCIDROn a cluster with a 10.96.0.0/28 range for Services, there is only 2^(32-28) - 2 = 14 IP addresses available. The kubernetes.default Service is always created; for this example, that leaves you with only 13 possible Services.for i in $(seq 1 13); do kubectl create service clusterip "test-$i" --tcp 80 -o json | jq -r .spec.clusterIP; done 10.96.0.11 10.96.0.5 10.96.0.12 10.96.0.13 10.96.0.14 10.96.0.2 10.96.0.3 10.96.0.4 10.96.0.6 10.96.0.7 10.96.0.8 10.96.0.9 error: failed to create ClusterIP service: Internal error occurred: failed to allocate a serviceIP: range is full You can increase the number of IP addresses available for Services, by creating a new ServiceCIDR that extends or adds new IP address ranges.cat  80/TCP 5s Validate that the Service gets cluster IPs from the IPv4 and IPv6 address blocks using kubectl describe. You may then validate access to the service via the IPs and ports.kubectl describe svc -l app.kubernetes.io/name=MyApp Name: my-service Namespace: default Labels: app.kubernetes.io/name=MyApp Annotations:  Selector: app.kubernetes.io/name=MyApp Type: ClusterIP IP Family Policy: PreferDualStack IP Families: IPv4,IPv6 IP: 10.0.216.242 IPs: 10.0.216.242,2001:db8:fd00::af55 Port:  80/TCP TargetPort: 9376/TCP Endpoints:  Session Affinity: None Events:  Create a dual-stack load balanced ServiceIf the cloud provider supports the provisioning of IPv6 enabled external load balancers, create the following Service with PreferDualStack in .spec.ipFamilyPolicy, IPv6 as the first element of the .spec.ipFamilies array and the type field set to LoadBalancer.service/networking/dual-stack-prefer-ipv6-lb-svc.yaml apiVersion: v1 kind: Service metadata: name: my-service labels: app.kubernetes.io/name: MyApp spec: ipFamilyPolicy: PreferDualStack ipFamilies: \- IPv6 type: LoadBalancer selector: app.kubernetes.io/name: MyApp ports: \- protocol: TCP port: 80 Check the Service:kubectl get svc -l app.kubernetes.io/name=MyApp Validate that the Service receives a CLUSTER-IP address from the IPv6 address block along with an EXTERNAL-IP. You may then validate access to the service via the IP and port.NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE my-service LoadBalancer 2001:db8:fd00::7ebc 2603:1030:805::5 80:30790/TCP 35s 4.15 - Extend kubectl with pluginsExtend kubectl by creating and installing kubectl plugins.This guide demonstrates how to install and write extensions for kubectl. By thinking of core kubectl commands as essential building blocks for interacting with a Kubernetes cluster, a cluster administrator can think of plugins as a means of utilizing these building blocks to create more complex behavior. Plugins extend kubectl with new sub-commands, allowing for new and custom features not included in the main distribution of kubectl.Before you beginYou need to have a working kubectl binary installed.Installing kubectl pluginsA plugin is a standalone executable file, whose name begins with kubectl-. To install a plugin, move its executable file to anywhere on your PATH.You can also discover and install kubectl plugins available in the open source using Krew. Krew is a plugin manager maintained by the Kubernetes SIG CLI community.Caution:Kubectl plugins available via the Krew plugin index are not audited for security. You should install and run third-party plugins at your own risk, since they are arbitrary programs running on your machine.Discovering pluginskubectl provides a command kubectl plugin list that searches your PATH for valid plugin executables. Executing this command causes a traversal of all files in your PATH. Any files that are executable, and begin with kubectl- will show up in the order in which they are present in your PATH in this command's output. A warning will be included for any files beginning with kubectl- that are not executable. A warning will also be included for any valid plugin files that overlap each other's name.You can use Krew to discover and install kubectl plugins from a community-curated plugin index.Create pluginskubectl allows plugins to add custom create commands of the shape kubectl create something by providing a kubectl-create-something binary in the PATH.LimitationsIt is currently not possible to create plugins that overwrite existing kubectl commands or extend commands other than create. For example, creating a plugin kubectl-version will cause that plugin to never be executed, as the existing kubectl version command will always take precedence over it. Due to this limitation, it is also not possible to use plugins to add new subcommands to existing kubectl commands. For example, adding a subcommand kubectl attach vm by naming your plugin kubectl-attach-vm will cause that plugin to be ignored.kubectl plugin list shows warnings for any valid plugins that attempt to do this.Writing kubectl pluginsYou can write a plugin in any programming language or script that allows you to write command-line commands.There is no plugin installation or pre-loading required. Plugin executables receive the inherited environment from the kubectl binary. A plugin determines which command path it wishes to implement based on its name. For example, a plugin named kubectl-foo provides a command kubectl foo. You must install the plugin executable somewhere in your PATH.Example plugin#!/bin/bash # optional argument handling if [[ "$1" == "version" ]] then echo "1.0.0" exit 0 fi # optional argument handling if [[ "$1" == "config" ]] then echo "$KUBECONFIG" exit 0 fi echo "I am a plugin named kubectl-foo" Using a pluginTo use a plugin, make the plugin executable:sudo chmod +x ./kubectl-foo and place it anywhere in your PATH:sudo mv ./kubectl-foo /usr/local/bin You may now invoke your plugin as a kubectl command:kubectl foo I am a plugin named kubectl-foo All args and flags are passed as-is to the executable:kubectl foo version 1.0.0 All environment variables are also passed as-is to the executable:export KUBECONFIG=~/.kube/config kubectl foo config /home//.kube/config KUBECONFIG=/etc/kube/config kubectl foo config /etc/kube/config Additionally, the first argument that is passed to a plugin will always be the full path to the location where it was invoked ($0 would equal /usr/local/bin/kubectl-foo in the example above).Naming a pluginAs seen in the example above, a plugin determines the command path that it will implement based on its filename. Every sub-command in the command path that a plugin targets, is separated by a dash (-). For example, a plugin that wishes to be invoked whenever the command kubectl foo bar baz is invoked by the user, would have the filename of kubectl-foo-bar-baz.Flags and argument handlingNote:The plugin mechanism does not create any custom, plugin-specific values or environment variables for a plugin process.An older kubectl plugin mechanism provided environment variables such as KUBECTL_PLUGINS_CURRENT_NAMESPACE; that no longer happens.kubectl plugins must parse and validate all of the arguments passed to them. See using the command line runtime package for details of a Go library aimed at plugin authors.Here are some additional cases where users invoke your plugin while providing additional flags and arguments. This builds upon the kubectl-foo-bar-baz plugin from the scenario above.If you run kubectl foo bar baz arg1 --flag=value arg2, kubectl's plugin mechanism will first try to find the plugin with the longest possible name, which in this case would be kubectl-foo-bar-baz-arg1. Upon not finding that plugin, kubectl then treats the last dash-separated value as an argument (arg1 in this case), and attempts to find the next longest possible name, kubectl-foo-bar-baz. Upon having found a plugin with this name, kubectl then invokes that plugin, passing all args and flags after the plugin's name as arguments to the plugin process.Example:# create a plugin echo -e '#!/bin/bash\n\necho "My first command-line argument was $1"' > kubectl-foo-bar-baz sudo chmod +x ./kubectl-foo-bar-baz # "install" your plugin by moving it to a directory in your $PATH sudo mv ./kubectl-foo-bar-baz /usr/local/bin # check that kubectl recognizes your plugin kubectl plugin list The following kubectl-compatible plugins are available: /usr/local/bin/kubectl-foo-bar-baz # test that calling your plugin via a "kubectl" command works # even when additional arguments and flags are passed to your # plugin executable by the user. kubectl foo bar baz arg1 --meaningless-flag=true My first command-line argument was arg1 As you can see, your plugin was found based on the kubectl command specified by a user, and all extra arguments and flags were passed as-is to the plugin executable once it was found.Names with dashes and underscoresAlthough the kubectl plugin mechanism uses the dash (-) in plugin filenames to separate the sequence of sub-commands processed by the plugin, it is still possible to create a plugin command containing dashes in its commandline invocation by using underscores (_) in its filename.Example:# create a plugin containing an underscore in its filename echo -e '#!/bin/bash\n\necho "I am a plugin with a dash in my name"' > ./kubectl-foo_bar sudo chmod +x ./kubectl-foo_bar # move the plugin into your $PATH sudo mv ./kubectl-foo_bar /usr/local/bin # You can now invoke your plugin via kubectl: kubectl foo-bar I am a plugin with a dash in my name Note that the introduction of underscores to a plugin filename does not prevent you from having commands such as kubectl foo_bar. The command from the above example, can be invoked using either a dash (-) or an underscore (_):# You can invoke your custom command with a dash kubectl foo-bar I am a plugin with a dash in my name # You can also invoke your custom command with an underscore kubectl foo_bar I am a plugin with a dash in my name Name conflicts and overshadowingIt is possible to have multiple plugins with the same filename in different locations throughout your PATH. For example, given a PATH with the following value: PATH=/usr/local/bin/plugins:/usr/local/bin/moreplugins, a copy of plugin kubectl-foo could exist in /usr/local/bin/plugins and /usr/local/bin/moreplugins, such that the output of the kubectl plugin list command is:PATH=/usr/local/bin/plugins:/usr/local/bin/moreplugins kubectl plugin list The following kubectl-compatible plugins are available: /usr/local/bin/plugins/kubectl-foo /usr/local/bin/moreplugins/kubectl-foo \- warning: /usr/local/bin/moreplugins/kubectl-foo is overshadowed by a similarly named plugin: /usr/local/bin/plugins/kubectl-foo error: one plugin warning was found In the above scenario, the warning under /usr/local/bin/moreplugins/kubectl-foo tells you that this plugin will never be executed. Instead, the executable that appears first in your PATH, /usr/local/bin/plugins/kubectl-foo, will always be found and executed first by the kubectl plugin mechanism.A way to resolve this issue is to ensure that the location of the plugin that you wish to use with kubectl always comes first in your PATH. For example, if you want to always use /usr/local/bin/moreplugins/kubectl-foo anytime that the kubectl command kubectl foo was invoked, change the value of your PATH to be /usr/local/bin/moreplugins:/usr/local/bin/plugins.Invocation of the longest executable filenameThere is another kind of overshadowing that can occur with plugin filenames. Given two plugins present in a user's PATH: kubectl-foo-bar and kubectl-foo-bar-baz, the kubectl plugin mechanism will always choose the longest possible plugin name for a given user command. Some examples below, clarify this further:# for a given kubectl command, the plugin with the longest possible filename will always be preferred kubectl foo bar baz Plugin kubectl-foo-bar-baz is executed kubectl foo bar Plugin kubectl-foo-bar is executed kubectl foo bar baz buz Plugin kubectl-foo-bar-baz is executed, with "buz" as its first argument kubectl foo bar buz Plugin kubectl-foo-bar is executed, with "buz" as its first argument This design choice ensures that plugin sub-commands can be implemented across multiple files, if needed, and that these sub-commands can be nested under a "parent" plugin command:ls ./plugin_command_tree kubectl-parent kubectl-parent-subcommand kubectl-parent-subcommand-subsubcommand Checking for plugin warningsYou can use the aforementioned kubectl plugin list command to ensure that your plugin is visible by kubectl, and verify that there are no warnings preventing it from being called as a kubectl command.kubectl plugin list The following kubectl-compatible plugins are available: test/fixtures/pkg/kubectl/plugins/kubectl-foo /usr/local/bin/kubectl-foo \- warning: /usr/local/bin/kubectl-foo is overshadowed by a similarly named plugin: test/fixtures/pkg/kubectl/plugins/kubectl-foo plugins/kubectl-invalid \- warning: plugins/kubectl-invalid identified as a kubectl plugin, but it is not executable error: 2 plugin warnings were found Using the command line runtime packageIf you're writing a plugin for kubectl and you're using Go, you can make use of the cli-runtime utility libraries.These libraries provide helpers for parsing or updating a user's kubeconfig file, for making REST-style requests to the API server, or to bind flags associated with configuration and printing.See the Sample CLI Plugin for an example usage of the tools provided in the CLI Runtime repo.Distributing kubectl pluginsIf you have developed a plugin for others to use, you should consider how you package it, distribute it and deliver updates to your users.KrewKrew offers a cross-platform way to package and distribute your plugins. This way, you use a single packaging format for all target platforms (Linux, Windows, macOS etc) and deliver updates to your users. Krew also maintains a plugin index so that other people can discover your plugin and install it.Native / platform specific package managementAlternatively, you can use traditional package managers such as, apt or yum on Linux, Chocolatey on Windows, and Homebrew on macOS. Any package manager will be suitable if it can place new executables placed somewhere in the user's PATH. As a plugin author, if you pick this option then you also have the burden of updating your kubectl plugin's distribution package across multiple platforms for each release.Source codeYou can publish the source code; for example, as a Git repository. If you choose this option, someone who wants to use that plugin must fetch the code, set up a build environment (if it needs compiling), and deploy the plugin. If you also make compiled packages available, or use Krew, that will make installs easier.What's nextCheck the Sample CLI Plugin repository for a detailed example of a plugin written in Go. In case of any questions, feel free to reach out to the SIG CLI team.Read about Krew, a package manager for kubectl plugins.4.16 - Manage HugePagesConfigure and manage huge pages as a schedulable resource in a cluster.FEATURE STATE: Kubernetes v1.14 [stable] (enabled by default: true)Kubernetes supports the allocation and consumption of pre-allocated huge pages by applications in a Pod. This page describes how users can consume huge pages.Before you beginKubernetes nodes must pre-allocate huge pages in order for the node to report its huge page capacity.A node can pre-allocate huge pages for multiple sizes, for instance, the following line in /etc/default/grub allocates 2*1GiB of 1 GiB and 512*2 MiB of 2 MiB pages:GRUB_CMDLINE_LINUX="hugepagesz=1G hugepages=2 hugepagesz=2M hugepages=512" The nodes will automatically discover and report all huge page resources as schedulable resources.When you describe the Node, you should see something similar to the following in the following in the Capacity and Allocatable sections:Capacity: cpu: ... ephemeral-storage: ... hugepages-1Gi: 2Gi hugepages-2Mi: 1Gi memory: ... pods: ... Allocatable: cpu: ... ephemeral-storage: ... hugepages-1Gi: 2Gi hugepages-2Mi: 1Gi memory: ... pods: ... Note:For dynamically allocated pages (after boot), the Kubelet needs to be restarted for the new allocations to be refrelected.APIHuge pages can be consumed via container level resource requirements using the resource name hugepages-, where  is the most compact binary notation using integer values supported on a particular node. For example, if a node supports 2048KiB and 1048576KiB page sizes, it will expose a schedulable resources hugepages-2Mi and hugepages-1Gi. Unlike CPU or memory, huge pages do not support overcommit. Note that when requesting hugepage resources, either memory or CPU resources must be requested as well.A pod may consume multiple huge page sizes in a single pod spec. In this case it must use medium: HugePages- notation for all volume mounts.apiVersion: v1 kind: Pod metadata: name: huge-pages-example spec: containers: \- name: example image: fedora:latest command: \- sleep \- inf volumeMounts: \- mountPath: /hugepages-2Mi name: hugepage-2mi \- mountPath: /hugepages-1Gi name: hugepage-1gi resources: limits: hugepages-2Mi: 100Mi hugepages-1Gi: 2Gi memory: 100Mi requests: memory: 100Mi volumes: \- name: hugepage-2mi emptyDir: medium: HugePages-2Mi \- name: hugepage-1gi emptyDir: medium: HugePages-1Gi A pod may use medium: HugePages only if it requests huge pages of one size.apiVersion: v1 kind: Pod metadata: name: huge-pages-example spec: containers: \- name: example image: fedora:latest command: \- sleep \- inf volumeMounts: \- mountPath: /hugepages name: hugepage resources: limits: hugepages-2Mi: 100Mi memory: 100Mi requests: memory: 100Mi volumes: \- name: hugepage emptyDir: medium: HugePages Huge page requests must equal the limits. This is the default if limits are specified, but requests are not.Huge pages are isolated at a container scope, so each container has own limit on their cgroup sandbox as requested in a container spec.EmptyDir volumes backed by huge pages may not consume more huge page memory than the pod request.Applications that consume huge pages via shmget() with SHM_HUGETLB must run with a supplemental group that matches proc/sys/vm/hugetlb_shm_group.Huge page usage in a namespace is controllable via ResourceQuota similar to other compute resources like cpu or memory using the hugepages- token.4.17 - Schedule GPUsConfigure and schedule GPUs for use as a resource by nodes in a cluster.FEATURE STATE: Kubernetes v1.26 [stable]Kubernetes includes stable support for managing AMD and NVIDIA GPUs (graphical processing units) across different nodes in your cluster, using device plugins.This page describes how users can consume GPUs, and outlines some of the limitations in the implementation.Using device pluginsKubernetes implements device plugins to let Pods access specialized hardware features such as GPUs.Note: This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the content guide before submitting a change. More information.As an administrator, you have to install GPU drivers from the corresponding hardware vendor on the nodes and run the corresponding device plugin from the GPU vendor. Here are some links to vendors' instructions:AMDIntelNVIDIAOnce you have installed the plugin, your cluster exposes a custom schedulable resource such as amd.com/gpu or nvidia.com/gpu.You can consume these GPUs from your containers by requesting the custom GPU resource, the same way you request cpu or memory. However, there are some limitations in how you specify the resource requirements for custom devices.GPUs are only supposed to be specified in the limits section, which means:You can specify GPU limits without specifying requests, because Kubernetes will use the limit as the request value by default.You can specify GPU in both limits and requests but these two values must be equal.You cannot specify GPU requests without specifying limits.Here's an example manifest for a Pod that requests a GPU:apiVersion: v1 kind: Pod metadata: name: example-vector-add spec: restartPolicy: OnFailure containers: \- name: example-vector-add image: "registry.example/example-vector-add:v42" resources: limits: gpu-vendor.example/example-gpu: 1 # requesting 1 GPU Manage clusters with different types of GPUsIf different nodes in your cluster have different types of GPUs, then you can use Node Labels and Node Selectors to schedule pods to appropriate nodes.For example:# Label your nodes with the accelerator type they have. kubectl label nodes node1 accelerator=example-gpu-x100 kubectl label nodes node2 accelerator=other-gpu-k915 That label key accelerator is just an example; you can use a different label key if you prefer.Automatic node labellingAs an administrator, you can automatically discover and label all your GPU enabled nodes by deploying Kubernetes Node Feature Discovery (NFD). NFD detects the hardware features that are available on each node in a Kubernetes cluster. Typically, NFD is configured to advertise those features as node labels, but NFD can also add extended resources, annotations, and node taints. NFD is compatible with all supported versions of Kubernetes. By default NFD create the feature labels for the detected features. Administrators can leverage NFD to also taint nodes with specific features, so that only pods that request those features can be scheduled on those nodes.You also need a plugin for NFD that adds appropriate labels to your nodes; these might be generic labels or they could be vendor specific. Your GPU vendor may provide a third party plugin for NFD; check their documentation for more details.apiVersion: v1 kind: Pod metadata: name: example-vector-add spec: restartPolicy: OnFailure # You can use Kubernetes node affinity to schedule this Pod onto a node # that provides the kind of GPU that its container needs in order to work affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: \- matchExpressions: \- key: "gpu.gpu-vendor.example/installed-memory" operator: Gt # (greater than) values: ["40535"] \- key: "feature.node.kubernetes.io/pci-10.present" # NFD Feature label values: ["true"] # (optional) only schedule on nodes with PCI device 10 containers: \- name: example-vector-add image: "registry.example/example-vector-add:v42" resources: limits: gpu-vendor.example/example-gpu: 1 # requesting 1 GPUGPU vendor implementationsIntelNVIDIA5 - TutorialsThis section of the Kubernetes documentation contains tutorials. A tutorial shows how to accomplish a goal that is larger than a single task. Typically a tutorial has several sections, each of which has a sequence of steps. Before walking through each tutorial, you may want to bookmark the Standardized Glossary page for later references.BasicsKubernetes Basics is an in-depth interactive tutorial that helps you understand the Kubernetes system and try out some basic Kubernetes features.Introduction to Kubernetes (edX)Hello MinikubeConfigurationConfiguring Redis Using a ConfigMapAuthoring PodsAdopting Sidecar ContainersStateless ApplicationsExposing an External IP Address to Access an Application in a ClusterExample: Deploying PHP Guestbook application with RedisStateful ApplicationsStatefulSet BasicsExample: WordPress and MySQL with Persistent VolumesExample: Deploying Cassandra with Stateful SetsRunning ZooKeeper, A CP Distributed SystemServicesConnecting Applications with ServicesUsing Source IPSecurityApply Pod Security Standards at Cluster levelApply Pod Security Standards at Namespace levelRestrict a Container's Access to Resources with AppArmorSeccompCluster ManagementRunning Kubelet in Standalone ModeWhat's nextIf you would like to write a tutorial, see Content Page Types for information about the tutorial page type.5.1 - Hello MinikubeThis tutorial shows you how to run a sample app on Kubernetes using minikube. The tutorial provides a container image that uses NGINX to echo back all the requests.ObjectivesDeploy a sample application to minikube.Run the app.View application logs.Before you beginThis tutorial assumes that you have already set up minikube. See Step 1 in minikube start for installation instructions.Note:Only execute the instructions in Step 1, Installation. The rest is covered on this page.You also need to install kubectl. See Install tools for installation instructions.Create a minikube clusterminikube start Open the DashboardOpen the Kubernetes dashboard. You can do this two different ways:Launch a browserURL copy and pasteOpen a new terminal, and run:# Start a new terminal, and leave this running. minikube dashboard Now, switch back to the terminal where you ran minikube start.Note:The dashboard command enables the dashboard add-on and opens the proxy in the default web browser. You can create Kubernetes resources on the dashboard such as Deployment and Service.To find out how to avoid directly invoking the browser from the terminal and get a URL for the web dashboard, see the "URL copy and paste" tab.By default, the dashboard is only accessible from within the internal Kubernetes virtual network. The dashboard command creates a temporary proxy to make the dashboard accessible from outside the Kubernetes virtual network.To stop the proxy, run Ctrl+C to exit the process. After the command exits, the dashboard remains running in the Kubernetes cluster. You can run the dashboard command again to create another proxy to access the dashboard.If you don't want minikube to open a web browser for you, run the dashboard subcommand with the \--url flag. minikube outputs a URL that you can open in the browser you prefer.Open a new terminal, and run:# Start a new terminal, and leave this running. minikube dashboard --url Now, you can use this URL and switch back to the terminal where you ran minikube start.Create a DeploymentA Kubernetes Pod is a group of one or more Containers, tied together for the purposes of administration and networking. The Pod in this tutorial has only one Container. A Kubernetes Deployment checks on the health of your Pod and restarts the Pod's Container if it terminates. Deployments are the recommended way to manage the creation and scaling of Pods.Use the kubectl create command to create a Deployment that manages a Pod. The Pod runs a Container based on the provided Docker image.# Run a test container image that includes a webserver kubectl create deployment hello-node --image=registry.k8s.io/e2e-test-images/agnhost:2.39 -- /agnhost netexec --http-port=8080 View the Deployment:kubectl get deployments The output is similar to:NAME READY UP-TO-DATE AVAILABLE AGE hello-node 1/1 1 1 1m (It may take some time for the pod to become available. If you see "0/1", try again in a few seconds.)View the Pod:kubectl get pods The output is similar to:NAME READY STATUS RESTARTS AGE hello-node-5f76cf6ccf-br9b5 1/1 Running 0 1m View cluster events:kubectl get events View the kubectl configuration:kubectl config view View application logs for a container in a pod (replace pod name with the one you got from kubectl get pods).Note:Replace hello-node-5f76cf6ccf-br9b5 in the kubectl logs command with the name of the pod from the kubectl get pods command output.kubectl logs hello-node-5f76cf6ccf-br9b5 The output is similar to:I0911 09:19:26.677397 1 log.go:195] Started HTTP server on port 8080 I0911 09:19:26.677586 1 log.go:195] Started UDP server on port 8081 Note:For more information about kubectl commands, see the kubectl overview.Create a ServiceBy default, the Pod is only accessible by its internal IP address within the Kubernetes cluster. To make the hello-node Container accessible from outside the Kubernetes virtual network, you have to expose the Pod as a Kubernetes Service.Warning:The agnhost container has a /shell endpoint, which is useful for debugging, but dangerous to expose to the public internet. Do not run this on an internet-facing cluster, or a production cluster.Expose the Pod to the public internet using the kubectl expose command:kubectl expose deployment hello-node --type=LoadBalancer --port=8080 The --type=LoadBalancer flag indicates that you want to expose your Service outside of the cluster.The application code inside the test image only listens on TCP port 8080. If you used kubectl expose to expose a different port, clients could not connect to that other port.View the Service you created:kubectl get services The output is similar to:NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE hello-node LoadBalancer 10.108.144.78  8080:30369/TCP 21s kubernetes ClusterIP 10.96.0.1  443/TCP 23m On cloud providers that support load balancers, an external IP address would be provisioned to access the Service. On minikube, the LoadBalancer type makes the Service accessible through the minikube service command.Run the following command:minikube service hello-node This opens up a browser window that serves your app and shows the app's response.Enable addonsThe minikube tool includes a set of built-in addons that can be enabled, disabled and opened in the local Kubernetes environment.List the currently supported addons:minikube addons list The output is similar to:addon-manager: enabled dashboard: enabled default-storageclass: enabled efk: disabled freshpod: disabled gvisor: disabled helm-tiller: disabled ingress: disabled ingress-dns: disabled logviewer: disabled metrics-server: disabled nvidia-driver-installer: disabled nvidia-gpu-device-plugin: disabled registry: disabled registry-creds: disabled storage-provisioner: enabled storage-provisioner-gluster: disabled Enable an addon, for example, metrics-server:minikube addons enable metrics-server The output is similar to:The 'metrics-server' addon is enabled View the Pod and Service you created by installing that addon:kubectl get pod,svc -n kube-system The output is similar to:NAME READY STATUS RESTARTS AGE pod/coredns-5644d7b6d9-mh9ll 1/1 Running 0 34m pod/coredns-5644d7b6d9-pqd2t 1/1 Running 0 34m pod/metrics-server-67fb648c5 1/1 Running 0 26s pod/etcd-minikube 1/1 Running 0 34m pod/influxdb-grafana-b29w8 2/2 Running 0 26s pod/kube-addon-manager-minikube 1/1 Running 0 34m pod/kube-apiserver-minikube 1/1 Running 0 34m pod/kube-controller-manager-minikube 1/1 Running 0 34m pod/kube-proxy-rnlps 1/1 Running 0 34m pod/kube-scheduler-minikube 1/1 Running 0 34m pod/storage-provisioner 1/1 Running 0 34m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/metrics-server ClusterIP 10.96.241.45  80/TCP 26s service/kube-dns ClusterIP 10.96.0.10  53/UDP,53/TCP 34m service/monitoring-grafana NodePort 10.99.24.54  80:30002/TCP 26s service/monitoring-influxdb ClusterIP 10.111.169.94  8083/TCP,8086/TCP 26s Check the output from metrics-server:kubectl top pods The output is similar to:NAME CPU(cores) MEMORY(bytes) hello-node-ccf4b9788-4jn97 1m 6Mi If you see the following message, wait, and try again:error: Metrics API not available Disable metrics-server:minikube addons disable metrics-server The output is similar to:metrics-server was successfully disabled Clean upNow you can clean up the resources you created in your cluster:kubectl delete service hello-node kubectl delete deployment hello-node Stop the Minikube clusterminikube stop Optionally, delete the Minikube VM:# Optional minikube delete If you want to use minikube again to learn more about Kubernetes, you don't need to delete it.ConclusionThis page covered the basic aspects to get a minikube cluster up and running. You are now ready to deploy applications.What's nextTutorial to deploy your first app on Kubernetes with kubectl.Learn more about Deployment objects.Learn more about Deploying applications.Learn more about Service objects.5.2 - Learn Kubernetes BasicsObjectivesThis tutorial provides a walkthrough of the basics of the Kubernetes cluster orchestration system. Each module contains some background information on major Kubernetes features and concepts, and a tutorial for you to follow along.Using the tutorials, you can learn to:Deploy a containerized application on a cluster.Scale the deployment.Update the containerized application with a new software version.Debug the containerized application.What can Kubernetes do for you?With modern web services, users expect applications to be available 24/7, and developers expect to deploy new versions of those applications several times a day. Containerization helps package software to serve these goals, enabling applications to be released and updated without downtime. Kubernetes helps you make sure those containerized applications run where and when you want, and helps them find the resources and tools they need to work. Kubernetes is a production-ready, open source platform designed with Google's accumulated experience in container orchestration, combined with best-of-breed ideas from the community.Kubernetes Basics Modules1. Create a Kubernetes cluster2. Deploy an app3. Explore your app4. Expose your app publicly5. Scale up your app6. Update your appWhat's nextTutorial Using Minikube to Create a Cluster5.2.1 - Create a ClusterLearn about Kubernetes cluster and create a simple cluster using Minikube.5.2.1.1 - Using Minikube to Create a ClusterObjectivesLearn what a Kubernetes cluster is.Learn what Minikube is.Start a Kubernetes cluster on your computer.Kubernetes ClustersKubernetes is a production-grade, open-source platform that orchestrates the placement (scheduling) and execution of application containers within and across computer clusters.Kubernetes coordinates a highly available cluster of computers that are connected to work as a single unit. The abstractions in Kubernetes allow you to deploy containerized applications to a cluster without tying them specifically to individual machines. To make use of this new model of deployment, applications need to be packaged in a way that decouples them from individual hosts: they need to be containerized. Containerized applications are more flexible and available than in past deployment models, where applications were installed directly onto specific machines as packages deeply integrated into the host. Kubernetes automates the distribution and scheduling of application containers across a cluster in a more efficient way. Kubernetes is an open-source platform and is production-ready.A Kubernetes cluster consists of two types of resources:The Control Plane coordinates the clusterNodes are the workers that run applicationsCluster DiagramThe Control Plane is responsible for managing the cluster. The Control Plane coordinates all activities in your cluster, such as scheduling applications, maintaining applications' desired state, scaling applications, and rolling out new updates.Control Planes manage the cluster and the nodes that are used to host the running applications.A node is a VM or a physical computer that serves as a worker machine in a Kubernetes cluster. Each node has a Kubelet, which is an agent for managing the node and communicating with the Kubernetes control plane. The node should also have tools for handling container operations, such as containerd or CRI-O. A Kubernetes cluster that handles production traffic should have a minimum of three nodes because if one node goes down, both an etcd member and a control plane instance are lost, and redundancy is compromised. You can mitigate this risk by adding more control plane nodes.When you deploy applications on Kubernetes, you tell the control plane to start the application containers. The control plane schedules the containers to run on the cluster's nodes. Node-level components, such as the kubelet, communicate with the control plane using the Kubernetes API, which the control plane exposes. End users can also use the Kubernetes API directly to interact with the cluster.A Kubernetes cluster can be deployed on either physical or virtual machines. To get started with Kubernetes development, you can use Minikube. Minikube is a lightweight Kubernetes implementation that creates a VM on your local machine and deploys a simple cluster containing only one node. Minikube is available for Linux, macOS, and Windows systems. The Minikube CLI provides basic bootstrapping operations for working with your cluster, including start, stop, status, and delete.What's nextTutorial Hello Minikube.Learn more about Cluster Architecture.5.2.2 - Deploy an App5.2.2.1 - Using kubectl to Create a DeploymentObjectivesLearn about application Deployments.Deploy your first app on Kubernetes with kubectl.Kubernetes DeploymentsA Deployment is responsible for creating and updating instances of your application.Note:This tutorial uses a container that requires the AMD64 architecture. If you are using minikube on a computer with a different CPU architecture, you could try using minikube with a driver that can emulate AMD64. For example, the Docker Desktop driver can do this.Once you have a running Kubernetes cluster, you can deploy your containerized applications on top of it. To do so, you create a Kubernetes Deployment. The Deployment instructs Kubernetes how to create and update instances of your application. Once you've created a Deployment, the Kubernetes control plane schedules the application instances included in that Deployment to run on individual Nodes in the cluster.Once the application instances are created, a Kubernetes Deployment controller continuously monitors those instances. If the Node hosting an instance goes down or is deleted, the Deployment controller replaces the instance with an instance on another Node in the cluster. This provides a self-healing mechanism to address machine failure or maintenance.In a pre-orchestration world, installation scripts would often be used to start applications, but they did not allow recovery from machine failure. By both creating your application instances and keeping them running across Nodes, Kubernetes Deployments provide a fundamentally different approach to application management.Deploying your first app on KubernetesApplications need to be packaged into one of the supported container formats in order to be deployed on Kubernetes.You can create and manage a Deployment by using the Kubernetes command line interface, kubectl. kubectl uses the Kubernetes API to interact with the cluster. In this module, you'll learn the most common kubectl commands needed to create Deployments that run your applications on a Kubernetes cluster.When you create a Deployment, you'll need to specify the container image for your application and the number of replicas that you want to run. You can change that information later by updating your Deployment; Module 5 and Module 6 of the bootcamp discuss how you can scale and update your Deployments.For your first Deployment, you'll use a hello-node application packaged in a Docker container that uses NGINX to echo back all the requests. (If you didn't already try creating a hello-node application and deploying it using a container, you can do that first by following the instructions from the Hello Minikube tutorial.You will need to have installed kubectl as well. If you need to install it, visit install tools install tools.Now that you know what Deployments are, let's deploy our first app!kubectl basicsThe common format of a kubectl command is: kubectl action resource.This performs the specified action (like create, describe or delete) on the specified resource (like node or deployment. You can use --help after the subcommand to get additional info about possible parameters (for example: kubectl get nodes --help).Check that kubectl is configured to talk to your cluster, by running the kubectl version command.Check that kubectl is installed and that you can see both the client and the server versions.To view the nodes in the cluster, run the kubectl get nodes command.You see the available nodes. Later, Kubernetes will choose where to deploy our application based on Node available resources.Deploy an appLet’s deploy our first app on Kubernetes with the kubectl create deployment command. We need to provide the deployment name and app image location (include the full repository url for images hosted outside Docker Hub).kubectl create deployment kubernetes-bootcamp --image=gcr.io/google-samples/kubernetes-bootcamp:v1 Great! You just deployed your first application by creating a deployment. This performed a few things for you:searched for a suitable node where an instance of the application could be run (we have only 1 available node)scheduled the application to run on that Nodeconfigured the cluster to reschedule the instance on a new Node when neededTo list your deployments use the kubectl get deployments command:kubectl get deployments We see that there is 1 deployment running a single instance of your app. The instance is running inside a container on your node.View the appPods that are running inside Kubernetes are running on a private, isolated network. By default they are visible from other pods and services within the same Kubernetes cluster, but not outside that network. When we use kubectl, we're interacting through an API endpoint to communicate with our application.We will cover other options on how to expose your application outside the Kubernetes cluster later, in Module 4. Also as a basic tutorial, we're not explaining what Pods are in any detail here, it will be covered in later topics.The kubectl proxy command can create a proxy that will forward communications into the cluster-wide, private network. The proxy can be terminated by pressing control-C and won't show any output while it's running.You need to open a second terminal window to run the proxy.kubectl proxy We now have a connection between our host (the terminal) and the Kubernetes cluster. The proxy enables direct access to the API from these terminals.You can see all those APIs hosted through the proxy endpoint. For example, we can query the version directly through the API using the curl command:curl http://localhost:8001/version Note:If port 8001 is not accessible, ensure that the kubectl proxy that you started above is running in the second terminal.The API server will automatically create an endpoint for each pod, based on the pod name, that is also accessible through the proxy.First we need to get the Pod name, and we'll store it in the environment variable POD_NAME.export POD_NAME=$(kubectl get pods -o go-template --template '{{range .items}}{{.metadata.name}}{{"\n"}}{{end}}') echo Name of the Pod: $POD_NAME You can access the Pod through the proxied API, by running:curl http://localhost:8001/api/v1/namespaces/default/pods/$POD_NAME:8080/proxy/ In order for the new Deployment to be accessible without using the proxy, a Service is required which will be explained in Module 4.What's nextTutorial Viewing Pods and Nodes.Learn more about Deployments.5.2.3 - Explore Your App5.2.3.1 - Viewing Pods and NodesObjectivesLearn about Kubernetes Pods.Learn about Kubernetes Nodes.Troubleshoot deployed applications.Kubernetes PodsA Pod is a group of one or more application containers (such as Docker) and includes shared storage (volumes), IP address and information about how to run them.When you created a Deployment in Module 2, Kubernetes created a Pod to host your application instance. A Pod is a Kubernetes abstraction that represents a group of one or more application containers (such as Docker), and some shared resources for those containers. Those resources include:Shared storage, as VolumesNetworking, as a unique cluster IP addressInformation about how to run each container, such as the container image version or specific ports to useA Pod models an application-specific "logical host" and can contain different application containers which are relatively tightly coupled. For example, a Pod might include both the container with your Node.js app as well as a different container that feeds the data to be published by the Node.js webserver. The containers in a Pod share an IP Address and port space, are always co-located and co-scheduled, and run in a shared context on the same Node.Pods are the atomic unit on the Kubernetes platform. When we create a Deployment on Kubernetes, that Deployment creates Pods with containers inside them (as opposed to creating containers directly). Each Pod is tied to the Node where it is scheduled, and remains there until termination (according to restart policy) or deletion. In case of a Node failure, identical Pods are scheduled on other available Nodes in the cluster.Pods overviewContainers should only be scheduled together in a single Pod if they are tightly coupled and need to share resources such as disk.NodesA Pod always runs on a Node. A Node is a worker machine in Kubernetes and may be either a virtual or a physical machine, depending on the cluster. Each Node is managed by the control plane. A Node can have multiple pods, and the Kubernetes control plane automatically handles scheduling the pods across the Nodes in the cluster. The control plane's automatic scheduling takes into account the available resources on each Node.Every Kubernetes Node runs at least:Kubelet, a process responsible for communication between the Kubernetes control plane and the Node; it manages the Pods and the containers running on a machine.A container runtime (like Docker) responsible for pulling the container image from a registry, unpacking the container, and running the application.Nodes overviewTroubleshooting with kubectlIn Module 2, you used the kubectl command-line interface. You'll continue to use it in Module 3 to get information about deployed applications and their environments. The most common operations can be done with the following kubectl subcommands:kubectl get - list resourceskubectl describe - show detailed information about a resourcekubectl logs - print the logs from a container in a podkubectl exec - execute a command on a container in a podYou can use these commands to see when applications were deployed, what their current statuses are, where they are running and what their configurations are.Now that we know more about our cluster components and the command line, let's explore our application.Check application configurationLet's verify that the application we deployed in the previous scenario is running. We'll use the kubectl get command and look for existing Pods:kubectl get pods If no pods are running, please wait a couple of seconds and list the Pods again. You can continue once you see one Pod running.Next, to view what containers are inside that Pod and what images are used to build those containers we run the kubectl describe pods command:kubectl describe pods We see here details about the Pod’s container: IP address, the ports used and a list of events related to the lifecycle of the Pod.The output of the describe subcommand is extensive and covers some concepts that we didn’t explain yet, but don’t worry, they will become familiar by the end of this tutorial.Note:The describe subcommand can be used to get detailed information about most of the Kubernetes primitives, including Nodes, Pods, and Deployments. The describe output is designed to be human readable, not to be scripted against.Show the app in the terminalRecall that Pods are running in an isolated, private network - so we need to proxy access to them so we can debug and interact with them. To do this, we'll use the kubectl proxy command to run a proxy in a second terminal. Open a new terminal window, and in that new terminal, run:kubectl proxy Now again, we'll get the Pod name and query that pod directly through the proxy. To get the Pod name and store it in the POD_NAME environment variable:export POD_NAME="$(kubectl get pods -o go-template --template '{{range .items}}{{.metadata.name}}{{"\n"}}{{end}}')" echo Name of the Pod: $POD_NAME To see the output of our application, run a curl request:curl http://localhost:8001/api/v1/namespaces/default/pods/$POD_NAME:8080/proxy/ The URL is the route to the API of the Pod.Note:We don't need to specify the container name, because we only have one container inside the pod.Executing commands on the containerWe can execute commands directly on the container once the Pod is up and running. For this, we use the exec subcommand and use the name of the Pod as a parameter. Let’s list the environment variables:kubectl exec "$POD_NAME" -- env Again, it's worth mentioning that the name of the container itself can be omitted since we only have a single container in the Pod.Next let’s start a bash session in the Pod’s container:kubectl exec -ti $POD_NAME -- bash We have now an open console on the container where we run our NodeJS application. The source code of the app is in the server.js file:cat server.js You can check that the application is up by running a curl command:curl http://localhost:8080 Note:Here we used localhost because we executed the command inside the NodeJS Pod. If you cannot connect to localhost:8080, check to make sure you have run the kubectl exec command and are launching the command from within the Pod.To close your container connection, type exit.What's nextTutorial Using A Service To Expose Your App.Learn more about Pods.Learn more about Nodes.5.2.4 - Expose Your App Publicly5.2.4.1 - Using a Service to Expose Your AppObjectivesLearn about a Service in Kubernetes.Understand how labels and selectors relate to a Service.Expose an application outside a Kubernetes cluster.Overview of Kubernetes ServicesKubernetes Pods are mortal. Pods have a lifecycle. When a worker node dies, the Pods running on the Node are also lost. A Replicaset might then dynamically drive the cluster back to the desired state via the creation of new Pods to keep your application running. As another example, consider an image-processing backend with 3 replicas. Those replicas are exchangeable; the front-end system should not care about backend replicas or even if a Pod is lost and recreated. That said, each Pod in a Kubernetes cluster has a unique IP address, even Pods on the same Node, so there needs to be a way of automatically reconciling changes among Pods so that your applications continue to function.A Kubernetes Service is an abstraction layer which defines a logical set of Pods and enables external traffic exposure, load balancing and service discovery for those Pods.A Service in Kubernetes is an abstraction which defines a logical set of Pods and a policy by which to access them. Services enable a loose coupling between dependent Pods. A Service is defined using YAML or JSON, like all Kubernetes object manifests. The set of Pods targeted by a Service is usually determined by a label selector (see below for why you might want a Service without including a selector in the spec).Although each Pod has a unique IP address, those IPs are not exposed outside the cluster without a Service. Services allow your applications to receive traffic. Services can be exposed in different ways by specifying a type in the spec of the Service:ClusterIP (default) - Exposes the Service on an internal IP in the cluster. This type makes the Service only reachable from within the cluster.NodePort - Exposes the Service on the same port of each selected Node in the cluster using NAT. Makes a Service accessible from outside the cluster using NodeIP:NodePort. Superset of ClusterIP.LoadBalancer - Creates an external load balancer in the current cloud (if supported) and assigns a fixed, external IP to the Service. Superset of NodePort.ExternalName - Maps the Service to the contents of the externalName field (e.g. foo.bar.example.com), by returning a CNAME record with its value. No proxying of any kind is set up. This type requires v1.7 or higher of kube-dns, or CoreDNS version 0.0.8 or higher.More information about the different types of Services can be found in the Using Source IP tutorial. Also see Connecting Applications with Services.Additionally, note that there are some use cases with Services that involve not defining a selector in the spec. A Service created without selector will also not create the corresponding Endpoints object. This allows users to manually map a Service to specific endpoints. Another possibility why there may be no selector is you are strictly using type: ExternalName.Services and LabelsA Service routes traffic across a set of Pods. Services are the abstraction that allows pods to die and replicate in Kubernetes without impacting your application. Discovery and routing among dependent Pods (such as the frontend and backend components in an application) are handled by Kubernetes Services.Services match a set of Pods using labels and selectors, a grouping primitive that allows logical operation on objects in Kubernetes. Labels are key/value pairs attached to objects and can be used in any number of ways:Designate objects for development, test, and productionEmbed version tagsClassify an object using tagsLabels can be attached to objects at creation time or later on. They can be modified at any time. Let's expose our application now using a Service and apply some labels.Step 1: Creating a new ServiceLet’s verify that our application is running. We’ll use the kubectl get command and look for existing Pods:kubectl get pods If no Pods are running then it means the objects from the previous tutorials were cleaned up. In this case, go back and recreate the deployment from the Using kubectl to create a Deployment tutorial. Please wait a couple of seconds and list the Pods again. You can continue once you see the one Pod running.Next, let’s list the current Services from our cluster:kubectl get services We have now a running Service called kubernetes-bootcamp. Here we see that the Service received a unique cluster-IP, an internal port and an external-IP (the IP of the Node).To find out what port was opened externally (for the type: NodePort Service) we’ll run the describe service subcommand:kubectl describe services/kubernetes-bootcamp Create an environment variable called NODE_PORT that has the value of the Node port assigned:export NODE_PORT="$(kubectl get services/kubernetes-bootcamp -o go-template='{{(index .spec.ports 0).nodePort}}')" echo "NODE_PORT=$NODE_PORT" Now we can test that the app is exposed outside of the cluster using curl, the IP address of the Node and the externally exposed port:curl http://"$(minikube ip):$NODE_PORT" Note:If you're running minikube with Docker Desktop as the container driver, a minikube tunnel is needed. This is because containers inside Docker Desktop are isolated from your host computer.In a separate terminal window, execute:minikube service kubernetes-bootcamp --url The output looks like this:http://127.0.0.1:51082 ! Because you are using a Docker driver on darwin, the terminal needs to be open to run it. Then use the given URL to access the app:curl 127.0.0.1:51082 And we get a response from the server. The Service is exposed.Step 2: Using labelsThe Deployment created automatically a label for our Pod. With the describe deployment subcommand you can see the name (the key) of that label:kubectl describe deployment Let’s use this label to query our list of Pods. We’ll use the kubectl get pods command with -l as a parameter, followed by the label values:kubectl get pods -l app=kubernetes-bootcamp You can do the same to list the existing Services:kubectl get services -l app=kubernetes-bootcamp Get the name of the Pod and store it in the POD_NAME environment variable:export POD_NAME="$(kubectl get pods -o go-template --template '{{range .items}}{{.metadata.name}}{{"\n"}}{{end}}')" echo "Name of the Pod: $POD_NAME" To apply a new label we use the label subcommand followed by the object type, object name and the new label:kubectl label pods "$POD_NAME" version=v1 This will apply a new label to our Pod (we pinned the application version to the Pod), and we can check it with the describe pod command:kubectl describe pods "$POD_NAME" We see here that the label is attached now to our Pod. And we can query now the list of pods using the new label:kubectl get pods -l version=v1 And we see the Pod.Step 3: Deleting a serviceTo delete Services you can use the delete service subcommand. Labels can be used also here:kubectl delete service -l app=kubernetes-bootcamp Confirm that the Service is gone:kubectl get services This confirms that our Service was removed. To confirm that route is not exposed anymore you can curl the previously exposed IP and port:curl http://"$(minikube ip):$NODE_PORT" This proves that the application is not reachable anymore from outside of the cluster. You can confirm that the app is still running with a curl from inside the pod:kubectl exec -ti $POD_NAME -- curl http://localhost:8080 We see here that the application is up. This is because the Deployment is managing the application. To shut down the application, you would need to delete the Deployment as well.What's nextTutorial Running Multiple Instances of Your App.Learn more about Service.5.2.5 - Scale Your App5.2.5.1 - Running Multiple Instances of Your AppObjectivesScale an existing app manually using kubectl.Scaling an applicationYou can create from the start a Deployment with multiple instances using the --replicas parameter for the kubectl create deployment command.Previously we created a Deployment, and then exposed it publicly via a Service. The Deployment created only one Pod for running our application. When traffic increases, we will need to scale the application to keep up with user demand.If you haven't worked through the earlier sections, start from Using minikube to create a cluster.Scaling is accomplished by changing the number of replicas in a Deployment.Note:If you are trying this after the previous section, then you may have deleted the service you created, or have created a Service of type: NodePort. In this section, it is assumed that a service with type: LoadBalancer is created for the kubernetes-bootcamp Deployment.If you have not deleted the Service created in the previous section, first delete that Service and then run the following command to create a new Service with its type set to LoadBalancer:kubectl expose deployment/kubernetes-bootcamp --type="LoadBalancer" --port 8080 Scaling overviewScaling is accomplished by changing the number of replicas in a Deployment.Scaling out a Deployment will ensure new Pods are created and scheduled to Nodes with available resources. Scaling will increase the number of Pods to the new desired state. Kubernetes also supports autoscaling of Pods, but it is outside of the scope of this tutorial. Scaling to zero is also possible, and it will terminate all Pods of the specified Deployment.Running multiple instances of an application will require a way to distribute the traffic to all of them. Services have an integrated load-balancer that will distribute network traffic to all Pods of an exposed Deployment. Services will monitor continuously the running Pods using endpoints, to ensure the traffic is sent only to available Pods.Once you have multiple instances of an application running, you would be able to do Rolling updates without downtime. We'll cover that in the next section of the tutorial. Now, let's go to the terminal and scale our application.Scaling a DeploymentTo list your Deployments, use the get deployments subcommand:kubectl get deployments The output should be similar to:NAME READY UP-TO-DATE AVAILABLE AGE kubernetes-bootcamp 1/1 1 1 11m We should have 1 Pod. If not, run the command again. This shows:NAME lists the names of the Deployments in the cluster.READY shows the ratio of CURRENT/DESIRED replicasUP-TO-DATE displays the number of replicas that have been updated to achieve the desired state.AVAILABLE displays how many replicas of the application are available to your users.AGE displays the amount of time that the application has been running.To see the ReplicaSet created by the Deployment, run:kubectl get rs Notice that the name of the ReplicaSet is always formatted as [DEPLOYMENT-NAME]-[RANDOM-STRING]. The random string is randomly generated and uses the pod-template-hash as a seed.Two important columns of this output are:DESIRED displays the desired number of replicas of the application, which you define when you create the Deployment. This is the desired state.CURRENT displays how many replicas are currently running. Next, let’s scale the Deployment to 4 replicas. We’ll use the kubectl scale command, followed by the Deployment type, name and desired number of instances:kubectl scale deployments/kubernetes-bootcamp --replicas=4 To list your Deployments once again, use get deployments:kubectl get deployments The change was applied, and we have 4 instances of the application available. Next, let’s check if the number of Pods changed:kubectl get pods -o wide There are 4 Pods now, with different IP addresses. The change was registered in the Deployment events log. To check that, use the describe subcommand:kubectl describe deployments/kubernetes-bootcamp You can also view in the output of this command that there are 4 replicas now.Load BalancingLet's check that the Service is load-balancing the traffic. To find out the exposed IP and Port we can use describe service as we learned in the previous part of the tutorial:kubectl describe services/kubernetes-bootcamp Create an environment variable called NODE_PORT that has a value as the Node port:export NODE_PORT="$(kubectl get services/kubernetes-bootcamp -o go-template='{{(index .spec.ports 0).nodePort}}')" echo NODE_PORT=$NODE_PORT Next, we’ll do a curl to the exposed IP address and port. Execute the command multiple times:curl http://"$(minikube ip):$NODE_PORT" We hit a different Pod with every request. This demonstrates that the load-balancing is working.The output should be similar to:Hello Kubernetes bootcamp! | Running on: kubernetes-bootcamp-644c5687f4-wp67j | v=1 Hello Kubernetes bootcamp! | Running on: kubernetes-bootcamp-644c5687f4-hs9dj | v=1 Hello Kubernetes bootcamp! | Running on: kubernetes-bootcamp-644c5687f4-4hjvf | v=1 Hello Kubernetes bootcamp! | Running on: kubernetes-bootcamp-644c5687f4-wp67j | v=1 Hello Kubernetes bootcamp! | Running on: kubernetes-bootcamp-644c5687f4-4hjvf | v=1 Note:If you're running minikube with Docker Desktop as the container driver, a minikube tunnel is needed. This is because containers inside Docker Desktop are isolated from your host computer.In a separate terminal window, execute:minikube service kubernetes-bootcamp --url The output looks like this:http://127.0.0.1:51082 ! Because you are using a Docker driver on darwin, the terminal needs to be open to run it. Then use the given URL to access the app:curl 127.0.0.1:51082 Scale DownTo scale down the Deployment to 2 replicas, run again the scale subcommand:kubectl scale deployments/kubernetes-bootcamp --replicas=2 List the Deployments to check if the change was applied with the get deployments subcommand:kubectl get deployments The number of replicas decreased to 2. List the number of Pods, with get pods:kubectl get pods -o wide This confirms that 2 Pods were terminated.What's nextTutorial Performing a Rolling Update.Learn more about ReplicaSet.Learn more about Autoscaling.5.2.6 - Update Your App5.2.6.1 - Performing a Rolling UpdateObjectivesPerform a rolling update using kubectl.Updating an applicationRolling updates allow Deployments' update to take place with zero downtime by incrementally updating Pods instances with new ones.Users expect applications to be available all the time, and developers are expected to deploy new versions of them several times a day. In Kubernetes this is done with rolling updates. A rolling update allows a Deployment update to take place with zero downtime. It does this by incrementally replacing the current Pods with new ones. The new Pods are scheduled on Nodes with available resources, and Kubernetes waits for those new Pods to start before removing the old Pods.In the previous module we scaled our application to run multiple instances. This is a requirement for performing updates without affecting application availability. By default, the maximum number of Pods that can be unavailable during the update and the maximum number of new Pods that can be created, is one. Both options can be configured to either numbers or percentages (of Pods). In Kubernetes, updates are versioned and any Deployment update can be reverted to a previous (stable) version.Rolling updates overviewIf a Deployment is exposed publicly, the Service will load-balance the traffic only to available Pods during the update.Similar to application Scaling, if a Deployment is exposed publicly, the Service will load-balance the traffic only to available Pods during the update. An available Pod is an instance that is available to the users of the application.Rolling updates allow the following actions:Promote an application from one environment to another (via container image updates)Rollback to previous versionsContinuous Integration and Continuous Delivery of applications with zero downtimeIn the following interactive tutorial, we'll update our application to a new version, and also perform a rollback.Update the version of the appTo list your Deployments, run the get deployments subcommand:kubectl get deployments To list the running Pods, run the get pods subcommand:kubectl get pods To view the current image version of the app, run the describe pods subcommand and look for the Image field:kubectl describe pods To update the image of the application to version 2, use the set image subcommand, followed by the deployment name and the new image version:kubectl set image deployments/kubernetes-bootcamp kubernetes-bootcamp=docker.io/jocatalin/kubernetes-bootcamp:v2 The command notified the Deployment to use a different image for your app and initiated a rolling update. Check the status of the new Pods, and view the old one terminating with the get pods subcommand:kubectl get pods Verify an updateFirst, check that the service is running, as you might have deleted it in previous tutorial step, run describe services/kubernetes-bootcamp. If it's missing, you can create it again with:kubectl expose deployment/kubernetes-bootcamp --type="NodePort" --port 8080 Create an environment variable called NODE_PORT that has the value of the Node port assigned:export NODE_PORT="$(kubectl get services/kubernetes-bootcamp -o go-template='{{(index .spec.ports 0).nodePort}}')" echo "NODE_PORT=$NODE_PORT" Next, do a curl to the exposed IP and port:curl http://"$(minikube ip):$NODE_PORT" Every time you run the curl command, you will hit a different Pod. Notice that all Pods are now running the latest version (v2).You can also confirm the update by running the rollout status subcommand:kubectl rollout status deployments/kubernetes-bootcamp To view the current image version of the app, run the describe pods subcommand:kubectl describe pods In the Image field of the output, verify that you are running the latest image version (v2).Roll back an updateLet’s perform another update, and try to deploy an image tagged with v10:kubectl set image deployments/kubernetes-bootcamp kubernetes-bootcamp=gcr.io/google-samples/kubernetes-bootcamp:v10 Use get deployments to see the status of the deployment:kubectl get deployments Notice that the output doesn't list the desired number of available Pods. Run the get pods subcommand to list all Pods:kubectl get pods Notice that some of the Pods have a status of ImagePullBackOff.To get more insight into the problem, run the describe pods subcommand:kubectl describe pods In the Events section of the output for the affected Pods, notice that the v10 image version did not exist in the repository.To roll back the deployment to your last working version, use the rollout undo subcommand:kubectl rollout undo deployments/kubernetes-bootcamp The rollout undo command reverts the deployment to the previous known state (v2 of the image). Updates are versioned and you can revert to any previously known state of a Deployment.Use the get pods subcommand to list the Pods again:kubectl get pods To check the image deployed on the running Pods, use the describe pods subcommand:kubectl describe pods The Deployment is once again using a stable version of the app (v2). The rollback was successful.Remember to clean up your local cluster.kubectl delete deployments/kubernetes-bootcamp services/kubernetes-bootcamp What's nextLearn more about Deployments.5.3 - Configuration5.3.1 - Updating Configuration via a ConfigMapThis page provides a step-by-step example of updating configuration within a Pod via a ConfigMap and builds upon the Configure a Pod to Use a ConfigMap task.At the end of this tutorial, you will understand how to change the configuration for a running application.This tutorial uses the alpine and nginx images as examples.Before you beginYou need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesYou need to have the curl command-line tool for making HTTP requests from the terminal or command prompt. If you do not have curl available, you can install it. Check the documentation for your local operating system.ObjectivesUpdate configuration via a ConfigMap mounted as a VolumeUpdate environment variables of a Pod via a ConfigMapUpdate configuration via a ConfigMap in a multi-container PodUpdate configuration via a ConfigMap in a Pod possessing a Sidecar ContainerUpdate configuration via a ConfigMap mounted as a VolumeUse the kubectl create configmap command to create a ConfigMap from literal values:kubectl create configmap sport --from-literal=sport=football Below is an example of a Deployment manifest with the ConfigMap sport mounted as a volume into the Pod's only container.deployments/deployment-with-configmap-as-volume.yaml apiVersion: apps/v1 kind: Deployment metadata: name: configmap-volume labels: app.kubernetes.io/name: configmap-volume spec: replicas: 3 selector: matchLabels: app.kubernetes.io/name: configmap-volume template: metadata: labels: app.kubernetes.io/name: configmap-volume spec: containers: \- name: alpine image: alpine:3 command: \- /bin/sh \- -c \- while true; do echo "$(date) My preferred sport is $(cat /etc/config/sport)"; sleep 10; done; ports: \- containerPort: 80 volumeMounts: \- name: config-volume mountPath: /etc/config volumes: \- name: config-volume configMap: name: sportCreate the Deployment:kubectl apply -f https://k8s.io/examples/deployments/deployment-with-configmap-as-volume.yaml Check the pods for this Deployment to ensure they are ready (matching by selector):kubectl get pods --selector=app.kubernetes.io/name=configmap-volume You should see an output similar to:NAME READY STATUS RESTARTS AGE configmap-volume-6b976dfdcf-qxvbm 1/1 Running 0 72s configmap-volume-6b976dfdcf-skpvm 1/1 Running 0 72s configmap-volume-6b976dfdcf-tbc6r 1/1 Running 0 72s On each node where one of these Pods is running, the kubelet fetches the data for that ConfigMap and translates it to files in a local volume. The kubelet then mounts that volume into the container, as specified in the Pod template. The code running in that container loads the information from the file and uses it to print a report to stdout. You can check this report by viewing the logs for one of the Pods in that Deployment:# Pick one Pod that belongs to the Deployment, and view its logs kubectl logs deployments/configmap-volume You should see an output similar to:Found 3 pods, using pod/configmap-volume-76d9c5678f-x5rgj Thu Jan 4 14:06:46 UTC 2024 My preferred sport is football Thu Jan 4 14:06:56 UTC 2024 My preferred sport is football Thu Jan 4 14:07:06 UTC 2024 My preferred sport is football Thu Jan 4 14:07:16 UTC 2024 My preferred sport is football Thu Jan 4 14:07:26 UTC 2024 My preferred sport is football Edit the ConfigMap:kubectl edit configmap sport In the editor that appears, change the value of key sport from football to cricket. Save your changes. The kubectl tool updates the ConfigMap accordingly (if you see an error, try again).Here's an example of how that manifest could look after you edit it:apiVersion: v1 data: sport: cricket kind: ConfigMap # You can leave the existing metadata as they are. # The values you'll see won't exactly match these. metadata: creationTimestamp: "2024-01-04T14:05:06Z" name: sport namespace: default resourceVersion: "1743935" uid: 024ee001-fe72-487e-872e-34d6464a8a23 You should see the following output:configmap/sport edited Tail (follow the latest entries in) the logs of one of the pods that belongs to this Deployment:kubectl logs deployments/configmap-volume --follow After few seconds, you should see the log output change as follows:Thu Jan 4 14:11:36 UTC 2024 My preferred sport is football Thu Jan 4 14:11:46 UTC 2024 My preferred sport is football Thu Jan 4 14:11:56 UTC 2024 My preferred sport is football Thu Jan 4 14:12:06 UTC 2024 My preferred sport is cricket Thu Jan 4 14:12:16 UTC 2024 My preferred sport is cricket When you have a ConfigMap that is mapped into a running Pod using either a configMap volume or a projected volume, and you update that ConfigMap, the running Pod sees the update almost immediately.However, your application only sees the change if it is written to either poll for changes, or watch for file updates.An application that loads its configuration once at startup will not notice a change.Note:The total delay from the moment when the ConfigMap is updated to the moment when new keys are projected to the Pod can be as long as kubelet sync period.Also check Mounted ConfigMaps are updated automatically.Update environment variables of a Pod via a ConfigMapUse the kubectl create configmap command to create a ConfigMap from literal values:kubectl create configmap fruits --from-literal=fruits=apples Below is an example of a Deployment manifest with an environment variable configured via the ConfigMap fruits.deployments/deployment-with-configmap-as-envvar.yaml apiVersion: apps/v1 kind: Deployment metadata: name: configmap-env-var labels: app.kubernetes.io/name: configmap-env-var spec: replicas: 3 selector: matchLabels: app.kubernetes.io/name: configmap-env-var template: metadata: labels: app.kubernetes.io/name: configmap-env-var spec: containers: \- name: alpine image: alpine:3 env: \- name: FRUITS valueFrom: configMapKeyRef: key: fruits name: fruits command: \- /bin/sh \- -c \- while true; do echo "$(date) The basket is full of $FRUITS"; sleep 10; done; ports: \- containerPort: 80Create the Deployment:kubectl apply -f https://k8s.io/examples/deployments/deployment-with-configmap-as-envvar.yaml Check the pods for this Deployment to ensure they are ready (matching by selector):kubectl get pods --selector=app.kubernetes.io/name=configmap-env-var You should see an output similar to:NAME READY STATUS RESTARTS AGE configmap-env-var-59cfc64f7d-74d7z 1/1 Running 0 46s configmap-env-var-59cfc64f7d-c4wmj 1/1 Running 0 46s configmap-env-var-59cfc64f7d-dpr98 1/1 Running 0 46s The key-value pair in the ConfigMap is configured as an environment variable in the container of the Pod. Check this by viewing the logs of one Pod that belongs to the Deployment.kubectl logs deployment/configmap-env-var You should see an output similar to:Found 3 pods, using pod/configmap-env-var-7c994f7769-l74nq Thu Jan 4 16:07:06 UTC 2024 The basket is full of apples Thu Jan 4 16:07:16 UTC 2024 The basket is full of apples Thu Jan 4 16:07:26 UTC 2024 The basket is full of apples Edit the ConfigMap:kubectl edit configmap fruits In the editor that appears, change the value of key fruits from apples to mangoes. Save your changes. The kubectl tool updates the ConfigMap accordingly (if you see an error, try again).Here's an example of how that manifest could look after you edit it:apiVersion: v1 data: fruits: mangoes kind: ConfigMap # You can leave the existing metadata as they are. # The values you'll see won't exactly match these. metadata: creationTimestamp: "2024-01-04T16:04:19Z" name: fruits namespace: default resourceVersion: "1749472" You should see the following output:configmap/fruits edited Tail the logs of the Deployment and observe the output for few seconds:# As the text explains, the output does NOT change kubectl logs deployments/configmap-env-var --follow Notice that the output remains unchanged, even though you edited the ConfigMap:Thu Jan 4 16:12:56 UTC 2024 The basket is full of apples Thu Jan 4 16:13:06 UTC 2024 The basket is full of apples Thu Jan 4 16:13:16 UTC 2024 The basket is full of apples Thu Jan 4 16:13:26 UTC 2024 The basket is full of apples Note:Although the value of the key inside the ConfigMap has changed, the environment variable in the Pod still shows the earlier value. This is because environment variables for a process running inside a Pod are not updated when the source data changes; if you wanted to force an update, you would need to have Kubernetes replace your existing Pods. The new Pods would then run with the updated information.You can trigger that replacement. Perform a rollout for the Deployment, using kubectl rollout:# Trigger the rollout kubectl rollout restart deployment configmap-env-var # Wait for the rollout to complete kubectl rollout status deployment configmap-env-var --watch=true Next, check the Deployment:kubectl get deployment configmap-env-var You should see an output similar to:NAME READY UP-TO-DATE AVAILABLE AGE configmap-env-var 3/3 3 3 12m Check the Pods:kubectl get pods --selector=app.kubernetes.io/name=configmap-env-var The rollout causes Kubernetes to make a new ReplicaSet for the Deployment; that means the existing Pods eventually terminate, and new ones are created. After few seconds, you should see an output similar to:NAME READY STATUS RESTARTS AGE configmap-env-var-6d94d89bf5-2ph2l 1/1 Running 0 13s configmap-env-var-6d94d89bf5-74twx 1/1 Running 0 8s configmap-env-var-6d94d89bf5-d5vx8 1/1 Running 0 11s Note:Please wait for the older Pods to fully terminate before proceeding with the next steps.View the logs for a Pod in this Deployment:# Pick one Pod that belongs to the Deployment, and view its logs kubectl logs deployment/configmap-env-var You should see an output similar to the below:Found 3 pods, using pod/configmap-env-var-6d9ff89fb6-bzcf6 Thu Jan 4 16:30:35 UTC 2024 The basket is full of mangoes Thu Jan 4 16:30:45 UTC 2024 The basket is full of mangoes Thu Jan 4 16:30:55 UTC 2024 The basket is full of mangoes This demonstrates the scenario of updating environment variables in a Pod that are derived from a ConfigMap. Changes to the ConfigMap values are applied to the Pod during the subsequent rollout. If Pods get created for another reason, such as scaling up the Deployment, then the new Pods also use the latest configuration values; if you don't trigger a rollout, then you might find that your app is running with a mix of old and new environment variable values.Update configuration via a ConfigMap in a multi-container PodUse the kubectl create configmap command to create a ConfigMap from literal values:kubectl create configmap color --from-literal=color=red Below is an example manifest for a Deployment that manages a set of Pods, each with two containers. The two containers share an emptyDir volume that they use to communicate. The first container runs a web server (nginx). The mount path for the shared volume in the web server container is /usr/share/nginx/html. The second helper container is based on alpine, and for this container the emptyDir volume is mounted at /pod-data. The helper container writes a file in HTML that has its content based on a ConfigMap. The web server container serves the HTML via HTTP.deployments/deployment-with-configmap-two-containers.yaml apiVersion: apps/v1 kind: Deployment metadata: name: configmap-two-containers labels: app.kubernetes.io/name: configmap-two-containers spec: replicas: 3 selector: matchLabels: app.kubernetes.io/name: configmap-two-containers template: metadata: labels: app.kubernetes.io/name: configmap-two-containers spec: volumes: \- name: shared-data emptyDir: {} \- name: config-volume configMap: name: color containers: \- name: nginx image: nginx volumeMounts: \- name: shared-data mountPath: /usr/share/nginx/html \- name: alpine image: alpine:3 volumeMounts: \- name: shared-data mountPath: /pod-data \- name: config-volume mountPath: /etc/config command: \- /bin/sh \- -c \- while true; do echo "$(date) My preferred color is $(cat /etc/config/color)" > /pod-data/index.html; sleep 10; done; Create the Deployment:kubectl apply -f https://k8s.io/examples/deployments/deployment-with-configmap-two-containers.yaml Check the pods for this Deployment to ensure they are ready (matching by selector):kubectl get pods --selector=app.kubernetes.io/name=configmap-two-containers You should see an output similar to:NAME READY STATUS RESTARTS AGE configmap-two-containers-565fb6d4f4-2xhxf 2/2 Running 0 20s configmap-two-containers-565fb6d4f4-g5v4j 2/2 Running 0 20s configmap-two-containers-565fb6d4f4-mzsmf 2/2 Running 0 20s Expose the Deployment (the kubectl tool creates a Service for you):kubectl expose deployment configmap-two-containers --name=configmap-service --port=8080 --target-port=80 Use kubectl to forward the port:# this stays running in the background kubectl port-forward service/configmap-service 8080:8080 & Access the service.curl http://localhost:8080 You should see an output similar to:Fri Jan 5 08:08:22 UTC 2024 My preferred color is red Edit the ConfigMap:kubectl edit configmap color In the editor that appears, change the value of key color from red to blue. Save your changes. The kubectl tool updates the ConfigMap accordingly (if you see an error, try again).Here's an example of how that manifest could look after you edit it:apiVersion: v1 data: color: blue kind: ConfigMap # You can leave the existing metadata as they are. # The values you'll see won't exactly match these. metadata: creationTimestamp: "2024-01-05T08:12:05Z" name: color namespace: configmap resourceVersion: "1801272" uid: 80d33e4a-cbb4-4bc9-ba8c-544c68e425d6 Loop over the service URL for few seconds.# Cancel this when you're happy with it (Ctrl-C) while true; do curl --connect-timeout 7.5 http://localhost:8080; sleep 10; done You should see the output change as follows:Fri Jan 5 08:14:00 UTC 2024 My preferred color is red Fri Jan 5 08:14:02 UTC 2024 My preferred color is red Fri Jan 5 08:14:20 UTC 2024 My preferred color is red Fri Jan 5 08:14:22 UTC 2024 My preferred color is red Fri Jan 5 08:14:32 UTC 2024 My preferred color is blue Fri Jan 5 08:14:43 UTC 2024 My preferred color is blue Fri Jan 5 08:15:00 UTC 2024 My preferred color is blue Update configuration via a ConfigMap in a Pod possessing a sidecar containerThe above scenario can be replicated by using a Sidecar Container as a helper container to write the HTML file.As a Sidecar Container is conceptually an Init Container, it is guaranteed to start before the main web server container.This ensures that the HTML file is always available when the web server is ready to serve it.If you are continuing from the previous scenario, you can reuse the ConfigMap named color for this scenario.If you are executing this scenario independently, use the kubectl create configmap command to create a ConfigMap from literal values:kubectl create configmap color --from-literal=color=blue Below is an example manifest for a Deployment that manages a set of Pods, each with a main container and a sidecar container. The two containers share an emptyDir volume that they use to communicate. The main container runs a web server (NGINX). The mount path for the shared volume in the web server container is /usr/share/nginx/html. The second container is a Sidecar Container based on Alpine Linux which acts as a helper container. For this container the emptyDir volume is mounted at /pod-data. The Sidecar Container writes a file in HTML that has its content based on a ConfigMap. The web server container serves the HTML via HTTP.deployments/deployment-with-configmap-and-sidecar-container.yaml apiVersion: apps/v1 kind: Deployment metadata: name: configmap-sidecar-container labels: app.kubernetes.io/name: configmap-sidecar-container spec: replicas: 3 selector: matchLabels: app.kubernetes.io/name: configmap-sidecar-container template: metadata: labels: app.kubernetes.io/name: configmap-sidecar-container spec: volumes: \- name: shared-data emptyDir: {} \- name: config-volume configMap: name: color containers: \- name: nginx image: nginx volumeMounts: \- name: shared-data mountPath: /usr/share/nginx/html initContainers: \- name: alpine image: alpine:3 restartPolicy: Always volumeMounts: \- name: shared-data mountPath: /pod-data \- name: config-volume mountPath: /etc/config command: \- /bin/sh \- -c \- while true; do echo "$(date) My preferred color is $(cat /etc/config/color)" > /pod-data/index.html; sleep 10; done; Create the Deployment:kubectl apply -f https://k8s.io/examples/deployments/deployment-with-configmap-and-sidecar-container.yaml Check the pods for this Deployment to ensure they are ready (matching by selector):kubectl get pods --selector=app.kubernetes.io/name=configmap-sidecar-container You should see an output similar to:NAME READY STATUS RESTARTS AGE configmap-sidecar-container-5fb59f558b-87rp7 2/2 Running 0 94s configmap-sidecar-container-5fb59f558b-ccs7s 2/2 Running 0 94s configmap-sidecar-container-5fb59f558b-wnmgk 2/2 Running 0 94s Expose the Deployment (the kubectl tool creates a Service for you):kubectl expose deployment configmap-sidecar-container --name=configmap-sidecar-service --port=8081 --target-port=80 Use kubectl to forward the port:# this stays running in the background kubectl port-forward service/configmap-sidecar-service 8081:8081 & Access the service.curl http://localhost:8081 You should see an output similar to:Sat Feb 17 13:09:05 UTC 2024 My preferred color is blue Edit the ConfigMap:kubectl edit configmap color In the editor that appears, change the value of key color from blue to green. Save your changes. The kubectl tool updates the ConfigMap accordingly (if you see an error, try again).Here's an example of how that manifest could look after you edit it:apiVersion: v1 data: color: green kind: ConfigMap # You can leave the existing metadata as they are. # The values you'll see won't exactly match these. metadata: creationTimestamp: "2024-02-17T12:20:30Z" name: color namespace: default resourceVersion: "1054" uid: e40bb34c-58df-4280-8bea-6ed16edccfaa Loop over the service URL for few seconds.# Cancel this when you're happy with it (Ctrl-C) while true; do curl --connect-timeout 7.5 http://localhost:8081; sleep 10; done You should see the output change as follows:Sat Feb 17 13:12:35 UTC 2024 My preferred color is blue Sat Feb 17 13:12:45 UTC 2024 My preferred color is blue Sat Feb 17 13:12:55 UTC 2024 My preferred color is blue Sat Feb 17 13:13:05 UTC 2024 My preferred color is blue Sat Feb 17 13:13:15 UTC 2024 My preferred color is green Sat Feb 17 13:13:25 UTC 2024 My preferred color is green Sat Feb 17 13:13:35 UTC 2024 My preferred color is green Update configuration via an immutable ConfigMap that is mounted as a volumeNote:Immutable ConfigMaps are especially used for configuration that is constant and is not expected to change over time. Marking a ConfigMap as immutable allows a performance improvement where the kubelet does not watch for changes.If you do need to make a change, you should plan to either:change the name of the ConfigMap, and switch to running Pods that reference the new namereplace all the nodes in your cluster that have previously run a Pod that used the old valuerestart the kubelet on any node where the kubelet previously loaded the old ConfigMapAn example manifest for an Immutable ConfigMap is shown below.configmap/immutable-configmap.yaml apiVersion: v1 data: company_name: "ACME, Inc." # existing fictional company name kind: ConfigMap immutable: true metadata: name: company-name-20150801Create the Immutable ConfigMap:kubectl apply -f https://k8s.io/examples/configmap/immutable-configmap.yaml Below is an example of a Deployment manifest with the Immutable ConfigMap company-name-20150801 mounted as a volume into the Pod's only container.deployments/deployment-with-immutable-configmap-as-volume.yaml apiVersion: apps/v1 kind: Deployment metadata: name: immutable-configmap-volume labels: app.kubernetes.io/name: immutable-configmap-volume spec: replicas: 3 selector: matchLabels: app.kubernetes.io/name: immutable-configmap-volume template: metadata: labels: app.kubernetes.io/name: immutable-configmap-volume spec: containers: \- name: alpine image: alpine:3 command: \- /bin/sh \- -c \- while true; do echo "$(date) The name of the company is $(cat /etc/config/company_name)"; sleep 10; done; ports: \- containerPort: 80 volumeMounts: \- name: config-volume mountPath: /etc/config volumes: \- name: config-volume configMap: name: company-name-20150801Create the Deployment:kubectl apply -f https://k8s.io/examples/deployments/deployment-with-immutable-configmap-as-volume.yaml Check the pods for this Deployment to ensure they are ready (matching by selector):kubectl get pods --selector=app.kubernetes.io/name=immutable-configmap-volume You should see an output similar to:NAME READY STATUS RESTARTS AGE immutable-configmap-volume-78b6fbff95-5gsfh 1/1 Running 0 62s immutable-configmap-volume-78b6fbff95-7vcj4 1/1 Running 0 62s immutable-configmap-volume-78b6fbff95-vdslm 1/1 Running 0 62s The Pod's container refers to the data defined in the ConfigMap and uses it to print a report to stdout. You can check this report by viewing the logs for one of the Pods in that Deployment:# Pick one Pod that belongs to the Deployment, and view its logs kubectl logs deployments/immutable-configmap-volume You should see an output similar to:Found 3 pods, using pod/immutable-configmap-volume-78b6fbff95-5gsfh Wed Mar 20 03:52:34 UTC 2024 The name of the company is ACME, Inc. Wed Mar 20 03:52:44 UTC 2024 The name of the company is ACME, Inc. Wed Mar 20 03:52:54 UTC 2024 The name of the company is ACME, Inc. Note:Once a ConfigMap is marked as immutable, it is not possible to revert this change nor to mutate the contents of the data or the binaryData field.In order to modify the behavior of the Pods that use this configuration, you will create a new immutable ConfigMap and edit the Deployment to define a slightly different pod template, referencing the new ConfigMap.Create a new immutable ConfigMap by using the manifest shown below:configmap/new-immutable-configmap.yaml apiVersion: v1 data: company_name: "Fiktivesunternehmen GmbH" # new fictional company name kind: ConfigMap immutable: true metadata: name: company-name-20240312kubectl apply -f https://k8s.io/examples/configmap/new-immutable-configmap.yaml You should see an output similar to:configmap/company-name-20240312 created Check the newly created ConfigMap:kubectl get configmap You should see an output displaying both the old and new ConfigMaps:NAME DATA AGE company-name-20150801 1 22m company-name-20240312 1 24s Modify the Deployment to reference the new ConfigMap.Edit the Deployment:kubectl edit deployment immutable-configmap-volume In the editor that appears, update the existing volume definition to use the new ConfigMap.volumes: \- configMap: defaultMode: 420 name: company-name-20240312 # Update this field name: config-volume You should see the following output:deployment.apps/immutable-configmap-volume edited This will trigger a rollout. Wait for all the previous Pods to terminate and the new Pods to be in a ready state.Monitor the status of the Pods:kubectl get pods --selector=app.kubernetes.io/name=immutable-configmap-volume NAME READY STATUS RESTARTS AGE immutable-configmap-volume-5fdb88fcc8-29v8n 1/1 Running 0 13s immutable-configmap-volume-5fdb88fcc8-52ddd 1/1 Running 0 14s immutable-configmap-volume-5fdb88fcc8-n5jx4 1/1 Running 0 15s immutable-configmap-volume-78b6fbff95-5gsfh 1/1 Terminating 0 32m immutable-configmap-volume-78b6fbff95-7vcj4 1/1 Terminating 0 32m immutable-configmap-volume-78b6fbff95-vdslm 1/1 Terminating 0 32m You should eventually see an output similar to:NAME READY STATUS RESTARTS AGE immutable-configmap-volume-5fdb88fcc8-29v8n 1/1 Running 0 43s immutable-configmap-volume-5fdb88fcc8-52ddd 1/1 Running 0 44s immutable-configmap-volume-5fdb88fcc8-n5jx4 1/1 Running 0 45s View the logs for a Pod in this Deployment:# Pick one Pod that belongs to the Deployment, and view its logs kubectl logs deployment/immutable-configmap-volume You should see an output similar to the below:Found 3 pods, using pod/immutable-configmap-volume-5fdb88fcc8-n5jx4 Wed Mar 20 04:24:17 UTC 2024 The name of the company is Fiktivesunternehmen GmbH Wed Mar 20 04:24:27 UTC 2024 The name of the company is Fiktivesunternehmen GmbH Wed Mar 20 04:24:37 UTC 2024 The name of the company is Fiktivesunternehmen GmbH Once all the deployments have migrated to use the new immutable ConfigMap, it is advised to delete the old one.kubectl delete configmap company-name-20150801 SummaryChanges to a ConfigMap mounted as a Volume on a Pod are available seamlessly after the subsequent kubelet sync.Changes to a ConfigMap that configures environment variables for a Pod are available after the subsequent rollout for the Pod.Once a ConfigMap is marked as immutable, it is not possible to revert this change (you cannot make an immutable ConfigMap mutable), and you also cannot make any change to the contents of the data or the binaryData field. You can delete and recreate the ConfigMap, or you can make a new different ConfigMap. When you delete a ConfigMap, running containers and their Pods maintain a mount point to any volume that referenced that existing ConfigMap.Cleaning upTerminate the kubectl port-forward commands in case they are running.Delete the resources created during the tutorial:kubectl delete deployment configmap-volume configmap-env-var configmap-two-containers configmap-sidecar-container immutable-configmap-volume kubectl delete service configmap-service configmap-sidecar-service kubectl delete configmap sport fruits color company-name-20240312 kubectl delete configmap company-name-20150801 # In case it was not handled during the task execution 5.3.2 - Configuring Redis using a ConfigMapThis page provides a real world example of how to configure Redis using a ConfigMap and builds upon the Configure a Pod to Use a ConfigMap task.ObjectivesCreate a ConfigMap with Redis configuration valuesCreate a Redis Pod that mounts and uses the created ConfigMapVerify that the configuration was correctly applied.Before you beginYou need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesTo check the version, enter kubectl version.The example shown on this page works with kubectl 1.14 and above.Understand Configure a Pod to Use a ConfigMap.Real World Example: Configuring Redis using a ConfigMapFollow the steps below to configure a Redis cache using data stored in a ConfigMap.First create a ConfigMap with an empty configuration block:cat <./example-redis-config.yaml apiVersion: v1 kind: ConfigMap metadata: name: example-redis-config data: redis-config: "" EOF Apply the ConfigMap created above, along with a Redis pod manifest:kubectl apply -f example-redis-config.yaml kubectl apply -f https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/config/redis-pod.yaml Examine the contents of the Redis pod manifest and note the following:A volume named config is created by spec.volumes[1]The key and path under spec.volumes[1].configMap.items[0] exposes the redis-config key from the example-redis-config ConfigMap as a file named redis.conf on the config volume.The config volume is then mounted at /redis-master by spec.containers[0].volumeMounts[1].This has the net effect of exposing the data in data.redis-config from the example-redis-config ConfigMap above as /redis-master/redis.conf inside the Pod.pods/config/redis-pod.yaml apiVersion: v1 kind: Pod metadata: name: redis spec: containers: \- name: redis image: redis:5.0.4 command: \- redis-server \- "/redis-master/redis.conf" env: \- name: MASTER value: "true" ports: \- containerPort: 6379 resources: limits: cpu: "0.1" volumeMounts: \- mountPath: /redis-master-data name: data \- mountPath: /redis-master name: config volumes: \- name: data emptyDir: {} \- name: config configMap: name: example-redis-config items: \- key: redis-config path: redis.conf Examine the created objects:kubectl get pod/redis configmap/example-redis-config You should see the following output:NAME READY STATUS RESTARTS AGE pod/redis 1/1 Running 0 8s NAME DATA AGE configmap/example-redis-config 1 14s Recall that we left redis-config key in the example-redis-config ConfigMap blank:kubectl describe configmap/example-redis-config You should see an empty redis-config key:Name: example-redis-config Namespace: default Labels:  Annotations:  Data ==== redis-config: Use kubectl exec to enter the pod and run the redis-cli tool to check the current configuration:kubectl exec -it redis -- redis-cli Check maxmemory:127.0.0.1:6379> CONFIG GET maxmemory It should show the default value of 0:1) "maxmemory" 2) "0" Similarly, check maxmemory-policy:127.0.0.1:6379> CONFIG GET maxmemory-policy Which should also yield its default value of noeviction:1) "maxmemory-policy" 2) "noeviction" Now let's add some configuration values to the example-redis-config ConfigMap:pods/config/example-redis-config.yaml apiVersion: v1 kind: ConfigMap metadata: name: example-redis-config data: redis-config: | maxmemory 2mb maxmemory-policy allkeys-lru Apply the updated ConfigMap:kubectl apply -f example-redis-config.yaml Confirm that the ConfigMap was updated:kubectl describe configmap/example-redis-config You should see the configuration values we just added:Name: example-redis-config Namespace: default Labels:  Annotations:  Data ==== redis-config: \---- maxmemory 2mb maxmemory-policy allkeys-lru Check the Redis Pod again using redis-cli via kubectl exec to see if the configuration was applied:kubectl exec -it redis -- redis-cli Check maxmemory:127.0.0.1:6379> CONFIG GET maxmemory It remains at the default value of 0:1) "maxmemory" 2) "0" Similarly, maxmemory-policy remains at the noeviction default setting:127.0.0.1:6379> CONFIG GET maxmemory-policy Returns:1) "maxmemory-policy" 2) "noeviction" The configuration values have not changed because the Pod needs to be restarted to grab updated values from associated ConfigMaps. Let's delete and recreate the Pod:kubectl delete pod redis kubectl apply -f https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/config/redis-pod.yaml Now re-check the configuration values one last time:kubectl exec -it redis -- redis-cli Check maxmemory:127.0.0.1:6379> CONFIG GET maxmemory It should now return the updated value of 2097152:1) "maxmemory" 2) "2097152" Similarly, maxmemory-policy has also been updated:127.0.0.1:6379> CONFIG GET maxmemory-policy It now reflects the desired value of allkeys-lru:1) "maxmemory-policy" 2) "allkeys-lru" Clean up your work by deleting the created resources:kubectl delete pod/redis configmap/example-redis-config What's nextLearn more about ConfigMaps.Follow an example of Updating configuration via a ConfigMap.5.3.3 - Adopting Sidecar ContainersThis section is relevant for people adopting a new built-in sidecar containers feature for their workloads.Sidecar container is not a new concept as posted in the blog post. Kubernetes allows running multiple containers in a Pod to implement this concept. However, running a sidecar container as a regular container has a lot of limitations being fixed with the new built-in sidecar containers support.FEATURE STATE: Kubernetes v1.33 [stable] (enabled by default: true)ObjectivesUnderstand the need for sidecar containersBe able to troubleshoot issues with the sidecar containersUnderstand options to universally "inject" sidecar containers to any workloadBefore you beginYou need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesYour Kubernetes server must be at or later than version 1.29.To check the version, enter kubectl version.Sidecar containers overviewSidecar containers are secondary containers that run along with the main application container within the same Pod. These containers are used to enhance or to extend the functionality of the primary app container by providing additional services, or functionalities such as logging, monitoring, security, or data synchronization, without directly altering the primary application code. You can read more in the Sidecar containers concept page.The concept of sidecar containers is not new and there are multiple implementations of this concept. As well as sidecar containers that you, the person defining the Pod, want to run, you can also find that some addons modify Pods - before the Pods start running - so that there are extra sidecar containers. The mechanisms to inject those extra sidecars are often mutating webhooks. For example, a service mesh addon might inject a sidecar that configures mutual TLS and encryption in transit between different Pods.While the concept of sidecar containers is not new, the native implementation of this feature in Kubernetes, however, is new. And as with every new feature, adopting this feature may present certain challenges.This tutorial explores challenges and solutions that can be experienced by end users as well as by authors of sidecar containers.Benefits of a built-in sidecar containerUsing Kubernetes' native support for sidecar containers provides several benefits:You can configure a native sidecar container to start ahead of init containers.The built-in sidecar containers can be authored to guarantee that they are terminated last. Sidecar containers are terminated with a SIGTERM signal once all the regular containers are completed and terminated. If the sidecar container isn’t gracefully shut down, a SIGKILL signal will be used to terminate it.With Jobs, when Pod's restartPolicy: OnFailure or restartPolicy: Never, native sidecar containers do not block Pod completion. With legacy sidecar containers, special care is needed to handle this situation.Also, with Jobs, built-in sidecar containers would keep being restarted once they are done, even if regular containers would not with Pod's restartPolicy: Never.See differences from init containers to learn more about it.Adopting built-in sidecar containersThe SidecarContainers feature gate is in beta state starting from Kubernetes version 1.29 and is enabled by default. Some clusters may have this feature disabled or have software installed that is incompatible with the feature.When this happens, the Pod may be rejected or the sidecar containers may block Pod startup, rendering the Pod useless. This condition is easy to detect as the Pod simply gets stuck on initialization. However, it is often unclear what caused the problem.Here are the considerations and troubleshooting steps that one can take while adopting sidecar containers for their workload.Ensure the feature gate is enabledAs a very first step, make sure that both API server and Nodes are at Kubernetes version v1.29 or later. The feature will break on clusters where Nodes are running earlier versions where it is not enabled.NoteThe feature can be enabled on nodes with the version 1.28. The behavior of built-in sidecar container termination was different in version 1.28, and it is not recommended to adjust the behavior of a sidecar to that behavior. However, if the only concern is the startup order, the above statement can be changed to Nodes running version 1.28 with the feature gate enabled.You should ensure that the feature gate is enabled for the API server(s) within the control plane and for all nodes.One of the ways to check the feature gate enablement is to run a command like this:For API Server:kubectl get --raw /metrics | grep kubernetes_feature_enabled | grep SidecarContainers For the individual node:kubectl get --raw /api/v1/nodes//proxy/metrics | grep kubernetes_feature_enabled | grep SidecarContainers If you see something like this:kubernetes_feature_enabled{name="SidecarContainers",stage="BETA"} 1 it means that the feature is enabled.Check for 3rd party tooling and mutating webhooksIf you experience issues when validating the feature, it may be an indication that one of the 3rd party tools or mutating webhooks are broken.When the SidecarContainers feature gate is enabled, Pods gain a new field in their API. Some tools or mutating webhooks might have been built with an earlier version of Kubernetes API.If tools pass unknown fields as-is using various patching strategies to mutate a Pod object, this will not be a problem. However, there are tools that will strip out unknown fields; if you have those, they must be recompiled with the v1.28+ version of Kubernetes API client code.The way to check this is to use the kubectl describe pod command with your Pod that has passed through mutating admission. If any tools stripped out the new field (restartPolicy:Always), you will not see it in the command output.If you hit an issue like this, please advise the author of the tools or the webhooks use one of the patching strategies for modifying objects instead of a full object update.NoteMutating webhook may update Pods based on some conditions. Thus, sidecar containers may work for some Pods and fail for others.Automatic injection of sidecarsIf you are using software that injects sidecars automatically, there are a few possible strategies you may follow to ensure that native sidecar containers can be used. All strategies are generally options you may choose to decide whether the Pod the sidecar will be injected to will land on a Node supporting the feature or not.As an example, you can follow this conversation in Istio community. The discussion explores the options listed below.Mark Pods that land to nodes supporting sidecars. You can use node labels and node affinity to mark nodes supporting sidecar containers and Pods landing on those nodes.Check Nodes compatibility on injection. During sidecar injection, you may use the following strategies to check node compatibility:query node version and assume the feature gate is enabled on the version 1.29+query node prometheus metrics and check feature enablement statusassume the nodes are running with a supported version skew from the API serverthere may be other custom ways to detect nodes compatibility.Develop a universal sidecar injector. The idea of a universal sidecar injector is to inject a sidecar container as a regular container as well as a native sidecar container. And have a runtime logic to decide which one will work. The universal sidecar injector is wasteful, as it will account for requests twice, but may be considered as a workable solution for special cases.One way would be on start of a native sidecar container detect the node version and exit immediately if the version does not support the sidecar feature.Consider a runtime feature detection design:Define an empty dir so containers can communicate with each otherInject an init container, let's call it NativeSidecar with restartPolicy=Always.NativeSidecar must write a file to an empty directory indicating the first run and exit immediately with exit code 0.NativeSidecar on restart (when native sidecars are supported) checks that file already exists in the empty dir and changes it - indicating that the built-in sidecar containers are supported and running.Inject regular container, let's call it OldWaySidecar.OldWaySidecar on start checks the presence of a file in an empty dir.If the file indicates that the NativeSidecar is NOT running, it assumes that the sidecar feature is not supported and works assuming it is the sidecar.If the file indicates that the NativeSidecar is running, it either does nothing and sleeps forever (in the case when Pod’s restartPolicy=Always) or exits immediately with exit code 0 (in the case when Pod’s restartPolicy!=Always).What's nextLearn more about sidecar containers.5.4 - SecuritySecurity is an important concern for most organizations and people who run Kubernetes clusters. You can find a basic security checklist elsewhere in the Kubernetes documentation.To learn how to deploy and manage security aspects of Kubernetes, you can follow the tutorials in this section.5.4.1 - Apply Pod Security Standards at the Cluster LevelNoteThis tutorial applies only for new clusters.Pod Security is an admission controller that carries out checks against the Kubernetes Pod Security Standards when new pods are created. It is a feature GA'ed in v1.25. This tutorial shows you how to enforce the baseline Pod Security Standard at the cluster level which applies a standard configuration to all namespaces in a cluster.To apply Pod Security Standards to specific namespaces, refer to Apply Pod Security Standards at the namespace level.If you are running a version of Kubernetes other than v1.33, check the documentation for that version.Before you beginInstall the following on your workstation:kindkubectlThis tutorial demonstrates what you can configure for a Kubernetes cluster that you fully control. If you are learning how to configure Pod Security Admission for a managed cluster where you are not able to configure the control plane, read Apply Pod Security Standards at the namespace level.Choose the right Pod Security Standard to applyPod Security Admission lets you apply built-in Pod Security Standards with the following modes: enforce, audit, and warn.To gather information that helps you to choose the Pod Security Standards that are most appropriate for your configuration, do the following:Create a cluster with no Pod Security Standards applied:kind create cluster --name psa-wo-cluster-pss The output is similar to:Creating cluster "psa-wo-cluster-pss" ... ✓ Ensuring node image (kindest/node:v1.33.0) 🖼 ✓ Preparing nodes 📦 ✓ Writing configuration 📜 ✓ Starting control-plane 🕹️ ✓ Installing CNI 🔌 ✓ Installing StorageClass 💾 Set kubectl context to "kind-psa-wo-cluster-pss" You can now use your cluster with: kubectl cluster-info --context kind-psa-wo-cluster-pss Thanks for using kind! 😊 Set the kubectl context to the new cluster:kubectl cluster-info --context kind-psa-wo-cluster-pss The output is similar to this:Kubernetes control plane is running at https://127.0.0.1:61350 CoreDNS is running at https://127.0.0.1:61350/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'. Get a list of namespaces in the cluster:kubectl get ns The output is similar to this:NAME STATUS AGE default Active 9m30s kube-node-lease Active 9m32s kube-public Active 9m32s kube-system Active 9m32s local-path-storage Active 9m26s Use --dry-run=server to understand what happens when different Pod Security Standards are applied:Privilegedkubectl label --dry-run=server --overwrite ns --all \ pod-security.kubernetes.io/enforce=privileged The output is similar to:namespace/default labeled namespace/kube-node-lease labeled namespace/kube-public labeled namespace/kube-system labeled namespace/local-path-storage labeled Baselinekubectl label --dry-run=server --overwrite ns --all \ pod-security.kubernetes.io/enforce=baseline The output is similar to:namespace/default labeled namespace/kube-node-lease labeled namespace/kube-public labeled Warning: existing pods in namespace "kube-system" violate the new PodSecurity enforce level "baseline:latest" Warning: etcd-psa-wo-cluster-pss-control-plane (and 3 other pods): host namespaces, hostPath volumes Warning: kindnet-vzj42: non-default capabilities, host namespaces, hostPath volumes Warning: kube-proxy-m6hwf: host namespaces, hostPath volumes, privileged namespace/kube-system labeled namespace/local-path-storage labeled Restrictedkubectl label --dry-run=server --overwrite ns --all \ pod-security.kubernetes.io/enforce=restricted The output is similar to:namespace/default labeled namespace/kube-node-lease labeled namespace/kube-public labeled Warning: existing pods in namespace "kube-system" violate the new PodSecurity enforce level "restricted:latest" Warning: coredns-7bb9c7b568-hsptc (and 1 other pod): unrestricted capabilities, runAsNonRoot != true, seccompProfile Warning: etcd-psa-wo-cluster-pss-control-plane (and 3 other pods): host namespaces, hostPath volumes, allowPrivilegeEscalation != false, unrestricted capabilities, restricted volume types, runAsNonRoot != true Warning: kindnet-vzj42: non-default capabilities, host namespaces, hostPath volumes, allowPrivilegeEscalation != false, unrestricted capabilities, restricted volume types, runAsNonRoot != true, seccompProfile Warning: kube-proxy-m6hwf: host namespaces, hostPath volumes, privileged, allowPrivilegeEscalation != false, unrestricted capabilities, restricted volume types, runAsNonRoot != true, seccompProfile namespace/kube-system labeled Warning: existing pods in namespace "local-path-storage" violate the new PodSecurity enforce level "restricted:latest" Warning: local-path-provisioner-d6d9f7ffc-lw9lh: allowPrivilegeEscalation != false, unrestricted capabilities, runAsNonRoot != true, seccompProfile namespace/local-path-storage labeled From the previous output, you'll notice that applying the privileged Pod Security Standard shows no warnings for any namespaces. However, baseline and restricted standards both have warnings, specifically in the kube-system namespace.Set modes, versions and standardsIn this section, you apply the following Pod Security Standards to the latest version:baseline standard in enforce mode.restricted standard in warn and audit mode.The baseline Pod Security Standard provides a convenient middle ground that allows keeping the exemption list short and prevents known privilege escalations.Additionally, to prevent pods from failing in kube-system, you'll exempt the namespace from having Pod Security Standards applied.When you implement Pod Security Admission in your own environment, consider the following:Based on the risk posture applied to a cluster, a stricter Pod Security Standard like restricted might be a better choice.Exempting the kube-system namespace allows pods to run as privileged in this namespace. For real world use, the Kubernetes project strongly recommends that you apply strict RBAC policies that limit access to kube-system, following the principle of least privilege. To implement the preceding standards, do the following:Create a configuration file that can be consumed by the Pod Security Admission Controller to implement these Pod Security Standards:mkdir -p /tmp/pss cat < /tmp/pss/cluster-level-pss.yaml apiVersion: apiserver.config.k8s.io/v1 kind: AdmissionConfiguration plugins: \- name: PodSecurity configuration: apiVersion: pod-security.admission.config.k8s.io/v1 kind: PodSecurityConfiguration defaults: enforce: "baseline" enforce-version: "latest" audit: "restricted" audit-version: "latest" warn: "restricted" warn-version: "latest" exemptions: usernames: [] runtimeClasses: [] namespaces: [kube-system] EOF Note:pod-security.admission.config.k8s.io/v1 configuration requires v1.25+. For v1.23 and v1.24, use v1beta1. For v1.22, use v1alpha1.Configure the API server to consume this file during cluster creation:cat < /tmp/pss/cluster-config.yaml kind: Cluster apiVersion: kind.x-k8s.io/v1alpha4 nodes: \- role: control-plane kubeadmConfigPatches: \- | kind: ClusterConfiguration apiServer: extraArgs: admission-control-config-file: /etc/config/cluster-level-pss.yaml extraVolumes: \- name: accf hostPath: /etc/config mountPath: /etc/config readOnly: false pathType: "DirectoryOrCreate" extraMounts: \- hostPath: /tmp/pss containerPath: /etc/config # optional: if set, the mount is read-only. # default false readOnly: false # optional: if set, the mount needs SELinux relabeling. # default false selinuxRelabel: false # optional: set propagation mode (None, HostToContainer or Bidirectional) # see https://kubernetes.io/docs/concepts/storage/volumes/#mount-propagation # default None propagation: None EOF Note:If you use Docker Desktop with kind on macOS, you can add /tmp as a Shared Directory under the menu item Preferences > Resources > File Sharing.Create a cluster that uses Pod Security Admission to apply these Pod Security Standards:kind create cluster --name psa-with-cluster-pss --config /tmp/pss/cluster-config.yaml The output is similar to this:Creating cluster "psa-with-cluster-pss" ... ✓ Ensuring node image (kindest/node:v1.33.0) 🖼 ✓ Preparing nodes 📦 ✓ Writing configuration 📜 ✓ Starting control-plane 🕹️ ✓ Installing CNI 🔌 ✓ Installing StorageClass 💾 Set kubectl context to "kind-psa-with-cluster-pss" You can now use your cluster with: kubectl cluster-info --context kind-psa-with-cluster-pss Have a question, bug, or feature request? Let us know! https://kind.sigs.k8s.io/#community 🙂 Point kubectl to the cluster:kubectl cluster-info --context kind-psa-with-cluster-pss The output is similar to this:Kubernetes control plane is running at https://127.0.0.1:63855 CoreDNS is running at https://127.0.0.1:63855/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'. Create a Pod in the default namespace:security/example-baseline-pod.yaml apiVersion: v1 kind: Pod metadata: name: nginx spec: containers: \- image: nginx name: nginx ports: \- containerPort: 80 kubectl apply -f https://k8s.io/examples/security/example-baseline-pod.yaml The pod is started normally, but the output includes a warning:Warning: would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "nginx" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nginx" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nginx" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nginx" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost") pod/nginx created Clean upNow delete the clusters which you created above by running the following command:kind delete cluster --name psa-with-cluster-pss kind delete cluster --name psa-wo-cluster-pss What's nextRun a shell script to perform all the preceding steps at once:Create a Pod Security Standards based cluster level ConfigurationCreate a file to let API server consume this configurationCreate a cluster that creates an API server with this configurationSet kubectl context to this new clusterCreate a minimal pod yaml fileApply this file to create a Pod in the new clusterPod Security AdmissionPod Security StandardsApply Pod Security Standards at the namespace level5.4.2 - Apply Pod Security Standards at the Namespace LevelNoteThis tutorial applies only for new clusters.Pod Security Admission is an admission controller that applies Pod Security Standards when pods are created. It is a feature GA'ed in v1.25. In this tutorial, you will enforce the baseline Pod Security Standard, one namespace at a time.You can also apply Pod Security Standards to multiple namespaces at once at the cluster level. For instructions, refer to Apply Pod Security Standards at the cluster level.Before you beginInstall the following on your workstation:kindkubectlCreate clusterCreate a kind cluster as follows:kind create cluster --name psa-ns-level The output is similar to this:Creating cluster "psa-ns-level" ... ✓ Ensuring node image (kindest/node:v1.33.0) 🖼 ✓ Preparing nodes 📦 ✓ Writing configuration 📜 ✓ Starting control-plane 🕹️ ✓ Installing CNI 🔌 ✓ Installing StorageClass 💾 Set kubectl context to "kind-psa-ns-level" You can now use your cluster with: kubectl cluster-info --context kind-psa-ns-level Not sure what to do next? 😅 Check out https://kind.sigs.k8s.io/docs/user/quick-start/ Set the kubectl context to the new cluster:kubectl cluster-info --context kind-psa-ns-level The output is similar to this:Kubernetes control plane is running at https://127.0.0.1:50996 CoreDNS is running at https://127.0.0.1:50996/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'. Create a namespaceCreate a new namespace called example:kubectl create ns example The output is similar to this:namespace/example created Enable Pod Security Standards checking for that namespaceEnable Pod Security Standards on this namespace using labels supported by built-in Pod Security Admission. In this step you will configure a check to warn on Pods that don't meet the latest version of the baseline pod security standard.kubectl label --overwrite ns example \ pod-security.kubernetes.io/warn=baseline \ pod-security.kubernetes.io/warn-version=latest You can configure multiple pod security standard checks on any namespace, using labels. The following command will enforce the baseline Pod Security Standard, but warn and audit for restricted Pod Security Standards as per the latest version (default value)kubectl label --overwrite ns example \ pod-security.kubernetes.io/enforce=baseline \ pod-security.kubernetes.io/enforce-version=latest \ pod-security.kubernetes.io/warn=restricted \ pod-security.kubernetes.io/warn-version=latest \ pod-security.kubernetes.io/audit=restricted \ pod-security.kubernetes.io/audit-version=latest Verify the Pod Security Standard enforcementCreate a baseline Pod in the example namespace:kubectl apply -n example -f https://k8s.io/examples/security/example-baseline-pod.yaml The Pod does start OK; the output includes a warning. For example:Warning: would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "nginx" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nginx" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nginx" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nginx" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost") pod/nginx created Create a baseline Pod in the default namespace:kubectl apply -n default -f https://k8s.io/examples/security/example-baseline-pod.yaml Output is similar to this:pod/nginx created The Pod Security Standards enforcement and warning settings were applied only to the example namespace. You could create the same Pod in the default namespace with no warnings.Clean upNow delete the cluster which you created above by running the following command:kind delete cluster --name psa-ns-level What's nextRun a shell script to perform all the preceding steps all at once.Create kind clusterCreate new namespaceApply baseline Pod Security Standard in enforce mode while applying restricted Pod Security Standard also in warn and audit mode.Create a new pod with the following pod security standards appliedPod Security AdmissionPod Security StandardsApply Pod Security Standards at the cluster level5.4.3 - Restrict a Container's Access to Resources with AppArmorFEATURE STATE: Kubernetes v1.31 [stable] (enabled by default: true)This page shows you how to load AppArmor profiles on your nodes and enforce those profiles in Pods. To learn more about how Kubernetes can confine Pods using AppArmor, see Linux kernel security constraints for Pods and containers.ObjectivesSee an example of how to load a profile on a NodeLearn how to enforce the profile on a PodLearn how to check that the profile is loadedSee what happens when a profile is violatedSee what happens when a profile cannot be loadedBefore you beginAppArmor is an optional kernel module and Kubernetes feature, so verify it is supported on your Nodes before proceeding:AppArmor kernel module is enabled -- For the Linux kernel to enforce an AppArmor profile, the AppArmor kernel module must be installed and enabled. Several distributions enable the module by default, such as Ubuntu and SUSE, and many others provide optional support. To check whether the module is enabled, check the /sys/module/apparmor/parameters/enabled file:cat /sys/module/apparmor/parameters/enabled Y The kubelet verifies that AppArmor is enabled on the host before admitting a pod with AppArmor explicitly configured.Container runtime supports AppArmor -- All common Kubernetes-supported container runtimes should support AppArmor, including containerd and CRI-O. Please refer to the corresponding runtime documentation and verify that the cluster fulfills the requirements to use AppArmor.Profile is loaded -- AppArmor is applied to a Pod by specifying an AppArmor profile that each container should be run with. If any of the specified profiles are not loaded in the kernel, the kubelet will reject the Pod. You can view which profiles are loaded on a node by checking the /sys/kernel/security/apparmor/profiles file. For example:ssh gke-test-default-pool-239f5d02-gyn2 "sudo cat /sys/kernel/security/apparmor/profiles | sort" apparmor-test-deny-write (enforce) apparmor-test-audit-write (enforce) docker-default (enforce) k8s-nginx (enforce) For more details on loading profiles on nodes, see Setting up nodes with profiles.Securing a PodNote:Prior to Kubernetes v1.30, AppArmor was specified through annotations. Use the documentation version selector to view the documentation with this deprecated API.AppArmor profiles can be specified at the pod level or container level. The container AppArmor profile takes precedence over the pod profile.securityContext: appArmorProfile: type:  Where  is one of:RuntimeDefault to use the runtime's default profileLocalhost to use a profile loaded on the host (see below)Unconfined to run without AppArmorSee Specifying AppArmor Confinement for full details on the AppArmor profile API.To verify that the profile was applied, you can check that the container's root process is running with the correct profile by examining its proc attr:kubectl exec  \-- cat /proc/1/attr/current The output should look something like this:cri-containerd.apparmor.d (enforce) ExampleThis example assumes you have already set up a cluster with AppArmor support.First, load the profile you want to use onto your Nodes. This profile blocks all file write operations:#include  profile k8s-apparmor-example-deny-write flags=(attach_disconnected) { #include  file, # Deny all file writes. deny /** w, } The profile needs to be loaded onto all nodes, since you don't know where the pod will be scheduled. For this example you can use SSH to install the profiles, but other approaches are discussed in Setting up nodes with profiles.# This example assumes that node names match host names, and are reachable via SSH. NODES=($( kubectl get node -o jsonpath='{.items[*].status.addresses[?(.type == "Hostname")].address}' )) for NODE in ${NODES[*]}; do ssh $NODE 'sudo apparmor_parser -q < profile k8s-apparmor-example-deny-write flags=(attach_disconnected) { #include  file, # Deny all file writes. deny /** w, } EOF' done Next, run a simple "Hello AppArmor" Pod with the deny-write profile:pods/security/hello-apparmor.yaml apiVersion: v1 kind: Pod metadata: name: hello-apparmor spec: securityContext: appArmorProfile: type: Localhost localhostProfile: k8s-apparmor-example-deny-write containers: \- name: hello image: busybox:1.28 command: [ "sh", "-c", "echo 'Hello AppArmor!' && sleep 1h" ] kubectl create -f hello-apparmor.yaml You can verify that the container is actually running with that profile by checking /proc/1/attr/current:kubectl exec hello-apparmor -- cat /proc/1/attr/current The output should be:k8s-apparmor-example-deny-write (enforce) Finally, you can see what happens if you violate the profile by writing to a file:kubectl exec hello-apparmor -- touch /tmp/test touch: /tmp/test: Permission denied error: error executing remote command: command terminated with non-zero exit code: Error executing in Docker Container: 1 To wrap up, see what happens if you try to specify a profile that hasn't been loaded:kubectl create -f /dev/stdin < Annotations: container.apparmor.security.beta.kubernetes.io/hello=localhost/k8s-apparmor-example-allow-write Status: Pending ... Events: Type Reason Age From Message \---- ------ ---- ---- ------- Normal Scheduled 10s default-scheduler Successfully assigned default/hello-apparmor to gke-test-default-pool-239f5d02-x1kf Normal Pulled 8s kubelet Successfully pulled image "busybox:1.28" in 370.157088ms (370.172701ms including waiting) Normal Pulling 7s (x2 over 9s) kubelet Pulling image "busybox:1.28" Warning Failed 7s (x2 over 8s) kubelet Error: failed to get container spec opts: failed to generate apparmor spec opts: apparmor profile not found k8s-apparmor-example-allow-write Normal Pulled 7s kubelet Successfully pulled image "busybox:1.28" in 90.980331ms (91.005869ms including waiting) An Event provides the error message with the reason, the specific wording is runtime-dependent: Warning Failed 7s (x2 over 8s) kubelet Error: failed to get container spec opts: failed to generate apparmor spec opts: apparmor profile not found AdministrationSetting up Nodes with profilesKubernetes 1.33 does not provide any built-in mechanisms for loading AppArmor profiles onto Nodes. Profiles can be loaded through custom infrastructure or tools like the Kubernetes Security Profiles Operator.The scheduler is not aware of which profiles are loaded onto which Node, so the full set of profiles must be loaded onto every Node. An alternative approach is to add a Node label for each profile (or class of profiles) on the Node, and use a node selector to ensure the Pod is run on a Node with the required profile.Authoring ProfilesGetting AppArmor profiles specified correctly can be a tricky business. Fortunately there are some tools to help with that:aa-genprof and aa-logprof generate profile rules by monitoring an application's activity and logs, and admitting the actions it takes. Further instructions are provided by the AppArmor documentation.bane is an AppArmor profile generator for Docker that uses a simplified profile language.To debug problems with AppArmor, you can check the system logs to see what, specifically, was denied. AppArmor logs verbose messages to dmesg, and errors can usually be found in the system logs or through journalctl. More information is provided in AppArmor failures.Specifying AppArmor confinementCaution:Prior to Kubernetes v1.30, AppArmor was specified through annotations. Use the documentation version selector to view the documentation with this deprecated API.AppArmor profile within security contextYou can specify the appArmorProfile on either a container's securityContext or on a Pod's securityContext. If the profile is set at the pod level, it will be used as the default profile for all containers in the pod (including init, sidecar, and ephemeral containers). If both a pod & container AppArmor profile are set, the container's profile will be used.An AppArmor profile has 2 fields:type (required) - indicates which kind of AppArmor profile will be applied. Valid options are:Localhosta profile pre-loaded on the node (specified by localhostProfile).RuntimeDefaultthe container runtime's default profile.Unconfinedno AppArmor enforcement.localhostProfile - The name of a profile loaded on the node that should be used. The profile must be preconfigured on the node to work. This option must be provided if and only if the type is Localhost.What's nextAdditional resources:Quick guide to the AppArmor profile languageAppArmor core policy reference5.4.4 - Restrict a Container's Syscalls with seccompFEATURE STATE: Kubernetes v1.19 [stable]Seccomp stands for secure computing mode and has been a feature of the Linux kernel since version 2.6.12. It can be used to sandbox the privileges of a process, restricting the calls it is able to make from userspace into the kernel. Kubernetes lets you automatically apply seccomp profiles loaded onto a node to your Pods and containers.Identifying the privileges required for your workloads can be difficult. In this tutorial, you will go through how to load seccomp profiles into a local Kubernetes cluster, how to apply them to a Pod, and how you can begin to craft profiles that give only the necessary privileges to your container processes.ObjectivesLearn how to load seccomp profiles on a nodeLearn how to apply a seccomp profile to a containerObserve auditing of syscalls made by a container processObserve behavior when a missing profile is specifiedObserve a violation of a seccomp profileLearn how to create fine-grained seccomp profilesLearn how to apply a container runtime default seccomp profileBefore you beginIn order to complete all steps in this tutorial, you must install kind and kubectl.The commands used in the tutorial assume that you are using Docker as your container runtime. (The cluster that kind creates may use a different container runtime internally). You could also use Podman but in that case, you would have to follow specific instructions in order to complete the tasks successfully.This tutorial shows some examples that are still beta (since v1.25) and others that use only generally available seccomp functionality. You should make sure that your cluster is configured correctly for the version you are using.The tutorial also uses the curl tool for downloading examples to your computer. You can adapt the steps to use a different tool if you prefer.Note:It is not possible to apply a seccomp profile to a container running with privileged: true set in the container's securityContext. Privileged containers always run as Unconfined.Download example seccomp profilesThe contents of these profiles will be explored later on, but for now go ahead and download them into a directory named profiles/ so that they can be loaded into the cluster.audit.jsonviolation.jsonfine-grained.jsonpods/security/seccomp/profiles/audit.json { "defaultAction": "SCMP_ACT_LOG" }pods/security/seccomp/profiles/violation.json { "defaultAction": "SCMP_ACT_ERRNO" }pods/security/seccomp/profiles/fine-grained.json { "defaultAction": "SCMP_ACT_ERRNO", "architectures": [ "SCMP_ARCH_X86_64", "SCMP_ARCH_X86", "SCMP_ARCH_X32" ], "syscalls": [ { "names": [ "accept4", "epoll_wait", "pselect6", "futex", "madvise", "epoll_ctl", "getsockname", "setsockopt", "vfork", "mmap", "read", "write", "close", "arch_prctl", "sched_getaffinity", "munmap", "brk", "rt_sigaction", "rt_sigprocmask", "sigaltstack", "gettid", "clone", "bind", "socket", "openat", "readlinkat", "exit_group", "epoll_create1", "listen", "rt_sigreturn", "sched_yield", "clock_gettime", "connect", "dup2", "epoll_pwait", "execve", "exit", "fcntl", "getpid", "getuid", "ioctl", "mprotect", "nanosleep", "open", "poll", "recvfrom", "sendto", "set_tid_address", "setitimer", "writev", "fstatfs", "getdents64", "pipe2", "getrlimit" ], "action": "SCMP_ACT_ALLOW" } ] }Run these commands:mkdir ./profiles curl -L -o profiles/audit.json https://k8s.io/examples/pods/security/seccomp/profiles/audit.json curl -L -o profiles/violation.json https://k8s.io/examples/pods/security/seccomp/profiles/violation.json curl -L -o profiles/fine-grained.json https://k8s.io/examples/pods/security/seccomp/profiles/fine-grained.json ls profiles You should see three profiles listed at the end of the final step:audit.json fine-grained.json violation.json Create a local Kubernetes cluster with kindFor simplicity, kind can be used to create a single node cluster with the seccomp profiles loaded. Kind runs Kubernetes in Docker, so each node of the cluster is a container. This allows for files to be mounted in the filesystem of each container similar to loading files onto a node.pods/security/seccomp/kind.yaml apiVersion: kind.x-k8s.io/v1alpha4 kind: Cluster nodes: \- role: control-plane extraMounts: \- hostPath: "./profiles" containerPath: "/var/lib/kubelet/seccomp/profiles"Download that example kind configuration, and save it to a file named kind.yaml:curl -L -O https://k8s.io/examples/pods/security/seccomp/kind.yaml You can set a specific Kubernetes version by setting the node's container image. See Nodes within the kind documentation about configuration for more details on this. This tutorial assumes you are using Kubernetes v1.33.As a beta feature, you can configure Kubernetes to use the profile that the container runtime prefers by default, rather than falling back to Unconfined. If you want to try that, see enable the use of RuntimeDefault as the default seccomp profile for all workloads before you continue.Once you have a kind configuration in place, create the kind cluster with that configuration:kind create cluster --config=kind.yaml After the new Kubernetes cluster is ready, identify the Docker container running as the single node cluster:docker ps You should see output indicating that a container is running with name kind-control-plane. The output is similar to:CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 6a96207fed4b kindest/node:v1.18.2 "/usr/local/bin/entr…" 27 seconds ago Up 24 seconds 127.0.0.1:42223->6443/tcp kind-control-plane If observing the filesystem of that container, you should see that the profiles/ directory has been successfully loaded into the default seccomp path of the kubelet. Use docker exec to run a command in the Pod:# Change 6a96207fed4b to the container ID you saw from "docker ps" docker exec -it 6a96207fed4b ls /var/lib/kubelet/seccomp/profiles audit.json fine-grained.json violation.json You have verified that these seccomp profiles are available to the kubelet running within kind.Create a Pod that uses the container runtime default seccomp profileMost container runtimes provide a sane set of default syscalls that are allowed or not. You can adopt these defaults for your workload by setting the seccomp type in the security context of a pod or container to RuntimeDefault.Note:If you have the seccompDefault configuration enabled, then Pods use the RuntimeDefault seccomp profile whenever no other seccomp profile is specified. Otherwise, the default is Unconfined.Here's a manifest for a Pod that requests the RuntimeDefault seccomp profile for all its containers:pods/security/seccomp/ga/default-pod.yaml apiVersion: v1 kind: Pod metadata: name: default-pod labels: app: default-pod spec: securityContext: seccompProfile: type: RuntimeDefault containers: \- name: test-container image: hashicorp/http-echo:1.0 args: \- "-text=just made some more syscalls!" securityContext: allowPrivilegeEscalation: falseCreate that Pod:kubectl apply -f https://k8s.io/examples/pods/security/seccomp/ga/default-pod.yaml kubectl get pod default-pod The Pod should be showing as having started successfully:NAME READY STATUS RESTARTS AGE default-pod 1/1 Running 0 20s Delete the Pod before moving to the next section:kubectl delete pod default-pod --wait --now Create a Pod with a seccomp profile for syscall auditingTo start off, apply the audit.json profile, which will log all syscalls of the process, to a new Pod.Here's a manifest for that Pod:pods/security/seccomp/ga/audit-pod.yaml apiVersion: v1 kind: Pod metadata: name: audit-pod labels: app: audit-pod spec: securityContext: seccompProfile: type: Localhost localhostProfile: profiles/audit.json containers: \- name: test-container image: hashicorp/http-echo:1.0 args: \- "-text=just made some syscalls!" securityContext: allowPrivilegeEscalation: falseNote:Older versions of Kubernetes allowed you to configure seccomp behavior using annotations. Kubernetes 1.33 only supports using fields within .spec.securityContext to configure seccomp, and this tutorial explains that approach.Create the Pod in the cluster:kubectl apply -f https://k8s.io/examples/pods/security/seccomp/ga/audit-pod.yaml This profile does not restrict any syscalls, so the Pod should start successfully.kubectl get pod audit-pod NAME READY STATUS RESTARTS AGE audit-pod 1/1 Running 0 30s In order to be able to interact with this endpoint exposed by this container, create a NodePort Service that allows access to the endpoint from inside the kind control plane container.kubectl expose pod audit-pod --type NodePort --port 5678 Check what port the Service has been assigned on the node.kubectl get service audit-pod The output is similar to:NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE audit-pod NodePort 10.111.36.142  5678:32373/TCP 72s Now you can use curl to access that endpoint from inside the kind control plane container, at the port exposed by this Service. Use docker exec to run the curl command within the container belonging to that control plane container:# Change 6a96207fed4b to the control plane container ID and 32373 to the port number you saw from "docker ps" docker exec -it 6a96207fed4b curl localhost:32373 just made some syscalls! You can see that the process is running, but what syscalls did it actually make? Because this Pod is running in a local cluster, you should be able to see those in /var/log/syslog on your local system. Open up a new terminal window and tail the output for calls from http-echo:# The log path on your computer might be different from "/var/log/syslog" tail -f /var/log/syslog | grep 'http-echo' You should already see some logs of syscalls made by http-echo, and if you run curl again inside the control plane container you will see more output written to the log.For example:Jul 6 15:37:40 my-machine kernel: [369128.669452] audit: type=1326 audit(1594067860.484:14536): auid=4294967295 uid=0 gid=0 ses=4294967295 pid=29064 comm="http-echo" exe="/http-echo" sig=0 arch=c000003e syscall=51 compat=0 ip=0x46fe1f code=0x7ffc0000 Jul 6 15:37:40 my-machine kernel: [369128.669453] audit: type=1326 audit(1594067860.484:14537): auid=4294967295 uid=0 gid=0 ses=4294967295 pid=29064 comm="http-echo" exe="/http-echo" sig=0 arch=c000003e syscall=54 compat=0 ip=0x46fdba code=0x7ffc0000 Jul 6 15:37:40 my-machine kernel: [369128.669455] audit: type=1326 audit(1594067860.484:14538): auid=4294967295 uid=0 gid=0 ses=4294967295 pid=29064 comm="http-echo" exe="/http-echo" sig=0 arch=c000003e syscall=202 compat=0 ip=0x455e53 code=0x7ffc0000 Jul 6 15:37:40 my-machine kernel: [369128.669456] audit: type=1326 audit(1594067860.484:14539): auid=4294967295 uid=0 gid=0 ses=4294967295 pid=29064 comm="http-echo" exe="/http-echo" sig=0 arch=c000003e syscall=288 compat=0 ip=0x46fdba code=0x7ffc0000 Jul 6 15:37:40 my-machine kernel: [369128.669517] audit: type=1326 audit(1594067860.484:14540): auid=4294967295 uid=0 gid=0 ses=4294967295 pid=29064 comm="http-echo" exe="/http-echo" sig=0 arch=c000003e syscall=0 compat=0 ip=0x46fd44 code=0x7ffc0000 Jul 6 15:37:40 my-machine kernel: [369128.669519] audit: type=1326 audit(1594067860.484:14541): auid=4294967295 uid=0 gid=0 ses=4294967295 pid=29064 comm="http-echo" exe="/http-echo" sig=0 arch=c000003e syscall=270 compat=0 ip=0x4559b1 code=0x7ffc0000 Jul 6 15:38:40 my-machine kernel: [369188.671648] audit: type=1326 audit(1594067920.488:14559): auid=4294967295 uid=0 gid=0 ses=4294967295 pid=29064 comm="http-echo" exe="/http-echo" sig=0 arch=c000003e syscall=270 compat=0 ip=0x4559b1 code=0x7ffc0000 Jul 6 15:38:40 my-machine kernel: [369188.671726] audit: type=1326 audit(1594067920.488:14560): auid=4294967295 uid=0 gid=0 ses=4294967295 pid=29064 comm="http-echo" exe="/http-echo" sig=0 arch=c000003e syscall=202 compat=0 ip=0x455e53 code=0x7ffc0000 You can begin to understand the syscalls required by the http-echo process by looking at the syscall= entry on each line. While these are unlikely to encompass all syscalls it uses, it can serve as a basis for a seccomp profile for this container.Delete the Service and the Pod before moving to the next section:kubectl delete service audit-pod --wait kubectl delete pod audit-pod --wait --now Create a Pod with a seccomp profile that causes violationFor demonstration, apply a profile to the Pod that does not allow for any syscalls.The manifest for this demonstration is:pods/security/seccomp/ga/violation-pod.yaml apiVersion: v1 kind: Pod metadata: name: violation-pod labels: app: violation-pod spec: securityContext: seccompProfile: type: Localhost localhostProfile: profiles/violation.json containers: \- name: test-container image: hashicorp/http-echo:1.0 args: \- "-text=just made some syscalls!" securityContext: allowPrivilegeEscalation: falseAttempt to create the Pod in the cluster:kubectl apply -f https://k8s.io/examples/pods/security/seccomp/ga/violation-pod.yaml The Pod creates, but there is an issue. If you check the status of the Pod, you should see that it failed to start.kubectl get pod violation-pod NAME READY STATUS RESTARTS AGE violation-pod 0/1 CrashLoopBackOff 1 6s As seen in the previous example, the http-echo process requires quite a few syscalls. Here seccomp has been instructed to error on any syscall by setting "defaultAction": "SCMP_ACT_ERRNO". This is extremely secure, but removes the ability to do anything meaningful. What you really want is to give workloads only the privileges they need.Delete the Pod before moving to the next section:kubectl delete pod violation-pod --wait --now Create a Pod with a seccomp profile that only allows necessary syscallsIf you take a look at the fine-grained.json profile, you will notice some of the syscalls seen in syslog of the first example where the profile set "defaultAction": "SCMP_ACT_LOG". Now the profile is setting "defaultAction": "SCMP_ACT_ERRNO", but explicitly allowing a set of syscalls in the "action": "SCMP_ACT_ALLOW" block. Ideally, the container will run successfully and you will see no messages sent to syslog.The manifest for this example is:pods/security/seccomp/ga/fine-pod.yaml apiVersion: v1 kind: Pod metadata: name: fine-pod labels: app: fine-pod spec: securityContext: seccompProfile: type: Localhost localhostProfile: profiles/fine-grained.json containers: \- name: test-container image: hashicorp/http-echo:1.0 args: \- "-text=just made some syscalls!" securityContext: allowPrivilegeEscalation: falseCreate the Pod in your cluster:kubectl apply -f https://k8s.io/examples/pods/security/seccomp/ga/fine-pod.yaml kubectl get pod fine-pod The Pod should be showing as having started successfully:NAME READY STATUS RESTARTS AGE fine-pod 1/1 Running 0 30s Open up a new terminal window and use tail to monitor for log entries that mention calls from http-echo:# The log path on your computer might be different from "/var/log/syslog" tail -f /var/log/syslog | grep 'http-echo' Next, expose the Pod with a NodePort Service:kubectl expose pod fine-pod --type NodePort --port 5678 Check what port the Service has been assigned on the node:kubectl get service fine-pod The output is similar to:NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE fine-pod NodePort 10.111.36.142  5678:32373/TCP 72s Use curl to access that endpoint from inside the kind control plane container:# Change 6a96207fed4b to the control plane container ID and 32373 to the port number you saw from "docker ps" docker exec -it 6a96207fed4b curl localhost:32373 just made some syscalls! You should see no output in the syslog. This is because the profile allowed all necessary syscalls and specified that an error should occur if one outside of the list is invoked. This is an ideal situation from a security perspective, but required some effort in analyzing the program. It would be nice if there was a simple way to get closer to this security without requiring as much effort.Delete the Service and the Pod before moving to the next section:kubectl delete service fine-pod --wait kubectl delete pod fine-pod --wait --now Enable the use of RuntimeDefault as the default seccomp profile for all workloadsFEATURE STATE: Kubernetes v1.27 [stable]To use seccomp profile defaulting, you must run the kubelet with the \--seccomp-default command line flag enabled for each node where you want to use it.If enabled, the kubelet will use the RuntimeDefault seccomp profile by default, which is defined by the container runtime, instead of using the Unconfined (seccomp disabled) mode. The default profiles aim to provide a strong set of security defaults while preserving the functionality of the workload. It is possible that the default profiles differ between container runtimes and their release versions, for example when comparing those from CRI-O and containerd.Note:Enabling the feature will neither change the Kubernetes securityContext.seccompProfile API field nor add the deprecated annotations of the workload. This provides users the possibility to rollback anytime without actually changing the workload configuration. Tools like crictl inspect can be used to verify which seccomp profile is being used by a container.Some workloads may require a lower amount of syscall restrictions than others. This means that they can fail during runtime even with the RuntimeDefault profile. To mitigate such a failure, you can:Run the workload explicitly as Unconfined.Disable the SeccompDefault feature for the nodes. Also making sure that workloads get scheduled on nodes where the feature is disabled.Create a custom seccomp profile for the workload.If you were introducing this feature into production-like cluster, the Kubernetes project recommends that you enable this feature gate on a subset of your nodes and then test workload execution before rolling the change out cluster-wide.You can find more detailed information about a possible upgrade and downgrade strategy in the related Kubernetes Enhancement Proposal (KEP): Enable seccomp by default.Kubernetes 1.33 lets you configure the seccomp profile that applies when the spec for a Pod doesn't define a specific seccomp profile. However, you still need to enable this defaulting for each node where you would like to use it.If you are running a Kubernetes 1.33 cluster and want to enable the feature, either run the kubelet with the --seccomp-default command line flag, or enable it through the kubelet configuration file. To enable the feature gate in kind, ensure that kind provides the minimum required Kubernetes version and enables the SeccompDefault feature in the kind configuration:kind: Cluster apiVersion: kind.x-k8s.io/v1alpha4 nodes: \- role: control-plane image: kindest/node:v1.28.0@sha256:9f3ff58f19dcf1a0611d11e8ac989fdb30a28f40f236f59f0bea31fb956ccf5c kubeadmConfigPatches: \- | kind: JoinConfiguration nodeRegistration: kubeletExtraArgs: seccomp-default: "true" \- role: worker image: kindest/node:v1.28.0@sha256:9f3ff58f19dcf1a0611d11e8ac989fdb30a28f40f236f59f0bea31fb956ccf5c kubeadmConfigPatches: \- | kind: JoinConfiguration nodeRegistration: kubeletExtraArgs: seccomp-default: "true" If the cluster is ready, then running a pod:kubectl run --rm -it --restart=Never --image=alpine alpine -- sh Should now have the default seccomp profile attached. This can be verified by using docker exec to run crictl inspect for the container on the kind worker:docker exec -it kind-worker bash -c \ 'crictl inspect $(crictl ps --name=alpine -q) | jq .info.runtimeSpec.linux.seccomp' { "defaultAction": "SCMP_ACT_ERRNO", "architectures": ["SCMP_ARCH_X86_64", "SCMP_ARCH_X86", "SCMP_ARCH_X32"], "syscalls": [ { "names": ["..."] } ] } What's nextYou can learn more about Linux seccomp:A seccomp OverviewSeccomp Security Profiles for Docker5.5 - Stateless Applications5.5.1 - Exposing an External IP Address to Access an Application in a ClusterThis page shows how to create a Kubernetes Service object that exposes an external IP address.Before you beginInstall kubectl.Use a cloud provider like Google Kubernetes Engine or Amazon Web Services to create a Kubernetes cluster. This tutorial creates an external load balancer, which requires a cloud provider.Configure kubectl to communicate with your Kubernetes API server. For instructions, see the documentation for your cloud provider.ObjectivesRun five instances of a Hello World application.Create a Service object that exposes an external IP address.Use the Service object to access the running application.Creating a service for an application running in five podsRun a Hello World application in your cluster:service/load-balancer-example.yaml apiVersion: apps/v1 kind: Deployment metadata: labels: app.kubernetes.io/name: load-balancer-example name: hello-world spec: replicas: 5 selector: matchLabels: app.kubernetes.io/name: load-balancer-example template: metadata: labels: app.kubernetes.io/name: load-balancer-example spec: containers: \- image: gcr.io/google-samples/hello-app:2.0 name: hello-world ports: \- containerPort: 8080 kubectl apply -f https://k8s.io/examples/service/load-balancer-example.yaml The preceding command creates a Deployment and an associated ReplicaSet. The ReplicaSet has five Pods each of which runs the Hello World application.Display information about the Deployment:kubectl get deployments hello-world kubectl describe deployments hello-world Display information about your ReplicaSet objects:kubectl get replicasets kubectl describe replicasets Create a Service object that exposes the deployment:kubectl expose deployment hello-world --type=LoadBalancer --name=my-service Display information about the Service:kubectl get services my-service The output is similar to:NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE my-service LoadBalancer 10.3.245.137 104.198.205.71 8080/TCP 54s Note:The type=LoadBalancer service is backed by external cloud providers, which is not covered in this example, please refer to this page for the details.Note:If the external IP address is shown as , wait for a minute and enter the same command again.Display detailed information about the Service:kubectl describe services my-service The output is similar to:Name: my-service Namespace: default Labels: app.kubernetes.io/name=load-balancer-example Annotations:  Selector: app.kubernetes.io/name=load-balancer-example Type: LoadBalancer IP: 10.3.245.137 LoadBalancer Ingress: 104.198.205.71 Port:  8080/TCP NodePort:  32377/TCP Endpoints: 10.0.0.6:8080,10.0.1.6:8080,10.0.1.7:8080 + 2 more... Session Affinity: None Events:  Make a note of the external IP address (LoadBalancer Ingress) exposed by your service. In this example, the external IP address is 104.198.205.71. Also note the value of Port and NodePort. In this example, the Port is 8080 and the NodePort is 32377.In the preceding output, you can see that the service has several endpoints: 10.0.0.6:8080,10.0.1.6:8080,10.0.1.7:8080 + 2 more. These are internal addresses of the pods that are running the Hello World application. To verify these are pod addresses, enter this command:kubectl get pods --output=wide The output is similar to:NAME ... IP NODE hello-world-2895499144-1jaz9 ... 10.0.1.6 gke-cluster-1-default-pool-e0b8d269-1afc hello-world-2895499144-2e5uh ... 10.0.1.8 gke-cluster-1-default-pool-e0b8d269-1afc hello-world-2895499144-9m4h1 ... 10.0.0.6 gke-cluster-1-default-pool-e0b8d269-5v7a hello-world-2895499144-o4z13 ... 10.0.1.7 gke-cluster-1-default-pool-e0b8d269-1afc hello-world-2895499144-segjf ... 10.0.2.5 gke-cluster-1-default-pool-e0b8d269-cpuc Use the external IP address (LoadBalancer Ingress) to access the Hello World application:curl http://: where  is the external IP address (LoadBalancer Ingress) of your Service, and  is the value of Port in your Service description. If you are using minikube, typing minikube service my-service will automatically open the Hello World application in a browser.The response to a successful request is a hello message:Hello, world! Version: 2.0.0 Hostname: 0bd46b45f32f Cleaning upTo delete the Service, enter this command:kubectl delete services my-service To delete the Deployment, the ReplicaSet, and the Pods that are running the Hello World application, enter this command:kubectl delete deployment hello-world What's nextLearn more about connecting applications with services.5.5.2 - Example: Deploying PHP Guestbook application with RedisThis tutorial shows you how to build and deploy a simple (not production ready), multi-tier web application using Kubernetes and Docker. This example consists of the following components:A single-instance Redis to store guestbook entriesMultiple web frontend instancesObjectivesStart up a Redis leader.Start up two Redis followers.Start up the guestbook frontend.Expose and view the Frontend Service.Clean up.Before you beginYou need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesYour Kubernetes server must be at or later than version v1.14.To check the version, enter kubectl version.Start up the Redis DatabaseThe guestbook application uses Redis to store its data.Creating the Redis DeploymentThe manifest file, included below, specifies a Deployment controller that runs a single replica Redis Pod.application/guestbook/redis-leader-deployment.yaml # SOURCE: https://cloud.google.com/kubernetes-engine/docs/tutorials/guestbook apiVersion: apps/v1 kind: Deployment metadata: name: redis-leader labels: app: redis role: leader tier: backend spec: replicas: 1 selector: matchLabels: app: redis template: metadata: labels: app: redis role: leader tier: backend spec: containers: \- name: leader image: "docker.io/redis:6.0.5" resources: requests: cpu: 100m memory: 100Mi ports: \- containerPort: 6379Launch a terminal window in the directory you downloaded the manifest files.Apply the Redis Deployment from the redis-leader-deployment.yaml file:kubectl apply -f https://k8s.io/examples/application/guestbook/redis-leader-deployment.yaml Query the list of Pods to verify that the Redis Pod is running:kubectl get pods The response should be similar to this:NAME READY STATUS RESTARTS AGE redis-leader-fb76b4755-xjr2n 1/1 Running 0 13s Run the following command to view the logs from the Redis leader Pod:kubectl logs -f deployment/redis-leader Creating the Redis leader ServiceThe guestbook application needs to communicate to the Redis to write its data. You need to apply a Service to proxy the traffic to the Redis Pod. A Service defines a policy to access the Pods.application/guestbook/redis-leader-service.yaml # SOURCE: https://cloud.google.com/kubernetes-engine/docs/tutorials/guestbook apiVersion: v1 kind: Service metadata: name: redis-leader labels: app: redis role: leader tier: backend spec: ports: \- port: 6379 targetPort: 6379 selector: app: redis role: leader tier: backendApply the Redis Service from the following redis-leader-service.yaml file:kubectl apply -f https://k8s.io/examples/application/guestbook/redis-leader-service.yaml Query the list of Services to verify that the Redis Service is running:kubectl get service The response should be similar to this:NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.0.0.1  443/TCP 1m redis-leader ClusterIP 10.103.78.24  6379/TCP 16s Note:This manifest file creates a Service named redis-leader with a set of labels that match the labels previously defined, so the Service routes network traffic to the Redis Pod.Set up Redis followersAlthough the Redis leader is a single Pod, you can make it highly available and meet traffic demands by adding a few Redis followers, or replicas.application/guestbook/redis-follower-deployment.yaml # SOURCE: https://cloud.google.com/kubernetes-engine/docs/tutorials/guestbook apiVersion: apps/v1 kind: Deployment metadata: name: redis-follower labels: app: redis role: follower tier: backend spec: replicas: 2 selector: matchLabels: app: redis template: metadata: labels: app: redis role: follower tier: backend spec: containers: \- name: follower image: us-docker.pkg.dev/google-samples/containers/gke/gb-redis-follower:v2 resources: requests: cpu: 100m memory: 100Mi ports: \- containerPort: 6379Apply the Redis Deployment from the following redis-follower-deployment.yaml file:kubectl apply -f https://k8s.io/examples/application/guestbook/redis-follower-deployment.yaml Verify that the two Redis follower replicas are running by querying the list of Pods:kubectl get pods The response should be similar to this:NAME READY STATUS RESTARTS AGE redis-follower-dddfbdcc9-82sfr 1/1 Running 0 37s redis-follower-dddfbdcc9-qrt5k 1/1 Running 0 38s redis-leader-fb76b4755-xjr2n 1/1 Running 0 11m Creating the Redis follower serviceThe guestbook application needs to communicate with the Redis followers to read data. To make the Redis followers discoverable, you must set up another Service.application/guestbook/redis-follower-service.yaml # SOURCE: https://cloud.google.com/kubernetes-engine/docs/tutorials/guestbook apiVersion: v1 kind: Service metadata: name: redis-follower labels: app: redis role: follower tier: backend spec: ports: # the port that this service should serve on \- port: 6379 selector: app: redis role: follower tier: backendApply the Redis Service from the following redis-follower-service.yaml file:kubectl apply -f https://k8s.io/examples/application/guestbook/redis-follower-service.yaml Query the list of Services to verify that the Redis Service is running:kubectl get service The response should be similar to this:NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1  443/TCP 3d19h redis-follower ClusterIP 10.110.162.42  6379/TCP 9s redis-leader ClusterIP 10.103.78.24  6379/TCP 6m10s Note:This manifest file creates a Service named redis-follower with a set of labels that match the labels previously defined, so the Service routes network traffic to the Redis Pod.Set up and Expose the Guestbook FrontendNow that you have the Redis storage of your guestbook up and running, start the guestbook web servers. Like the Redis followers, the frontend is deployed using a Kubernetes Deployment.The guestbook app uses a PHP frontend. It is configured to communicate with either the Redis follower or leader Services, depending on whether the request is a read or a write. The frontend exposes a JSON interface, and serves a jQuery-Ajax-based UX.Creating the Guestbook Frontend Deploymentapplication/guestbook/frontend-deployment.yaml # SOURCE: https://cloud.google.com/kubernetes-engine/docs/tutorials/guestbook apiVersion: apps/v1 kind: Deployment metadata: name: frontend spec: replicas: 3 selector: matchLabels: app: guestbook tier: frontend template: metadata: labels: app: guestbook tier: frontend spec: containers: \- name: php-redis image: us-docker.pkg.dev/google-samples/containers/gke/gb-frontend:v5 env: \- name: GET_HOSTS_FROM value: "dns" resources: requests: cpu: 100m memory: 100Mi ports: \- containerPort: 80 Apply the frontend Deployment from the frontend-deployment.yaml file:kubectl apply -f https://k8s.io/examples/application/guestbook/frontend-deployment.yaml Query the list of Pods to verify that the three frontend replicas are running:kubectl get pods -l app=guestbook -l tier=frontend The response should be similar to this:NAME READY STATUS RESTARTS AGE frontend-85595f5bf9-5tqhb 1/1 Running 0 47s frontend-85595f5bf9-qbzwm 1/1 Running 0 47s frontend-85595f5bf9-zchwc 1/1 Running 0 47s Creating the Frontend ServiceThe Redis Services you applied is only accessible within the Kubernetes cluster because the default type for a Service is ClusterIP. ClusterIP provides a single IP address for the set of Pods the Service is pointing to. This IP address is accessible only within the cluster.If you want guests to be able to access your guestbook, you must configure the frontend Service to be externally visible, so a client can request the Service from outside the Kubernetes cluster. However a Kubernetes user can use kubectl port-forward to access the service even though it uses a ClusterIP.Note:Some cloud providers, like Google Compute Engine or Google Kubernetes Engine, support external load balancers. If your cloud provider supports load balancers and you want to use it, uncomment type: LoadBalancer.application/guestbook/frontend-service.yaml # SOURCE: https://cloud.google.com/kubernetes-engine/docs/tutorials/guestbook apiVersion: v1 kind: Service metadata: name: frontend labels: app: guestbook tier: frontend spec: # if your cluster supports it, uncomment the following to automatically create # an external load-balanced IP for the frontend service. # type: LoadBalancer #type: LoadBalancer ports: # the port that this service should serve on \- port: 80 selector: app: guestbook tier: frontendApply the frontend Service from the frontend-service.yaml file:kubectl apply -f https://k8s.io/examples/application/guestbook/frontend-service.yaml Query the list of Services to verify that the frontend Service is running:kubectl get services The response should be similar to this:NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE frontend ClusterIP 10.97.28.230  80/TCP 19s kubernetes ClusterIP 10.96.0.1  443/TCP 3d19h redis-follower ClusterIP 10.110.162.42  6379/TCP 5m48s redis-leader ClusterIP 10.103.78.24  6379/TCP 11m Viewing the Frontend Service via kubectl port-forwardRun the following command to forward port 8080 on your local machine to port 80 on the service.kubectl port-forward svc/frontend 8080:80 The response should be similar to this:Forwarding from 127.0.0.1:8080 -> 80 Forwarding from [::1]:8080 -> 80 load the page http://localhost:8080 in your browser to view your guestbook.Viewing the Frontend Service via LoadBalancerIf you deployed the frontend-service.yaml manifest with type: LoadBalancer you need to find the IP address to view your Guestbook.Run the following command to get the IP address for the frontend Service.kubectl get service frontend The response should be similar to this:NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE frontend LoadBalancer 10.51.242.136 109.197.92.229 80:32372/TCP 1m Copy the external IP address, and load the page in your browser to view your guestbook.Note:Try adding some guestbook entries by typing in a message, and clicking Submit. The message you typed appears in the frontend. This message indicates that data is successfully added to Redis through the Services you created earlier.Scale the Web FrontendYou can scale up or down as needed because your servers are defined as a Service that uses a Deployment controller.Run the following command to scale up the number of frontend Pods:kubectl scale deployment frontend --replicas=5 Query the list of Pods to verify the number of frontend Pods running:kubectl get pods The response should look similar to this:NAME READY STATUS RESTARTS AGE frontend-85595f5bf9-5df5m 1/1 Running 0 83s frontend-85595f5bf9-7zmg5 1/1 Running 0 83s frontend-85595f5bf9-cpskg 1/1 Running 0 15m frontend-85595f5bf9-l2l54 1/1 Running 0 14m frontend-85595f5bf9-l9c8z 1/1 Running 0 14m redis-follower-dddfbdcc9-82sfr 1/1 Running 0 97m redis-follower-dddfbdcc9-qrt5k 1/1 Running 0 97m redis-leader-fb76b4755-xjr2n 1/1 Running 0 108m Run the following command to scale down the number of frontend Pods:kubectl scale deployment frontend --replicas=2 Query the list of Pods to verify the number of frontend Pods running:kubectl get pods The response should look similar to this:NAME READY STATUS RESTARTS AGE frontend-85595f5bf9-cpskg 1/1 Running 0 16m frontend-85595f5bf9-l9c8z 1/1 Running 0 15m redis-follower-dddfbdcc9-82sfr 1/1 Running 0 98m redis-follower-dddfbdcc9-qrt5k 1/1 Running 0 98m redis-leader-fb76b4755-xjr2n 1/1 Running 0 109m Cleaning upDeleting the Deployments and Services also deletes any running Pods. Use labels to delete multiple resources with one command.Run the following commands to delete all Pods, Deployments, and Services.kubectl delete deployment -l app=redis kubectl delete service -l app=redis kubectl delete deployment frontend kubectl delete service frontend The response should look similar to this:deployment.apps "redis-follower" deleted deployment.apps "redis-leader" deleted deployment.apps "frontend" deleted service "frontend" deleted Query the list of Pods to verify that no Pods are running:kubectl get pods The response should look similar to this:No resources found in default namespace. What's nextComplete the Kubernetes Basics Interactive TutorialsUse Kubernetes to create a blog using Persistent Volumes for MySQL and WordpressRead more about connecting applications with servicesRead more about using labels effectively5.6 - Stateful Applications5.6.1 - StatefulSet BasicsThis tutorial provides an introduction to managing applications with StatefulSets. It demonstrates how to create, delete, scale, and update the Pods of StatefulSets.Before you beginBefore you begin this tutorial, you should familiarize yourself with the following Kubernetes concepts:PodsCluster DNSHeadless ServicesPersistentVolumesPersistentVolume ProvisioningThe kubectl command line toolYou need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesYou should configure kubectl to use a context that uses the default namespace. If you are using an existing cluster, make sure that it's OK to use that cluster's default namespace to practice. Ideally, practice in a cluster that doesn't run any real workloads.It's also useful to read the concept page about StatefulSets.Note:This tutorial assumes that your cluster is configured to dynamically provision PersistentVolumes. You'll also need to have a default StorageClass. If your cluster is not configured to provision storage dynamically, you will have to manually provision two 1 GiB volumes prior to starting this tutorial and set up your cluster so that those PersistentVolumes map to the PersistentVolumeClaim templates that the StatefulSet defines.ObjectivesStatefulSets are intended to be used with stateful applications and distributed systems. However, the administration of stateful applications and distributed systems on Kubernetes is a broad, complex topic. In order to demonstrate the basic features of a StatefulSet, and not to conflate the former topic with the latter, you will deploy a simple web application using a StatefulSet.After this tutorial, you will be familiar with the following.How to create a StatefulSetHow a StatefulSet manages its PodsHow to delete a StatefulSetHow to scale a StatefulSetHow to update a StatefulSet's PodsCreating a StatefulSetBegin by creating a StatefulSet (and the Service that it relies upon) using the example below. It is similar to the example presented in the StatefulSets concept. It creates a headless Service, nginx, to publish the IP addresses of Pods in the StatefulSet, web.application/web/web.yaml apiVersion: v1 kind: Service metadata: name: nginx labels: app: nginx spec: ports: \- port: 80 name: web clusterIP: None selector: app: nginx \--- apiVersion: apps/v1 kind: StatefulSet metadata: name: web spec: serviceName: "nginx" replicas: 2 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: \- name: nginx image: registry.k8s.io/nginx-slim:0.21 ports: \- containerPort: 80 name: web volumeMounts: \- name: www mountPath: /usr/share/nginx/html volumeClaimTemplates: \- metadata: name: www spec: accessModes: [ "ReadWriteOnce" ] resources: requests: storage: 1Gi You will need to use at least two terminal windows. In the first terminal, use kubectl get to watch the creation of the StatefulSet's Pods.# use this terminal to run commands that specify --watch # end this watch when you are asked to start a new watch kubectl get pods --watch -l app=nginx In the second terminal, use kubectl apply to create the headless Service and StatefulSet:kubectl apply -f https://k8s.io/examples/application/web/web.yaml service/nginx created statefulset.apps/web created The command above creates two Pods, each running an NGINX webserver. Get the nginx Service...kubectl get service nginx NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE nginx ClusterIP None  80/TCP 12s ...then get the web StatefulSet, to verify that both were created successfully:kubectl get statefulset web NAME READY AGE web 2/2 37s Ordered Pod creationA StatefulSet defaults to creating its Pods in a strict order.For a StatefulSet with n replicas, when Pods are being deployed, they are created sequentially, ordered from {0..n-1}. Examine the output of the kubectl get command in the first terminal. Eventually, the output will look like the example below.# Do not start a new watch; # this should already be running kubectl get pods --watch -l app=nginx NAME READY STATUS RESTARTS AGE web-0 0/1 Pending 0 0s web-0 0/1 Pending 0 0s web-0 0/1 ContainerCreating 0 0s web-0 1/1 Running 0 19s web-1 0/1 Pending 0 0s web-1 0/1 Pending 0 0s web-1 0/1 ContainerCreating 0 0s web-1 1/1 Running 0 18s Notice that the web-1 Pod is not launched until the web-0 Pod is Running (see Pod Phase) and Ready (see type in Pod Conditions).Later in this tutorial you will practice parallel startup.Note:To configure the integer ordinal assigned to each Pod in a StatefulSet, see Start ordinal.Pods in a StatefulSetPods in a StatefulSet have a unique ordinal index and a stable network identity.Examining the Pod's ordinal indexGet the StatefulSet's Pods:kubectl get pods -l app=nginx NAME READY STATUS RESTARTS AGE web-0 1/1 Running 0 1m web-1 1/1 Running 0 1m As mentioned in the StatefulSets concept, the Pods in a StatefulSet have a sticky, unique identity. This identity is based on a unique ordinal index that is assigned to each Pod by the StatefulSet controller.The Pods' names take the form -. Since the web StatefulSet has two replicas, it creates two Pods, web-0 and web-1.Using stable network identitiesEach Pod has a stable hostname based on its ordinal index. Use kubectl exec to execute the hostname command in each Pod:for i in 0 1; do kubectl exec "web-$i" -- sh -c 'hostname'; done web-0 web-1 Use kubectl run to execute a container that provides the nslookup command from the dnsutils package. Using nslookup on the Pods' hostnames, you can examine their in-cluster DNS addresses:kubectl run -i --tty --image busybox:1.28 dns-test --restart=Never --rm which starts a new shell. In that new shell, run:# Run this in the dns-test container shell nslookup web-0.nginx The output is similar to:Server: 10.0.0.10 Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local Name: web-0.nginx Address 1: 10.244.1.6 nslookup web-1.nginx Server: 10.0.0.10 Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local Name: web-1.nginx Address 1: 10.244.2.6 (and now exit the container shell: exit)The CNAME of the headless service points to SRV records (one for each Pod that is Running and Ready). The SRV records point to A record entries that contain the Pods' IP addresses.In one terminal, watch the StatefulSet's Pods:# Start a new watch # End this watch when you've seen that the delete is finished kubectl get pod --watch -l app=nginx In a second terminal, use kubectl delete to delete all the Pods in the StatefulSet:kubectl delete pod -l app=nginx pod "web-0" deleted pod "web-1" deleted Wait for the StatefulSet to restart them, and for both Pods to transition to Running and Ready:# This should already be running kubectl get pod --watch -l app=nginx NAME READY STATUS RESTARTS AGE web-0 0/1 ContainerCreating 0 0s NAME READY STATUS RESTARTS AGE web-0 1/1 Running 0 2s web-1 0/1 Pending 0 0s web-1 0/1 Pending 0 0s web-1 0/1 ContainerCreating 0 0s web-1 1/1 Running 0 34s Use kubectl exec and kubectl run to view the Pods' hostnames and in-cluster DNS entries. First, view the Pods' hostnames:for i in 0 1; do kubectl exec web-$i -- sh -c 'hostname'; done web-0 web-1 then, run:kubectl run -i --tty --image busybox:1.28 dns-test --restart=Never --rm which starts a new shell.In that new shell, run:# Run this in the dns-test container shell nslookup web-0.nginx The output is similar to:Server: 10.0.0.10 Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local Name: web-0.nginx Address 1: 10.244.1.7 nslookup web-1.nginx Server: 10.0.0.10 Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local Name: web-1.nginx Address 1: 10.244.2.8 (and now exit the container shell: exit)The Pods' ordinals, hostnames, SRV records, and A record names have not changed, but the IP addresses associated with the Pods may have changed. In the cluster used for this tutorial, they have. This is why it is important not to configure other applications to connect to Pods in a StatefulSet by the IP address of a particular Pod (it is OK to connect to Pods by resolving their hostname).Discovery for specific Pods in a StatefulSetIf you need to find and connect to the active members of a StatefulSet, you should query the CNAME of the headless Service (nginx.default.svc.cluster.local). The SRV records associated with the CNAME will contain only the Pods in the StatefulSet that are Running and Ready.If your application already implements connection logic that tests for liveness and readiness, you can use the SRV records of the Pods ( web-0.nginx.default.svc.cluster.local, web-1.nginx.default.svc.cluster.local), as they are stable, and your application will be able to discover the Pods' addresses when they transition to Running and Ready.If your application wants to find any healthy Pod in a StatefulSet, and therefore does not need to track each specific Pod, you could also connect to the IP address of a type: ClusterIP Service, backed by the Pods in that StatefulSet. You can use the same Service that tracks the StatefulSet (specified in the serviceName of the StatefulSet) or a separate Service that selects the right set of Pods.Writing to stable storageGet the PersistentVolumeClaims for web-0 and web-1:kubectl get pvc -l app=nginx The output is similar to:NAME STATUS VOLUME CAPACITY ACCESSMODES AGE www-web-0 Bound pvc-15c268c7-b507-11e6-932f-42010a800002 1Gi RWO 48s www-web-1 Bound pvc-15c79307-b507-11e6-932f-42010a800002 1Gi RWO 48s The StatefulSet controller created two PersistentVolumeClaims that are bound to two PersistentVolumes.As the cluster used in this tutorial is configured to dynamically provision PersistentVolumes, the PersistentVolumes were created and bound automatically.The NGINX webserver, by default, serves an index file from /usr/share/nginx/html/index.html. The volumeMounts field in the StatefulSet's spec ensures that the /usr/share/nginx/html directory is backed by a PersistentVolume.Write the Pods' hostnames to their index.html files and verify that the NGINX webservers serve the hostnames:for i in 0 1; do kubectl exec "web-$i" -- sh -c 'echo "$(hostname)" > /usr/share/nginx/html/index.html'; done for i in 0 1; do kubectl exec -i -t "web-$i" -- curl http://localhost/; done web-0 web-1 Note:If you instead see 403 Forbidden responses for the above curl command, you will need to fix the permissions of the directory mounted by the volumeMounts (due to a bug when using hostPath volumes), by running:for i in 0 1; do kubectl exec web-$i -- chmod 755 /usr/share/nginx/html; donebefore retrying the curl command above.In one terminal, watch the StatefulSet's Pods:# End this watch when you've reached the end of the section. # At the start of "Scaling a StatefulSet" you'll start a new watch. kubectl get pod --watch -l app=nginx In a second terminal, delete all of the StatefulSet's Pods:kubectl delete pod -l app=nginx pod "web-0" deleted pod "web-1" deleted Examine the output of the kubectl get command in the first terminal, and wait for all of the Pods to transition to Running and Ready.# This should already be running kubectl get pod --watch -l app=nginx NAME READY STATUS RESTARTS AGE web-0 0/1 ContainerCreating 0 0s NAME READY STATUS RESTARTS AGE web-0 1/1 Running 0 2s web-1 0/1 Pending 0 0s web-1 0/1 Pending 0 0s web-1 0/1 ContainerCreating 0 0s web-1 1/1 Running 0 34s Verify the web servers continue to serve their hostnames:for i in 0 1; do kubectl exec -i -t "web-$i" -- curl http://localhost/; done web-0 web-1 Even though web-0 and web-1 were rescheduled, they continue to serve their hostnames because the PersistentVolumes associated with their PersistentVolumeClaims are remounted to their volumeMounts. No matter what node web-0and web-1 are scheduled on, their PersistentVolumes will be mounted to the appropriate mount points.Scaling a StatefulSetScaling a StatefulSet refers to increasing or decreasing the number of replicas (horizontal scaling). This is accomplished by updating the replicas field. You can use either kubectl scale or kubectl patch to scale a StatefulSet.Scaling upScaling up means adding more replicas. Provided that your app is able to distribute work across the StatefulSet, the new larger set of Pods can perform more of that work.In one terminal window, watch the Pods in the StatefulSet:# If you already have a watch running, you can continue using that. # Otherwise, start one. # End this watch when there are 5 healthy Pods for the StatefulSet kubectl get pods --watch -l app=nginx In another terminal window, use kubectl scale to scale the number of replicas to 5:kubectl scale sts web --replicas=5 statefulset.apps/web scaled Examine the output of the kubectl get command in the first terminal, and wait for the three additional Pods to transition to Running and Ready.# This should already be running kubectl get pod --watch -l app=nginx NAME READY STATUS RESTARTS AGE web-0 1/1 Running 0 2h web-1 1/1 Running 0 2h NAME READY STATUS RESTARTS AGE web-2 0/1 Pending 0 0s web-2 0/1 Pending 0 0s web-2 0/1 ContainerCreating 0 0s web-2 1/1 Running 0 19s web-3 0/1 Pending 0 0s web-3 0/1 Pending 0 0s web-3 0/1 ContainerCreating 0 0s web-3 1/1 Running 0 18s web-4 0/1 Pending 0 0s web-4 0/1 Pending 0 0s web-4 0/1 ContainerCreating 0 0s web-4 1/1 Running 0 19s The StatefulSet controller scaled the number of replicas. As with StatefulSet creation, the StatefulSet controller created each Pod sequentially with respect to its ordinal index, and it waited for each Pod's predecessor to be Running and Ready before launching the subsequent Pod.Scaling downScaling down means reducing the number of replicas. For example, you might do this because the level of traffic to a service has decreased, and at the current scale there are idle resources.In one terminal, watch the StatefulSet's Pods:# End this watch when there are only 3 Pods for the StatefulSet kubectl get pod --watch -l app=nginx In another terminal, use kubectl patch to scale the StatefulSet back down to three replicas:kubectl patch sts web -p '{"spec":{"replicas":3}}' statefulset.apps/web patched Wait for web-4 and web-3 to transition to Terminating.# This should already be running kubectl get pods --watch -l app=nginx NAME READY STATUS RESTARTS AGE web-0 1/1 Running 0 3h web-1 1/1 Running 0 3h web-2 1/1 Running 0 55s web-3 1/1 Running 0 36s web-4 0/1 ContainerCreating 0 18s NAME READY STATUS RESTARTS AGE web-4 1/1 Running 0 19s web-4 1/1 Terminating 0 24s web-4 1/1 Terminating 0 24s web-3 1/1 Terminating 0 42s web-3 1/1 Terminating 0 42s Ordered Pod terminationThe control plane deleted one Pod at a time, in reverse order with respect to its ordinal index, and it waited for each Pod to be completely shut down before deleting the next one.Get the StatefulSet's PersistentVolumeClaims:kubectl get pvc -l app=nginx NAME STATUS VOLUME CAPACITY ACCESSMODES AGE www-web-0 Bound pvc-15c268c7-b507-11e6-932f-42010a800002 1Gi RWO 13h www-web-1 Bound pvc-15c79307-b507-11e6-932f-42010a800002 1Gi RWO 13h www-web-2 Bound pvc-e1125b27-b508-11e6-932f-42010a800002 1Gi RWO 13h www-web-3 Bound pvc-e1176df6-b508-11e6-932f-42010a800002 1Gi RWO 13h www-web-4 Bound pvc-e11bb5f8-b508-11e6-932f-42010a800002 1Gi RWO 13h There are still five PersistentVolumeClaims and five PersistentVolumes. When exploring a Pod's stable storage, you saw that the PersistentVolumes mounted to the Pods of a StatefulSet are not deleted when the StatefulSet's Pods are deleted. This is still true when Pod deletion is caused by scaling the StatefulSet down.Updating StatefulSetsThe StatefulSet controller supports automated updates. The strategy used is determined by the spec.updateStrategy field of the StatefulSet API object. This feature can be used to upgrade the container images, resource requests and/or limits, labels, and annotations of the Pods in a StatefulSet.There are two valid update strategies, RollingUpdate (the default) and OnDelete.RollingUpdateThe RollingUpdate update strategy will update all Pods in a StatefulSet, in reverse ordinal order, while respecting the StatefulSet guarantees.You can split updates to a StatefulSet that uses the RollingUpdate strategy into partitions, by specifying .spec.updateStrategy.rollingUpdate.partition. You'll practice that later in this tutorial.First, try a simple rolling update.In one terminal window, patch the web StatefulSet to change the container image again:kubectl patch statefulset web --type='json' -p='[{"op": "replace", "path": "/spec/template/spec/containers/0/image", "value":"registry.k8s.io/nginx-slim:0.24"}]' statefulset.apps/web patched In another terminal, watch the Pods in the StatefulSet:# End this watch when the rollout is complete # # If you're not sure, leave it running one more minute kubectl get pod -l app=nginx --watch The output is similar to:NAME READY STATUS RESTARTS AGE web-0 1/1 Running 0 7m web-1 1/1 Running 0 7m web-2 1/1 Running 0 8m web-2 1/1 Terminating 0 8m web-2 1/1 Terminating 0 8m web-2 0/1 Terminating 0 8m web-2 0/1 Terminating 0 8m web-2 0/1 Terminating 0 8m web-2 0/1 Terminating 0 8m web-2 0/1 Pending 0 0s web-2 0/1 Pending 0 0s web-2 0/1 ContainerCreating 0 0s web-2 1/1 Running 0 19s web-1 1/1 Terminating 0 8m web-1 0/1 Terminating 0 8m web-1 0/1 Terminating 0 8m web-1 0/1 Terminating 0 8m web-1 0/1 Pending 0 0s web-1 0/1 Pending 0 0s web-1 0/1 ContainerCreating 0 0s web-1 1/1 Running 0 6s web-0 1/1 Terminating 0 7m web-0 1/1 Terminating 0 7m web-0 0/1 Terminating 0 7m web-0 0/1 Terminating 0 7m web-0 0/1 Terminating 0 7m web-0 0/1 Terminating 0 7m web-0 0/1 Pending 0 0s web-0 0/1 Pending 0 0s web-0 0/1 ContainerCreating 0 0s web-0 1/1 Running 0 10s The Pods in the StatefulSet are updated in reverse ordinal order. The StatefulSet controller terminates each Pod, and waits for it to transition to Running and Ready prior to updating the next Pod. Note that, even though the StatefulSet controller will not proceed to update the next Pod until its ordinal successor is Running and Ready, it will restore any Pod that fails during the update to that Pod's existing version.Pods that have already received the update will be restored to the updated version, and Pods that have not yet received the update will be restored to the previous version. In this way, the controller attempts to continue to keep the application healthy and the update consistent in the presence of intermittent failures.Get the Pods to view their container images:for p in 0 1 2; do kubectl get pod "web-$p" --template '{{range $i, $c := .spec.containers}}{{$c.image}}{{end}}'; echo; done registry.k8s.io/nginx-slim:0.24 registry.k8s.io/nginx-slim:0.24 registry.k8s.io/nginx-slim:0.24 All the Pods in the StatefulSet are now running the previous container image.Note:You can also use kubectl rollout status sts/ to view the status of a rolling update to a StatefulSetStaging an updateYou can split updates to a StatefulSet that uses the RollingUpdate strategy into partitions, by specifying .spec.updateStrategy.rollingUpdate.partition.For more context, you can read Partitioned rolling updates in the StatefulSet concept page.You can stage an update to a StatefulSet by using the partition field within .spec.updateStrategy.rollingUpdate. For this update, you will keep the existing Pods in the StatefulSet unchanged whilst you change the pod template for the StatefulSet. Then you - or, outside of a tutorial, some external automation - can trigger that prepared update.First, patch the web StatefulSet to add a partition to the updateStrategy field:# The value of "partition" determines which ordinals a change applies to # Make sure to use a number bigger than the last ordinal for the # StatefulSet kubectl patch statefulset web -p '{"spec":{"updateStrategy":{"type":"RollingUpdate","rollingUpdate":{"partition":3}}}}' statefulset.apps/web patched Patch the StatefulSet again to change the container image that this StatefulSet uses:kubectl patch statefulset web --type='json' -p='[{"op": "replace", "path": "/spec/template/spec/containers/0/image", "value":"registry.k8s.io/nginx-slim:0.21"}]' statefulset.apps/web patched Delete a Pod in the StatefulSet:kubectl delete pod web-2 pod "web-2" deleted Wait for the replacement web-2 Pod to be Running and Ready:# End the watch when you see that web-2 is healthy kubectl get pod -l app=nginx --watch NAME READY STATUS RESTARTS AGE web-0 1/1 Running 0 4m web-1 1/1 Running 0 4m web-2 0/1 ContainerCreating 0 11s web-2 1/1 Running 0 18s Get the Pod's container image:kubectl get pod web-2 --template '{{range $i, $c := .spec.containers}}{{$c.image}}{{end}}' registry.k8s.io/nginx-slim:0.24 Notice that, even though the update strategy is RollingUpdate the StatefulSet restored the Pod with the original container image. This is because the ordinal of the Pod is less than the partition specified by the updateStrategy.Rolling out a canaryYou're now going to try a canary rollout of that staged change.You can roll out a canary (to test the modified template) by decrementing the partition you specified above.Patch the StatefulSet to decrement the partition:# The value of "partition" should match the highest existing ordinal for # the StatefulSet kubectl patch statefulset web -p '{"spec":{"updateStrategy":{"type":"RollingUpdate","rollingUpdate":{"partition":2}}}}' statefulset.apps/web patched The control plane triggers replacement for web-2 (implemented by a graceful delete followed by creating a new Pod once the deletion is complete). Wait for the new web-2 Pod to be Running and Ready.# This should already be running kubectl get pod -l app=nginx --watch NAME READY STATUS RESTARTS AGE web-0 1/1 Running 0 4m web-1 1/1 Running 0 4m web-2 0/1 ContainerCreating 0 11s web-2 1/1 Running 0 18s Get the Pod's container:kubectl get pod web-2 --template '{{range $i, $c := .spec.containers}}{{$c.image}}{{end}}' registry.k8s.io/nginx-slim:0.21 When you changed the partition, the StatefulSet controller automatically updated the web-2 Pod because the Pod's ordinal was greater than or equal to the partition.Delete the web-1 Pod:kubectl delete pod web-1 pod "web-1" deleted Wait for the web-1 Pod to be Running and Ready.# This should already be running kubectl get pod -l app=nginx --watch The output is similar to:NAME READY STATUS RESTARTS AGE web-0 1/1 Running 0 6m web-1 0/1 Terminating 0 6m web-2 1/1 Running 0 2m web-1 0/1 Terminating 0 6m web-1 0/1 Terminating 0 6m web-1 0/1 Terminating 0 6m web-1 0/1 Pending 0 0s web-1 0/1 Pending 0 0s web-1 0/1 ContainerCreating 0 0s web-1 1/1 Running 0 18s Get the web-1 Pod's container image:kubectl get pod web-1 --template '{{range $i, $c := .spec.containers}}{{$c.image}}{{end}}' registry.k8s.io/nginx-slim:0.24 web-1 was restored to its original configuration because the Pod's ordinal was less than the partition. When a partition is specified, all Pods with an ordinal that is greater than or equal to the partition will be updated when the StatefulSet's .spec.template is updated. If a Pod that has an ordinal less than the partition is deleted or otherwise terminated, it will be restored to its original configuration.Phased roll outsYou can perform a phased roll out (e.g. a linear, geometric, or exponential roll out) using a partitioned rolling update in a similar manner to how you rolled out a canary. To perform a phased roll out, set the partition to the ordinal at which you want the controller to pause the update.The partition is currently set to 2. Set the partition to 0:kubectl patch statefulset web -p '{"spec":{"updateStrategy":{"type":"RollingUpdate","rollingUpdate":{"partition":0}}}}' statefulset.apps/web patched Wait for all of the Pods in the StatefulSet to become Running and Ready.# This should already be running kubectl get pod -l app=nginx --watch The output is similar to:NAME READY STATUS RESTARTS AGE web-0 1/1 Running 0 3m web-1 0/1 ContainerCreating 0 11s web-2 1/1 Running 0 2m web-1 1/1 Running 0 18s web-0 1/1 Terminating 0 3m web-0 1/1 Terminating 0 3m web-0 0/1 Terminating 0 3m web-0 0/1 Terminating 0 3m web-0 0/1 Terminating 0 3m web-0 0/1 Terminating 0 3m web-0 0/1 Pending 0 0s web-0 0/1 Pending 0 0s web-0 0/1 ContainerCreating 0 0s web-0 1/1 Running 0 3s Get the container image details for the Pods in the StatefulSet:for p in 0 1 2; do kubectl get pod "web-$p" --template '{{range $i, $c := .spec.containers}}{{$c.image}}{{end}}'; echo; done registry.k8s.io/nginx-slim:0.21 registry.k8s.io/nginx-slim:0.21 registry.k8s.io/nginx-slim:0.21 By moving the partition to 0, you allowed the StatefulSet to continue the update process.OnDeleteYou select this update strategy for a StatefulSet by setting the .spec.template.updateStrategy.type to OnDelete.Patch the web StatefulSet to use the OnDelete update strategy:kubectl patch statefulset web -p '{"spec":{"updateStrategy":{"type":"OnDelete", "rollingUpdate": null}}}' statefulset.apps/web patched When you select this update strategy, the StatefulSet controller does not automatically update Pods when a modification is made to the StatefulSet's .spec.template field. You need to manage the rollout yourself - either manually, or using separate automation.Deleting StatefulSetsStatefulSet supports both non-cascading and cascading deletion. In a non-cascading delete, the StatefulSet's Pods are not deleted when the StatefulSet is deleted. In a cascading delete, both the StatefulSet and its Pods are deleted.Read Use Cascading Deletion in a Cluster to learn about cascading deletion generally.Non-cascading deleteIn one terminal window, watch the Pods in the StatefulSet.# End this watch when there are no Pods for the StatefulSet kubectl get pods --watch -l app=nginx Use kubectl delete to delete the StatefulSet. Make sure to supply the --cascade=orphan parameter to the command. This parameter tells Kubernetes to only delete the StatefulSet, and to not delete any of its Pods.kubectl delete statefulset web --cascade=orphan statefulset.apps "web" deleted Get the Pods, to examine their status:kubectl get pods -l app=nginx NAME READY STATUS RESTARTS AGE web-0 1/1 Running 0 6m web-1 1/1 Running 0 7m web-2 1/1 Running 0 5m Even though web has been deleted, all of the Pods are still Running and Ready. Delete web-0:kubectl delete pod web-0 pod "web-0" deleted Get the StatefulSet's Pods:kubectl get pods -l app=nginx NAME READY STATUS RESTARTS AGE web-1 1/1 Running 0 10m web-2 1/1 Running 0 7m As the web StatefulSet has been deleted, web-0 has not been relaunched.In one terminal, watch the StatefulSet's Pods.# Leave this watch running until the next time you start a watch kubectl get pods --watch -l app=nginx In a second terminal, recreate the StatefulSet. Note that, unless you deleted the nginx Service (which you should not have), you will see an error indicating that the Service already exists.kubectl apply -f https://k8s.io/examples/application/web/web.yaml statefulset.apps/web created service/nginx unchanged Ignore the error. It only indicates that an attempt was made to create the nginx headless Service even though that Service already exists.Examine the output of the kubectl get command running in the first terminal.# This should already be running kubectl get pods --watch -l app=nginx NAME READY STATUS RESTARTS AGE web-1 1/1 Running 0 16m web-2 1/1 Running 0 2m NAME READY STATUS RESTARTS AGE web-0 0/1 Pending 0 0s web-0 0/1 Pending 0 0s web-0 0/1 ContainerCreating 0 0s web-0 1/1 Running 0 18s web-2 1/1 Terminating 0 3m web-2 0/1 Terminating 0 3m web-2 0/1 Terminating 0 3m web-2 0/1 Terminating 0 3m When the web StatefulSet was recreated, it first relaunched web-0. Since web-1 was already Running and Ready, when web-0 transitioned to Running and Ready, it adopted this Pod. Since you recreated the StatefulSet with replicas equal to 2, once web-0 had been recreated, and once web-1 had been determined to already be Running and Ready, web-2 was terminated.Now take another look at the contents of the index.html file served by the Pods' webservers:for i in 0 1; do kubectl exec -i -t "web-$i" -- curl http://localhost/; done web-0 web-1 Even though you deleted both the StatefulSet and the web-0 Pod, it still serves the hostname originally entered into its index.html file. This is because the StatefulSet never deletes the PersistentVolumes associated with a Pod. When you recreated the StatefulSet and it relaunched web-0, its original PersistentVolume was remounted.Cascading deleteIn one terminal window, watch the Pods in the StatefulSet.# Leave this running until the next page section kubectl get pods --watch -l app=nginx In another terminal, delete the StatefulSet again. This time, omit the \--cascade=orphan parameter.kubectl delete statefulset web statefulset.apps "web" deleted Examine the output of the kubectl get command running in the first terminal, and wait for all of the Pods to transition to Terminating.# This should already be running kubectl get pods --watch -l app=nginx NAME READY STATUS RESTARTS AGE web-0 1/1 Running 0 11m web-1 1/1 Running 0 27m NAME READY STATUS RESTARTS AGE web-0 1/1 Terminating 0 12m web-1 1/1 Terminating 0 29m web-0 0/1 Terminating 0 12m web-0 0/1 Terminating 0 12m web-0 0/1 Terminating 0 12m web-1 0/1 Terminating 0 29m web-1 0/1 Terminating 0 29m web-1 0/1 Terminating 0 29m As you saw in the Scaling Down section, the Pods are terminated one at a time, with respect to the reverse order of their ordinal indices. Before terminating a Pod, the StatefulSet controller waits for the Pod's successor to be completely terminated.Note:Although a cascading delete removes a StatefulSet together with its Pods, the cascade does not delete the headless Service associated with the StatefulSet. You must delete the nginx Service manually.kubectl delete service nginx service "nginx" deleted Recreate the StatefulSet and headless Service one more time:kubectl apply -f https://k8s.io/examples/application/web/web.yaml service/nginx created statefulset.apps/web created When all of the StatefulSet's Pods transition to Running and Ready, retrieve the contents of their index.html files:for i in 0 1; do kubectl exec -i -t "web-$i" -- curl http://localhost/; done web-0 web-1 Even though you completely deleted the StatefulSet, and all of its Pods, the Pods are recreated with their PersistentVolumes mounted, and web-0 and web-1 continue to serve their hostnames.Finally, delete the nginx Service...kubectl delete service nginx service "nginx" deleted ...and the web StatefulSet:kubectl delete statefulset web statefulset "web" deleted Pod management policyFor some distributed systems, the StatefulSet ordering guarantees are unnecessary and/or undesirable. These systems require only uniqueness and identity.You can specify a Pod management policy to avoid this strict ordering; either OrderedReady (the default), or Parallel.OrderedReady Pod managementOrderedReady pod management is the default for StatefulSets. It tells the StatefulSet controller to respect the ordering guarantees demonstrated above.Use this when your application requires or expects that changes, such as rolling out a new version of your application, happen in the strict order of the ordinal (pod number) that the StatefulSet provides. In other words, if you have Pods app-0, app-1 and app-2, Kubernetes will update app-0 first and check it. Once the checks are good, Kubernetes updates app-1 and finally app-2.If you added two more Pods, Kubernetes would set up app-3 and wait for that to become healthy before deploying app-4.Because this is the default setting, you've already practised using it.Parallel Pod managementThe alternative, Parallel pod management, tells the StatefulSet controller to launch or terminate all Pods in parallel, and not to wait for Pods to become Running and Ready or completely terminated prior to launching or terminating another Pod.The Parallel pod management option only affects the behavior for scaling operations. Updates are not affected; Kubernetes still rolls out changes in order. For this tutorial, the application is very simple: a webserver that tells you its hostname (because this is a StatefulSet, the hostname for each Pod is different and predictable).application/web/web-parallel.yaml apiVersion: v1 kind: Service metadata: name: nginx labels: app: nginx spec: ports: \- port: 80 name: web clusterIP: None selector: app: nginx \--- apiVersion: apps/v1 kind: StatefulSet metadata: name: web spec: serviceName: "nginx" podManagementPolicy: "Parallel" replicas: 2 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: \- name: nginx image: registry.k8s.io/nginx-slim:0.24 ports: \- containerPort: 80 name: web volumeMounts: \- name: www mountPath: /usr/share/nginx/html volumeClaimTemplates: \- metadata: name: www spec: accessModes: [ "ReadWriteOnce" ] resources: requests: storage: 1Gi This manifest is identical to the one you downloaded above except that the .spec.podManagementPolicy of the web StatefulSet is set to Parallel.In one terminal, watch the Pods in the StatefulSet.# Leave this watch running until the end of the section kubectl get pod -l app=nginx --watch In another terminal, reconfigure the StatefulSet for Parallel Pod management:kubectl apply -f https://k8s.io/examples/application/web/web-parallel.yaml service/nginx updated statefulset.apps/web updated Keep the terminal open where you're running the watch. In another terminal window, scale the StatefulSet:kubectl scale statefulset/web --replicas=5 statefulset.apps/web scaled Examine the output of the terminal where the kubectl get command is running. It may look something likeweb-3 0/1 Pending 0 0s web-3 0/1 Pending 0 0s web-3 0/1 Pending 0 7s web-3 0/1 ContainerCreating 0 7s web-2 0/1 Pending 0 0s web-4 0/1 Pending 0 0s web-2 1/1 Running 0 8s web-4 0/1 ContainerCreating 0 4s web-3 1/1 Running 0 26s web-4 1/1 Running 0 2s The StatefulSet launched three new Pods, and it did not wait for the first to become Running and Ready prior to launching the second and third Pods.This approach is useful if your workload has a stateful element, or needs Pods to be able to identify each other with predictable naming, and especially if you sometimes need to provide a lot more capacity quickly. If this simple web service for the tutorial suddenly got an extra 1,000,000 requests per minute then you would want to run some more Pods - but you also would not want to wait for each new Pod to launch. Starting the extra Pods in parallel cuts the time between requesting the extra capacity and having it available for use.Cleaning upYou should have two terminals open, ready for you to run kubectl commands as part of cleanup.kubectl delete sts web # sts is an abbreviation for statefulset You can watch kubectl get to see those Pods being deleted.# end the watch when you've seen what you need to kubectl get pod -l app=nginx --watch web-3 1/1 Terminating 0 9m web-2 1/1 Terminating 0 9m web-3 1/1 Terminating 0 9m web-2 1/1 Terminating 0 9m web-1 1/1 Terminating 0 44m web-0 1/1 Terminating 0 44m web-0 0/1 Terminating 0 44m web-3 0/1 Terminating 0 9m web-2 0/1 Terminating 0 9m web-1 0/1 Terminating 0 44m web-0 0/1 Terminating 0 44m web-2 0/1 Terminating 0 9m web-2 0/1 Terminating 0 9m web-2 0/1 Terminating 0 9m web-1 0/1 Terminating 0 44m web-1 0/1 Terminating 0 44m web-1 0/1 Terminating 0 44m web-0 0/1 Terminating 0 44m web-0 0/1 Terminating 0 44m web-0 0/1 Terminating 0 44m web-3 0/1 Terminating 0 9m web-3 0/1 Terminating 0 9m web-3 0/1 Terminating 0 9m During deletion, a StatefulSet removes all Pods concurrently; it does not wait for a Pod's ordinal successor to terminate prior to deleting that Pod.Close the terminal where the kubectl get command is running and delete the nginx Service:kubectl delete svc nginx Delete the persistent storage media for the PersistentVolumes used in this tutorial.kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE www-web-0 Bound pvc-2bf00408-d366-4a12-bad0-1869c65d0bee 1Gi RWO standard 25m www-web-1 Bound pvc-ba3bfe9c-413e-4b95-a2c0-3ea8a54dbab4 1Gi RWO standard 24m www-web-2 Bound pvc-cba6cfa6-3a47-486b-a138-db5930207eaf 1Gi RWO standard 15m www-web-3 Bound pvc-0c04d7f0-787a-4977-8da3-d9d3a6d8d752 1Gi RWO standard 15m www-web-4 Bound pvc-b2c73489-e70b-4a4e-9ec1-9eab439aa43e 1Gi RWO standard 14m kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-0c04d7f0-787a-4977-8da3-d9d3a6d8d752 1Gi RWO Delete Bound default/www-web-3 standard 15m pvc-2bf00408-d366-4a12-bad0-1869c65d0bee 1Gi RWO Delete Bound default/www-web-0 standard 25m pvc-b2c73489-e70b-4a4e-9ec1-9eab439aa43e 1Gi RWO Delete Bound default/www-web-4 standard 14m pvc-ba3bfe9c-413e-4b95-a2c0-3ea8a54dbab4 1Gi RWO Delete Bound default/www-web-1 standard 24m pvc-cba6cfa6-3a47-486b-a138-db5930207eaf 1Gi RWO Delete Bound default/www-web-2 standard 15m kubectl delete pvc www-web-0 www-web-1 www-web-2 www-web-3 www-web-4 persistentvolumeclaim "www-web-0" deleted persistentvolumeclaim "www-web-1" deleted persistentvolumeclaim "www-web-2" deleted persistentvolumeclaim "www-web-3" deleted persistentvolumeclaim "www-web-4" deleted kubectl get pvc No resources found in default namespace. Note:You also need to delete the persistent storage media for the PersistentVolumes used in this tutorial. Follow the necessary steps, based on your environment, storage configuration, and provisioning method, to ensure that all storage is reclaimed.5.6.2 - Example: Deploying WordPress and MySQL with Persistent VolumesThis tutorial shows you how to deploy a WordPress site and a MySQL database using Minikube. Both applications use PersistentVolumes and PersistentVolumeClaims to store data.A PersistentVolume (PV) is a piece of storage in the cluster that has been manually provisioned by an administrator, or dynamically provisioned by Kubernetes using a StorageClass. A PersistentVolumeClaim (PVC) is a request for storage by a user that can be fulfilled by a PV. PersistentVolumes and PersistentVolumeClaims are independent from Pod lifecycles and preserve data through restarting, rescheduling, and even deleting Pods.Warning:This deployment is not suitable for production use cases, as it uses single instance WordPress and MySQL Pods. Consider using WordPress Helm Chart to deploy WordPress in production.Note:The files provided in this tutorial are using GA Deployment APIs and are specific to kubernetes version 1.9 and later. If you wish to use this tutorial with an earlier version of Kubernetes, please update the API version appropriately, or reference earlier versions of this tutorial.ObjectivesCreate PersistentVolumeClaims and PersistentVolumesCreate a kustomization.yaml witha Secret generatorMySQL resource configsWordPress resource configsApply the kustomization directory by kubectl apply -k ./Clean upBefore you beginYou need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesTo check the version, enter kubectl version.The example shown on this page works with kubectl 1.27 and above.Download the following configuration files:mysql-deployment.yamlwordpress-deployment.yamlCreate PersistentVolumeClaims and PersistentVolumesMySQL and Wordpress each require a PersistentVolume to store data. Their PersistentVolumeClaims will be created at the deployment step.Many cluster environments have a default StorageClass installed. When a StorageClass is not specified in the PersistentVolumeClaim, the cluster's default StorageClass is used instead.When a PersistentVolumeClaim is created, a PersistentVolume is dynamically provisioned based on the StorageClass configuration.Warning:In local clusters, the default StorageClass uses the hostPath provisioner. hostPath volumes are only suitable for development and testing. With hostPath volumes, your data lives in /tmp on the node the Pod is scheduled onto and does not move between nodes. If a Pod dies and gets scheduled to another node in the cluster, or the node is rebooted, the data is lost.Note:If you are bringing up a cluster that needs to use the hostPath provisioner, the --enable-hostpath-provisioner flag must be set in the controller-manager component.Note:If you have a Kubernetes cluster running on Google Kubernetes Engine, please follow this guide.Create a kustomization.yamlAdd a Secret generatorA Secret is an object that stores a piece of sensitive data like a password or key. Since 1.14, kubectl supports the management of Kubernetes objects using a kustomization file. You can create a Secret by generators in kustomization.yaml.Add a Secret generator in kustomization.yaml from the following command. You will need to replace YOUR_PASSWORD with the password you want to use.cat <./kustomization.yaml secretGenerator: \- name: mysql-pass literals: \- password=YOUR_PASSWORD EOF Add resource configs for MySQL and WordPressThe following manifest describes a single-instance MySQL Deployment. The MySQL container mounts the PersistentVolume at /var/lib/mysql. The MYSQL_ROOT_PASSWORD environment variable sets the database password from the Secret.application/wordpress/mysql-deployment.yaml apiVersion: v1 kind: Service metadata: name: wordpress-mysql labels: app: wordpress spec: ports: \- port: 3306 selector: app: wordpress tier: mysql clusterIP: None \--- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: mysql-pv-claim labels: app: wordpress spec: accessModes: \- ReadWriteOnce resources: requests: storage: 20Gi \--- apiVersion: apps/v1 kind: Deployment metadata: name: wordpress-mysql labels: app: wordpress spec: selector: matchLabels: app: wordpress tier: mysql strategy: type: Recreate template: metadata: labels: app: wordpress tier: mysql spec: containers: \- image: mysql:8.0 name: mysql env: \- name: MYSQL_ROOT_PASSWORD valueFrom: secretKeyRef: name: mysql-pass key: password \- name: MYSQL_DATABASE value: wordpress \- name: MYSQL_USER value: wordpress \- name: MYSQL_PASSWORD valueFrom: secretKeyRef: name: mysql-pass key: password ports: \- containerPort: 3306 name: mysql volumeMounts: \- name: mysql-persistent-storage mountPath: /var/lib/mysql volumes: \- name: mysql-persistent-storage persistentVolumeClaim: claimName: mysql-pv-claim The following manifest describes a single-instance WordPress Deployment. The WordPress container mounts the PersistentVolume at /var/www/html for website data files. The WORDPRESS_DB_HOST environment variable sets the name of the MySQL Service defined above, and WordPress will access the database by Service. The WORDPRESS_DB_PASSWORD environment variable sets the database password from the Secret kustomize generated.application/wordpress/wordpress-deployment.yaml apiVersion: v1 kind: Service metadata: name: wordpress labels: app: wordpress spec: ports: \- port: 80 selector: app: wordpress tier: frontend type: LoadBalancer \--- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: wp-pv-claim labels: app: wordpress spec: accessModes: \- ReadWriteOnce resources: requests: storage: 20Gi \--- apiVersion: apps/v1 kind: Deployment metadata: name: wordpress labels: app: wordpress spec: selector: matchLabels: app: wordpress tier: frontend strategy: type: Recreate template: metadata: labels: app: wordpress tier: frontend spec: containers: \- image: wordpress:6.2.1-apache name: wordpress env: \- name: WORDPRESS_DB_HOST value: wordpress-mysql \- name: WORDPRESS_DB_PASSWORD valueFrom: secretKeyRef: name: mysql-pass key: password \- name: WORDPRESS_DB_USER value: wordpress ports: \- containerPort: 80 name: wordpress volumeMounts: \- name: wordpress-persistent-storage mountPath: /var/www/html volumes: \- name: wordpress-persistent-storage persistentVolumeClaim: claimName: wp-pv-claim Download the MySQL deployment configuration file.curl -LO https://k8s.io/examples/application/wordpress/mysql-deployment.yaml Download the WordPress configuration file.curl -LO https://k8s.io/examples/application/wordpress/wordpress-deployment.yaml Add them to kustomization.yaml file.cat <>./kustomization.yaml resources: \- mysql-deployment.yaml \- wordpress-deployment.yaml EOF Apply and VerifyThe kustomization.yaml contains all the resources for deploying a WordPress site and a MySQL database. You can apply the directory bykubectl apply -k ./ Now you can verify that all objects exist.Verify that the Secret exists by running the following command:kubectl get secrets The response should be like this:NAME TYPE DATA AGE mysql-pass-c57bb4t7mf Opaque 1 9s Verify that a PersistentVolume got dynamically provisioned.kubectl get pvc Note:It can take up to a few minutes for the PVs to be provisioned and bound.The response should be like this:NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE mysql-pv-claim Bound pvc-8cbd7b2e-4044-11e9-b2bb-42010a800002 20Gi RWO standard 77s wp-pv-claim Bound pvc-8cd0df54-4044-11e9-b2bb-42010a800002 20Gi RWO standard 77s Verify that the Pod is running by running the following command:kubectl get pods Note:It can take up to a few minutes for the Pod's Status to be RUNNING.The response should be like this:NAME READY STATUS RESTARTS AGE wordpress-mysql-1894417608-x5dzt 1/1 Running 0 40s Verify that the Service is running by running the following command:kubectl get services wordpress The response should be like this:NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE wordpress LoadBalancer 10.0.0.89  80:32406/TCP 4m Note:Minikube can only expose Services through NodePort. The EXTERNAL-IP is always pending.Run the following command to get the IP Address for the WordPress Service:minikube service wordpress --url The response should be like this:http://1.2.3.4:32406 Copy the IP address, and load the page in your browser to view your site.You should see the WordPress set up page similar to the following screenshot.Warning:Do not leave your WordPress installation on this page. If another user finds it, they can set up a website on your instance and use it to serve malicious content.Either install WordPress by creating a username and password or delete your instance.Cleaning upRun the following command to delete your Secret, Deployments, Services and PersistentVolumeClaims:kubectl delete -k ./ What's nextLearn more about Introspection and DebuggingLearn more about JobsLearn more about Port ForwardingLearn how to Get a Shell to a Container5.6.3 - Example: Deploying Cassandra with a StatefulSetThis tutorial shows you how to run Apache Cassandra on Kubernetes. Cassandra, a database, needs persistent storage to provide data durability (application state). In this example, a custom Cassandra seed provider lets the database discover new Cassandra instances as they join the Cassandra cluster.StatefulSets make it easier to deploy stateful applications into your Kubernetes cluster. For more information on the features used in this tutorial, see StatefulSet.Note:Cassandra and Kubernetes both use the term node to mean a member of a cluster. In this tutorial, the Pods that belong to the StatefulSet are Cassandra nodes and are members of the Cassandra cluster (called a ring). When those Pods run in your Kubernetes cluster, the Kubernetes control plane schedules those Pods onto Kubernetes Nodes.When a Cassandra node starts, it uses a seed list to bootstrap discovery of other nodes in the ring. This tutorial deploys a custom Cassandra seed provider that lets the database discover new Cassandra Pods as they appear inside your Kubernetes cluster.ObjectivesCreate and validate a Cassandra headless Service.Use a StatefulSet to create a Cassandra ring.Validate the StatefulSet.Modify the StatefulSet.Delete the StatefulSet and its Pods.Before you beginYou need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. It is recommended to run this tutorial on a cluster with at least two nodes that are not acting as control plane hosts. If you do not already have a cluster, you can create one by using minikube or you can use one of these Kubernetes playgrounds:KillercodaKodeKloudPlay with KubernetesTo complete this tutorial, you should already have a basic familiarity with Pods, Services, and StatefulSets.Additional Minikube setup instructionsCaution:Minikube defaults to 2048MB of memory and 2 CPU. Running Minikube with the default resource configuration results in insufficient resource errors during this tutorial. To avoid these errors, start Minikube with the following settings:minikube start --memory 5120 --cpus=4 Creating a headless Service for CassandraIn Kubernetes, a Service describes a set of Pods that perform the same task.The following Service is used for DNS lookups between Cassandra Pods and clients within your cluster:application/cassandra/cassandra-service.yaml apiVersion: v1 kind: Service metadata: labels: app: cassandra name: cassandra spec: clusterIP: None ports: \- port: 9042 selector: app: cassandra Create a Service to track all Cassandra StatefulSet members from the cassandra-service.yaml file:kubectl apply -f https://k8s.io/examples/application/cassandra/cassandra-service.yaml Validating (optional)Get the Cassandra Service.kubectl get svc cassandra The response isNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE cassandra ClusterIP None  9042/TCP 45s If you don't see a Service named cassandra, that means creation failed. Read Debug Services for help troubleshooting common issues.Using a StatefulSet to create a Cassandra ringThe StatefulSet manifest, included below, creates a Cassandra ring that consists of three Pods.Note:This example uses the default provisioner for Minikube. Please update the following StatefulSet for the cloud you are working with.application/cassandra/cassandra-statefulset.yaml apiVersion: apps/v1 kind: StatefulSet metadata: name: cassandra labels: app: cassandra spec: serviceName: cassandra replicas: 3 selector: matchLabels: app: cassandra template: metadata: labels: app: cassandra spec: terminationGracePeriodSeconds: 500 containers: \- name: cassandra image: gcr.io/google-samples/cassandra:v13 imagePullPolicy: Always ports: \- containerPort: 7000 name: intra-node \- containerPort: 7001 name: tls-intra-node \- containerPort: 7199 name: jmx \- containerPort: 9042 name: cql resources: limits: cpu: "500m" memory: 1Gi requests: cpu: "500m" memory: 1Gi securityContext: capabilities: add: \- IPC_LOCK lifecycle: preStop: exec: command: \- /bin/sh \- -c \- nodetool drain env: \- name: MAX_HEAP_SIZE value: 512M \- name: HEAP_NEWSIZE value: 100M \- name: CASSANDRA_SEEDS value: "cassandra-0.cassandra.default.svc.cluster.local" \- name: CASSANDRA_CLUSTER_NAME value: "K8Demo" \- name: CASSANDRA_DC value: "DC1-K8Demo" \- name: CASSANDRA_RACK value: "Rack1-K8Demo" \- name: POD_IP valueFrom: fieldRef: fieldPath: status.podIP readinessProbe: exec: command: \- /bin/bash \- -c \- /ready-probe.sh initialDelaySeconds: 15 timeoutSeconds: 5 # These volume mounts are persistent. They are like inline claims, # but not exactly because the names need to match exactly one of # the stateful pod volumes. volumeMounts: \- name: cassandra-data mountPath: /cassandra_data # These are converted to volume claims by the controller # and mounted at the paths mentioned above. # do not use these in production until ssd GCEPersistentDisk or other ssd pd volumeClaimTemplates: \- metadata: name: cassandra-data spec: accessModes: [ "ReadWriteOnce" ] storageClassName: fast resources: requests: storage: 1Gi \--- kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: fast provisioner: k8s.io/minikube-hostpath parameters: type: pd-ssd Create the Cassandra StatefulSet from the cassandra-statefulset.yaml file:# Use this if you are able to apply cassandra-statefulset.yaml unmodified kubectl apply -f https://k8s.io/examples/application/cassandra/cassandra-statefulset.yaml If you need to modify cassandra-statefulset.yaml to suit your cluster, download https://k8s.io/examples/application/cassandra/cassandra-statefulset.yaml and then apply that manifest, from the folder you saved the modified version into:# Use this if you needed to modify cassandra-statefulset.yaml locally kubectl apply -f cassandra-statefulset.yaml Validating the Cassandra StatefulSetGet the Cassandra StatefulSet:kubectl get statefulset cassandra The response should be similar to:NAME DESIRED CURRENT AGE cassandra 3 0 13s The StatefulSet resource deploys Pods sequentially.Get the Pods to see the ordered creation status:kubectl get pods -l="app=cassandra" The response should be similar to:NAME READY STATUS RESTARTS AGE cassandra-0 1/1 Running 0 1m cassandra-1 0/1 ContainerCreating 0 8s It can take several minutes for all three Pods to deploy. Once they are deployed, the same command returns output similar to:NAME READY STATUS RESTARTS AGE cassandra-0 1/1 Running 0 10m cassandra-1 1/1 Running 0 9m cassandra-2 1/1 Running 0 8m Run the Cassandra nodetool inside the first Pod, to display the status of the ring.kubectl exec -it cassandra-0 -- nodetool status The response should look something like:Datacenter: DC1-K8Demo ====================== Status=Up/Down |/ State=Normal/Leaving/Joining/Moving \-- Address Load Tokens Owns (effective) Host ID Rack UN 172.17.0.5 83.57 KiB 32 74.0% e2dd09e6-d9d3-477e-96c5-45094c08db0f Rack1-K8Demo UN 172.17.0.4 101.04 KiB 32 58.8% f89d6835-3a42-4419-92b3-0e62cae1479c Rack1-K8Demo UN 172.17.0.6 84.74 KiB 32 67.1% a6a1e8c2-3dc5-4417-b1a0-26507af2aaad Rack1-K8Demo Modifying the Cassandra StatefulSetUse kubectl edit to modify the size of a Cassandra StatefulSet.Run the following command:kubectl edit statefulset cassandra This command opens an editor in your terminal. The line you need to change is the replicas field. The following sample is an excerpt of the StatefulSet file:# Please edit the object below. Lines beginning with a '#' will be ignored, # and an empty file will abort the edit. If an error occurs while saving this file will be # reopened with the relevant failures. # apiVersion: apps/v1 kind: StatefulSet metadata: creationTimestamp: 2016-08-13T18:40:58Z generation: 1 labels: app: cassandra name: cassandra namespace: default resourceVersion: "323" uid: 7a219483-6185-11e6-a910-42010a8a0fc0 spec: replicas: 3 Change the number of replicas to 4, and then save the manifest.The StatefulSet now scales to run with 4 Pods.Get the Cassandra StatefulSet to verify your change:kubectl get statefulset cassandra The response should be similar to:NAME DESIRED CURRENT AGE cassandra 4 4 36m Cleaning upDeleting or scaling a StatefulSet down does not delete the volumes associated with the StatefulSet. This setting is for your safety because your data is more valuable than automatically purging all related StatefulSet resources.Warning:Depending on the storage class and reclaim policy, deleting the PersistentVolumeClaims may cause the associated volumes to also be deleted. Never assume you'll be able to access data if its volume claims are deleted.Run the following commands (chained together into a single command) to delete everything in the Cassandra StatefulSet:grace=$(kubectl get pod cassandra-0 -o=jsonpath='{.spec.terminationGracePeriodSeconds}') \ && kubectl delete statefulset -l app=cassandra \ && echo "Sleeping ${grace} seconds" 1>&2 \ && sleep $grace \ && kubectl delete persistentvolumeclaim -l app=cassandra Run the following command to delete the Service you set up for Cassandra:kubectl delete service -l app=cassandra Cassandra container environment variablesThe Pods in this tutorial use the gcr.io/google-samples/cassandra:v13 image from Google's container registry. The Docker image above is based on debian-base and includes OpenJDK 8.This image includes a standard Cassandra installation from the Apache Debian repo. By using environment variables you can change values that are inserted into cassandra.yaml.Environment variableDefault valueCASSANDRA_CLUSTER_NAME'Test Cluster'CASSANDRA_NUM_TOKENS32CASSANDRA_RPC_ADDRESS0.0.0.0What's nextLearn how to Scale a StatefulSet.Learn more about the KubernetesSeedProviderSee more custom Seed Provider Configurations5.6.4 - Running ZooKeeper, A Distributed System CoordinatorThis tutorial demonstrates running Apache Zookeeper on Kubernetes using StatefulSets, PodDisruptionBudgets, and PodAntiAffinity.Before you beginBefore starting this tutorial, you should be familiar with the following Kubernetes concepts:PodsCluster DNSHeadless ServicesPersistentVolumesPersistentVolume ProvisioningStatefulSetsPodDisruptionBudgetsPodAntiAffinitykubectl CLIYou must have a cluster with at least four nodes, and each node requires at least 2 CPUs and 4 GiB of memory. In this tutorial you will cordon and drain the cluster's nodes. This means that the cluster will terminate and evict all Pods on its nodes, and the nodes will temporarily become unschedulable. You should use a dedicated cluster for this tutorial, or you should ensure that the disruption you cause will not interfere with other tenants.This tutorial assumes that you have configured your cluster to dynamically provision PersistentVolumes. If your cluster is not configured to do so, you will have to manually provision three 20 GiB volumes before starting this tutorial.ObjectivesAfter this tutorial, you will know the following.How to deploy a ZooKeeper ensemble using StatefulSet.How to consistently configure the ensemble.How to spread the deployment of ZooKeeper servers in the ensemble.How to use PodDisruptionBudgets to ensure service availability during planned maintenance.ZooKeeperApache ZooKeeper is a distributed, open-source coordination service for distributed applications. ZooKeeper allows you to read, write, and observe updates to data. Data are organized in a file system like hierarchy and replicated to all ZooKeeper servers in the ensemble (a set of ZooKeeper servers). All operations on data are atomic and sequentially consistent. ZooKeeper ensures this by using the Zab consensus protocol to replicate a state machine across all servers in the ensemble.The ensemble uses the Zab protocol to elect a leader, and the ensemble cannot write data until that election is complete. Once complete, the ensemble uses Zab to ensure that it replicates all writes to a quorum before it acknowledges and makes them visible to clients. Without respect to weighted quorums, a quorum is a majority component of the ensemble containing the current leader. For instance, if the ensemble has three servers, a component that contains the leader and one other server constitutes a quorum. If the ensemble can not achieve a quorum, the ensemble cannot write data.ZooKeeper servers keep their entire state machine in memory, and write every mutation to a durable WAL (Write Ahead Log) on storage media. When a server crashes, it can recover its previous state by replaying the WAL. To prevent the WAL from growing without bound, ZooKeeper servers will periodically snapshot them in memory state to storage media. These snapshots can be loaded directly into memory, and all WAL entries that preceded the snapshot may be discarded.Creating a ZooKeeper ensembleThe manifest below contains a Headless Service, a Service, a PodDisruptionBudget, and a StatefulSet.application/zookeeper/zookeeper.yaml apiVersion: v1 kind: Service metadata: name: zk-hs labels: app: zk spec: ports: \- port: 2888 name: server \- port: 3888 name: leader-election clusterIP: None selector: app: zk \--- apiVersion: v1 kind: Service metadata: name: zk-cs labels: app: zk spec: ports: \- port: 2181 name: client selector: app: zk \--- apiVersion: policy/v1 kind: PodDisruptionBudget metadata: name: zk-pdb spec: selector: matchLabels: app: zk maxUnavailable: 1 \--- apiVersion: apps/v1 kind: StatefulSet metadata: name: zk spec: selector: matchLabels: app: zk serviceName: zk-hs replicas: 3 updateStrategy: type: RollingUpdate podManagementPolicy: OrderedReady template: metadata: labels: app: zk spec: affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: \- labelSelector: matchExpressions: \- key: "app" operator: In values: \- zk topologyKey: "kubernetes.io/hostname" containers: \- name: kubernetes-zookeeper imagePullPolicy: Always image: "registry.k8s.io/kubernetes-zookeeper:1.0-3.4.10" resources: requests: memory: "1Gi" cpu: "0.5" ports: \- containerPort: 2181 name: client \- containerPort: 2888 name: server \- containerPort: 3888 name: leader-election command: \- sh \- -c \- "start-zookeeper \ \--servers=3 \ \--data_dir=/var/lib/zookeeper/data \ \--data_log_dir=/var/lib/zookeeper/data/log \ \--conf_dir=/opt/zookeeper/conf \ \--client_port=2181 \ \--election_port=3888 \ \--server_port=2888 \ \--tick_time=2000 \ \--init_limit=10 \ \--sync_limit=5 \ \--heap=512M \ \--max_client_cnxns=60 \ \--snap_retain_count=3 \ \--purge_interval=12 \ \--max_session_timeout=40000 \ \--min_session_timeout=4000 \ \--log_level=INFO" readinessProbe: exec: command: \- sh \- -c \- "zookeeper-ready 2181" initialDelaySeconds: 10 timeoutSeconds: 5 livenessProbe: exec: command: \- sh \- -c \- "zookeeper-ready 2181" initialDelaySeconds: 10 timeoutSeconds: 5 volumeMounts: \- name: datadir mountPath: /var/lib/zookeeper securityContext: runAsUser: 1000 fsGroup: 1000 volumeClaimTemplates: \- metadata: name: datadir spec: accessModes: [ "ReadWriteOnce" ] resources: requests: storage: 10Gi Open a terminal, and use the kubectl apply command to create the manifest.kubectl apply -f https://k8s.io/examples/application/zookeeper/zookeeper.yaml This creates the zk-hs Headless Service, the zk-cs Service, the zk-pdb PodDisruptionBudget, and the zk StatefulSet.service/zk-hs created service/zk-cs created poddisruptionbudget.policy/zk-pdb created statefulset.apps/zk created Use kubectl get to watch the StatefulSet controller create the StatefulSet's Pods.kubectl get pods -w -l app=zk Once the zk-2 Pod is Running and Ready, use CTRL-C to terminate kubectl.NAME READY STATUS RESTARTS AGE zk-0 0/1 Pending 0 0s zk-0 0/1 Pending 0 0s zk-0 0/1 ContainerCreating 0 0s zk-0 0/1 Running 0 19s zk-0 1/1 Running 0 40s zk-1 0/1 Pending 0 0s zk-1 0/1 Pending 0 0s zk-1 0/1 ContainerCreating 0 0s zk-1 0/1 Running 0 18s zk-1 1/1 Running 0 40s zk-2 0/1 Pending 0 0s zk-2 0/1 Pending 0 0s zk-2 0/1 ContainerCreating 0 0s zk-2 0/1 Running 0 19s zk-2 1/1 Running 0 40s The StatefulSet controller creates three Pods, and each Pod has a container with a ZooKeeper server.Facilitating leader electionBecause there is no terminating algorithm for electing a leader in an anonymous network, Zab requires explicit membership configuration to perform leader election. Each server in the ensemble needs to have a unique identifier, all servers need to know the global set of identifiers, and each identifier needs to be associated with a network address.Use kubectl exec to get the hostnames of the Pods in the zk StatefulSet.for i in 0 1 2; do kubectl exec zk-$i -- hostname; done The StatefulSet controller provides each Pod with a unique hostname based on its ordinal index. The hostnames take the form of -. Because the replicas field of the zk StatefulSet is set to 3, the Set's controller creates three Pods with their hostnames set to zk-0, zk-1, and zk-2.zk-0 zk-1 zk-2 The servers in a ZooKeeper ensemble use natural numbers as unique identifiers, and store each server's identifier in a file called myid in the server's data directory.To examine the contents of the myid file for each server use the following command.for i in 0 1 2; do echo "myid zk-$i";kubectl exec zk-$i -- cat /var/lib/zookeeper/data/myid; done Because the identifiers are natural numbers and the ordinal indices are non-negative integers, you can generate an identifier by adding 1 to the ordinal.myid zk-0 1 myid zk-1 2 myid zk-2 3 To get the Fully Qualified Domain Name (FQDN) of each Pod in the zk StatefulSet use the following command.for i in 0 1 2; do kubectl exec zk-$i -- hostname -f; done The zk-hs Service creates a domain for all of the Pods, zk-hs.default.svc.cluster.local.zk-0.zk-hs.default.svc.cluster.local zk-1.zk-hs.default.svc.cluster.local zk-2.zk-hs.default.svc.cluster.local The A records in Kubernetes DNS resolve the FQDNs to the Pods' IP addresses. If Kubernetes reschedules the Pods, it will update the A records with the Pods' new IP addresses, but the A records names will not change.ZooKeeper stores its application configuration in a file named zoo.cfg. Use kubectl exec to view the contents of the zoo.cfg file in the zk-0 Pod.kubectl exec zk-0 -- cat /opt/zookeeper/conf/zoo.cfg In the server.1, server.2, and server.3 properties at the bottom of the file, the 1, 2, and 3 correspond to the identifiers in the ZooKeeper servers' myid files. They are set to the FQDNs for the Pods in the zk StatefulSet.clientPort=2181 dataDir=/var/lib/zookeeper/data dataLogDir=/var/lib/zookeeper/log tickTime=2000 initLimit=10 syncLimit=2000 maxClientCnxns=60 minSessionTimeout= 4000 maxSessionTimeout= 40000 autopurge.snapRetainCount=3 autopurge.purgeInterval=0 server.1=zk-0.zk-hs.default.svc.cluster.local:2888:3888 server.2=zk-1.zk-hs.default.svc.cluster.local:2888:3888 server.3=zk-2.zk-hs.default.svc.cluster.local:2888:3888 Achieving consensusConsensus protocols require that the identifiers of each participant be unique. No two participants in the Zab protocol should claim the same unique identifier. This is necessary to allow the processes in the system to agree on which processes have committed which data. If two Pods are launched with the same ordinal, two ZooKeeper servers would both identify themselves as the same server.kubectl get pods -w -l app=zk NAME READY STATUS RESTARTS AGE zk-0 0/1 Pending 0 0s zk-0 0/1 Pending 0 0s zk-0 0/1 ContainerCreating 0 0s zk-0 0/1 Running 0 19s zk-0 1/1 Running 0 40s zk-1 0/1 Pending 0 0s zk-1 0/1 Pending 0 0s zk-1 0/1 ContainerCreating 0 0s zk-1 0/1 Running 0 18s zk-1 1/1 Running 0 40s zk-2 0/1 Pending 0 0s zk-2 0/1 Pending 0 0s zk-2 0/1 ContainerCreating 0 0s zk-2 0/1 Running 0 19s zk-2 1/1 Running 0 40s The A records for each Pod are entered when the Pod becomes Ready. Therefore, the FQDNs of the ZooKeeper servers will resolve to a single endpoint, and that endpoint will be the unique ZooKeeper server claiming the identity configured in its myid file.zk-0.zk-hs.default.svc.cluster.local zk-1.zk-hs.default.svc.cluster.local zk-2.zk-hs.default.svc.cluster.local This ensures that the servers properties in the ZooKeepers' zoo.cfg files represents a correctly configured ensemble.server.1=zk-0.zk-hs.default.svc.cluster.local:2888:3888 server.2=zk-1.zk-hs.default.svc.cluster.local:2888:3888 server.3=zk-2.zk-hs.default.svc.cluster.local:2888:3888 When the servers use the Zab protocol to attempt to commit a value, they will either achieve consensus and commit the value (if leader election has succeeded and at least two of the Pods are Running and Ready), or they will fail to do so (if either of the conditions are not met). No state will arise where one server acknowledges a write on behalf of another.Sanity testing the ensembleThe most basic sanity test is to write data to one ZooKeeper server and to read the data from another.The command below executes the zkCli.sh script to write world to the path /hello on the zk-0 Pod in the ensemble.kubectl exec zk-0 -- zkCli.sh create /hello world WATCHER:: WatchedEvent state:SyncConnected type:None path:null Created /hello To get the data from the zk-1 Pod use the following command.kubectl exec zk-1 -- zkCli.sh get /hello The data that you created on zk-0 is available on all the servers in the ensemble.WATCHER:: WatchedEvent state:SyncConnected type:None path:null world cZxid = 0x100000002 ctime = Thu Dec 08 15:13:30 UTC 2016 mZxid = 0x100000002 mtime = Thu Dec 08 15:13:30 UTC 2016 pZxid = 0x100000002 cversion = 0 dataVersion = 0 aclVersion = 0 ephemeralOwner = 0x0 dataLength = 5 numChildren = 0 Providing durable storageAs mentioned in the ZooKeeper Basics section, ZooKeeper commits all entries to a durable WAL, and periodically writes snapshots in memory state, to storage media. Using WALs to provide durability is a common technique for applications that use consensus protocols to achieve a replicated state machine.Use the kubectl delete command to delete the zk StatefulSet.kubectl delete statefulset zk statefulset.apps "zk" deleted Watch the termination of the Pods in the StatefulSet.kubectl get pods -w -l app=zk When zk-0 if fully terminated, use CTRL-C to terminate kubectl.zk-2 1/1 Terminating 0 9m zk-0 1/1 Terminating 0 11m zk-1 1/1 Terminating 0 10m zk-2 0/1 Terminating 0 9m zk-2 0/1 Terminating 0 9m zk-2 0/1 Terminating 0 9m zk-1 0/1 Terminating 0 10m zk-1 0/1 Terminating 0 10m zk-1 0/1 Terminating 0 10m zk-0 0/1 Terminating 0 11m zk-0 0/1 Terminating 0 11m zk-0 0/1 Terminating 0 11m Reapply the manifest in zookeeper.yaml.kubectl apply -f https://k8s.io/examples/application/zookeeper/zookeeper.yaml This creates the zk StatefulSet object, but the other API objects in the manifest are not modified because they already exist.Watch the StatefulSet controller recreate the StatefulSet's Pods.kubectl get pods -w -l app=zk Once the zk-2 Pod is Running and Ready, use CTRL-C to terminate kubectl.NAME READY STATUS RESTARTS AGE zk-0 0/1 Pending 0 0s zk-0 0/1 Pending 0 0s zk-0 0/1 ContainerCreating 0 0s zk-0 0/1 Running 0 19s zk-0 1/1 Running 0 40s zk-1 0/1 Pending 0 0s zk-1 0/1 Pending 0 0s zk-1 0/1 ContainerCreating 0 0s zk-1 0/1 Running 0 18s zk-1 1/1 Running 0 40s zk-2 0/1 Pending 0 0s zk-2 0/1 Pending 0 0s zk-2 0/1 ContainerCreating 0 0s zk-2 0/1 Running 0 19s zk-2 1/1 Running 0 40s Use the command below to get the value you entered during the sanity test, from the zk-2 Pod.kubectl exec zk-2 zkCli.sh get /hello Even though you terminated and recreated all of the Pods in the zk StatefulSet, the ensemble still serves the original value.WATCHER:: WatchedEvent state:SyncConnected type:None path:null world cZxid = 0x100000002 ctime = Thu Dec 08 15:13:30 UTC 2016 mZxid = 0x100000002 mtime = Thu Dec 08 15:13:30 UTC 2016 pZxid = 0x100000002 cversion = 0 dataVersion = 0 aclVersion = 0 ephemeralOwner = 0x0 dataLength = 5 numChildren = 0 The volumeClaimTemplates field of the zk StatefulSet's spec specifies a PersistentVolume provisioned for each Pod.volumeClaimTemplates: \- metadata: name: datadir annotations: volume.alpha.kubernetes.io/storage-class: anything spec: accessModes: [ "ReadWriteOnce" ] resources: requests: storage: 20Gi The StatefulSet controller generates a PersistentVolumeClaim for each Pod in the StatefulSet.Use the following command to get the StatefulSet's PersistentVolumeClaims.kubectl get pvc -l app=zk When the StatefulSet recreated its Pods, it remounts the Pods' PersistentVolumes.NAME STATUS VOLUME CAPACITY ACCESSMODES AGE datadir-zk-0 Bound pvc-bed742cd-bcb1-11e6-994f-42010a800002 20Gi RWO 1h datadir-zk-1 Bound pvc-bedd27d2-bcb1-11e6-994f-42010a800002 20Gi RWO 1h datadir-zk-2 Bound pvc-bee0817e-bcb1-11e6-994f-42010a800002 20Gi RWO 1h The volumeMounts section of the StatefulSet's container template mounts the PersistentVolumes in the ZooKeeper servers' data directories.volumeMounts: \- name: datadir mountPath: /var/lib/zookeeper When a Pod in the zk StatefulSet is (re)scheduled, it will always have the same PersistentVolume mounted to the ZooKeeper server's data directory. Even when the Pods are rescheduled, all the writes made to the ZooKeeper servers' WALs, and all their snapshots, remain durable.Ensuring consistent configurationAs noted in the Facilitating Leader Election and Achieving Consensus sections, the servers in a ZooKeeper ensemble require consistent configuration to elect a leader and form a quorum. They also require consistent configuration of the Zab protocol in order for the protocol to work correctly over a network. In our example we achieve consistent configuration by embedding the configuration directly into the manifest.Get the zk StatefulSet.kubectl get sts zk -o yaml … command: \- sh \- -c \- "start-zookeeper \ \--servers=3 \ \--data_dir=/var/lib/zookeeper/data \ \--data_log_dir=/var/lib/zookeeper/data/log \ \--conf_dir=/opt/zookeeper/conf \ \--client_port=2181 \ \--election_port=3888 \ \--server_port=2888 \ \--tick_time=2000 \ \--init_limit=10 \ \--sync_limit=5 \ \--heap=512M \ \--max_client_cnxns=60 \ \--snap_retain_count=3 \ \--purge_interval=12 \ \--max_session_timeout=40000 \ \--min_session_timeout=4000 \ \--log_level=INFO" … The command used to start the ZooKeeper servers passed the configuration as command line parameter. You can also use environment variables to pass configuration to the ensemble.Configuring loggingOne of the files generated by the zkGenConfig.sh script controls ZooKeeper's logging. ZooKeeper uses Log4j, and, by default, it uses a time and size based rolling file appender for its logging configuration.Use the command below to get the logging configuration from one of Pods in the zk StatefulSet.kubectl exec zk-0 cat /usr/etc/zookeeper/log4j.properties The logging configuration below will cause the ZooKeeper process to write all of its logs to the standard output file stream.zookeeper.root.logger=CONSOLE zookeeper.console.threshold=INFO log4j.rootLogger=${zookeeper.root.logger} log4j.appender.CONSOLE=org.apache.log4j.ConsoleAppender log4j.appender.CONSOLE.Threshold=${zookeeper.console.threshold} log4j.appender.CONSOLE.layout=org.apache.log4j.PatternLayout log4j.appender.CONSOLE.layout.ConversionPattern=%d{ISO8601} [myid:%X{myid}] - %-5p [%t:%C{1}@%L] - %m%n This is the simplest possible way to safely log inside the container. Because the applications write logs to standard out, Kubernetes will handle log rotation for you. Kubernetes also implements a sane retention policy that ensures application logs written to standard out and standard error do not exhaust local storage media.Use kubectl logs to retrieve the last 20 log lines from one of the Pods.kubectl logs zk-0 --tail 20 You can view application logs written to standard out or standard error using kubectl logs and from the Kubernetes Dashboard.2016-12-06 19:34:16,236 [myid:1] - INFO [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@827] - Processing ruok command from /127.0.0.1:52740 2016-12-06 19:34:16,237 [myid:1] - INFO [Thread-1136:NIOServerCnxn@1008] - Closed socket connection for client /127.0.0.1:52740 (no session established for client) 2016-12-06 19:34:26,155 [myid:1] - INFO [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@192] - Accepted socket connection from /127.0.0.1:52749 2016-12-06 19:34:26,155 [myid:1] - INFO [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@827] - Processing ruok command from /127.0.0.1:52749 2016-12-06 19:34:26,156 [myid:1] - INFO [Thread-1137:NIOServerCnxn@1008] - Closed socket connection for client /127.0.0.1:52749 (no session established for client) 2016-12-06 19:34:26,222 [myid:1] - INFO [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@192] - Accepted socket connection from /127.0.0.1:52750 2016-12-06 19:34:26,222 [myid:1] - INFO [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@827] - Processing ruok command from /127.0.0.1:52750 2016-12-06 19:34:26,226 [myid:1] - INFO [Thread-1138:NIOServerCnxn@1008] - Closed socket connection for client /127.0.0.1:52750 (no session established for client) 2016-12-06 19:34:36,151 [myid:1] - INFO [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@192] - Accepted socket connection from /127.0.0.1:52760 2016-12-06 19:34:36,152 [myid:1] - INFO [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@827] - Processing ruok command from /127.0.0.1:52760 2016-12-06 19:34:36,152 [myid:1] - INFO [Thread-1139:NIOServerCnxn@1008] - Closed socket connection for client /127.0.0.1:52760 (no session established for client) 2016-12-06 19:34:36,230 [myid:1] - INFO [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@192] - Accepted socket connection from /127.0.0.1:52761 2016-12-06 19:34:36,231 [myid:1] - INFO [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@827] - Processing ruok command from /127.0.0.1:52761 2016-12-06 19:34:36,231 [myid:1] - INFO [Thread-1140:NIOServerCnxn@1008] - Closed socket connection for client /127.0.0.1:52761 (no session established for client) 2016-12-06 19:34:46,149 [myid:1] - INFO [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@192] - Accepted socket connection from /127.0.0.1:52767 2016-12-06 19:34:46,149 [myid:1] - INFO [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@827] - Processing ruok command from /127.0.0.1:52767 2016-12-06 19:34:46,149 [myid:1] - INFO [Thread-1141:NIOServerCnxn@1008] - Closed socket connection for client /127.0.0.1:52767 (no session established for client) 2016-12-06 19:34:46,230 [myid:1] - INFO [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@192] - Accepted socket connection from /127.0.0.1:52768 2016-12-06 19:34:46,230 [myid:1] - INFO [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@827] - Processing ruok command from /127.0.0.1:52768 2016-12-06 19:34:46,230 [myid:1] - INFO [Thread-1142:NIOServerCnxn@1008] - Closed socket connection for client /127.0.0.1:52768 (no session established for client) Kubernetes integrates with many logging solutions. You can choose a logging solution that best fits your cluster and applications. For cluster-level logging and aggregation, consider deploying a sidecar container to rotate and ship your logs.Configuring a non-privileged userThe best practices to allow an application to run as a privileged user inside of a container are a matter of debate. If your organization requires that applications run as a non-privileged user you can use a SecurityContext to control the user that the entry point runs as.The zk StatefulSet's Pod template contains a SecurityContext.securityContext: runAsUser: 1000 fsGroup: 1000 In the Pods' containers, UID 1000 corresponds to the zookeeper user and GID 1000 corresponds to the zookeeper group.Get the ZooKeeper process information from the zk-0 Pod.kubectl exec zk-0 -- ps -elf As the runAsUser field of the securityContext object is set to 1000, instead of running as root, the ZooKeeper process runs as the zookeeper user.F S UID PID PPID C PRI NI ADDR SZ WCHAN STIME TTY TIME CMD 4 S zookeep+ 1 0 0 80 0 - 1127 - 20:46 ? 00:00:00 sh -c zkGenConfig.sh && zkServer.sh start-foreground 0 S zookeep+ 27 1 0 80 0 - 1155556 - 20:46 ? 00:00:19 /usr/lib/jvm/java-8-openjdk-amd64/bin/java -Dzookeeper.log.dir=/var/log/zookeeper -Dzookeeper.root.logger=INFO,CONSOLE -cp /usr/bin/../build/classes:/usr/bin/../build/lib/*.jar:/usr/bin/../share/zookeeper/zookeeper-3.4.9.jar:/usr/bin/../share/zookeeper/slf4j-log4j12-1.6.1.jar:/usr/bin/../share/zookeeper/slf4j-api-1.6.1.jar:/usr/bin/../share/zookeeper/netty-3.10.5.Final.jar:/usr/bin/../share/zookeeper/log4j-1.2.16.jar:/usr/bin/../share/zookeeper/jline-0.9.94.jar:/usr/bin/../src/java/lib/*.jar:/usr/bin/../etc/zookeeper: -Xmx2G -Xms2G -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.local.only=false org.apache.zookeeper.server.quorum.QuorumPeerMain /usr/bin/../etc/zookeeper/zoo.cfg By default, when the Pod's PersistentVolumes is mounted to the ZooKeeper server's data directory, it is only accessible by the root user. This configuration prevents the ZooKeeper process from writing to its WAL and storing its snapshots.Use the command below to get the file permissions of the ZooKeeper data directory on the zk-0 Pod.kubectl exec -ti zk-0 -- ls -ld /var/lib/zookeeper/data Because the fsGroup field of the securityContext object is set to 1000, the ownership of the Pods' PersistentVolumes is set to the zookeeper group, and the ZooKeeper process is able to read and write its data.drwxr-sr-x 3 zookeeper zookeeper 4096 Dec 5 20:45 /var/lib/zookeeper/data Managing the ZooKeeper processThe ZooKeeper documentation mentions that "You will want to have a supervisory process that manages each of your ZooKeeper server processes (JVM)." Utilizing a watchdog (supervisory process) to restart failed processes in a distributed system is a common pattern. When deploying an application in Kubernetes, rather than using an external utility as a supervisory process, you should use Kubernetes as the watchdog for your application.Updating the ensembleThe zk StatefulSet is configured to use the RollingUpdate update strategy.You can use kubectl patch to update the number of cpus allocated to the servers.kubectl patch sts zk --type='json' -p='[{"op": "replace", "path": "/spec/template/spec/containers/0/resources/requests/cpu", "value":"0.3"}]' statefulset.apps/zk patched Use kubectl rollout status to watch the status of the update.kubectl rollout status sts/zk waiting for statefulset rolling update to complete 0 pods at revision zk-5db4499664... Waiting for 1 pods to be ready... Waiting for 1 pods to be ready... waiting for statefulset rolling update to complete 1 pods at revision zk-5db4499664... Waiting for 1 pods to be ready... Waiting for 1 pods to be ready... waiting for statefulset rolling update to complete 2 pods at revision zk-5db4499664... Waiting for 1 pods to be ready... Waiting for 1 pods to be ready... statefulset rolling update complete 3 pods at revision zk-5db4499664... This terminates the Pods, one at a time, in reverse ordinal order, and recreates them with the new configuration. This ensures that quorum is maintained during a rolling update.Use the kubectl rollout history command to view a history or previous configurations.kubectl rollout history sts/zk The output is similar to this:statefulsets "zk" REVISION 1 2 Use the kubectl rollout undo command to roll back the modification.kubectl rollout undo sts/zk The output is similar to this:statefulset.apps/zk rolled back Handling process failureRestart Policies control how Kubernetes handles process failures for the entry point of the container in a Pod. For Pods in a StatefulSet, the only appropriate RestartPolicy is Always, and this is the default value. For stateful applications you should never override the default policy.Use the following command to examine the process tree for the ZooKeeper server running in the zk-0 Pod.kubectl exec zk-0 -- ps -ef The command used as the container's entry point has PID 1, and the ZooKeeper process, a child of the entry point, has PID 27.UID PID PPID C STIME TTY TIME CMD zookeep+ 1 0 0 15:03 ? 00:00:00 sh -c zkGenConfig.sh && zkServer.sh start-foreground zookeep+ 27 1 0 15:03 ? 00:00:03 /usr/lib/jvm/java-8-openjdk-amd64/bin/java -Dzookeeper.log.dir=/var/log/zookeeper -Dzookeeper.root.logger=INFO,CONSOLE -cp /usr/bin/../build/classes:/usr/bin/../build/lib/*.jar:/usr/bin/../share/zookeeper/zookeeper-3.4.9.jar:/usr/bin/../share/zookeeper/slf4j-log4j12-1.6.1.jar:/usr/bin/../share/zookeeper/slf4j-api-1.6.1.jar:/usr/bin/../share/zookeeper/netty-3.10.5.Final.jar:/usr/bin/../share/zookeeper/log4j-1.2.16.jar:/usr/bin/../share/zookeeper/jline-0.9.94.jar:/usr/bin/../src/java/lib/*.jar:/usr/bin/../etc/zookeeper: -Xmx2G -Xms2G -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.local.only=false org.apache.zookeeper.server.quorum.QuorumPeerMain /usr/bin/../etc/zookeeper/zoo.cfg In another terminal watch the Pods in the zk StatefulSet with the following command.kubectl get pod -w -l app=zk In another terminal, terminate the ZooKeeper process in Pod zk-0 with the following command.kubectl exec zk-0 -- pkill java The termination of the ZooKeeper process caused its parent process to terminate. Because the RestartPolicy of the container is Always, it restarted the parent process.NAME READY STATUS RESTARTS AGE zk-0 1/1 Running 0 21m zk-1 1/1 Running 0 20m zk-2 1/1 Running 0 19m NAME READY STATUS RESTARTS AGE zk-0 0/1 Error 0 29m zk-0 0/1 Running 1 29m zk-0 1/1 Running 1 29m If your application uses a script (such as zkServer.sh) to launch the process that implements the application's business logic, the script must terminate with the child process. This ensures that Kubernetes will restart the application's container when the process implementing the application's business logic fails.Testing for livenessConfiguring your application to restart failed processes is not enough to keep a distributed system healthy. There are scenarios where a system's processes can be both alive and unresponsive, or otherwise unhealthy. You should use liveness probes to notify Kubernetes that your application's processes are unhealthy and it should restart them.The Pod template for the zk StatefulSet specifies a liveness probe. livenessProbe: exec: command: \- sh \- -c \- "zookeeper-ready 2181" initialDelaySeconds: 15 timeoutSeconds: 5 The probe calls a bash script that uses the ZooKeeper ruok four letter word to test the server's health.OK=$(echo ruok | nc 127.0.0.1 $1) if [ "$OK" == "imok" ]; then exit 0 else exit 1 fi In one terminal window, use the following command to watch the Pods in the zk StatefulSet.kubectl get pod -w -l app=zk In another window, using the following command to delete the zookeeper-ready script from the file system of Pod zk-0.kubectl exec zk-0 -- rm /opt/zookeeper/bin/zookeeper-ready When the liveness probe for the ZooKeeper process fails, Kubernetes will automatically restart the process for you, ensuring that unhealthy processes in the ensemble are restarted.kubectl get pod -w -l app=zk NAME READY STATUS RESTARTS AGE zk-0 1/1 Running 0 1h zk-1 1/1 Running 0 1h zk-2 1/1 Running 0 1h NAME READY STATUS RESTARTS AGE zk-0 0/1 Running 0 1h zk-0 0/1 Running 1 1h zk-0 1/1 Running 1 1h Testing for readinessReadiness is not the same as liveness. If a process is alive, it is scheduled and healthy. If a process is ready, it is able to process input. Liveness is a necessary, but not sufficient, condition for readiness. There are cases, particularly during initialization and termination, when a process can be alive but not ready.If you specify a readiness probe, Kubernetes will ensure that your application's processes will not receive network traffic until their readiness checks pass.For a ZooKeeper server, liveness implies readiness. Therefore, the readiness probe from the zookeeper.yaml manifest is identical to the liveness probe. readinessProbe: exec: command: \- sh \- -c \- "zookeeper-ready 2181" initialDelaySeconds: 15 timeoutSeconds: 5 Even though the liveness and readiness probes are identical, it is important to specify both. This ensures that only healthy servers in the ZooKeeper ensemble receive network traffic.Tolerating Node failureZooKeeper needs a quorum of servers to successfully commit mutations to data. For a three server ensemble, two servers must be healthy for writes to succeed. In quorum based systems, members are deployed across failure domains to ensure availability. To avoid an outage, due to the loss of an individual machine, best practices preclude co-locating multiple instances of the application on the same machine.By default, Kubernetes may co-locate Pods in a StatefulSet on the same node. For the three server ensemble you created, if two servers are on the same node, and that node fails, the clients of your ZooKeeper service will experience an outage until at least one of the Pods can be rescheduled.You should always provision additional capacity to allow the processes of critical systems to be rescheduled in the event of node failures. If you do so, then the outage will only last until the Kubernetes scheduler reschedules one of the ZooKeeper servers. However, if you want your service to tolerate node failures with no downtime, you should set podAntiAffinity.Use the command below to get the nodes for Pods in the zk StatefulSet.for i in 0 1 2; do kubectl get pod zk-$i --template {{.spec.nodeName}}; echo ""; done All of the Pods in the zk StatefulSet are deployed on different nodes.kubernetes-node-cxpk kubernetes-node-a5aq kubernetes-node-2g2d This is because the Pods in the zk StatefulSet have a PodAntiAffinity specified.affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: \- labelSelector: matchExpressions: \- key: "app" operator: In values: \- zk topologyKey: "kubernetes.io/hostname" The requiredDuringSchedulingIgnoredDuringExecution field tells the Kubernetes Scheduler that it should never co-locate two Pods which have app label as zk in the domain defined by the topologyKey. The topologyKey kubernetes.io/hostname indicates that the domain is an individual node. Using different rules, labels, and selectors, you can extend this technique to spread your ensemble across physical, network, and power failure domains.Surviving maintenanceIn this section you will cordon and drain nodes. If you are using this tutorial on a shared cluster, be sure that this will not adversely affect other tenants.The previous section showed you how to spread your Pods across nodes to survive unplanned node failures, but you also need to plan for temporary node failures that occur due to planned maintenance.Use this command to get the nodes in your cluster.kubectl get nodes This tutorial assumes a cluster with at least four nodes. If the cluster has more than four, use kubectl cordon to cordon all but four nodes. Constraining to four nodes will ensure Kubernetes encounters affinity and PodDisruptionBudget constraints when scheduling zookeeper Pods in the following maintenance simulation.kubectl cordon  Use this command to get the zk-pdb PodDisruptionBudget.kubectl get pdb zk-pdb The max-unavailable field indicates to Kubernetes that at most one Pod from zk StatefulSet can be unavailable at any time.NAME MIN-AVAILABLE MAX-UNAVAILABLE ALLOWED-DISRUPTIONS AGE zk-pdb N/A 1 1 In one terminal, use this command to watch the Pods in the zk StatefulSet.kubectl get pods -w -l app=zk In another terminal, use this command to get the nodes that the Pods are currently scheduled on.for i in 0 1 2; do kubectl get pod zk-$i --template {{.spec.nodeName}}; echo ""; done The output is similar to this:kubernetes-node-pb41 kubernetes-node-ixsl kubernetes-node-i4c4 Use kubectl drain to cordon and drain the node on which the zk-0 Pod is scheduled.kubectl drain $(kubectl get pod zk-0 --template {{.spec.nodeName}}) --ignore-daemonsets --force --delete-emptydir-data The output is similar to this:node "kubernetes-node-pb41" cordoned WARNING: Deleting pods not managed by ReplicationController, ReplicaSet, Job, or DaemonSet: fluentd-cloud-logging-kubernetes-node-pb41, kube-proxy-kubernetes-node-pb41; Ignoring DaemonSet-managed pods: node-problem-detector-v0.1-o5elz pod "zk-0" deleted node "kubernetes-node-pb41" drained As there are four nodes in your cluster, kubectl drain, succeeds and the zk-0 is rescheduled to another node.NAME READY STATUS RESTARTS AGE zk-0 1/1 Running 2 1h zk-1 1/1 Running 0 1h zk-2 1/1 Running 0 1h NAME READY STATUS RESTARTS AGE zk-0 1/1 Terminating 2 2h zk-0 0/1 Terminating 2 2h zk-0 0/1 Terminating 2 2h zk-0 0/1 Terminating 2 2h zk-0 0/1 Pending 0 0s zk-0 0/1 Pending 0 0s zk-0 0/1 ContainerCreating 0 0s zk-0 0/1 Running 0 51s zk-0 1/1 Running 0 1m Keep watching the StatefulSet's Pods in the first terminal and drain the node on which zk-1 is scheduled.kubectl drain $(kubectl get pod zk-1 --template {{.spec.nodeName}}) --ignore-daemonsets --force --delete-emptydir-data The output is similar to this:"kubernetes-node-ixsl" cordoned WARNING: Deleting pods not managed by ReplicationController, ReplicaSet, Job, or DaemonSet: fluentd-cloud-logging-kubernetes-node-ixsl, kube-proxy-kubernetes-node-ixsl; Ignoring DaemonSet-managed pods: node-problem-detector-v0.1-voc74 pod "zk-1" deleted node "kubernetes-node-ixsl" drained The zk-1 Pod cannot be scheduled because the zk StatefulSet contains a PodAntiAffinity rule preventing co-location of the Pods, and as only two nodes are schedulable, the Pod will remain in a Pending state.kubectl get pods -w -l app=zk The output is similar to this:NAME READY STATUS RESTARTS AGE zk-0 1/1 Running 2 1h zk-1 1/1 Running 0 1h zk-2 1/1 Running 0 1h NAME READY STATUS RESTARTS AGE zk-0 1/1 Terminating 2 2h zk-0 0/1 Terminating 2 2h zk-0 0/1 Terminating 2 2h zk-0 0/1 Terminating 2 2h zk-0 0/1 Pending 0 0s zk-0 0/1 Pending 0 0s zk-0 0/1 ContainerCreating 0 0s zk-0 0/1 Running 0 51s zk-0 1/1 Running 0 1m zk-1 1/1 Terminating 0 2h zk-1 0/1 Terminating 0 2h zk-1 0/1 Terminating 0 2h zk-1 0/1 Terminating 0 2h zk-1 0/1 Pending 0 0s zk-1 0/1 Pending 0 0s Continue to watch the Pods of the StatefulSet, and drain the node on which zk-2 is scheduled.kubectl drain $(kubectl get pod zk-2 --template {{.spec.nodeName}}) --ignore-daemonsets --force --delete-emptydir-data The output is similar to this:node "kubernetes-node-i4c4" cordoned WARNING: Deleting pods not managed by ReplicationController, ReplicaSet, Job, or DaemonSet: fluentd-cloud-logging-kubernetes-node-i4c4, kube-proxy-kubernetes-node-i4c4; Ignoring DaemonSet-managed pods: node-problem-detector-v0.1-dyrog WARNING: Ignoring DaemonSet-managed pods: node-problem-detector-v0.1-dyrog; Deleting pods not managed by ReplicationController, ReplicaSet, Job, or DaemonSet: fluentd-cloud-logging-kubernetes-node-i4c4, kube-proxy-kubernetes-node-i4c4 There are pending pods when an error occurred: Cannot evict pod as it would violate the pod's disruption budget. pod/zk-2 Use CTRL-C to terminate kubectl.You cannot drain the third node because evicting zk-2 would violate zk-budget. However, the node will remain cordoned.Use zkCli.sh to retrieve the value you entered during the sanity test from zk-0.kubectl exec zk-0 zkCli.sh get /hello The service is still available because its PodDisruptionBudget is respected.WatchedEvent state:SyncConnected type:None path:null world cZxid = 0x200000002 ctime = Wed Dec 07 00:08:59 UTC 2016 mZxid = 0x200000002 mtime = Wed Dec 07 00:08:59 UTC 2016 pZxid = 0x200000002 cversion = 0 dataVersion = 0 aclVersion = 0 ephemeralOwner = 0x0 dataLength = 5 numChildren = 0 Use kubectl uncordon to uncordon the first node.kubectl uncordon kubernetes-node-pb41 The output is similar to this:node "kubernetes-node-pb41" uncordoned zk-1 is rescheduled on this node. Wait until zk-1 is Running and Ready.kubectl get pods -w -l app=zk The output is similar to this:NAME READY STATUS RESTARTS AGE zk-0 1/1 Running 2 1h zk-1 1/1 Running 0 1h zk-2 1/1 Running 0 1h NAME READY STATUS RESTARTS AGE zk-0 1/1 Terminating 2 2h zk-0 0/1 Terminating 2 2h zk-0 0/1 Terminating 2 2h zk-0 0/1 Terminating 2 2h zk-0 0/1 Pending 0 0s zk-0 0/1 Pending 0 0s zk-0 0/1 ContainerCreating 0 0s zk-0 0/1 Running 0 51s zk-0 1/1 Running 0 1m zk-1 1/1 Terminating 0 2h zk-1 0/1 Terminating 0 2h zk-1 0/1 Terminating 0 2h zk-1 0/1 Terminating 0 2h zk-1 0/1 Pending 0 0s zk-1 0/1 Pending 0 0s zk-1 0/1 Pending 0 12m zk-1 0/1 ContainerCreating 0 12m zk-1 0/1 Running 0 13m zk-1 1/1 Running 0 13m Attempt to drain the node on which zk-2 is scheduled.kubectl drain $(kubectl get pod zk-2 --template {{.spec.nodeName}}) --ignore-daemonsets --force --delete-emptydir-data The output is similar to this:node "kubernetes-node-i4c4" already cordoned WARNING: Deleting pods not managed by ReplicationController, ReplicaSet, Job, or DaemonSet: fluentd-cloud-logging-kubernetes-node-i4c4, kube-proxy-kubernetes-node-i4c4; Ignoring DaemonSet-managed pods: node-problem-detector-v0.1-dyrog pod "heapster-v1.2.0-2604621511-wht1r" deleted pod "zk-2" deleted node "kubernetes-node-i4c4" drained This time kubectl drain succeeds.Uncordon the second node to allow zk-2 to be rescheduled.kubectl uncordon kubernetes-node-ixsl The output is similar to this:node "kubernetes-node-ixsl" uncordoned You can use kubectl drain in conjunction with PodDisruptionBudgets to ensure that your services remain available during maintenance. If drain is used to cordon nodes and evict pods prior to taking the node offline for maintenance, services that express a disruption budget will have that budget respected. You should always allocate additional capacity for critical services so that their Pods can be immediately rescheduled.Cleaning upUse kubectl uncordon to uncordon all the nodes in your cluster.You must delete the persistent storage media for the PersistentVolumes used in this tutorial. Follow the necessary steps, based on your environment, storage configuration, and provisioning method, to ensure that all storage is reclaimed.5.7 - Cluster Management5.7.1 - Running Kubelet in Standalone ModeThis tutorial shows you how to run a standalone kubelet instance.You may have different motivations for running a standalone kubelet. This tutorial is aimed at introducing you to Kubernetes, even if you don't have much experience with it. You can follow this tutorial and learn about node setup, basic (static) Pods, and how Kubernetes manages containers.Once you have followed this tutorial, you could try using a cluster that has a control plane to manage pods and nodes, and other types of objects. For example, Hello, minikube.You can also run the kubelet in standalone mode to suit production use cases, such as to run the control plane for a highly available, resiliently deployed cluster. This tutorial does not cover the details you need for running a resilient control plane.ObjectivesInstall cri-o, and kubelet on a Linux system and run them as systemd services.Launch a Pod running nginx that listens to requests on TCP port 80 on the Pod's IP address.Learn how the different components of the solution interact among themselves.Caution:The kubelet configuration used for this tutorial is insecure by design and should not be used in a production environment.Before you beginAdmin (root) access to a Linux system that uses systemd and iptables (or nftables with iptables emulation).Access to the Internet to download the components needed for the tutorial, such as:A container runtime that implements the Kubernetes (CRI).Network plugins (these are often known as Container Networking Interface (CNI))Required CLI tools: curl, tar, jq.Prepare the systemSwap configurationBy default, kubelet fails to start if swap memory is detected on a node. This means that swap should either be disabled or tolerated by kubelet.Note:If you configure the kubelet to tolerate swap, the kubelet still configures Pods (and the containers in those Pods) not to use swap space. To find out how Pods can actually use the available swap, you can read more about swap memory management on Linux nodes.If you have swap memory enabled, either disable it or add failSwapOn: false to the kubelet configuration file.To check if swap is enabled:sudo swapon --show If there is no output from the command, then swap memory is already disabled.To disable swap temporarily:sudo swapoff -a To make this change persistent across reboots:Make sure swap is disabled in either /etc/fstab or systemd.swap, depending on how it was configured on your system.Enable IPv4 packet forwardingTo check if IPv4 packet forwarding is enabled:cat /proc/sys/net/ipv4/ip_forward If the output is 1, it is already enabled. If the output is 0, then follow next steps.To enable IPv4 packet forwarding, create a configuration file that sets the net.ipv4.ip_forward parameter to 1:sudo tee /etc/sysctl.d/k8s.conf < crio-install Run the installer script:sudo bash crio-install Enable and start the crio service:sudo systemctl daemon-reload sudo systemctl enable --now crio.service Quick test:sudo systemctl is-active crio.service The output is similar to:active Detailed service check:sudo journalctl -f -u crio.service Install network pluginsThe cri-o installer installs and configures the cni-plugins package. You can verify the installation running the following command:/opt/cni/bin/bridge --version The output is similar to:CNI bridge plugin v1.5.1 CNI protocol versions supported: 0.1.0, 0.2.0, 0.3.0, 0.3.1, 0.4.0, 1.0.0 To check the default configuration:cat /etc/cni/net.d/11-crio-ipv4-bridge.conflist The output is similar to:{ "cniVersion": "1.0.0", "name": "crio", "plugins": [ { "type": "bridge", "bridge": "cni0", "isGateway": true, "ipMasq": true, "hairpinMode": true, "ipam": { "type": "host-local", "routes": [ { "dst": "0.0.0.0/0" } ], "ranges": [ [{ "subnet": "10.85.0.0/16" }] ] } } ] } Note:Make sure that the default subnet range (10.85.0.0/16) does not overlap with any of your active networks. If there is an overlap, you can edit the file and change it accordingly. Restart the service after the change.Download and set up the kubeletDownload the latest stable release of the kubelet.x86-64ARM64 curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubelet" curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/arm64/kubelet" Configure:sudo mkdir -p /etc/kubernetes/manifests sudo tee /etc/kubernetes/kubelet.yaml < static-web.yaml apiVersion: v1 kind: Pod metadata: name: static-web spec: containers: \- name: web image: nginx ports: \- name: web containerPort: 80 protocol: TCP EOF Copy the static-web.yaml manifest file to the /etc/kubernetes/manifests directory.sudo cp static-web.yaml /etc/kubernetes/manifests/ Find out information about the kubelet and the PodThe Pod networking plugin creates a network bridge (cni0) and a pair of veth interfaces for each Pod (one of the pair is inside the newly made Pod, and the other is at the host level).Query the kubelet's API endpoint at http://localhost:10255/pods:curl http://localhost:10255/pods | jq '.' To obtain the IP address of the static-web Pod:curl http://localhost:10255/pods | jq '.items[].status.podIP' The output is similar to:"10.85.0.4" Connect to the nginx server Pod on http://: (port 80 is the default), in this case:curl http://10.85.0.4 The output is similar to: